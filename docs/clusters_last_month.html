



<!DOCTYPE html>
<html lang="en">
  
  <head>
    
      <meta charset="utf-8">
      <title>Clustering papers on Supervised Learning by Classification</title>
      
      
        
          
        
        
          
        <script type="text/javascript" src="https://cdn.pydata.org/bokeh/release/bokeh-1.4.0.min.js"></script>
        <script type="text/javascript" src="https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.4.0.min.js"></script>
        <script type="text/javascript">
            Bokeh.set_log_level("info");
        </script>
        
      
      
    
  </head>
  
  
  <body>
    
      
        
          
          
            
              <div class="bk-root" id="59d5eaae-bd4c-4d32-bc5e-3c56dfb706b9" data-root-id="1087"></div>
            
          
        
      
      
        <script type="application/json" id="1176">
          {"51b4bb2a-fa7a-4669-baa2-0c956413ced0":{"roots":{"references":[{"attributes":{"args":{"out_text":{"id":"1062","type":"Paragraph"},"p":{"id":"1014","subtype":"Figure","type":"Plot"},"slider":{"id":"1074","type":"Slider"},"source":{"id":"1011","type":"ColumnDataSource"},"text":{"id":"1075","type":"TextInput"},"topics":["hyperspectral, spectral, proposed, attention, images, cnn, self, deep","multi, analysis, algorithms, fault, proposed, graph, imbalance","proposed, study, research, analysis, high, cancer, compared"]},"code":"\n\t\t\t\tvar key = text.value;\n\t\t\t\tkey = key.toLowerCase();\n\t\t\t\tvar cluster = slider.value;\n                var clusters_count = slider.end;\n                var data = source.data; \n                \n                \n                x = data['x'];\n                y = data['y'];\n                x_backup = data['x_backup'];\n                y_backup = data['y_backup'];\n                labels = data['cluster'];\n                abstract = data['abstract'];\n                title = data['title'];\n                author = data['author'];\n                if (cluster == clusters_count) {\n                    out_text.text = 'Keywords: Slide to specific cluster to see the keywords.';\n                    for (i = 0; i &lt; x.length; i++) {\n\t\t\t\t\t\tif(abstract[i].toLowerCase().includes(key) || \n                        (title[i] &amp;&amp; title[i].toLowerCase().includes(key)) ||\n                        (author[i] &amp;&amp; author[i].toLowerCase().includes(key))) {\n\t\t\t\t\t\t\tx[i] = x_backup[i];\n\t\t\t\t\t\t\ty[i] = y_backup[i];\n\t\t\t\t\t\t} else {\n\t\t\t\t\t\t\tx[i] = undefined;\n\t\t\t\t\t\t\ty[i] = undefined;\n\t\t\t\t\t\t}\n                    }\n                }\n                else {\n                    out_text.text = 'Keywords: ' + topics[Number(cluster)];\n                    for (i = 0; i &lt; x.length; i++) {\n                        if(labels[i] == cluster) {\n\t\t\t\t\t\t\tif(abstract[i].toLowerCase().includes(key)\n                            || (title[i] &amp;&amp; title[i].toLowerCase().includes(key))\n                            || (author[i] &amp;&amp; author[i].toLowerCase().includes(key))) {\n\t\t\t\t\t\t\t\tx[i] = x_backup[i];\n\t\t\t\t\t\t\t\ty[i] = y_backup[i];\n\t\t\t\t\t\t\t} else {\n\t\t\t\t\t\t\t\tx[i] = undefined;\n\t\t\t\t\t\t\t\ty[i] = undefined;\n\t\t\t\t\t\t\t}\n                        } else {\n                            x[i] = undefined;\n                            y[i] = undefined;\n                        }\n                    }\n                }\n            source.change.emit();\n            "},"id":"1063","type":"CustomJS"},{"attributes":{"children":[{"id":"1076","type":"Div"}]},"id":"1078","type":"Row"},{"attributes":{},"id":"1021","type":"LinearScale"},{"attributes":{"callback":null,"data":{"abstract":["Twitter Sentiment Analysis is the way of identifying sentiments and opinions in tweets. The main computational steps in this process are determining the polarity or sentiment of the tweet and then categorizing them into the positive tweet or negative tweet. The primary issue with Twitter sentiment analysis is the identification of the most suitable sentiment classifier that can correctly classify the tweets. Generally, base classification technique like Naive Bayes classifier, Random Forest classifier, SVMs and Logistic Regression are being used. In this paper, an ensemble classifier has been proposed that combines the base learning classifier to form a single classifier, with an aim of improving the performance and accuracy of sentiment classification technique. The results show that the proposed ensemble classifier performs better than stand-alone classifiers and majority voting ensemble classifier. In addition, the role of data pre-processing and feature representation in sentiment classification technique is also explored as part of this work.","Wafer Bin Map (WBM) defect patterns are a critical aspect of identifying the root cause of manufacturing defects in the semiconductor industry. Semi-supervised learning (SSL) approaches have gained popularity for this purpose, as they can leverage both labeled and unlabeled data to improve model performance. However, SSL of WBM defect patterns is challenging due to class imbalance, where some defect classes have many more examples than others. Most of the existing SSL approaches assume a balanced dataset and often fail to provide satisfactory results when applied to imbalanced class problems. To address this issue, this work proposes a novel Dual-Head Convolutional Neural Network (CNN) architecture that contains two classifier heads. One classifier head maximizes overall classification scores, while the other aims to maximize per-class classification scores, providing equal attention to both majority and minority classes. The proposed CNN architecture uses pseudo-labels selected based on the outputs of these two classifiers to expand the labeled training set, which is then used to retrain the CNN. In this way, highly confident pseudo-labels are selected even from the minority classes, leading to better model training. Experiments show that the proposed approach is effective in handling class-imbalanced classification of WBM defect patterns, reporting state-of-the-art classification with an F1 score of 0.918, accuracy of 98.2% and a mean per-class accuracy of 91.7% using a lightweight ResNet-10 model as the backbone on the real-world public WBM dataset, WM-811K. The proposed approach\u2019s success suggests that it could be a valuable tool for improving the accuracy and reliability of WBM defect pattern classification in semiconductor manufacturing. The code is available at https://github.com/M-Siyamalan/SSL-DHCNN.\nHighlights\n\u2022\nNovel semi-supervised approach is proposed for wafer bin map defect classification.\n\u2022\nThe approach effectively handles imbalanced class semi-supervised classification.\n\u2022\nExperimental results show that the proposed approach outperforms other approaches.","One of the main challenges in target-dependent sentiment classification (TDSC) is dealing with sentences that contain multiple targets with varying polarities. Traditional sentiment analysis has shown the effectiveness of language characteristics. Therefore, we propose a method to extract target semantic-related tokens from sentences in order to simplify the sentiment classification task. To achieve this, we establish six grammatical principles that utilize grammatical knowledge to filter the relevant descriptions of targets. Since a target is typically a noun and acts as a subject, we summarize the six rules to extract the contexts contained in the objects and subordinate clauses. We use dependency parsing to analyze the grammatical relations between the target and its context. We design a data pre-processing method called Text Filtering (TF) to automate this procedure. After executing the TF algorithm, we pass the target-related words to a simple classifier to predict their sentiment polarities. Rather than feeding these features directly to a network and letting it learn features on its own, our approach employs dependency relations to extract context linked to the target. This provides the network with meaningful and representative features, resulting in superior results. We conduct ablation studies to investigate the effectiveness of the proposed TF algorithm. In the restaurant hard dataset, our approach improves accuracy by 13.76% and macro-F1 by 14.65% compared to a CNN-based method where TF is not implemented.\nHighlights\n\u2022\nIncorporate linguistic knowledge to the model by a two-step framework for effective handling of multiple targets.\n\u2022\nTarget-related contexts were extracted by the summarized rules which distinguish content and function words.\n\u2022\nA hierarchical and recursive data process method was designed specifically for TDSC named Text filtering (TF).","Geophysical reservoir characterization is a significant task in the oil and gas industry and elastic logs prediction of subsurface formations is a fundamental aspect of this process. However, elastic log prediction in a high-dimensional and complex geological environment, such as the Lower Indus Basin Pakistan, poses a significant challenge where traditional empirical methods often fail to provide competitively accurate results. Therefore, this study proposes a novel machine learning approach that combines unsupervised clustering (K-means) and ensemble-based machine learning (random forest) to improve prediction accuracy. By clustering data based on statistical similarity and ensemble algorithms to each cluster, the methodology addresses the challenges of sonic log prediction in the Lower Indus Basin (Pakistan). This approach was evaluated using real-world data, outperforming several baseline methods with a root mean square error of 98% accuracy. Its effectiveness to predict elastic log makes it a valuable tool in reservoir characterization, earthquake analysis, and geothermal energy exploration. Overall, combining this methodology with other techniques can enhance seismic data analysis and enable better decision-making in the oil and gas industry. This novel approach presents an effective solution for predicting sonic log in the Lower Indus Basin and contributes to advancements in geophysical reservoir characterization.\nHighlights\n\u2022\nA novel machine learning method is proposed to predict elastic log response.\n\u2022\nThis method integrates K-means clustering with random forest for prediction.\n\u2022\nThe accuracy of predictions significantly improved in complex geological settings.\n\u2022\nThis method can predict petrophysical parameters in complex geological environment.\n\u2022\nThis proposed approach effectively reduces cumbersome human efforts and save time.","Functional magnetic resonance imaging (fMRI) is a non-invasive technique measuring brain activity by detecting blood flow changes, enabling the study of cognitive processes and brain states. However, the high dimensionality of resting-state (rs) fMRI data poses challenges for machine learning applications. Feature extraction (FE) and feature selection (FS) are critical for developing efficient machine learning models. Transforming raw data into meaningful features and selecting the most relevant ones, allows models to achieve improved generalization, accuracy, and robustness. Previous studies demonstrated the effectiveness of FE and FS methods for analyzing rs-fMRI data for Autism Spectrum Disorder (ASD) classification. In this study, we apply a random walks technique for correlation-based brain networks to extract features from rs-fMRI data, specifically the number of random walkers on each brain area. We then select significant features, i.e., brain areas with a statistically significant difference in the number of random walkers between neurotypical and ASD subjects. Our random walks-based FE and FS approach reduces the number of brain areas used in the classification and converts the functional connectivity matrix into a manageable vector, enabling faster computation. We examined 16 pipelines and tested support vector machines (SVM) and logistic regression for classification, identifying the optimal pipeline to consist of no filtering, no global signal regression (GSR), and FS, achieving a 76.54% classification accuracy with SVM. Our findings suggest that random walks capture a wide range of interactions and dynamics in brain networks, providing a deeper characterization of their structure and function, ultimately enhancing classification performance.","Multi-source information fusion is an effective method to handle pattern classification problems. Dempster\u2013Shafer evidence theory (DSET) plays an important role in handling uncertainty problems in multi-source information fusion. However, highly conflicting evidence in DSET may cause counter-intuitive fusion results. Belief divergence theory is one of the solutions to conflict management, which is also beneficial for the improvement of accuracies of pattern classification. In this paper, a novel belief divergence measurement method, fractal belief Jensen\u2013Shannon (F B J S) divergence is proposed to better measure the discrepancy between Basic probability assignments (BPAs) and address the problem of highly conflicting evidence in DSET. The proposed F B J S divergence is the first belief divergence that incorporates the belief divergence theory and the concept of fractal. In addition, it has the properties of non-negativeness and symmetry. Then, based on F B J S divergence, a novel multi-source information fusion algorithm is proposed. Ultimately, the proposed algorithm is effectively applied to solve a pattern classification problem with a higher classification accuracy.","This paper introduces new flexible loss functions for binary classification in Gradient-Boosted Decision Trees (GBDT) that combine Dice-based and cross-entropy-based losses and offer link functions from either a generalized extreme value (GEV) or exponentiated exponential logistic (EEL) distribution. Testing 27 different GBDT models using XGBoost on a Freddie Mac mortgage loan database showed that the choice of the loss function is useful. Specifically, when the class imbalance ratio (IR) is less than 99, using a skewed GEV distribution-based link function in XGBoost enhances discriminatory power and classification accuracy while retaining a simple model structure, which is particularly important in credit scoring applications. In cases where class imbalances are severe, typically between IRs of 99 and 200, we found that an advanced loss function, which is composed of a symmetric hybrid loss function and a link derived from a positively skewed EEL distribution, outperforms other XGBoost variants. Based on our findings, the accuracy improvements of these proposed extensions result in lower misclassification costs, which are especially evident when IR is below 99, which results in higher profitability for the business. Furthermore, the study highlights the transparency associated with GBDT, which is also an integral component of financial applications. Researchers and practitioners can use these insights to create more accurate and discriminative machine learning models, with possible extensions to other GBDT implementations and machine learning techniques that take into account loss functions. The source code for the proposed approach is publicly available at https://github.com/jm-ml/flexible-losses-for-binary-classification-with-GBDT.","Highlights\n\u2022\nA novel hyperspectral band selection network is proposed for seed classification.\n\u2022\nBand attention and sparse constraint (SC) maximizes the removal of redundant bands.\n\u2022\nA new loss function is defined to solve the gradient update problem caused by SC.\n\u2022\nThe selected bands carry important spectrochemical information.\n\u2022\nThis method is also applicable to other hyperspectral data of food and agro-products.\nAbstract\nThe development of a real-time online system for rapid and nondestructive identification of seed varieties can greatly improve production efficiency in modern agriculture. Hyperspectral imaging (HSI) is a powerful tool for seed variety identification. Nevertheless, hyperspectral data are not only high in dimensionality but also contain redundant information, which is very unfriendly to real-time online applications. Selecting a few representative bands from the entire working spectral region can significantly reduce the equipment cost and computational load of HSI. In the field of food and agr-products quality evaluation, Band selection (BS) methods based on chemometrics have been dominant for a long time. Most of these methods, however, fail to take full account of the nonlinearities and global interactions between spectral bands, which may result in the selection of some adjacent bands that still retain more redundant information. In this paper, a novel BS network is proposed, which is composed of sparse band attention module and classification net module. The former is used to generate weight of each band, and sparse constraint is applied to the weights of redundant bands, while the latter is used to achieve high-performance classification with reweighted data. Furthermore, to solve the problem of gradient updating caused by sparse constraint, a auxiliary loss function is defined to assist optimization. Finally, comparative experiments is conducted on our maize seed hyperspectral dataset. The results demonstrate that the presented method selects a subset of informative bands with less redundant information to obtain better classification performance and outperforms several other existing BS methods.","Feature selections facilitate classification learning in various data environments. Aiming at interval-valued decision systems (IVDSs), feature selections rely on information measures and similarity degrees, whereas current selection algorithms on credibility-based condition entropy and classical similarity degree are accompanied with some measurement limitations and advancement space. In this paper based on IVDSs, three coverage-credibility-based condition entropies and one geometry-probabilistic similarity degree are proposed across two dimensions of informationization and granulation, and they improve the existing condition entropy and similarity degree; accordingly, 4 \u00d7 2 feature selections emerge for optimization and applicability, and they systematically contain one initial selection algorithm and seven new/robuster algorithms. At first, three-way granular measures (i.e., credibility, coverage, and integrated coverage-credibility) are formulated in IVDSs, and three novel condition entropies are established by implementing three information structures on coverage-credibility. These condition entropies acquire in-depth improvements, hierarchical algorithms, size relationships, maximum/minimum conditions, and granulation non-monotonicity. Then, the probabilistic similarity degree is defined by a six-piecewise function with quadratic factors, and this new measure gains the geometry-probability mechanism and high-quality improvement. Furthermore, feature selections are determined by preserving condition entropies and by mining feature significances, so eight selection algorithms are obtained by combining condition entropies and similarity degrees. Finally, data experiments are performed to validate relevant uncertainty measures and feature selections, and seven constructional selection algorithms outperform three contrastive algorithms to achieve better classification performances.\nHighlights\n\u2022\n3 improved CEs are constructed by 3 information structures on coverage-credibilities.\n\u2022\n3 improved CEs gain mechanisms, algorithms, properties, granulation non-monotonicity.\n\u2022\nNew PSD gives the probability semantics, function reinforcement, quality improvement.\n\u2022\nNon-monotonic FSs and heuristic algorithms are designed by preserving 3 CEs on 2 SDs.\n\u2022\n7 combined FS algorithms outperform 3 current algorithms from classification effects.","Dempster\u2013Shafer evidence theory (DSET) is extensively employed in multi-source data fusion applications. Nonetheless, when belief probability assignments (BPAs) exhibit considerable conflict, unexpected results can occur. To address this limitation, the high-order fractals are explored and a K-order fractal-based Kullback\u2013Leibler divergence (KO-FKL) is introduced, which defines the K-order as the optimal fractal epoch. This measure is employed to quantify the divergence between BPAs and demonstrates superior performance in assessing the conflict between two BPAs in numerical examples, compared to existing belief divergence methods. To utilize the KO-FKL divergence measure to real-world problems, a novel KO-FKL-based multi-source data fusion (KO-FKL-MSDF) algorithm is designed. Through comparisons with well-known related methods, our proposed KO-FKL-MSDF algorithm demonstrates superiority and enhanced robustness. Lastly, the KO-FKL-MSDF algorithm is applied to real-world classification problems, underlining its high practical applicability.\nHighlights\n\u2022\nHigh-order fractal BPA and KL divergence using continuous Pignistic process.\n\u2022\nOptimal fractal high-order K via proposed Difference of Deng entropy convergence.\n\u2022\nK-order fractal KL divergence outperforms existing measures in plentiful examples.\n\u2022\nKO-FKL multi-source data fusion algorithm via extensive validation and analysis.\n\u2022\nKO-FKL-MSDF algorithm excels in pattern classification on multiple datasets.","Skip Background Section\nBackground\nSeismic signals are useful for earthquake detection and classification. Therefore, various artificial intelligence (AI) models have been used with seismic signals to develop automated earthquake detection systems. Our primary goal is to present an accurate feature engineering model for earthquake detection and classification using seismic signals.\nSkip Material and model Section\nMaterial and model\nWe have used a public dataset in this work containing three categories: (1) noise, (2) P waves, and (3) S waves. P and S waves are used to define earthquakes. We have presented two applied use cases using this dataset: (i) earthquake detection and (ii) wave classification. In this work, a new textural feature extractor has been presented by using a graph pattern similar to a butterfly. Thus, this feature extraction function is named Butterfly pattern (BFPat). We have created a new feature engineering architecture by deploying BFPat, statistics, and wavelet packet decomposition (WPD) functions. The recommended BFPat and statistics have been applied to the wavelet bands created by WPD and the raw seismic signals. Multilevel features have been extracted from both frequency and space domains. The used dataset contains signals with three channels. Using these three channels, seven signals have been created. Seven feature vectors have been created from 7 input signals used in this study. The most meaningful/informative features from the generated feature set are then selected using the iterative neighborhood component analysis feature selector method. Seven chosen feature vectors have been considered as inputs of the two shallow classifiers: k nearest neighbors (kNN) and support vector machine (SVM). A total of 14 (=7 \u00d7 2) results have been obtained in the classification phase. A majority voting process was applied in the last phase to choose the best results and improve the classification performance.\nSkip Results Section\nResults\nWe have presented two use cases for our new BFPat method in this work to obtain superior results. Our model reached an accuracy of 99.58% in detecting the earthquake detection and 93.13% accuracy in 3-class classifications of waves.\nSkip Conclusions Section\nConclusions\nOur recommended model has achieved over 90% classification performance for both cases. Also, we have presented the most valuable channel and combinations in our work. Our developed system is ready to be tested with a bigger database.","Remote Sensing (RS) has been widely utilized in various Earth Observation (EO) missions, including land cover classification and environmental monitoring. Unlike computer vision tasks on natural images, collecting remote sensing data is more challenging. To fully exploit the available data and leverage the complementary information across different data sources, we propose a novel approach called Multimodal Transformer for Remote Sensing (RsMmFormer) for image classification, which utilizes both Hyperspectral Image (HSI) and Light Detection and Ranging (LiDAR) data. In contrast to the conventional Vision Transformer (ViT), which does not incorporate the inherent biases and assumptions of convolutions, we improve our RsMmFormer model by incorporating convolutional layers. This allows us to integrate the favorable characteristics of convolutional neural networks (CNNs). Next, we introduce the Multi-scale Multi-head Self-Attention (MSMHSA) module, which enables learning detailed representations, facilitating the detection of small targets occupying only a few pixels in the remote sensing image. The proposed MSMHSA module facilitates the integration of Hyperspectral Imaging (HSI) and LiDAR data in a progressive and detailed manner, effectively attending to both global and local contexts using self-attention mechanisms. Comprehensive experiments conducted on popular benchmarks such as Trento and MUUFL showcase the effectiveness and superiority of our proposed RsMmFormer model for remote sensing image classification.","This paper studies the effect of word representations on gender classification using deep learning. There are two main objectives: how well do popular deep learning architectures, namely LSTM and CNNs, perform on gender classification task and investigate how the choice of word representation effects the performance. Three networks, LSTM, CNN and LeNet-5, were trained on a dataset containing about 18000 names from India, Western countries, Sri Lanka and Japan. These names, encoded using the popular One-Hot representation and Word Embeddings in addition to Integer representation and an Enhanced Integer representation (proposed in this paper), were given as Input and the performance is evaluated on accuracy, training times and size of input layer. Experimental results show that LSTM in combination with word embedding derived from the proposed Enhanced Integer representation gives the best performance of about 85%. One-Hot representation is superior to Integer and Enhanced Integer representation but appears to perform lower than word embeddings.","SVM utilizes the hinge loss function and maximum margin to find the separating hyperplane. In SVM, only the boundary instances/support vectors confine the separating hyperplane, making it susceptible to noisy samples near the decision boundary. This work proposes a novel noise-robust eagle loss function and presents Eagle-SVM based on the proposed loss function. The formulation of the eagle loss function was motivated by examining the state-of-the-art loss assignment policy. It allocates the loss value as per instance significance. The more important instances are assigned higher loss values, whereas those corresponding to outliers and noise are assigned lower loss values. The experiments were conducted on the benchmark datasets downloaded from the UCI repository to compare the performance of the proposed variant of SVM with hinge loss SVM, pinball loss SVM, \u03f5-pinball loss SVM, SVM-CL, relabel SVM, 2medianSVM and 2meanSVM. The experimental results demonstrate that the eagle loss SVM outperforms all the state-of-the-art variants of SVM and is robust due to the incorporation of the novel loss assignment policy.\nHighlights\n\u2022\nA noise-robust loss function entitled Eagle Loss Function.\n\u2022\nNoise-robust EagleSVM using Eagle Loss Function.\n\u2022\nRobust classifiers towards feature noise and target noise.","Recognizing the species composition of an ecosystem is essential for conservation and land management. This study presents the software Class3Dp, a supervised classifier of vegetation species for coloured point clouds. Class3Dp is run through a graphical user interface (GUI) that allows for the selection of training samples from RGB or MS (multispectral) clouds and their classification based on geometric, spectral and neighbourhood features, along with different machine learning methods, obtaining the point cloud classified according to the classes (species) introduced. A case study is shown where a classification of ground and vegetation is carried out, obtaining an overall accuracy (OA) of 0.94 in the RGB classification and 0.95 in the MS. Points classified as vegetation were re-classified in the species Anthyllis cytisoides L., Chamaerops humilis L., Cistus monspeliensis L., Pistacia lentiscus L. and Quercus coccifera L., obtaining an OA of 0.86 in the RGB classification and 0.87 in the MS.\nHighlights\n\u2022\nClass3Dp is a supervised classifier software of coloured point clouds based on 3D and spectral information.\n\u2022\nThe software is designed to classify plant species in RGB and multispectral point clouds.\n\u2022\nClass3Dp calculates up to 48 features and supports five machine learning models.","Airborne pollen identification is crucial to help patients prevent pollinosis symptoms. Existing data-driven methods rely on large-scale pollen images with simple backgrounds. In real scenarios, the background is complex and the data scale is small. Therefore, these methods suffer from two challenges: (1) Irrelevant information interference; (2) Incomplete feature attention. To overcome these challenges, we propose a prior knowledge-guided deep feature learning (PK-DFL) for real-world optical microscope image classification. Its main steps are as follows: Pollen location is designed to locate pollen grains based on color features, aiming to boost the accuracy of shape and texture prior feature extraction. Shape-texture awareness helps to extract the shape and texture of pollen grains via predefined feature extractors (i.e., a set of shape descriptors and an improved SFTA). These features are used to construct two types of prior knowledge, namely shape-texture attention maps (STA maps) and shape-texture feature vectors (STF vectors). Pollen classification uses a deep network (CNN) to classify pollen via imitating the pollen identification procedure of palynologists. It uses STA maps to weight pollen images and convolutional feature maps for instructing the CNN to focus on critical areas of pollen images (for the first challenge). STF vectors are employed to obtain the inter-class similarity of pollen via template matching. This information is further converted to soft targets that are used to supervise the CNN attending to comprehensive key features (for the second challenge). Extensive experiments on real-world datasets demonstrate the effectiveness of our PK-DFL (with accuracy and F1-score over 88%).\nHighlights\n\u2022\nA deep learning model for identifying allergic pollen like a palynologist.\n\u2022\nPollen location and predefined feature extractors for shape-texture extraction.\n\u2022\nAttention instruction manner for guiding CNNs to focus on critical areas.\n\u2022\nSoft target supervision manner for guiding CNNs to attend to key features.\n\u2022\nExtensive experiments for performance evaluation (accuracy over 88%).","The North Atlantic Right Whale (NARW) population is currently teetering on the brink of extinction, with a mere approximate count of 350 individuals remaining. These animals have been protected under the Endangered Species Act since 1970. Today, the survival of right whales is imperiled primarily due to vessel collisions, net entanglements, and habitat degradation. This paper presents a novel system of animal-computer interaction founded on the identification of bioacoustic signatures. Initially, NARWs\u2019 vocalizations were transformed into spectrograms, which were subsequently inputted into a Convolutional Neural Network (CNN). To enhance robustness against environmental noise, techniques such as time warping, frequency masking, and time masking were employed. The outcomes of our study indicate that the proposed system holds potential for establishing a closed-loop interaction framework between vessels and NARWs. This framework could enable vessels to adapt their speed or avoid routes frequented by NARWs. Furthermore, this article discusses the potential benefits of employing networked sensors, such as Internet of Things (IoT) devices, to augment NARW monitoring and data collection efforts.","Alzheimer disease (AD) is a chronic neurological disorder in which the loss of brain cells causes dementia. Early and accurate diagnosis of AD will lead to better treatment of the disease before irreversible brain damage has been occurred. This paper proposes the classification of Alzheimer's disease using 3D structural Magnetic Resonance Imaging (sMRI) images through 3D convolutional neural networks (CNNs). Most existing methods utilizing 3D subject-level CNNs for Alzheimer's disease classification design a single model which relies on a very large training dataset for improved generalization. Herein, we address this issue through 3D transfer learning which makes use of knowledge gained from a pre-trained task. We train 3D versions of five classical 2D image classification architectures\u2014ResNet, ResNeXt, SE-ResNet, SE-ResNeXt, and SE-Net\u2014by initializing each model with pre-trained weights from their 2D counterparts, and combine their predictions through a weighted average method. The weights assigned to each model of the ensemble are optimized to achieve a performance better than any single 3D CNN model. With a relatively smaller training dataset, the proposed model obtains 97.27%, 82.33%, 90.41%, 84.22%, 84.26%, and 77.1% accuracies for the Alzheimer\u2019s disease (AD) versus cognitively normal (CN), early mild cognitive impairment (EMCI) versus CN, late mild cognitive impairment (LMCI) versus CN, EMCI versus AD, LMCI versus AD, and EMCI versus LMCI classification tasks, outperforming current state-of-the-art methods, and indicating the effectiveness of our proposed model.","In an actual industrial scenario, machines typically operate normally for the majority of the time, with malfunctions occurring only occasionally. As a result, there is very little recorded data on defects. Consequently, the fault diagnostic dataset becomes imbalanced, with a significantly lower number of fault samples compared to normal samples. Furthermore, with the rapid development of the manufacturing industry, the increasing complexity of machines and equipment leads to various challenges in collecting fault data, such as noise, within-class imbalance, multi-class imbalance, and time series imbalance. It is worth noting that this study is the first to comprehensively summarize these four specific challenges. Therefore, addressing these issues has become a critical research focus and a pain point in the field of fault diagnosis, and numerous solutions have emerged. This study provides a comprehensive overview of these solutions at three levels: data preprocessing, feature extraction, and classifier improvement. It also describes the applications of imbalanced data classification methods, including pure resampling techniques, as well as sampling techniques that combine resampling algorithms with feature extraction and classifier improvement in industrial scenarios. Finally, we summarize the challenges facing imbalanced data classification research and suggest potential directions for future studies.\nHighlights\n\u2022\nThis paper showcases using imbalance classification for diagnosing industrial faults.\n\u2022\nThis study is the first to comprehensively summarize these five specific challenges.\n\u2022\nChallenges include noise, class overlap, and imbalance within/multi-class/time series.\n\u2022\nStudy covers data pre-processing, feature extraction, and classifier improvement.\n\u2022\nStudy suggests the future research directions for imbalanced fault diagnosis.","The traditional k-mean clustering algorithm has some drawbacks, such as the need to manually determine the initial K value in advance, and the value may not match the real data distribution, and is susceptible to noise, thus causing classification errors. In this paper, we take the lead in improving the contour coefficient solving code and the k-mean clustering algorithm, so that the two improved codes can cooperate with each other to improve the accuracy of the results of the algorithm.\nFirstly, we use variance comparison to sub-classify the dataset, aiming to find the initial clustering center of the dataset, use the k-means clustering algorithm to classify the dataset, and finally introduce the contour coefficient to evaluate the classification results. Finally, we apply this algorithm to an example of artifact classification and train the improved algorithm with a large amount of data, and the results demonstrate that the contour coefficient-k-mean clustering algorithm yields high accuracy in the classification results.","Graphical abstract\nDisplay Omitted\nAbstract\nA challenge in gas turbine fault diagnosis is that labeled fault samples are relatively rare and much fewer than normal samples. Conventional data augmentation techniques generate fault samples in original data spaces, resulting in the issue that synthetic fault samples highly overlap with normal samples. Aiming at the issue, a feature-level data augmentation method, namely feature-level SMOTE, is developed by integrating deep Siamese multi-head self-attention network (DSMHSA) with synthetic minority over-sampling technique (SMOTE) to reduce inter-class imbalance and overlap simultaneously. First, the DSMHSA maps original data into a feature space with better inter-class separability, in which inter-class samples stay far away from one another. Second, the SMOTE generates synthetic fault samples in the well-separable space, in order to balance the data set. Finally, the effectiveness of the developed feature-level SMOTE in imbalanced fault diagnosis has been evaluated through two case studies including the real gas turbine fault dataset and the public robot execution failures dataset. To be specific, its average balanced accuracy is 90.38% on the gas turbine dataset, yielding 9.67%, 13.94%, and 12.39% improvements compared to the OUPS, A-SUWO, and NRAS, respectively.","Text Classification (TC) is a fundamental task in the information retrieval community. Nowadays, the mainstay TC methods are built on the deep neural networks, which can learn much more discriminative text features than the traditional shallow learning methods. Among existing deep TC methods, the ones based on Graph Neural Network (GNN) have attracted more attention due to the superior performance. Technically, the GNN-based TC methods mainly transform the full training dataset to a graph of texts; however, they often neglect the dependency between words, so as to miss potential semantic information of texts, which may be significant to exactly represent them. To solve the aforementioned problem, we generate graphs of words instead, so as to capture the dependency information of words. Specifically, each text is translated into a graph of words, where neighboring words are linked. We learn the node features of words by a GNN-like procedure and then aggregate them as the graph feature to represent the current text. To further improve the text representations, we suggest a contrastive learning regularization term. Specifically, we generate two augmented text graphs for each original text graph, we constrain the representations of the two augmented graphs from the same text close and the ones from different texts far away. We propose various techniques to generate the augmented graphs. Upon those ideas, we develop a novel deep TC model, namely Text-level Graph Networks with Contrastive Learning (TGNcl). We conduct a number of experiments to evaluate the proposed TGNcl model. The empirical results demonstrate that TGNcl can outperform the existing state-of-the-art TC models.","Lung cancer (LC) remains a leading cause of death worldwide. Early diagnosis is critical to protect innocent human lives. Computed tomography (CT) scans are one of the primary imaging modalities for lung cancer diagnosis. However, manual CT scan analysis is time-consuming and prone to errors/not accurate. Considering these shortcomings, computational methods especially machine learning and deep learning algorithms are leveraged as an alternative to accelerate the accurate detection of CT scans as cancerous, and non-cancerous. In the present article, we proposed a novel transfer learning-based predictor called, Lung-EffNet for lung cancer classification. Lung-EffNet is built based on the architecture of EfficientNet and further modified by adding top layers in the classification head of the model. Lung-EffNet is evaluated by utilizing five variants of EfficientNet i.e., B0\u2013B4. The experiments are conducted on the benchmark dataset \u201cIQ-OTH/NCCD\u201d for lung cancer patients grouped as benign, malignant, or normal based on the presence or absence of lung cancer. The class imbalance issue was handled through multiple data augmentation methods to overcome the biases. The developed model Lung-EffNet attained 99.10% of accuracy and a score of 0.97 to 0.99 of ROC on the test set. We compared the efficacy of the proposed fine-tuned pre-trained EfficientNet with other pre-trained CNN architectures. The predicted outcomes demonstrate that EfficientNetB1 based Lung-EffNet outperforms other CNNs in terms of both accuracy and efficiency. Moreover, it is faster and requires fewer parameters to train than other CNN based models, making it a good choice for large-scale deployment in clinical settings and a promising tool for automated lung cancer diagnosis from CT scan images.\nHighlights\n\u2022\nWe developed a novel transfer learning framework using EfficientNetB1 for lung cancer classification.\n\u2022\nWe solved the severe imbalance issue by using augmentation method to overcome the skewness of data.\n\u2022\nWe compared the novel designed model with other advanced methods in terms of execution time and computational complexity to demonstrate the performance of EfficientNet over other classification models.\n\u2022\nOur proposed model Lung-EffNet demonstrates superior performance in comparison to existing methods and targeting lung cancer from CT scan images.","In this study, we compare the classification accuracy achievable with linear support vector machine (L-SVM), K-nearest neighbor (KNN), and multilayer perceptron (MLP) methods for a multi-class EEG signal. This can be done in three phases. In phase one, band-pass filtering is applied to raw electroencephalogram (EEG) signals to decompose into five different frequency subbands. In phase two, we extract 10 important features from each subband. In phase three, these extracted feature sets are used as input to L-SVM, KNN, and MLP classifiers which categorize the sample data into three classes namely yoga, meditation, and combined yoga\u2013meditation. Various performance measures for each classifier are evaluated and then compared to know which classifier is effective in the classification of the EEG data into yoga, meditation, and combined yoga\u2013meditation groups. Performance measures such as confusion matrix, accuracy, sensitivity, specificity, precision, and F1 score are used to validate the performance of classifiers. Kruskal\u2013Wallis test has been conducted to compare the classification performance of the linear SVM, KNN, and MLP classifier models. By comparing the classification accuracy between the three classifiers, L-SVM achieved the highest accuracy of 91.67%.","Research on machine activity recognition (MAR) is drawing more attention because MAR can provide productivity monitoring for efficiency optimization, better maintenance scheduling, product design improvement, and potential material savings. A particular challenge of MAR for human-operated machines is the overlap when transiting from one activity to another: during transitions, operators often perform two activities simultaneously, e.g., lifting the fork already while approaching a rack, so the exact time when one activity ends and another begins is uncertain. Machine learning models are often uncertain during such activity transitions, and we propose a novel ensemble-based method adapted to fuzzy transitions in a forklift MAR problem. Unlike traditional ensembles, where models in the ensemble are trained on different subsets of data, or with costs that force them to be diverse in their responses, our approach is to train a single model that predicts several activity labels, each under a different context. These individual predictions are not made by independent networks but are made using a structure that allows for sharing important features, i.e., a context ensemble. The results show that the gated recurrent unit network can provide medium or strong confident context ensembles for 95% of the cases in the test set, and the final forklift MAR result achieves accuracies of 97% for driving and 90% for load-handling activities. This study is the first to highlight the overlapping activity issue in MAR problems and to demonstrate that the recognition results can be significantly improved by designing a machine learning framework that addresses this issue.","Data-driven fault diagnosis approaches have attracted considerable attention in the past few years, and promising diagnostic performance has been achieved with sufficient monitoring data. However, in real industrial scenarios, individual users often struggle to collect enough labeled data. Meanwhile, direct data aggregation from multiple users is not always feasible due to data privacy concerns and conflicts of interest. To solve this issue, a novel federated contrastive prototype learning scheme is proposed for collaborative fault diagnosis of rotating machinery. The collaborative modeling between the central server and multiple clients is implemented to establish a global fault diagnostic model with data privacy. A contrastive prototype learning module is designed to align the prototypes of the same classes across different clients while separating them away from other class prototypes, thus effectively eliminating distribution discrepancies across clients and learning domain-invariant discriminative features. To remove the bias of the global model during federated communication, an unbiased prototype learning module is constructed, which aligns the class prototypes of different clients to the global prototype center and enhances the generalization ability of the proposed approach under unseen conditions. Experimental results on two self-built testbeds and a laboratory dataset demonstrate that the proposed approach is a potential solution for real-world fault diagnosis applications.","Ensemble methods and conventional base class learners have effectively been applied in the realm of educational data mining to ameliorate the accuracy and consistency in prediction. Primarily in the contemporary study, researchers conducted empirical results on pedagogical real dataset acquired from University of Kashmir, using miscellaneous base classifiers viz. j48, random forest and random tree, to predict the performance of students. However, in the later phase, the pedagogical dataset was subjected to more proficient version of stacking viz. stackingC, with the principle objective to ameliorate the performance of students. Furthermore, the dataset was deployed with filtering procedures to corroborate any improvement in results, after the application of techniques such as synthetic minority oversampling technique (SMOTE) and spread sub-sampling method. Moreover, in case of ensemble stackingC, hybridization of predicted output was carried out with three base classifier vis-a- vis j48, random forest and random tree, and the classifier achieved paramount accuracy of 95.65% in predicting the actual class of students. The findings have by and large noticeably corroborated that the stackingC classifier, attained significant prediction accuracy of 95.96% when undergone through undersampling (spread sub-sampling) and 96.11% using oversampling (SMOTE). As a subject of corollary, it calls upon the researchers to broaden the canvas of literature by employing the analogous methods to uncover the diverse patterns hidden in academic datasets.","Credit scoring concerns with developing empirical model to support financial decision making process for financial institutions. It makes use of applicants\u2019 historical data and statistical or machine learning techniques to access the risk associated with an applicant. However, the data may have redundant and irrelevant information and features, which degrades the classification accuracy and increases the complexity. So, effective feature selection technique can resolve the problem of credit scoring dataset with huge number of features. In various studies, it is shown that ensemble classifier improves the classification performances as compared to its base classifiers. This study focuses to combine the benefits of feature selection and ensemble framework. For feature selection an approach based on feature clustering have been proposed in this study. Moreover, dataset with selected features is applied on five base classifiers and output obtained by base classifiers are aggregated by weighted voting approach for prediction of final output. For validating the proposed approach, three real world credit scoring datasets are utilized and results compared with some existing feature selection techniques in terms of classification accuracy and F1 -score.","Skip Abstract Section\nAbstract\nThe concealment of improvised explosive devices in dustbins aimed at destroying people and property is causing the mass removal of dustbins from public places and vehicular public transport in cities around the world. Such action of dustbin removal results in littering, stench, pests, contamination of water bodies, the spread of diseases, and increased greenhouse gases. The current solutions to the problem are blast-resistant dustbins which are bulky and expensive, and transparent dustbins which display the awful appearance of wastes in public places. This article proposes equipping dustbins with artificial intelligence-based classifiers to detect explosives concealed in wastes in public dustbins to minimise the risk to public safety. There was the need to construct a new database of explosive images to augment the existing TrashNet dataset. Then, through transfer learning using eight state-of-the-art convolutional neural networks as base models, the augmented dataset was used to search for optimum convolutional neural networks to detect explosives. One of the trained networks based on DenseNet-121 achieved the Top-1 accuracy of 80% with about 26 minutes learning time, which is 6.7% better than the Top-1 accuracy achieved by the base model on the benchmark ImageNet dataset. This finding demonstrates that the designed neural networks are promising cutting-edge techniques for detecting explosives concealed in dustbins to threaten public safety. To the best of our knowledge, this is the first time that convolutional neural networks have been proposed to identify explosives concealed in dustbins.\nSkip Graphical abstract Section\nGraphical abstract","Pulmonary Embolism (PE) \u2014 a non-cardiac cause of cardiac arrest is a strenuous job to perform as it is non-specific in presentation and shares various overlapping features with various other clinical disorders like Myocardial Infarction, Pneumonia, etc. This paper delineates a method to identify Pulmonary Embolism (PE) as a reason for Heart Failure using Deep Reinforcement Learning (DRL) algorithm. The methodology has been partitioned into two phases. Phase I acts towards the creation of a novel dataset as there was no data available for this particular problem statement. Phase II deals with the scope and application of the DRL algorithm on the above-invented dataset. The dataset formed is imbalanced in its essence. To effectively tackle the imbalanced dataset challenge, a cutting-edge imbalanced classification algorithm rooted in the power of Deep Reinforcement Learning (DRL) has been embraced. The classification model has been drawn up as a Sequential Decision-Making procedure and is resolved by making use of the DRL algorithm viz Double Deep Q-Network (DDQN). A classification action is performed by an agent on a single sample at each time step. Classification actions are assessed by the environment, based on the assessment, a reward is given to an agent. In order to make the agent more responsive towards the minority class samples, rewards belonging to the minority class are higher. Under the supervision of this reward system, the agent finally learns the optimal classification policy for this imbalanced dataset problem. Comparative analysis of the DDQN algorithm with other Deep Reinforcement Learning algorithms, including Dueling DQN, and Proximal Policy Optimization (PPO) algorithms has been performed in order to arrive at the best learning technique for the dataset. Furthermore, the DDQN algorithm has been evaluated against a selection of Machine Learning (ML) algorithms. From the experimentation, it is demonstrated that the DDQN model outperformed other approaches with an accuracy of 99.99%, G-mean 0.9999, Recall 1.0000, and Specificity 0.9999.","3D printing has the potential to revolutionize industrial manufacturing through efficient and sustainable techniques. Fused Deposition Modeling (FDM) is a broadly deployed technique among various 3D printing methods. However, the surface quality of FDM is greatly influenced by multiple factors, making it challenging to unravel the relationship between printing quality and parameter settings. To break through this bottleneck, this study proposes an intelligent approach that combines Transfer Learning (TL)-based Feature Extractor (FE) and Gradient-Boosting Decision Trees (GBDT) to investigate the effects of FDM printing parameters on surface quality. Experiments are conducted in the laboratory to validate the effectiveness of the FE-GBDT, which is then compared with the exemplary Machine Learning (ML) algorithms. The results show that our proposed TL model can achieve high precision and accuracy over 0.9900, demonstrating the efficacy of FE-GBDT in deciphering the impact of FDM printing parameters on surface quality. The contribution of each parameter is evaluated and indicates that layer height could dramatically affect the surface quality with an importance score of 0.626. The results provide valuable insights for the 3D printing community, proving that the FE-GBDT approach offers improved generalization, faster training, enhanced feature extraction, addressing data scarcity, and the ability to leverage the strengths of both approaches for superior performance across various tasks.","Deep learning-based approaches for three-dimensional (3D) grid understanding and processing tasks have been extensively studied in recent years. Despite the great success in various scenarios, the existing approaches fail to effectively utilize the velocity information in the flow field, resulting in the actual requirements of post-processing tasks being difficult to meet by the extracted features. To fully integrate structural information in the 3D grid and velocity information, this paper constructs a flow-field-aware network (FFANet) for 3D grid classification and segmentation tasks. The main innovations include: (i) using the self-attention mechanism to build a multi-scale feature learning network to learn the distribution feature of the velocity field and structure feature of different scales in the 3D flow field grid, respectively, for generating a global feature with more discriminative representation information; (ii) constructing a fine-grained semantic learning network based on a co-attention mechanism to adaptively learn the weight matrix between the above two features to enhance the effective semantic utilization of the global feature; (iii) according to the practical requirements of post-processing in numerical simulation, we designed two downstream tasks: 1) surface grid identification task and 2) feature edge extraction task. The experimental results show that the accuracy (Acc) and intersection-over-union (IoU) performance of the FFANet compared favourably to the 3D mesh data analysis approaches.","Deep NLP models are correlation-based learning, which has a critical limitation of over-fitting over spurious features and shows poor generalization capability in the out-of-distribution (OOD) setting. Existing methods encourage the model to exploit causal features and exclude spurious correlations through Counterfactually Augmented Data (CAD) and feature regularization. However, those methods still face challenges due to the low quality of counterfactual generation and the high cost of annotation. This paper proposes an improved method for OOD generalization motivated by causal inference tools. Specifically, taking the topic of the text as the confounder of the input and the label, the model fits the causal correlation between the representations and the label through the backdoor adjustment to alleviate the exploitation of the spurious correlations. The proposed method is evaluated on the counterfactual adversarial test set (movie review text) and the challenging test set with synonym perturbation (financial news text) provided in the previous work. The experimental results show that this method improves the OOD generalization capability of the sentiment classification models in these two attacks while preserving the predictive ability, especially in the case of long text.","Supervised and unsupervised classification is crucial in many areas where different types of data sets are common, such as biology, medicine, or industry, among others. A key consideration is that some units are more typical of the group they belong to than others. For this reason, fuzzy classification approaches are necessary. In this paper, a fuzzy supervised classification method, which is based on the construction of prototypes, is proposed. The method obtains the prototypes from an objective function that includes label information and a distance-based depth function. It works with any distance and it can deal with data sets of a wide nature variety. It can further be applied to data sets where the use of Euclidean distance is not suitable and to high-dimensional data (data sets in which the number of features p is larger than the number of observations n, often written as p &gt; &gt; n). In addition, the model can also cope with unsupervised classification, thus becoming an interesting alternative to other fuzzy clustering methods. With synthetic data sets along with high-dimensional real biomedical and industrial data sets, we demonstrate the good performance of the supervised and unsupervised fuzzy proposed procedures.\nHighlights\n\u2022\nNew fuzzy classification methodology based on the construction of prototypes\n\u2022\nDistance-based, it overcomes the curse of dimensionality\n\u2022\nIt can be applied to a large spectrum of data, when the Euclidean distance is not suitable\n\u2022\nIt identifies K observations, selected between the deepest observations\n\u2022\nSupervised and unsupervised approaches integrated in the objective function","Skip Abstract Section\nAbstract\nThe disadvantages of modern methods of classifying objects and processes are considered. We propose a method of constructing three-dimensional classifications that allows the elimination of some of these shortcomings, based on the system-object approach, using the ideas of multidimensional classification and natural classification. Three basic system characteristics are used as classification planes: structural (node), functional (function), and substantive (object), which allows for the classification by types of functional request to the system from a higher-order system (supersystem) by types of system formation processes and by the obtained results. Each classification is a tree-type graph with one vertex that is common to all three planes. The formal description of the three-dimensional graph by means of descriptive logic is presented, which not only allows for the phenomena and objects of the subject area to be classified, but also for the cause-and-effect relations existing in this area to be traced. The procedures for the use of three-dimensional system-object classification for forecasting and management support are described. An example of three-dimensional classification for functional diagnostic devices is given.","Machinery fault diagnosis based on deep learning methods is cost-effective to guarantee safety and reliability of mechanical systems. Due to the variability of machinery working condition and difficulty of data obtaining under different health states, it is desirable to enhance the generalization capability to unseen working conditions for the fault diagnosis models trained by available data sets under limited number of working conditions. Considering that labeling industrial data is also a laborious work, this paper proposes a novel semi-supervised domain generalization model, termed domain-invariant feature fusion networks (DIFFN) for intelligent fault diagnosis under unseen target working conditions. The main contributions are that, intra-domain-invariant features are considered to capture the intrinsic semantic information within the domain and are fused with inter-domain-invariant features to enhance the discrimination and generalization abilities in fault diagnosis. First, a domain-invariant representation learning method is established to learn the inter- and intra-domain-invariant features using two network branches and fuse them via a fusion module. Second, a mutual learning strategy is designed to enable the network branches and the fusion module to learn from each other, thereby improving the discrimination of the extracted features for accurate fault diagnosis. Lastly, a feature divergence maximization strategy is embedded between the two network branches to improve the generalization ability of the fault diagnosis model. Experiments on two bearing data sets demonstrate that the proposed model has better diagnostic accuracy and stability over state-of-the-art semi-supervised domain generalization methods, indicating its great potential for application in generalization fault diagnosis of machinery under unseen target working conditions.\nHighlights\n\u2022\nDIFFN is proposed to realize semi-supervised generalization fault diagnosis.\n\u2022\nInter- and intra-domain-invariant features are sufficiently exploited.\n\u2022\nA mutual learning strategy is designed to promote the discrimination of features.\n\u2022\nFeature divergence maximization strategy enhances the feature diversification.\n\u2022\nBearing generalization diagnosis experiments validate the superiority of DIFFN.","Transformers with long-range dependency and data specificity act as an effective means of classifying insect pests in agricultural engineering. Although many methods have been proposed to confine the range of self-attention within a local region to reduce the computation complexity, none of them can reduce the number of model parameters. Moreover, the self-attention mechanism usually causes query tokens to focus excessively on image patches, which limits the effective receptive field and the long-range dependence. To address these issues, this paper establishes a novel Dilated-Windows-based Vision Transformer with Efficient-Suppressive-self-attention (DWViT-ES) architecture, which includes efficient-self-attention (ESA), dilated window (DW), and suppressive-self-attention (SSA) as its core components. The ESA simplifies the successive linear Transformations to reduce the number of model parameters and computational costs. Meanwhile, the DW and SSA expand the effective receptive field of self-attention mechanism to prevent query tokens from focusing on similar and close regions, thereby preventing the loss of useful information. Finally, experiments show that the DWViT-ES only has 19.6 M parameters and 3.5G FLOPs (over 20% reductions vs. 19.6 M and 4.5G of Swin-T). Meanwhile, the DWViT-ES training from scratch has 71.6% top-1 accuracy on the IP102 dataset (2.4% absolute improvement of Swin-T); after fine-tuning on Imagenet-1K, the DWViT-ES achieves 76.0% and 78.7% top-1 accuracy on IP102 and CPB (0.1% and 0.9% absolute improvement of Swin-T), respectively. Meanwhile, practical deployment on a mobile-embedded device is presented, which validates the feasibility of the DWViT-ES.","Today's ever\u2010increasing generation of streaming data demands novel data mining approaches tailored to mining dynamic data streams. Data streams are non\u2010static in nature, continuously generated, and endless. They often suffer from class imbalance and undergo temporal drift. To address the classification of consecutive data instances within imbalanced data streams, this research introduces a new ensemble classification algorithm called Rarity Updated Ensemble with Oversampling (RUEO). The RUEO approach is specifically designed to exhibit robustness against class imbalance by incorporating an imbalance\u2010specific criterion to assess the efficacy of the base classifiers and employing an oversampling technique to reduce the imbalance in the training data. The RUEO algorithm was evaluated on a set of 20 data streams and compared against 14 baseline algorithms. On average, the proposed RUEO algorithm achieves an average\u2010accuracy of 0.69 on the real\u2010world data streams, while the chunk\u2010based algorithms AWE, AUE, and KUE achieve average\u2010accuracies of 0.48, 0.65, and 0.66, respectively. The statistical analysis, conducted using the Wilcoxon test, reveals a statistically significant improvement in average\u2010accuracy for the proposed RUEO algorithm when compared to 12 out of the 14 baseline algorithms. The source code and experimental results of this research work will be publicly available at https://github.com/vkiani/RUEO.","Intention recognition of non-cooperative target is an important basis for battlefield command decision-making. Recent advances suggest recognizing target intention from a perspective of data-driven. However, existing data-driven models do not consider complementary information between features to enhance their robustness in battlefield environments. To solve the problem, this paper constructs a novel neural network fusion model with information classification processing and information fusion to achieve target intention recognition. The model first designs the cross-classification processing method according to attributes\u2019 correlations and variation characteristics. Then, an interactive feature-level fusion method is proposed to model the fine-grained correlations between attributes to discover salient features. Finally, a decision-level fusion method based on Dempster\u2013Shafer theory is proposed to fuse the complementary information among attributes. The experimental results show that the recognition accuracy of the proposed model can reach 89.63%, and it can be maintained above 75% under the conditions of severe attribute missing or noise interference. It is demonstrated that the proposed model has higher accuracy and robustness in battlefield incomplete information environments.\nHighlights\n\u2022\nAn artificial neural network model with information classification processing and information fusion is constructed for target intention recognition.\n\u2022\nAn interactive feature-level fusion method is proposed to model the fine-grained correlations between attributes to discover salient features.\n\u2022\nA decision-level fusion method based on Dempster\u2013Shafer theory is proposed to fuse the complementary information among attributes.\n\u2022\nExperiments demonstrate that the proposed model has higher accuracy and robustness in battlefield incomplete information environments.","Origin: Warts are produced and developed on the human body due to infection induced by Human Papillomavirus. The most influenced zone of warts are hands and feet particularly, which is bit irritating and difficult to recoup in later stages. The major challenge in treating warts is the diversity of treatment method applicable on different patients, so it becomes difficult to recognize specific treatment method to be adopted in order to treat this infection. Ramifications of machine learning techniques in the medical domain have become crucial nowadays for early disease detection and developing expert systems. Objective: This research work focuses on enhancing predictive accuracy of J48, which is a binary decision tree based classifier by adding attributes based on genetic programming. These genetically tuned attribute construction not only just upgrades the classification capabilities of J48 classifier but also additionally expand the information space, intending J48 for giving more exact predictions for wart treatment method identification. Method: For their experimental setup, authors have chosen immunotherapy and cryotherapy datasets from UCI machine learning repositories, which includes instances of patients responses against treated with immunotherapy and cryotherapy methods for both plantar and common warts. The investigation has been led with the help of WEKA tool, which is an open source for performing data mining operations. Finding: After experimentation, it is found after inclusion of attributes generated through genetic programming, the classification accuracy of J48 can be increased by a substantial amount with less error rate. The result shows significant performance improvements in classification accuracy of J48 by 82.22% to 96.66% and 93.33% to 98.88% for immunotherapy and cryotherapy datasets, implemented with J48 and J48+GA respectively.","Training deep learning models on long-tailed datasets is a challenging task since the classification performance of tail classes with fewer samples is always unsatisfactory. Currently, many long-tailed methods have achieved success. However, some methods always improve tail-class performance at the expense of head-class performance due to limited model capability. To address this issue, we propose a novel algorithm-level method inspired by information theory to balance the information space of each class and boost tail-class performance while minimizing head-class sacrifice. Our method involves actively eliminating the redundant feature information of head classes to save space for tail classes during training. Specifically, we use a bilateral-expert model and design a duplicate information disentanglement (DID) module that can extract duplicate and redundant information from bilateral-expert features. This allows us to develop a head diversity loss to decrease the extracted duplicate and redundant information of head classes and a tail distillation loss to increase the label information of tail classes. The joint result of these two losses allows our model to fully leverage the information space for improved tail-class performance without compromising head-class performance. The effectiveness and practicability of our method are verified by five datasets with long-tailed distributions for visual recognition or fault diagnosis tasks. Experimental results demonstrate that our method outperforms currently available mainstream methods, which we attribute to the effectiveness of our proposed DID module and the incorporation of two long-tailed losses.","This paper focuses on the classification problem of multi-view data, aiming to improve the classification accuracy of current algorithms on multi-view data. Previous multi-view classification algorithms are usually based on exploiting the complementarity of different views and fusing features from different views. A representative category is the graph-based method, which builds a graph matrix for each view, and then fuses the graph matrices of different views to obtain a unified graph. These methods have the following problems: firstly, the graph matrix is simply based on sample similarity usually; secondly, the learned graph matrix does not change dynamically; thirdly, the weight of the graph representation matrix for a single view cannot be learned in the unified graph matrix. Therefore, this paper designs a Two-step classification algorithm based on Dynamic Graph-ELM, called TSDGELM. In the TSDGELM, the dynamic Graph-ELM is used to obtain the graph representation matrix of each view to save the local neighbor information of the data, and then a joint graph learning algorithm is designed based on the GBS (Graph-Based System) mechanism to fuse the graph matrix of the single-view, and finally the united graph is input into the classifier. To evaluate the effectiveness of the proposed method in this work, we conduct a series of experiments on eight datasets, and the results demonstrate the superiority of the proposed method.","Feature selection, as an important pre-processing technique, can efficiently mitigate the issue of \u201cthe curse of dimensionality\u201d by selecting discriminative features especially for multi-label learning, a discriminative feature subset can improve the classification accuracy. The existing feature selection methods for multi-label classification address the problem of label ambiguity by with logical labels. However, the significance of each label is often different in many practical applications. Using logical label to train the model may result in unsatisfactory performance due to not considering the importance of related labels with each sample. To address this issue, a novel multi-label feature selection algorithm is proposed with two-step: label enhancement and label correlations-based feature selection with label enhancement. In the step of label enhancement, a framework of label enhancement based on deep forest is utilized to transform the logical label to label distribution, which contains rich semantic information and then guides a more correct exploration of semantic correlations. In the step of feature selection, a novel multi-label feature selection algorithm is proposed based on label distribution data. Firstly, the samples are divided into multiple different clusters by using spectral clustering in the label space. Then, the label correlations can be reflected by multiple different clusters. Finally, the l 2 , 1-norm is used to construct an objective function to achieve multi-label feature selection. Experimental results demonstrate that competitiveness of the proposed algorithm over six state-of-the-art multi-label feature selection algorithms on eighteen benchmark datasets in terms of six widely accepted evaluation metrics.","Traditional techniques for network traffic classification are no longer effective in handling the complexities of dynamic network environments. Moreover, deep learning methods, while powerful, demand substantial spatial and computational resources, resulting in increased latency and instability. In this paper, we propose an innovative approach to network traffic classification utilising an LSTM structure. This approach incorporates network pruning, knowledge refinement, and Generative Adversarial Networks (GAN) to reduce model size, accelerate training speed without compromising accuracy, and address challenges associated with unbalanced datasets in classification problems. Our methodology involves the pruning of unimportant filters from the teacher model, followed by retraining and knowledge distillation to generate the student model. Experimental show that the size of the pruned teacher model is only 25.69% of the original, resulting in a noteworthy 28.16% improvement in training speed. Additionally, the classification performance of various unbalanced traffic categories, such as VoIP and streaming, shows significant enhancement.","Highlights\n\u2022\nIntroduction of a novel model that integrates OBIA and the attention mechanism for precise orchard mapping.\n\u2022\nThe contribution of SAR is studied in orchard classification.\n\u2022\nThe model obtains more accurate classification results than RNN-based and CNN-based deep learning models.\n\u2022\nThe uncertainty of the super-pixel size on classification accuracy is investigated.\nAbstract\nReliable and accurate classification of orchards is important for the dynamic monitoring of large-scale orchards and food security evaluation. At present, the very similar spectral profiles of different fruit trees and the high susceptibility of optical data to interference by weather conditions limit the resolution of orchard classification. Synthetic Aperture Radar (SAR) imagery provides an advanced solution to this problem, with the advantages of being immune to weather conditions and advances in deep learning techniques. This paper presents an orchard classification model (STCM) with optical and SAR fusion, which integrates the advantages of the Simple Non-Iterative Clustering (SNIC) super-pixel algorithm with a deep learning algorithm based on a multi-headed attention mechanism, and it achieves the best classification accuracy of above 0.82 in comparison with existing deep learning models. Meanwhile, the model has exceptional classification accuracy and robustness in a study area with fragmented plots, many fruit tree species, and various distributions of optical images. In the absence of optical data, the classification accuracy of the STCM model using only SAR data is around 0.70, which makes the model have a promising potential application value. This study provides a technical solution for accurately obtaining different orchard categories in high-resolution remotely sensed orchard images. The study helps to improve the management and operation of orchards and provides a strong basis for decision-making in the fruit industry to enhance the sustainability of the global fruit industry.","Graph neural networks (GNNs) have been shown to be useful in a variety of graph classification tasks, from bioinformatics to social networks. However, most GNNs represent the graph using local neighbourhood aggregation. This mechanism is inherently difficult to learn about the global structure of a graph and does not have enough expressive power to distinguish simple non-isomorphic graphs. To overcome the limitation, here we propose multi-head heat kernel convolution for graph representation. Unlike the conventional approach of aggregating local information from neighbours using an adjacency matrix, the proposed method uses multiple heat kernels to learn the local information and the global structure simultaneously. The proposed algorithm outperforms the competing methods in most benchmark datasets or at least shows comparable performance.","Plenty of models have been presented to handle the hypergraph node classification. However, very few of these methods consider contrastive learning, which is popular due to its great power to represent instances. This paper makes an attempt to leverage contrastive learning to hypergraph representation learning. Specifically, we propose a novel method called Collaborative Contrastive Learning (CCL), which incorporates a generated standard graph with the hypergraph. The main technical contribution here is that we develop a collaborative contrastive schema, which performs contrast between the node views obtained from the standard graph and hypergraph in each network layer, thus making the contrast collaborative. To be precise, in the first layer, the view from the standard graph is used to augment that from the hypergraph. Then, in the next layer, the augmented features are adopted to train a new representation to augment the view from the standard graph conversely. With this setting, the learning procedure is alternated between the standard graph and hypergraph. As a result, the learning on the standard graph and hypergraph is collaborative and leads to the final informative node representation. Experimental results on several widely used datasets validate the effectiveness of the proposed model.\nHighlights\n\u2022\nCCL is proposed to handle hypergraph node classification problem.\n\u2022\nCCL considers the contrast in each layer from GCN and HGCN.\n\u2022\nCCL exploits the convolutional networks on the standard graph and hypergraph.\n\u2022\nExperimental results demonstrate the superior performance of the proposed model.","The most common application of artificial immune networks (AINs) is on unsupervised learning tasks. This is due to the fact that AINs are inspired by the adaptive immune system, which consists of a network of antibodies that self-organises to form a memory of external antigens. The self-organising nature of AINs makes them a natural approach for solving problems involving learning and adapting to patterns or structures present in a dataset to form an abstract representation. Training AINs in this fashion means that the dataset need not have class labels because the typical aim of the learning process is not to perform classification. However, there have been attempts to use AINs for classification tasks by considering the resulting clusters of antibodies as representative of the classes present in a dataset. This has also been done when applying AINs to the task of recognising handwritten characters. However, in all the approaches found in the literature, the common method was to leave the task of discovering classes to the AINs. Doing so is contrary to how other models are trained to do classification tasks where data samples are provided along with their class labels to guide the learning process. Therefore, this paper presents a novel supervised learning approach to training AINs for multi-class classification. The proposed approach was tested on the MNIST handwritten digits dataset and achieved a classification accuracy of 99.45%.","Micro-expressions are rapid and subtle facial movements that can reflect the most real emotional state hidden in the human heart. Classifying different micro-expressions is still challenging because of their short duration and low intensity. This paper proposes new neural network models, Simplified SE-DenseNet-cc and SE-ResNet-cc, incorporating Eulerian video magnification (EVM) to enlarge micro-expression movements. Important features can be selectively enhanced, and unimportant features can be compressed using SE-block. The experimental results show that our proposed methods perform better than most of the algorithms in CASME-II and SMIC.","Highlights\n\u2022\nComprehensive wheat lodging analysis in terms of ratio, location, and angle was conducted.\n\u2022\nAuto crop plot dataset generation method was developed.\n\u2022\nImbalanced data challenge was addressed by changing the conventional loss function.\n\u2022\nEfficientNet-B7 produced exceptional performance in identifying and categorizing wheat lodging.\n\u2022\nLRCN gave a high performance for the classification of concatenated dataset.\nAbstract\nCrop lodging in agricultural fields is one of the major factors that limit cereal crop yields. Wheat, the most popular cereal crop in most countries, is also affected by this phenomenon, which may result in a significant decrease in both yield and quality. Therefore, addressing wheat lodging is crucial for producers. This study aims to detect and identify wheat lodging through aerial images and classify its severity based on ratio, angle and location of the lodging. To achieve this goal, a multi-task approach was proposed involving three phases. First, automatic dataset generation methodology was conducted on orthomosaic imagery of three dates. Next, a comprehensive assessment of wheat lodging (ratio, angle and location) was performed, which has received little research attention. Third, applying and improving selected classification models for classifying image datasets was conducted. Combining convolutional neural networks and temporal sequences in a single model provided an opportunity to use spatiotemporal information extracted from the wheat image datasets. Time dependency and individual dates were both considered in the classification task. The limited number of data and imbalanced classes challenges, resulting from real field conditions data collection, were overcome by applying a new loss function to the classifier models. The overall accuracy of wheat lodging classification reached over 91% in these two states using the proposed approach. Based on this research, wheat lodging was detected more accurately by the proposed models despite the small and imbalanced dataset. The developed methodology paves the way for comprehensive and automatic wheat lodging detection, and the methodology can be adapted for similar crops that suffer lodging issues with suitable modifications.","Breast cancer is a major cause of concern on a global scale due to its high incidence rate. It is one of the leading causes of death for women, if left untreated. Dynamic contrast-enhanced magnetic resonance imaging (DCE-MRI) is increasingly being used in the evaluation of breast cancer. Prior studies neglected to take into account breast cancer characteristics and features that might be helpful for distinguishing the four molecular subtypes of breast cancer. The use of breast DCE-MRI to identify the molecular subtypes is now the focus of research in breast cancer analysis. It offers breast cancer patients a better chance for an early and effective treatment plan. A manually annotated dataset of 1359 DCE-MRI images was used in this study, with 70% used for training and the remaining for testing. Twelve deep features were extracted from this dataset. The dataset was initially preprocessed through placing the ROIs by a radiologist experienced in breast MRI interpretation, then deep features are extracted using the proposed convolutional neural network (CNN). Finally, the deep features extracted are classified into molecular subtypes of breast cancer using the support vector machine (SVM). The effectiveness of the predictive model was assessed using accuracy and area under curve (AUC) measures. The test was performed on unseen held-out data. The maximum achieved accuracy and AUC were 99.78% and 100% respectively, with substantially a low complexity rate.","Handwritten character recognition is a significant image classification task. We present a model that is the first of it\u2019s kind as it is the first ever deep learning model designed to classify all basic and compound Bangla handwritten characters along with all Bangla numerals under the same filters. In this work, we propose a new architecture that can potentially ensure the network to learn sufficient number of filters with fewer parameters and time complexity. Furthermore, we also devised a technique to select a specific portion of the network that has almost the same learning capability as the entire network. Moreover, we demonstrated how this technique can enhance classification accuracy while making the neural network unbiased in terms of view point. Our proposed model is also a demonstration of managing variable amount of filters without adding the load on number of parameters required. Through the coarse of this work we came up with a image classifier that can classify all meaningful Bangla handwritten characters and numerals of different shapes. Consequently, we have achieved an accuracy rate of 97.21%. The paper provides conclusive results as well as adequate proof behind all the methodologies presented.","Highlights\n\u2022\nPresent an overview of ASC methods covering earlier works and recent advances.\n\u2022\nReview core techniques including data processing, feature acquisition, and modeling.\n\u2022\nSummarize available resources for ASC and analyze ASC tasks in DCASE challenges.\n\u2022\nDiscuss limitations and open challenges of current ASC algorithms.\n\u2022\nProvide suggestions for future research.\nAbstract\nAcoustic scene classification (ASC) has gained significant interest recently due to its diverse applications. Various audio signal processing and machine learning methods have been proposed for ASC. The volume and scope of ASC publications covering theories, algorithms, and applications have also been expanded. However, no recent comprehensive surveys exist to collect and organize the knowledge, impeding the ability of researchers and its applications. To fill this gap, we present an up-to-date overview of ASC methods, covering earlier works and recent advances. In this work, we first define a general framework for ASC, starting with a historical review of previous research in the ASC field. Then, we review core techniques for ASC that have achieved good performance. Focus on machine learning based ASC systems, this work summarizes and groups the existing techniques in terms of data processing, feature acquisition, and modeling. Furthermore, we provide a summary of the available resources for ASC research and analyze ASC tasks in Detection and Classification of Acoustic Scenes and Events (DCASE) challenges. Finally, we discuss limitations of the current ASC algorithms and open challenges to possible future developments toward practical applications of ASC systems.","Fault detection and classification is an important part of assessing the structural and system health status. The classification and detection of faults and faulty units is mostly done with statistical methods. After the data are measured and collected, the use of statistical software is necessary. Currently, many statistical software packages are being developed for the R programming language, as a result of R implementation being open source and free to use. This paper focuses on the rebmix R package, which concentrates on mixture model estimation. Mixture models, in particular Gaussian mixture models, are the main driver for many practical applications, such as clustering and classification. Hence, in this paper, we have expanded the rebmix for the estimation of the Gaussian mixtures. The results acquired on three different fault classification datasets were promising. Additionally, the process of obtaining those results is shown in detail, giving the researchers in the fault classification field useful resources for their research.\nHighlights\n\u2022\nREBMIX algorithm is derived for Gaussian mixture model estimation.\n\u2022\nMain methods and classes of the corresponding R package rebmix are described.\n\u2022\nThree different datasets for fault detection and classification are processed.\n\u2022\nThe R package rebmix has achieved better results than other popular R packages.","The burner combustion tuning is a complex problem that has been studied through flame monitoring and characterization. It has been observed that the flame electromagnetic spectrum and flickering contain specific flame information in combustion processes. This information is helpful for combustion stoichiometry tuning on burners. This paper described a method for selecting the best flame feature subset that can be computed from the scanner signal, in order to get the flame index and induce combustion stoichiometry on burners under specific combustion conditions. We propose a method for selecting a reduced subset with only the useful flame features for flame index classification. To extract the most relevant flame features we use a feature subset selection (FSS) algorithm and to determine the combustion state in burners, five flame indices were defined that represent the most common flame states in oil fuel-fired boilers. FSS includes complete, sequential, and random searches in order to eliminate redundant and noisy flame features to decrease the flame feature set dimension. A probabilistic neural network (PNN) algorithm was implemented for flame feature clustering. Signals from the actual flame scanner system and relevant variables from the boiler data acquisition system were used by the algorithms to calculate the burner flame index. A set of parametric tests was done in a heavy oil-fired boiler under well-known flame and index conditions to train and test the flame classifier. The results showed that only the four more relevant features are enough to classify flames with a good performance (92.3% accuracy), which is useful for burner combustion monitoring and optimization.\nHighlights\n\u2022\nA new approach to defining/selecting critical features in the combustion process.\n\u2022\nA new approach to classifying flames in industrial heavy oil-fired boilers.\n\u2022\nThe proposed algorithms are tested &amp; evaluated in a real industrial process.\n\u2022\nThe overall methodology ensures that the combustion process guarantees safety.","Fine-grained image classification is a challenging task due to the small inter-class variance, the large intra-class difference, and the small training data. Traditional methods typically rely on large-scale training samples with annotated part annotations, making them costly and severely limiting their application area. In this paper, we propose an effective and weakly supervised fine-grained classification framework. In this framework, a discriminative class-specific spectral feature is learned by intra-class spectral coupling and inter-class spectral decoupling under the weak supervision of image-level category labels, and then the new input images are classified based on the learned class-specific spectral feature. Different from existing strong supervised methods, the proposed technique creatively combines weak supervision of the image-level category labels with unsupervised spectral graph decomposition, not relying on large-scale training samples with dense part annotations, which are heavily labor-consuming. The performance of the proposed methods has been verified on four kinds of typical datasets: the JAFFE dataset, the Yale database, the UCI-CMU face database, and the neural foramina dataset. The satisfactory classification results have been achieved by the proposed method in expression recognition on the JAFFE dataset (with a mean accuracy of 95.31%), face recognition on the Yale database (with a mean accuracy of 98.79%), object recognition on the UCI-CMU face database (with a mean accuracy of 96.96%), and disease grading on the neural foramina dataset (with a mean accuracy of 92.09%). Compared with most state-of-the-art methods, the proposed method has superior classification performance in the small data set.\nHighlights\n\u2022\nFully mine and exploit the discriminative potentials of region correlations for fine-grained image classification in weak supervision.\n\u2022\nSpectral graph captures the internal region structure to ensure the image\u2019s comparison in a natural part-based fashion.\n\u2022\nIntra-class spectral synchronization aligns the spectral representations of with-class images to enhance intra-class similarity.","In this study, the authors combined the research on loose particle signal and component signal identification with the research on loose particle material identification for the first time, providing comprehensive and reliable loose particle detection results. Based on this, a signal detection and material identification method for loose particles inside sealed relays based on fusion classification model is proposed. Due to the limitations of technical means and confidentiality management, the authors made a real sealed relay sample, and took it as the research object. Through the steps of data acquisition, signal processing, feature engineering, and model training, the dedicated component identification feature library and material identification feature library was constructed, respectively, the component identification model and material identification model based on parameter-optimized SVM with linear kernel and XGBoost was trained, respectively. For the seal relay to be tested, through the steps of data acquisition, signal processing and feature engineering, the data set to be tested was created. The component identification model was used to identify component signals with loose particle signals, and the material identification model was used to identify the materials of loose particles. The majority voting process was used to convert the classification results into identification results, resulting in loose particle detection and material identification results. In addition, the general procedure steps of the proposed method for physical testing were given, and the identification accuracy for device-level loose particle detection was newly proposed. The loose particle testing event containing thirty-seven identification tasks shows that, the achieved identification accuracy was 97.30%, and 92.16% of the average classification accuracy was achieved by the component identification model, 80.41% of the average classification accuracy was achieved by the material identification model. This effectively demonstrates the feasibility and practicality of the proposed method in this paper. It is an important supplement to the loose particle detection research, and provides references for signal detection in similar fields.\nHighlights\n\u2022\nFirst combine the loose particle signal detection and material identification study.\n\u2022\nFirst construct the dedicated feature library and perform feature optimization.\n\u2022\nTrain the optimal component identification model and material identification model.\n\u2022\nNewly add majority voting steps to obtain the identification results.\n\u2022\nNewly propose the identification accuracy for device-level loose particle detection.","The aim of this research is to evaluate the performance of a classification model on nonlinear data. The study utilizes accuracy, precision, sensitivity, and specificity metrics based on data from e-commerce X sellers. The classification model is developed using the Support Vector Machine (SVM) approach, employing different kernel functions such as linear, polynomial, and Radial Basis Function (RBF). By comparing the performance scores of each model, the best model is determined. The results indicate that the SVM model with a linear kernel outperforms the others, demonstrating the highest performance scores. This approach is applied to predict the status of sellers on e-commerce X.","A progressive neurodegenerative disease affecting motor neurons, Amyotrophic Lateral Sclerosis (ALS) requires early diagnosis as quickly as possible. For such situations, surface electromyography (S-EMG) is widely used as a non-invasive diagnostic tool to measure muscles' activity through electrodes placed on the skin's surface. Artificial intelligence (AI) approaches can be employed to analyze captured signals and distinguish abnormal patterns. However, previous work focused primarily on spatial information. It does not consider temporal information, effectively capturing the dynamic nature of muscle activity and identifying subtle abnormalities that might indicate ALS. Therefore, we fill the gap in this study by proposing a combination of CNN, Long-Short-Term Memory Networks (LSTM), and attention mechanisms to exploit temporal information in EMG signals. Stability assessment using K-fold cross-validation ensures reliable model performance. Our results demonstrate that combining spatial and temporal information can enhance performance and acquire 98.15% and 98.45% for CNN and LSTM, and CNN, LSTM, and Attention combination. In addition, our proposed model remains stable compared to previous work.","As the importance of machine learning tools for decision support continues to grow, interpretability has emerged as a key factor. Rule-based classification algorithms, such as decision trees and rule induction, enable high local interpretability by providing transparent reasoning rules in an IF-THEN format. In this context, it is essential to provide concise and clear rules and conditions to achieve high local interpretability. This study proposes a novel Concise Algorithm, designed to effectively remove irrelevant conditions from classification rules. We present a framework incorporating the Concise Algorithm, which employs the One-Sided-Maximum decision tree algorithm for rule generation, followed by the application of the Concise Algorithm to remove irrelevant conditions. This proposed framework produces a rule-based classification model that exhibits an enhanced predictive performance-interpretability trade-off compared to benchmark methods (CART, Ripper, CN2, and modified One-Sided-Maximum), as demonstrated by empirical tests conducted on 19 UCI datasets. A case study focusing on the breast-cancer-wisconsin dataset provides a comprehensive analysis of the rule and condition generation processes.","Multi-label learning has attracted a great deal of research interests as it has a wide range of real-world applications. Although many multi-label learning methods have been proposed, very few of them have addressed the problem of class imbalance distribution in multi-label data. Moreover, most of the existing class imbalance multi-label learning algorithms only focus on solving the class imbalance problem, without taking into account the correlations among labels. To address these issues simultaneously, we propose to combine the well-known ensemble of classifier chain (ECC) algorithm with various binary-class imbalance learning techniques such as sampling, cost-sensitive learning, and threshold moving. This approach creates a new algorithm family called ECC++, designed specifically for class imbalance multi-label learning. ECC is already an excellent ensemble high-order binary relevance multi-label learning algorithm that is well-suited to exploiting correlations among labels. Combining it with binary-class imbalance learning techniques enables each link in a classifier chain (CC) to overcome the negative effect of skewed data distribution. ECC++ is a dynamic algorithm family that can be extended arbitrarily by applying any new binary-class imbalance learning techniques. To demonstrate the effectiveness and superiority of the proposed ECC++ algorithm family, we developed several ECC++ family members using some popular binary-class imbalance learning techniques. We then compared them with several state-of-the-art class imbalance multi-label learning algorithms on twelve benchmark and four real-world multi-label datasets. Our experimental results showed the effectiveness and superiority of the proposed ECC++ algorithm family over existing class imbalance multi-label learning algorithms. In conclusion, the proposed ECC++ algorithm family combines the strengths of the well-established ECC algorithm and binary-class imbalance learning techniques, resulting in a superior methodology for class imbalance multi-label learning.","Predicting the secondary structure of proteins is a challenging task. A large variety approaches exist that include observation using equipment\u2019s and theoretical evaluation, in which the optimal structure is determined. The secondary structure determines 3D tertiary structure of protein, on which features and functionalities of protein depend. This paper use classification technique, Random Forest to build a model which is able to determine structure of unknown proteins. The dataset included the amide frequencies of proteins whose structure is known. Machine learning model is developed that can predict the structure of protein that still need to be exploited. The accuracy of the model is determined using ROC curve. The results confirm the performance of the model constructed using amides dataset.","Traditional SVM algorithms for multi-class (k &gt; 2 classes) classification tasks include \u201cone-against-one\u201d, \u201cone-against-rest\u201d, and \u201cone-against-one-against-rest\u201d, which build k(k\u22121)/2 or k classifiers for space partitioning and classification decision. However, they may cause a variety of problems, such as an imbalanced problem, a high temporal complexity, and trouble establishing the decision boundary. In this study, we use the notion of minimizing structural risks (SRM) to recognize k classes by designing only one optimization problem, which we call M3HS-SVM. The M3HS-SVM offers numerous benefits. In summary, the following points should be emphasized: (1) Rather than dividing the space with hyper-planes, M3HS-SVM describes the structural characteristics of various classes of data and trains the hyper-sphere classifier of each class based on the data distribution. (2) M3HS-SVM inherits all of the advantages of classical binary SVM, such as the maximization spirit, the use of kernel techniques to solve nonlinear separable problems, and excellent generalization ability. (3) In the dual problem, we develop an SMO algorithm to effectively reduce the complexity of time and space. We eventually validate the preceding statement with comprehensive experiments. The experiment findings show that our method outperforms other mainstream methods in terms of computing time and classification performance on synthetic datasets, UCI datasets, and NDC datasets.","Imbalanced data distribution is a common feature in real-world datasets. For imbalanced data, the imbalanced characteristics of the classes have two negative effects on classification results, one of which is that the minority class is highly likely masked by the majority class so as to weaken the ability of the classifiers to identify the minority class. Another effect is that irrelevant attributes hidden in imbalanced data can create much noise to interfere the classifiers, thereby leading to that the classifiers could mistakenly treat noise as the minority classes. In this scenario, the performance of the classifiers is rapidly declined and the classifiers obtain incorrect classification results. To address this issue, this paper proposed a conformal transformation twin-hypersphere with fuzzy. The critical thought is that using conformal transformation to explore the regions containing minority classes, by so doing, minority classes can be more likely to be noticed by the classifier. Using the proposed fuzzy function assesses the contributions of points to the hypersphere training, through evaluating the contributions, noise can be determined, thereby increasing the ability of the classifier to noise resistance. Results on the synthetic and real datasets show that the proposed method outperforms the competitors in classification accuracy and noise resistance. Results also imply that the proposed method does not exhibit exponential calculation time, meaning that the method is suitable for the classification of large-scale imbalanced datasets. We demonstrate that conformal transformation can assist those non-linear kernels to find those hard-to-observe regions containing minority classes, thereby strengthening the adaptability of the classifiers to imbalanced data following complex distributions. Moreover, the non-linear kernels using conformal transformation can adapt to the situation where different sub-regions in sample space require different nonlinearities.","In multi-label classification, the expansion of output dimension seriously interferes learning performance, and even fails to build a joint prediction model. In order to restrain the proliferation of multi-label classifier\u2019s hypothesis space, the current works focus on the application of global positive label correlation. However, the \u201cblack or white\u201d mechanism ignore other possible forms of label correlation, such as negative or neutral correlation. By introducing the doctrine of the mean, three-way decision (3WD) theory provides a solution for in-depth research on local label correlation, and aims to handle the uncertainty of multi-label learning tasks. In this paper, a novel learning algorithm for multi-label joint classification, namely ML-3WD, is proposed by considering the 3WD label correlation from the perspective of samples. According to the weights of different features on any label, the comprehensive loss of each sample to three action strategies can be measured. Obviously, the 3WD rules for any label variable in multi-label output space is obtained. By aggregating the cutting thresholds between different labels, the division principles of 3WD label correlation are further established. Given any multi-label sample, the local fuzzy membership to co-occurrence or mutual state for label pair is examined based on kernelized fuzzy rough sets. The 3WD local label relevance of each sample is confirmed, that is, positive, negative or neutral. The global application strategy for multi-label classification is utilized to avoid over-fitting induced by local mining strategy. Based on the integral mean of the distribution of 3WD local label relevance in multi-label sample space, two different versions of empirical label relevance are constructed. By constraining the relative position between sub-separation hyperplanes, the 3WD label correlation distribution-based model for multi-label joint classification is designed. The experiment results on fifteen real world multi-label datasets reflect that our algorithm achieves good classification ability and versatility. The impact of core parameters on learning performance is also dissected.\nHighlights\n\u2022\nThe doctrine of the mean in three-way decision theory, which accords with human behavior cognition, inspires us to enrich the \u201dblack or white\u201d mining mechanism on label correlation. By adding buffer processing to the determined correlation, three possible forms for label correlation are first considered, they are positive, negative or neutral. A multi-label classification algorithm is designed according to 3WD label correlation from the perspective of samples, where global empirical label relevance is explicitly applied to restrict the sub-separation hyperplanes of different labels.","Counterfactually-Augmented Data (CAD) \u2013 minimal editing of sentences to flip the corresponding labels \u2013 has the potential to improve the Out-Of-Distribution (OOD) generalization capability of language models, as CAD induces language models to exploit domain-independent causal features and exclude spurious correlations. However, the empirical results of CAD\u2019s OOD generalization are not as efficient as anticipated.In this study, we attribute the inefficiency to the myopia phenomenon caused by CAD: language models only focus on causal features that are edited in the augmentation operation and exclude other non-edited causal features. Therefore, the potential of CAD is not fully exploited. To address this issue, we analyze the myopia phenomenon in feature space from the perspective of Fisher\u2019s Linear Discriminant, then we introduce two additional constraints based on CAD\u2019s structural properties (dataset-level and sentence-level) to help language models extract more complete causal features in CAD, thereby mitigating the myopia phenomenon and improving OOD generalization capability. We evaluate our method on two tasks: Sentiment Analysis and Natural Language Inference, and the experimental results demonstrate that our method could unlock the potential of CAD and improve the OOD generalization performance of language models by 1.0% to 5.9%.\nHighlights\n\u2022\nExclusion of non-edited causal features causes CAD inefficiency in OOD generalization.\n\u2022\nThis inefficiency is analyzed in feature space by Fisher\u2019s linear discriminant.\n\u2022\nTwo constraints based on CAD structural properties help to extract causal features.\n\u2022\nCAD\u2019s potential for OOD generalization is unlocked.","In this paper, a novel knowledge distillation (KD)-based pedestrian attribute recognition (PAR) model is developed, where a multi-label mixed feature learning network (MMFL-Net) is designed and adopted as the student model. In particular, by applying the grouped depth-wise separable convolution, re-parameterization and coordinate attention mechanism, not only the multi-scale receptive field information is sufficiently fused and spatially dependent robust features are extracted, the model complexity is also effectively kept acceptable. To alleviate the imbalance of category samples, an attribute weight parameter is proposed and considered when calculating the multi-label loss. Moreover, the Jensen\u2013Shannon (JS) divergence-based KD scheme can facilitate the learning of MMFL-Net from the teacher model, which benefits strong fitting ability of the deep feature correlations so as to realize a highly generalized model. The proposed KD-PAR is comprehensively evaluated through many of experiments, and experimental results show the effectiveness and superiority of the proposed model as compared with other advanced MLL-based methods and state-of-the-art PAR models, which efficiently achieves the balance between accuracy and complexity. When facing the complex scenes such as blurry background, similar object interference, and target occlusion, the proposed KD-PAR can even present satisfactory recognition results with strong robustness, thereby providing a feasible and practical solution to the PAR tasks.\nHighlights\n\u2022\nThe proposed KD-PAR can achieve multi-label mixed feature learning for PAR tasks.\n\u2022\nThe JS-based divergence KD scheme is beneficial for learning generalized features.\n\u2022\nAttribute weight can alleviate performance degradation caused by imbalanced samples.\n\u2022\nStructural re-parameterization refines feature presentation with less complexity.","Machine learning applications in remote sensing often require a labour-intensive feature engineering step, if only a small number of samples is available and transfer learning is not applicable. Here, we are introducing the concept of Spatial Variation Sequences, which allows to apply methodologies from automated time-series feature engineering to remote sensing applications of static images. The presented example application detects swimming pools from four-channel satellite images with an\nF\n1\n-score of 0.95, by generating spatial variation sequences from a modified swimming pool index. The automated feature engineering approach reduced the dimensionality of the classification problem by 99.7%. A more traditional approach using transfer learning on pre-trained Convolutional Neural Networks (CNN) was evaluated in parallel for comparison. The CNN approach boasted a higher performance of\nF\n1\n-score of 0.98 but required the use of pre-trained weights. The comparable performance of the FE and CNN approach demonstrates that time-series feature extraction is a valuable alternative to traditional remote sensing methods in the presence of data scarcity or the need of significant dimensionality reduction.","The prevalence of skin cancer, specifically melanoma, constitutes a significant global health concern, thus giving rise to intricate detection challenges that demand immediate attention and comprehensive solutions. In this study, we investigate the application of deep learning models for melanoma detection. Five pre-trained models, including VGG-16, ResNet50, InceptionV3, DenseNet-121, and Xception, are evaluated through a series of experiments. The models undergo the same training process with transfer learning, freezing all layers and modifying the classification layer. The experiments reveal that ResNet50 consistently outperforms the other models, demonstrating superior accuracy, precision, recall, and F1 score. Notably, ResNet50 exhibits exceptional accuracy and F1 score, achieving around 93% in both. This study sheds light on the potential use of deep learning in enhancing melanoma diagnosis and underscores the need for robust and accurate classification systems for early detection and effective treatment of skin cancer.","In microseismic monitoring, various types of vibration events are often collected. Realizing the automatic identification of microseismic events in many suspected events is the basis of monitoring timeliness. However, due to the different sampling methods of microseismic data provided by different products, the data often contains different waveform sizes and sampling frequencies. This makes it difficult for existing approaches to be widely used in different projects without data preprocessing. In this paper, we propose the Universal Automatic Classification Network (UACNet), a deep learning approach that automatically identifies microseismic data in engineering without preprocessing. The UACNet model includes multiple convolution layers, adaptive average pooling layers, fully connected layers, and UAC blocks. UAC block is a residual structure with multiple convolutional layers and reset and update gates. The adaptive average pooling layer unifies the input size, and the UAC block functions as a feature extraction network to mine sufficient features from data. We test the proposed UACNet on engineering data and compare it with existing common and advanced methods. As a result, UACNet passed the ablation study, and the classification accuracy of UACNet is 95.62%, which is higher than 89.14% of CNN, 91.24% of ResNet, 91.04% of CapsNet, and 86.16% of RTFN, respectively. Moreover, the influence of waveform size, sampling rate, signal-to-noise ratios, and amplitude on the accuracy of UACNet is analyzed. The results show that UACNet can overcome the influence of these factors and truly realize automatic real-time classification of microseismic signals without preprocessing.","Due to the enormous amount of user-generated content being generated on the web, labeling such data is a time-consuming and expensive endeavor. As a result, we have limited annotated data and the vast majority of data are unlabeled. Analysis reveals that extracting (external) knowledge from unlabeled data and integrating it with knowledge extracted from labeled data is a beneficial task for text information processing, in particular text classification. In this paper, we present a hybrid approach for classifying sentiments that employs external knowledge, which is categorized as either general-purpose sentiment knowledge or domain-related knowledge. General-purpose sentiment knowledge is extracted from sentiment lexicons, whereas domain-related knowledge is extracted from unlabeled data from the same or related domains. Similar domains for a given domain are identified based on their similarity score in terms of overlapping features. The proposed approach utilizes both forms of external knowledge and combines them with logistic regression to train an improved classification model. The classification model uses the conventional gradient descent algorithm for optimization, and its convergence analysis indicates that it is convex and converges to the global optimum. The proposed approach is empirically evaluated and compared to three baselines and one state-of-the-art method using standard performance evaluation metrics on a multi-domain sentiment dataset. The experiment results are encouraging, demonstrating that the proposed approach considerably outperforms the baseline approaches and outperforms the state-of-the-art approach by up to 2% in terms of both f-score and accuracy.\nHighlights\n\u2022\nA hybrid approach for document-level sentiment classification that makes use of external knowledge.\n\u2022\nA domain-specific knowledge extraction approach from unlabeled text documents.\n\u2022\nA detailed convergence analysis of the proposed optimization model used in the proposed approach.","Breast cancer (BC) is the most common cancer amongst women that threatens the health of women, initial diagnosis of BC becomes essential. Though there were several means to diagnose BC, the standard way is pathological analysis. Precise diagnosis of BC necessitates experienced histopathologists and needs more effort and time for completing this task. Recently, machine learning (ML) was successfully implemented in text classification, image recognition, and object recognition. With the emergence of computer aided diagnoses (CAD) technology, ML was effectively implemented for BC diagnosis. Histopathological image classification depends on deep learning (DL), particularly convolution neural network (CNN), which frequently needs a large amount of labelled training models, whereas the labelled data was hard to obtain. This study develops an Aquila Optimizer(AO) with Hybrid ResNet-DenseNet Enabled Breast Cancer Classification on Histopathological Images (AOHRD-BC2HI). The proposed AOHRD-BC2HI technique inspects the histopathological images for the diagnosis of breast cancer. To accomplish this, the presented AOHRD-BC2HI technique uses hybridization of Resnet with Densenet (HRD) model for feature extraction. Moreover, the HRD method can be enforced for feature extracting procedure in which the DenseNet (feature value memory by concatenation) and ResNet (refinement of feature value by addition) were interpreted. For BC detection and classification, the DSAE model is utilized. The AO algorithm is exploited to improve the detection performance of DSAE model. The experimental validation of the presented AOHRD-BC2HI approach is tested using benchmark dataset and the results are investigated under distinct measures.Also the proposed model achieved the accuracy of 96%. The comparative result reports the improved performance of the presented AOHRD-BC2HI technique over other recent methods.","Text classification first needs to convert the text into embedding vectors. Considering that static word embedding models such as Word2vec do not consider the position information of word and the difference of its role in different documents, while dynamic word embedding models such as Bert consume a large amount of time. An improved word embedding model based on pre-trained Word2vec is proposed, which achieves better classification accuracy and much lower classification time than Bert. At first, the concept of Term Document Frequency (TDF) is proposed on the basis of TF-IDF, and the TF-IDF-TDF of each word in different documents is calculated. Then, The positional encoding is added. Finally, in order to reduce the misleading of words with low importance, a filter is designed to set the embedding vector with low importance to zero. Considering that the sequence length that the deep learning model can handle is limited, and the text sequence exceeding the Maximum Sequence Length (MSL) set by the deep learning model will be directly truncated and discarded, an adaptive segmentation model is proposed, which can set different segmentation strategies for different texts according to the length of the text and the MSL. In order to maintain the continuity of adjacent text after segmentation, an adjacent-segment-vector-attended co-attention network is designed. In addition, the multi-channel convolution and the capsule network are designed to further extract deep hidden features. Multiple comparative experiment results show that the proposed model achieves the best Accuracy and Micro-F1 on five long text baseline datasets and six short text baseline datasets. In addition, when the MSL is not set too large compared with the document length in the dataset, the classification results of the proposed model are not affected by it.\nHighlights\n\u2022\nAn pre-trained Word2vec based embedding model is proposed.\n\u2022\nAn adaptive segmented text classification model is proposed.\n\u2022\nAn adjacent-segment-vector-attended co-attention network is designed.\n\u2022\nThe multi-channel convolution and capsule network are used to extract deep hidden features.","Weighting strategy prevails in machine learning. For example, a common approach in robust machine learning is to exert low weights on samples which are likely to be noisy or quite hard. This study summarizes another less-explored strategy, namely, perturbation. Various incarnations of perturbation have been utilized but it has not been explicitly revealed. Learning with perturbation is called perturbation learning and a systematic taxonomy is constructed for it in this study. In our taxonomy, learning with perturbation is divided on the basis of the perturbation targets, directions, inference manners, and granularity levels. Many existing learning algorithms including some classical ones can be understood with the constructed taxonomy. Alternatively, these algorithms share the same component, namely, perturbation in their procedures. Furthermore, a family of new learning algorithms can be obtained by varying existing learning algorithms with our taxonomy. Specifically, three concrete new learning algorithms are proposed for robust machine learning. Extensive experiments on image classification and text sentiment analysis verify the effectiveness of the three new algorithms. Learning with perturbation can also be used in other various learning scenarios, such as imbalanced learning, clustering, regression, and so on.","This paper describes the classification of facial expressions using EEG data. The entire procedure aims at controlling an electric wheelchair with a Brain Computer Interface (BCI) headset. The goal is to help the people who are suffering from locked-in syndrome to move or to pass the necessary signals. The headset consists of the electroencephalogram (EEG) cap comprising 16 electrodes attached to the amplifier out of which 14 electrodes are used for data acquisition while two are used as reference and ground. The EEG cap is placed on the head of the subject and various expressions (blink, eyebrows raise, smile, etc.) are performed on the subject. Muscle activities due to facial expressions can be observed from the recorded EEG signals. Expressions are classified and necessary signals are generated. The features are extracted using the wavelet packet transform processing method and classified primarily using Support Vector Machine (SVM).","The feature selection problem has become a key undertaking within machine learning. For classification problems, it is known to reduce the computational complexity of parameter estimation, but it also adds an important contribution to the explainability aspects of the results. An evolution strategy for feature selection is proposed in this paper. Feature weights are evolved with decision trees that use the Nash equilibrium concept to split node data. Trees are maintained until the variation in probabilities induced by feature weights stagnates. Predictions are made based on the information provided by all the trees. Numerical experiments illustrate the performance of the approach compared to other classification methods.","Latent Dirichlet allocation model (LDA) has been widely used in topic modeling. Recent works have shown the effectiveness of integrating neural network mechanisms with this generative model for learning text representation. However, one of the significant setbacks of LDA is that it is based on a Dirichlet prior that has a restrictive covariance structure. All its variables are considered to be negatively correlated, which makes the model restrictive. In a practical sense, topics can be positively or negatively correlated. To address this problem, we proposed a generalized Dirichlet variational autoencoder (GD-VAE) for topic modeling. The Generalized Dirichlet (GD) distribution has a more general covariance structure than the Dirichlet distribution because it takes into account both positively and negatively correlated topics in the corpus. Our proposed model leverages rejection sampling variational inference using a reparameterization trick for effective training. GD-VAE compares favorably to recent works on topic models on several benchmark corpora. Experiments show that accounting for topics\u2019 positive and negative correlations results in better performance. We further validate the superiority of our proposed framework on two image data sets. GD-VAE demonstrates its significance as an integral part of a classification architecture. For reproducibility and further research purposes, code for this work can be found at https://github.com/hormone03/GD-VAE.\nHighlights\n\u2022\nWe propose GD-VAE to capture correlations and learn complex distributions.\n\u2022\nWe show that capturing all correlations leads to improved performance in GD-VAE.\n\u2022\nWe address training instability by introducing a weighted objective function.\n\u2022\nThe comprehensive experiments show that GD-VAE outperformed state-of-the-art models.\n\u2022\nWe demonstrate the effectiveness of GD-VAE on data augmentation with image data sets.","The pre-trained model based on the Transformer architecture is currently the most widely used model in the field of Natural Language Processing (NLP), and feature fusion technology is the process of aggregating features from different sources to form an augmented feature representation that contains more information. In multi-modal or multi-branch NLP models, feature fusion is a commonly used technique, but for models with only a single feature source, feature fusion technology can be difficult to apply. Therefore, this paper proposes a new probabilistic-controlled late fusion encoder-decoder architecture, called the Feature Fusion Gate (FFG), based on both feature fusion technology and Mixup technology to aggregate the feature representations from the last two layers of the NLP pre-trained model to better capture semantic information in samples. During the aggregation process, FFG utilizes controlled noise as a regularization technique to help the model achieve better generalization performance. Experimental results on eight NLP benchmark datasets show that FFG outperforms three other baseline methods and consistently achieves significant performance improvements across DistilBERT, BERT and RoBERTa.","Sentiment analysis is a technique that analyzes the attitudes and emotions of people towards some product, service etc. Sentiment analysis of some product or service can be beneficial in predicting future scope of it. However, manually analyzing a large number of documents in a limited time can be a tedious and challenging task. Hence, several attempts have been made in the literature to solve this problem and several sentiment analysis techniques have been proposed. However, these approaches do not consider or do not give much weighted to\u2018emoticons\u2019 present in the sentence. Emotions are very popular these days and have become an integral part of written communication. Hence, in this paper, we propose a novel algorithm, based on \u2018emoticon score learning\u2019 for identifying sentiment of a given sentence. We test the proposed algorithm on 1000 tweets. Experimental results show that the proposed algorithm is effective in sentiment classification and give accuracy of 91.1%. Additionally, the proposed algorithm is able to detect sentences consisting of both positive and negative sentiments.","Accurate transformer fault diagnosis is crucial for maintaining the power system stability. Due the complex operation condition of the transformer, its faults are with the characteristic of multi-class faults, class-imbalance, and limited diagnosis data of availability. Additionally, some fault samples are only with overheating or discharge labels when collected, it is a challenge that how to how to use these samples. To address these issues, in this paper, a novel transformer fault diagnosis method based on a hybrid model of Res-Variational-Auto-Encoder (ResVAE) and ensemble learning (EL) model is proposed. Through a self-strengthening strategy, fault characteristics are extracted category-by-category by using a residual convolutional neural network, and low dimensional characteristics are mapped into characteristic fusion samples by VAE. Based on this strategy, an offline pre-training model is built based on ResVAE and EL. The hybrid model can obtain more information from offline source domain, enabling the EL to diagnose multiple fault types as well as undetermined faults. Considering 11 categories of imbalanced classification scenarios with limited sample sizes, the comparison is made between eight expansion and six diagnosis algorithms. The results show that the offline pre-training EL model increased the diagnostic accuracy up to 11.224% compared with tradition ratios method. The ResVAE-EL model achieves the highest diagnostic accuracy of 91.011%, which is 10.112% higher than that of the single offline pre-training model.","The aim of ordinal classification is to predict the ordered labels of the output from a set of observed inputs. Interval-valued data refers to data in the form of intervals. For the first time, interval-valued data and interval-valued functional data are considered as inputs in an ordinal classification problem. Six ordinal classifiers for interval data and interval-valued functional data are proposed. Three of them are parametric, one of them is based on ordinal binary decompositions and the other two are based on ordered logistic regression. The other three methods are based on the use of distances between interval data and kernels on interval data. One of the methods uses the weighted k-nearest-neighbor technique for ordinal classification. Another method considers kernel principal component analysis plus an ordinal classifier. And the sixth method, which is the method that performs best, uses a kernel-induced ordinal random forest. They are compared with na\u00efve approaches in an extensive experimental study with synthetic and original real data sets, about human global development, and weather data. The results show that considering ordering and interval-valued information improves the accuracy. The source code and data sets are available at https://github.com/aleixalcacer/OCFIVD.\nHighlights\n\u2022\nSix ordinal classifiers for interval data-valued data are proposed.\n\u2022\nOrdinal methods for interval-valued functional data are also proposed.\n\u2022\nConsidering ordering and interval-valued information improves the accuracy.\n\u2022\nA kernel-induced ordinal random forest performs best.","Railway switches are critical components in the rail system. Operation and maintenance tasks are essential to ensure proper functioning and avoid any failure that can cause delays, reducing operational safety. Data from condition monitoring systems requires advanced analysis tools. This paper presents the analysis of power output data of railway switches. A novel approach is proposed based on statistical analysis techniques combined with Machine Learning techniques to classify power curves by analyzing different sections of the power curves. These curves are studied statistically to classify them into normal and non-normal curves. Then, a dataset is generated with normal and non-normal labelled curves. Shapelets and k-Nearest Neighbour classification algorithms are applied to these data with good results (accuracy, sensitivity and specificity above 88% in each case). As a further analysis, a second dataset with the sectioned curves is done to detect non-normal curves without analyzing the complete curve. For this case study, k-Nearest Neighbour algorithm is able to classify with higher accuracy on the last section of the curve.","Uploading research articles to a database can be a complex process with multiple HTML fields. The complexity of this process makes users less efficient and productive. This study explored the use of the Multilayer Perceptron (MLP) algorithm using language-independent features (count and Boolean features) to create a multi-class text classifier that can classify 10 research article components (e.g., title, authors, abstract, etc.). The text classifier was developed to pave the way for a streamlined manual data entry process for uploading research articles to a database by implementing a single textarea in favor of using multiple HTML fields. The text samples were obtained from multiple sources using web scraping technology, consolidated, cleaned, and standardized. The 12 language-independent features were generated based on the textual formats of sample texts (e.g., count of capitalized letters, digits, punctuation, checking the existence of a URL pattern, etc.). Recursive feature elimination with cross validation (RFECV) was used to determine the optimal number of input features. The hyperparameter values of the model were determined through the grid search technique. A trial-and-error process was conducted to determine the number of hidden layers. The model achieved 95%, 94%, and 95% scores for micro, macro, and weighted average f1-scores, respectively. The model performed well in classifying research article components. However, it is sensitive to textual format (e.g., lower or upper case, punctuation used, etc.). For subsequent research in this area, this study recommends investigating the use of both language-dependent and language-independent features to address the limitations of the current model.","Few-Shot Class-Incremental Learning (FSCIL) is to learn novel classes with few data points incrementally, without forgetting old classes. It is very hard to capture the underlying patterns and traits of the few-shot classes. To meet the challenges, we propose a Self-supervised Contrastive Feature Refinement (SCFR) framework which tackles the FSCIL issue from three aspects. Firstly, we employ a self-supervised learning framework to make the network to learn richer representations and promote feature refinement. Meanwhile, we design virtual classes to improve the models robustness and generalization during training process. To prevent catastrophic forgetting, we attach Gaussian Noise to encountered prototypes to recall the distribution of known classes and maintain stability in the embedding space. SCFR offers a systematic solution which can effectively mitigate the issues of catastrophic forgetting and over-fitting. Experiments on widely recognized datasets, including CUB200, miniImageNet and CIFAR100, show remarkable performance than other mainstream works.","Real-world data usually obeys a long-tailed distribution, where a few classes have higher number of samples compared to the other classes. Recent studies have been proposed to alleviate the extreme data imbalance from different perspectives. In this paper, we experimentally find that due to the easily confusing visual features between some head- and tail classes, the cross-entropy model is prone to misclassify tail samples to similar head classes. Therefore, to alleviate the influence of the confusion on model performance and improve the classification of tail classes, we propose a Similarity Window Reweighting and Margin (SWRM) algorithm, where the SWRM consists of Similarity Window Reweighting (SWR) and Similarity Window Margin (SWM) algorithms. For the confusable head- and tail classes, SWR assigns larger weights to tail classes and smaller weights to head classes. Therefore, the model can enlarge the importance of tail classes and effectively improve their classification. Moreover, SWR considers the difference in label frequency and the impact of category similarity simultaneously, so that the weight coefficients are more reasonable and efficacious. SWM generates adaptive margins that are proportional to the ratio of the classifier\u2019s weight norm, thus promoting the learning of tail classifier with small weight norm. Our SWRM effectively eliminates the confusion between head- and tail classes and alleviates the misclassification issues. Extensive experiments on three long-tailed datasets, i.e., CIFAR100-LT, ImageNet-LT and Places-LT, verify our proposed method\u2019s effectiveness and superiority over comparative methods.","The metric-based learning framework has been widely used in data-scarce few-shot visual classification. However, the current loss function limits the effectiveness of metric learning. One issue is that the nearest neighbor classification technique used greatly narrows the value range of similarity between the query and class prototypes, which limits the guiding ability of the loss function. The other issue is that the episode-based training setting randomizes the class combination in each iteration, which reduces the perception of the traditional softmax losses for effective learning from episodes with various data distributions.To solve these problems, we first review some variants of the softmax loss from a unified perspective, and then propose a novel Dynamically-Scaled Softmax Loss (DSSL). By adding a probability regulator (for scaling probabilities) and a loss regulator (for scaling losses), the loss function can adaptively adjust the prediction distribution and the training weights of the samples, which forces the model to focus on more informative samples. Finally, we found the proposed DSSL strategy for few-shot classifiers can achieve competitive results on four generic benchmarks and a fine-grained benchmark, demonstrating the effectiveness of improving the distinguishability (for base classes) and generalizability (for novel classes) of the learned feature space.\nHighlights\n\u2022\nThe deep connections between some known losses were established and revealed.\n\u2022\nThe new framework are suitable for small sample scenarios with intuitive motivation.\n\u2022\nDetailed implementation was conducted on sufficient datasets with good result.","Under real-world conditions, faulty samples of key components (e.g., bearings and cutting tools, etc.) are typically limited and sparse. Additionally, their historical data is characterized by time-series and imbalance characteristics. In other words, the training samples are not only limited and noisy, but also exhibit both within-class and between-class imbalance. These factors present significant challenges in the realm of fault monitoring modeling. To tackle these challenges, this paper presents an innovative fault diagnosis method rooted in the extended NI-MWMOTE and LS-SVM. NI-MWMOTE stands as an advanced noise-immunity majority weighted minority oversampling technique, originally introduced in our prior research, and it has exhibited exceptional competitiveness in noisy imbalanced benchmark datasets. It champions an adaptive noise processing strategy leveraging the distribution characteristics of noisy imbalanced data and the essence of machine learning. Specifically, it employs Euclidean distance and neighbor density to differentiate between spurious noise and true noise, and it determines the optimal processing strategy based on misclassification error and iteration. Furthermore, it employs unsupervised aggregative hierarchical clustering, misclassification error, and majority-weighted minority oversampling in a collaborative manner to address both within-class and between-class imbalanced problems. The primary contribution of our paper lies in the context of the monitoring scenario mentioned above. We have expanded the hyper-parameter range of NI-MWMOTE, corrected and optimized its built-in noise function to enhance the interpretability of the model, and successfully applied it in conjunction with LS-SVM to this particular setting. Notably, this marks the pioneering endeavor within our established knowledge sphere into the domain of tool wear state monitoring. The results suggest that, when compared to 11 well-known algorithms, our framework demonstrates significant competitiveness in real-world scenarios characterized under data-limited and noise-imbalanced scenarios for bearings and cutting tools fault diagnosis. This establishes a solid theoretical and practical foundation for similar scenarios.","Nowadays, many classification algorithms have been applied to various industries to help them work out their problems met in real-life scenarios. However, in many binary classification tasks, samples in the minority class only make up a small part of all instances, which leads to the datasets we get usually suffer from high imbalance ratio. Existing models sometimes treat minority classes as noise or ignore them as outliers encountering data skewing. In order to solve this problem, we propose a bagging ensemble learning framework A S E (Anomaly Scoring Based Ensemble Learning). This framework has a scoring system based on anomaly detection algorithms which can guide the resampling strategy by divided samples in the majority class into subspaces. Then specific number of instances will be under-sampled from each subspace to construct subsets by combining with the minority class. And we calculate the weights of base classifiers trained by the subsets according to the classification result of the anomaly detection model and the statistics of the subspaces. Experiments have been conducted which show that our ensemble learning model can dramatically improve the performance of base classifiers and is more efficient than other existing methods under a wide range of imbalance ratio, data scale and data dimension. A S E can be combined with various classifiers and every part of our framework has been proved to be reasonable and necessary.\nHighlights\n\u2022\nIntroduce a scoring system based on anomaly detection to the resampling strategy.\n\u2022\nThe proposed weighting functions are intuitive and easy to understand.\n\u2022\nPropose an efficient ensemble learning framework.\n\u2022\nExcellent performance on different real-world imbalanced classification tasks.","Braille character recognition(BCR) is a basic step in building and designing any Braille assistive technology. Each Braille character is represented by a 2 \u00d7 3 matrix of raised dots (called a cell), which can be read by touch. This study introduces a generalized recognition approach based on an ensemble of transfer learning models for BCR. The study experiments are performed on two benchmark English Braille datasets (handwritten Braille \u2013 Omniglot (HBO), and Braille character (BC)), and a new dataset of Arabic Braille characters collected by our group called Arabic Braille (AB). First, we investigate the performance of 17- transfer learning models on the three datasets. Then, we build three ensemble approaches based on majority voting from the most effective two, three, and four models in each dataset. The experimental results reveal that the ensemble of DarkNet-53, GoogleNet, SqueezeNet, and DenseNet-201 is a more generalizable ensemble approach for BCR. It achieves a higher F1 score and lesser generalization error ( E t e s t ) value than each individual transfer learning model. The F1 scores of the introduced ensemble reached 89.42%, 99.58%, and 97.11% on the HBO, BC, and AB datasets, respectively, with E t e s t values of 10.47%, 0.43%, and 3.23%. While the F1 scores of the DarkNet-53 which is the most effective single model on the three datasets are 87.54%, 99.14%, and 94.73, with E t e s t values of 12.79%, 0.85%, and 5.31%, respectively.","Examples from living systems at various levels of the biological hierarchy and also from natural food products show that ultra-weak photon emission (UPE) has potential applications in the rating of vital functions and quality testing. In this study, the UPE of chicken eggs has been tested regarding the possibility of egg quality verification. The UPE from intact eggs and separated egg parts were subjected to supervised and unsupervised classification methods according to different housing types. The results of unsupervised egg grouping substantially agreed with the types of hen rearing. The Cohen\u2019s Kappa test score for the K-means method was up to K = 0 . 63. Supervised Support Vector Machine (SVM) classifier with radial kernel function achieved a relatively high accuracy (AC), up to 88%, also confirmed by the value of the K-statistics up to 0.81. This study shows that the best result of egg types classification can be obtained using UPE emission data from all egg parts.\nHighlights\n\u2022\nBiophoton emission of egg components can be used as a measure of the egg quality.\n\u2022\nOrganic housing type eggs are characterised by the highest levels of photon emission.\n\u2022\nUltra-weak photon emission of egg components allows classification of egg types.","Uncertainty measures exhibit algebraic and informational perspectives, and the two-view measure integration facilitates feature selections in classification learning. According to neighborhood decision systems (NDSs), two basic algorithms of feature selections (called JE-FS and DE-FS) already exist by using joint and decisional entropies, respectively, but they have advancement space for informationally fusing algebraic measures. In this paper on NDSs, three-way fusion measures are systematically constructed by combining three-way algebraic and informational measures, and thus three-level feature selections are hierarchically investigated by using corresponding monotonic and nonmonotonic measures and strategies. At first, the accuracy, granularity, and composite granularity-accuracy constitute three-way algebraic measures, while the joint, conditional, and decisional entropies (JE, CE, DE) formulate three-way informational measures. Then, three-way algebraic and informational measures are combined via normalization and multiplication, so three-way fusion measures based on JE, CE, DE are established. These new measures acquire granulation monotonicity and nonmonotonicity. Furthermore by relevant measures and monotonicity/nonmonotonicity, three-level feature selections (with null, single, and double fusion levels) related to JE, CE, DE are proposed, and corresponding heuristic algorithms are designed by monotonic and nonmonotonic principles. 4 \u00d7 3 = 12 selection algorithms comprehensively emerge, and they extend and improve current JE-FS and DE-FS. Finally by data experiments, related uncertainty measures and granulation properties are validated, and all 12 selection algorithms are compared in classification learning. As a result, new algorithms outperform JE-FS and DE-FS for classification performance, and the algorithmic improvements accord with the fusion-hierarchical deepening and entropy-systematic development of uncertainty measures.\nHighlights\n\u2022\nThree-way algebraic and informational measures induce three-way fusion measures.\n\u2022\nThree-way fusion measures acquire granulation monotonicity and nonmonotonicity.\n\u2022\nThree-level feature selections offer 4 \u00d7 3 = 12 monotonic/nonmonotonic algorithms.\n\u2022\nExtended heuristic algorithms improve 2 current algorithms on classification effects.","Cervical cancer (CC) is one of the most prevalent malignancies affecting women globally, with a particularly notable impact and high mortality in regions with limited economic resources. This underscores the imperative for the expeditious development of techniques that facilitate timely and precise detection, thereby augmenting treatment efficacy, enhancing survival rates, and mitigating the burden of healthcare expenditure. The intricacies of cervical cancer detection are inherently aligned with the challenges of fine-grained visual classification. This study focuses on the integration of bilinear pooling within convolutional neural networks (CNNs) and addresses the problem of the computational complexity of bilinear features with the fortification of the bilinear CNN with a random projection paradigm (RP-BCNN). The aim is the simultaneous achievement of improved classification precision and streamlined processing temporalities. The proposed methodology entails the introduction of a dyadic feature extraction protocol in which the input cellular image is subjected to twin feature extraction modalities. The feature maps obtained from this process undergo element-wise multiplication via an outer-product operation, thereby engendering composite feature representations. Subsequently, a judiciously designed random projection procedure is invoked to reduce dimensionality, yielding a more succinct yet informative image descriptor. Empirical evaluations of the introduced model predicted on the RP-BCNN framework yielded commendable outcomes. Notably, an accuracy of 0.9983 was achieved for the dual-label classification scenarios, and an accuracy of 0.9530 was realized in the context of multiclass classification encompassing seven distinct labels. The proposed model achieves an optimal equilibrium between classification accuracy and processing efficiency, thus constituting a potent instrument for the classification of cervical cancer. Further, it holds promise for the refinement of diagnostic accuracy, thereby providing a vantage point for embracing sophisticated techniques in the realm of medical image analysis.","Data classification is the most common task in machine learning, and feature selection is the key step in the classification task. Common feature selection methods mainly analyze the maximum correlation and minimum redundancy between feature factors and tags while ignoring the impact of the number of key features, which will inevitably lead to waste in subsequent classification training. To solve this problem, a feature selection algorithm (SSMI) based on the combination of sinusoidal sequences and mutual information is proposed. First, the mutual information between each feature and tag is calculated, and the interference information in high-dimensional data is removed according to the mutual information value. Second, a sine function is constructed, and sine ordering is carried out according to the mutual information value and feature mean value between different categories of the same feature. By adjusting the period and phase value of the sequence, the feature set with the largest difference is found, and the subset of key features is obtained. Finally, three machine learning classifiers (KNN, RF, SVM) are used to classify key feature subsets, and several feature selection algorithms (JMI, mRMR, CMIM, SFS, etc.) are compared to verify the advantages and disadvantages of different algorithms. Compared with other feature selection methods, the SSMI algorithm obtains the least number of key features, with an average reduction of 15 features. The average classification accuracy has been improved by 3% on the KNN classifier. On the HBV and SDHR datasets, the SSMI algorithm achieved classification accuracy of 81.26% and 83.12%, with sensitivity and specificity results of 76.28%, 87.39% and 68.14%, 86.11%, respectively. This shows that the SSMI algorithm can achieve higher classification accuracy with a smaller feature subset.","Classification of Indonesian crops is a critical task in developing farming and getting more understanding of agriculture. However, there is no clear task in classifying types of crops in Indonesia. Transfer learning has been used successfully in a variety of image classification applications. Thus, in this paper, we collected images of Indonesian crops from the internet randomly and proposed a classification by using transfer learning of deep learning with four pre-trained models: EffficientNet- B0, ResNet18, VGG19, and AlexNet. In the experiment, augmentation techniques such as random horizontal flip, random vertical flip, and random affine were utilized to prevent the network from overfitting. The result found that EfficientNet-B0 outperformed other models with an accuracy of 82.55. Then, the model struggled to distinguish between crops in the same family. According to the results, although transfer learning can work well to classify images of Indonesian agricultural crops, some improvements are still required to address existing issues.","To improve the classification accuracy of hand movements from sEMG signals, this paper puts forward a unified hand gesture classification framework which exploits the potentials of variational mode decomposition (VMD) and multi-class support vector machine (SVM). Acquiring the sEMG signals from 25 intact subjects for ten functional activities in real-time, we implement a non-recursive adaptive decomposition technique to sEMG signals and perform power spectral analysis to identify the dominant narrow-band intrinsic mode functions (IMFs) that contain prominent biomarkers. Subsequently, to compute the optimal feature vectors from a set of entropy measures, this work investigates the performance of two techniques namely minimum redundancy and maximum relevance (MRMR) technique and kernel principal component analysis (kPCA). After extracting the optimal set of entropy features, the proposed approach implements a multi-class SVM based on one-vs-one (OVO) strategy to classify the hand gestures. The performance of the multi-class SVM compared with those of the K-nearest neighbor (KNN) and na\u00efve bayes (NB) classifiers highlight that multi-class SVM offers superior performance with an average classification accuracy of 99.98%. Moreover, for statistical analysis of the experimental results, this work performs Friedman test to analyze the significance of the SVM, KNN and NB classifier performances. Finally, the performance comparison of the proposed approach with those of the state-of-the-art techniques highlights the superiority of the proposed framework to improve the hand gesture classification accuracy.\nHighlights\n\u2022\nVMD augmented multi-class SVM framework is presented for gesture recognition.\n\u2022\nOptimal entropy measures from decomposed IMFs are extracted through kPCA technique.\n\u2022\nA maximum classification accuracy of 99.98% is achieved using multi-class SVM.\n\u2022\nStatistical analysis of ML classifier models is performed using Friedman test.","The most malignant tumors of the central nervous system are adult-type diffuse gliomas. Historically, glioma subtype classification has been based on morphological features. However, since 2016, WHO recognizes that molecular evaluation is critical for subtyping. Among molecular markers, the mutation status of IDH1 and the codeletion of 1p/19q are crucial for the precise diagnosis of these malignancies. In pathology laboratories, however, manual screening for those markers is time-consuming and susceptible to error. To overcome these limitations, we propose a novel multimodal biomarker classification method that integrates image features derived from brain magnetic resonance imaging and histopathological exams. The proposed model consists of two branches, the first branch takes as input a multi-scale Hematoxylin and Eosin whole slide image, and the second branch uses the pre-segmented region of interest from the magnetic resonance imaging. Both branches are based on convolutional neural networks. After passing the exams by the two embedding branches, the output feature vectors are concatenated, and a multi-layer perceptron is used to classify the glioma biomarkers as a multi-class problem. In this work, several fusion strategies were studied, including a cascade model with mid-fusion; a mid-fusion model, a late fusion model, and a mid-context fusion model. The models were tested using a publicly available data set from The Cancer Genome Atlas. Our cross-validated classification models achieved an area under the curve of 0.874, 0.863, and 0.815 for the proposed multimodal, magnetic resonance imaging, and Hematoxylin and Eosin stain slide images respectively, indicating our multimodal model outperforms its unimodal counterparts and the state-of-the-art glioma biomarker classification methods.","Real-world industrial scenarios pose a challenging task known as few-shot class-incremental learning (FSCIL), which aims to recognize new classes using a few samples while not forgetting the old classes. Despite the recent advance of FSCIL, most existing methods rely on a single metric for making incremental relation predictions, which is unilateral and lacks stability. In this paper, we remedy this issue from two aspects. Specifically, to make convincing relation predictions, we first propose a relation complementation strategy that aggregates different metric models to investigate the comprehensive relation of classifier weights and test features. Then, to make the proposed strategy well fit the incremental scenarios, we design a pseudo incremental relation complementation learning scheme that constructs the learning tasks by mimicking the data setting in real incremental sessions. Taken together, our proposed method dubbed Relation Complementation Network (RCN) achieves the state-of-the-art performance on miniImageNet, CIFAR100 and CUB200. Our code is available at https://github.com/YeZiLaiXi/KT-RCN.git.","Belief rule-based classification system (BRBCS) is a useful model to handle classification problems. In our previous work, a novel data-driven BRBCS with batch-by-batch observation, online learning and multi-weights (BRBCS-BOM) is proposed. It can powerfully obtain knowledge from data. However, in some applications of industrial engineering, it can be hard to obtain enough training samples. The knowledge contained in data is not enough to deal with the problem well. It is necessary to adopt expert-driven BRBCS as an important supplement (hybrid-driven). In this paper, a hybrid-driven BRBCS-BOM with expert intervention (HBRBCS-BOM/E2) is proposed. It adopts a modified inheriting-and-learning hybrid-driven mode. Generally, it inherits the belief rules generated from training samples and make a secondary optimization for these inherited belief rules based on learning from expert-driven BRBCS. Moreover, a novel mode of expert intervention is proposed, based on the reliability evaluation. It can diversely-and-precisely obtain some important online new training samples for enhancement of data-knowledge, making hybrid-driven model better. The related experiments on an industrial engineering classification problem, called abnormity recognition of synthetical balance of material and energy, have demonstrated that the proposed HBRBCS-BOM/E2 not only makes an effective improvement for data-driven BRBCS-BOM from above two ways, but also has a more advanced performance compared with other existing high-performance BRBCS.\nHighlights\n\u2022\nA hybrid-driven mode is proposed based on swapping the inheriting-and-learning.\n\u2022\nA mode of expert intervention is proposed based on the reliability evaluation.\n\u2022\nProposed model can handle the abnormity recognition in electrolytic cell well.","Hierarchical text classification aims to assign text to multiple labels in a label set stored in a tree structure. The current algorithms mainly introduce the priori information of the label hierarchy, but the implicit correlation between labels in the hierarchy is rarely applied. At the same time, we also found that the inherent class imbalance of chain labels will also lead to poor classification effects of lower-level labels through a large number of studies. Therefore, a label structure enhanced hierarchy aware global model (LSE-HiAGM) is proposed. Firstly, the common density coefficient of labels is defined to measure the importance of a pair of labels in the hierarchical structure. Secondly, the common density coefficient is used as the weight of the label to update the topological structure features, so that the label can be linked with all labels globally. Finally, the topological structure feature, text features, and label hierarchical features are fused to make full use of all features to improve the embedding quality of low-level labels. In addition, to alleviate the class imbalance problem, a new loss function is used to constrain the model training. The probability of the label being sampled relative to all the labels of the sample is taken as the weight of the loss function. Therefore, a small penalty is imposed on the upper label and a large penalty on the lower label. A large number of experiments on datasets such as RCV1, WOS and NYT show that LSE-HiAGM performs better than the baseline models in hierarchical text classification.","Credit risk assessment is a crucial element in credit risk management. With the extensive research on consumer credit risk assessment in recent decades, the abundance of literature on this topic can be overwhelming for researchers. Therefore, this article aims to provide a more systematic and comprehensive analysis from three perspectives: classification algorithms, data traits, and learning methods. Firstly, the state-of-the-art classification algorithms are categorized into traditional single classifiers, intelligent single classifiers, hybrid and ensemble multiple classifiers. Secondly, considering the diversity of data traits in the credit dataset, data traits are divided into external structure information traits, data quality traits, data quantity traits, and internal information traits. Data traits-driven modeling framework based on multiple classifiers is proposed for solving credit risk assessment. Thirdly, considering the differences in data modeling methods, learning methods are classified into data status, label status, and structure form. Furthermore, model interpretability, model bias, model multi-pattern, and model fairness are discussed. Finally, the limitations and future research directions are presented. This review article serves as a helpful guide for researchers and practitioners in the field of credit risk modeling and analysis.","A classification system for hazardous materials in air traffic control was investigated using the Human Factors Analysis and Classification System (HFACS) framework and natural language processing to prevent hazardous situations in air traffic control. Based on the development of the HFACS standard, an air traffic control hazard classification system will be created. The dangerous data of the aviation safety management system is selected by dead bodies, classified and marked in 5 levels. TFIDF TextRank text classification method based on key content extraction and text classification model based on CNN and BERT model were used in the experiment to solve the problem of small samples, many labels and random samples in hazardous environment of air pollution control. The results show that the total cost of model training time and classification accuracy is the highest when the keywords are around 8. As the number of points increases, the time spent in dimensioning decreases and affects accuracy. When the number of points reaches about 93, the time spent in determining the size increases, but the accuracy of the allocation remains close to 0.7, but the increase in the value of time leads to a decrease in the total cost. It has been proven that extracting key content can solve text classification problems for small companies and contribute to further research in the development of security systems.","Accurate and timely crop classification results play a crucial role in providing data support for agricultural policy-making and crop yield estimation. However, the current development of crop classification faces a bottleneck in improving classification performance due to limited labeled samples and saturated classification algorithms. In this study, we propose a novel method to improve crop classification performance by leveraging unlabeled remote sensing data (URSD). Importantly, our method does not necessitate a large number of labeled samples or significant modifications to the classification algorithm. Instead, it relies on a unique self-supervised training approach and a substantial amount of URSD. Specifically, we develop a self-supervised classification framework based on a Multilayer Perceptron (MLP) and introduce a self-supervised training approach that takes into account both temporal and spectral factors. Additionally, we construct a historical sample classification model based on crop growth knowledge, emphasizing the correlation of local time series. We evaluate the proposed method using four study areas in China. The analysis of pre-training data types reveals that our method not only improves the classification performance of current year samples but also demonstrates noticeable improvement in classifying historical samples. The classification method analysis demonstrates the ability of our proposed self-supervised learning training approach to accumulate more prior knowledge. Overall, these results highlight the advantages of our method in terms of classification efficiency and performance improvement.","MERS-CoV, which belongs to the beta-coronaviruses together with SARS-CoV-2, although it has received relatively less attention by the COVID-19 pandemic, there is a sufficient possibility of new MERS-CoV lineages and variants. Previous studies have discussed the possibility of frequent recombination of MERS-CoV. We thus present a highly accurate method for the phylogenetic analysis and classification of MERS-CoV including recombinant sequences. We collected the sequences of S protein from MERS-CoV and divided them into five phylogenetic groups, of which recombinant sequences were divided into seven types. Physicochemical properties of amino acids were then calculated from the S protein sequences, and the results were used for the random forest model, Na\u00efve Bayes classification, and k-nearest neighbor method. We also constructed several feature subsets based on the ranked amino acid properties and applied them to the random forest model. In each dataset, the amino acid physicochemical properties were ranked differently. Using this information, classification of MERS-CoV based on machine learning algorithms showed that the random forest model had the best accuracy and area under the curve compared with the k-nearest neighbor and Na\u00efve Bayes classification methods. Several feature subsets were constructed using the correlation feature selection algorithm and applied to the random forest model. Overall, the performance of the classifier was improved compared to that when using all features. Coronaviruses including MERS-CoV continue to evolve into new forms through recombination or mutation. We thus present a method to increase the accuracy of their classification using additional information of the viral protein sequence, and confirm that a subunit consisting of optimal prominent features can improve the performance of the classifier by removing the unnecessary characteristic information.","In this study, we propose a new method called 'multi-label mapping' (MLM) for solving multi-class classification problems by mapping them into multi-label problems. The MLM method frequently selects different combinations of base classes, merges the classes, and assigns a new label to each class. Six standard datasets are selected from the UCI machine learning repository to evaluate the proposed method. Experimental results demonstrate that the MLM method reduces 50% to 96.66% of the number of required SVM classifiers and 25.76% to 72.27% of the training-testing time in comparison with the OVA and OVO methods. It also yields a better performance in terms of accuracy, precision, recall, and overfitting. Due to the need for a very low number of the SVM binary classifiers, a low training-testing time, and an acceptable prediction error, the presented method is a potential candidate for use in pattern recognition applications and multi-class classification problems.","Each year, millions of pilgrims come to Saudi Arabia to perform Hajj and Umrah. To help pilgrims with their difficulties and make plans to enhance the Hajj and Umrah services, this article offers a deep learning-based framework to categorize and analyze pilgrims\u2019 posts on social media networks X (formerly Twitter), Facebook and Instagram. We extracted Arabic posts related to Hajj/Umrah then tokenized and pre-processed the dataset to remove unnecessary parts such as punctuation. Posts are manually labeled into five classes: Health, Organization, Security, Services, and Worship. Labeled posts are collected to build a Long Short-Term Memory (LSTM) network classifier for deep learning. To increase the prediction accuracy, we customized the LSTM classification layer and used the sum of squares error (SSE) loss function instead of the default cross-entropy loss function. A rigorous simulation is run to assess the system\u2019s effectiveness. Every time a simulation round occurs, the trained network is tasked with identifying the classes of unlabeled posts (testing data). Similar tests are performed using the K-Nearest Neighbors (KNN) and Linear Discriminant Analysis (LDA) algorithms for comparative analysis. Accuracy, confusion matrix, precision, sensitivity, specificity, and F 1 score are used to quantify the system performance. The maximum mean accuracy of the proposed framework, KNN and LDA are 85.65%, 68.47%, and 59.06% respectively. The proposed framework needs to be integrated into a larger system called the Intelligent Pilgrim Service System (IPSS). We described the IPSS architecture, which comprises modules for data generation, data storage (on the cloud), and a control center. We described a hashtag mechanism as a part of the IPSS. This hashtag mechanism will provide both general and focused advice to millions of pilgrims related to their social media posts. We discussed scenarios of how the IPSS can enhance the Hajj and Umrah services.\nHighlights\n\u2022\nA tagged dataset of social media posts from X (formerly Twitter), Facebook, and Instagram related to Hajj and Umrah.\n\u2022\nA customized edge deep learning-based framework to classify social media posts.\n\u2022\nNine parameters to measure the performance of the system.\n\u2022\nAn architecture of the Intelligent Pilgrim Service System (IPSS) is provided.\n\u2022\nA novel hashtag mechanism is presented that provides on-time advice to pilgrims.","Homicide involving multiple victims has a significant negative effect on society. Criminal profiling consists of determining the traits of an unknown offender based on those of the crime and the victims, with a view to their identification. To provide the most likely profile of the perpetrator of a multi-victim homicide, we propose a predictive model of supervised machine learning based on a Bayesian Network. Conventional classifiers can generate the perpetrator\u2019s profile according to the traits of each of the victims of the same homicide, but the profiles may differ from one another. To address this issue, we consider the Multi-Instance (MI) learning framework, in which the victims of the same incident form a bag, and each bag is associated with a unique label for each of the perpetrator\u2019s features. We introduce the unanimity MI assumption in this domain, and accordingly allocate a label to the bag based on the labels and probabilities the Bayesian Network has assigned its instances, using a combination rule from those of the ensemble of classifiers. We apply this methodology to the Federal Bureau of Investigation (FBI) homicide database to compare three combination rules empirically in the validation process, as well as theoretically, using the one that ultimately proves to be the best to build the final model, which is then applied in some illustrative examples to achieve the criminal profile.\nHighlights\n\u2022\nPredictive model of Machine Learning based on Bayesian Networks.\n\u2022\nUseful for criminal profiling of multi-victim homicides.\n\u2022\nMulti-Instance learning using combination rules of the ensembles of classifiers.\n\u2022\nApplication to the FBI homicide dataset.","Breast cancer characterization remains a significant and challenging issue in contemporary medicine. Accurately distinguishing between malignant and benign breast lesions is crucial for effective diagnosis and treatment. The anatomical structure of malignant breast ultrasound images is more chaotic than that of benign images due to disease pathologies. However, texture-based analysis alone often fails to identify the extent of chaoticness in malignant breast ultrasound images due to their vague appearance with normal echo patterns, leading to missed diagnoses and increased mortality rates. To address this issue, we proposed an angular feature-based multilevel breast cancer classification framework mBCCf that aims to improve the accuracy and efficiency of classification. The proposed framework mimics the radiologist interpretation procedure by identifying the chaoticness on the periphery of the breast lesion in a breast ultrasound image (level-1). If the lesion contains an acute angle in any part of the periphery, it can be characterized as malignant or otherwise benign. However, solely relying on level-1 analysis may result in misclassification, especially when benign lesions exhibit echo patterns that resemble malignant ones. To overcome this limitation and to make the proposed system highly sensitive, advanced texture-based analysis (using combined shape, texture, and angular features) is performed (level-2). Finally, the performance of the proposed system is evaluated using a cross-dataset (consisting of 1293 breast ultrasound images) and compared with the different individual feature extraction techniques. Encouragingly, our system demonstrated an accuracy of 96.99% for classifying malignant and benign tumors, which is also validated using statistical analysis. The implications of our research lie in its potential to significantly improve breast cancer diagnosis by providing a reliable, efficient, and sensitive tool for radiologists.","Highlights\n\u2022\nCalculating the KNN for each instance and leveraging instance correlation to expand the original feature space.\n\u2022\nUsing mutual information to measure feature redundancy and extracting high-quality feature subsets with low dimensions.\n\u2022\nDeveloping a multi-label learning approach that considers label correlation, instance correlation, and feature redundancy.\nAbstract\nMulti-label classification is a machine-learning task that simultaneously processes instances associated with multiple labels. Label-specific feature learning selects each label's most discriminative feature subset, effectively reducing the feature dimension and improving the classification performance. However, most methods only consider label correlation, ignoring the correlation between instances and feature redundancy. To solve this problem, a multi-label classification method based on instance correlation and feature redundancy is proposed. The proposed method merges instance correlation by updating the data set and removes redundant features by calculating mutual information. By jointly considering label correlation, instance correlation, and feature redundancy, our method promotes effective multi-label feature selection. The experimental results on ten data sets demonstrate the effectiveness of the proposed method.","Accurate liver cancer classification is essential, as it substantially influences the selection of effective treatment strategies and impacts patient prognosis. Convolutional neural network (CNN) classifiers typically require extensive labeled datasets for training to attain decent performance. However, the process of obtaining labeled data through manual labeling is time-consuming, potentially biased, and costly when applied to large datasets. This study utilizes the Simple Siamese (SimSiam) contrastive self-supervised learning approach to enhance the classification of liver tumours, especially considering the limited availability of labeled computed tomography (CT) scans of liver cancer. We integrate SimSiam with three baseline CNN-based classifiers - Inception, Xception, and ResNet152 - and pretrain them with two loss functions: mean squared error (MSE) and cosine similarity (COS). Our findings show consistent improvements for three classifiers compared to the baseline models. Specifically, the ResNet152 model exhibits the highest performance among the evaluated networks. With MSE and COS losses, the classification accuracy for ResNet152 improves by 1.27% and 2.53%, respectively. The classification accuracy of the Inception model improves by 3.95% and 5.26%. Similarly, Xception\u2019s validation accuracy demonstrates an increase of 2.60% with both loss functions, compared to the baseline models. We validate our pipeline via our multi-resolution in-house abdominal CT scans of primary and secondary liver cancers, including 155 patients with hepatocellular carcinoma, 198 patients with colorectal liver metastases, and 107 patients with intrahepatic cholangiocarcinoma. Source code available at: https://github.com/Ramtin-Mojtahedi/SimSiam-LiverCancer-CL.","Ship classification based on machine learning (ML) has proven to be a significant underwater acoustic research direction. One of the critical challenges rests with how to embed domain signal knowledge into ML models to obtain suitable features that highly correlate with the classification and create better predictors. In this paper, a novel ML-based ship classification model, Hierarchical Underwater Acoustic Transformer (HUAT), is proposed to improve the classification performance. Firstly, the Detection of Envelope Modulation on Noise (DEMON) spectra of ship-radiated noise signals are estimated by cyclostationary analysis. The motivation for using a DEMON-based preprocessing scheme is that valuable propeller information can be revealed by exploiting the second-order cyclostationarity of ship-radiated noise signals. Secondly, the useful features of DEMON spectra are enhanced using a multi-head self-attention module, and the potential features of the Mel spectrograms are extracted employing a Convolutional Neural Network (CNN) module. The two kinds of features are fused to provide ship classification patterns. The challenge of feature learning in the deep classification model is reduced by leveraging domain-related classification knowledge. Finally, the Swin Transformer, based on shifted window self-attention mechanism, is used to learn high-level feature representations and conduct ship classification. Experimental results show that the HUAT model achieves excellent classification performance on ship-radiated noise datasets, ShipsEar and DeepShip. And its classification efficiency is better than the model based on traditional Transformer architecture. In addition, the proposed method provides technical support for the underwater intelligent system capable of automatically sensing sailing vessels and recognizing vessel types.\nHighlights\n\u2022\nThe Detection of Envelope Modulation on Noise (DEMON) spectra of ship-radiated noise signals are estimated by cyclostationary analysis.\n\u2022\nA novel feature fusion strategy is proposed for embedding domain signal knowledge into ML models to obtain suitable features highly correlated with the classification.\n\u2022\nThe valuable features of DEMON spectra and the Mel spectrogram of ship signals are extracted using two different feature extractors.\n\u2022\nThe Swin Transformer is used to learn high-level feature representations and conduct ship classification.","The application of hyperspectral imaging with computer-aided technology has promising prospects, and achieving real-time, efficient, and non-destructive detection, especially for food and agricultural products, undoubtedly poses a great challenge. Hyperspectral data processing has many complications, such as large volume, high redundancy, and difficulty extracting useful features. Therefore, this study develops a lightweight end-to-end unified framework for deep neural networks with excellent generalization to conserve memory space and computation. To improve the classification accuracy and performance of the core model LSAC-net, we combine an attention mechanism based on the temporal convolution method with a complementary model-scaling technique. Experimental results on our own datasets for the production year of citri reticulate pericarpium, production origin of Pu\u2019er tea, and process mode of coffee beans show that our model outperforms several other state-of-the-art models.\nHighlights\n\u2022\nA valid approach for the assessment of agri-food products by hyperspectral images is proposed.\n\u2022\nThe network is an integration of convolution and self-attention.\n\u2022\nRobustness of the accuracy by efficient use of hyperspectral image data.\n\u2022\nAn equilibrium model performance achieved by the compound scaling strategy.","This paper proposes a novel hybrid approach that combines a mixture of non-central Wishart distribution (MoNCW) model and a feed forward neural network (FFNN) to accurately estimate both the number and orientations of white matter fibers in biological tissues in brain. While the MoNCW model performs well in determining fiber orientations when the separation angle is greater than 50\u00b0, accurately clustering these orientations is still a significant challenge. To tackle this issue, the authors introduce a machine learning (ML) model that can precisely identify the number of fibers per voxel. The ML model is then integrated with the MoNCW model to improve the accuracy of fiber orientation estimation. The FFNN is trained using simulated datasets, which contain signal vectors for single, as well as two and three crossing fiber voxels, using a sequential model. The FFNN is particularly effective in solving classification problems, as it can process input data through multiple layers to produce output values that correspond to class labels. In this study, three classes are labelled as 1, 2, and 3, representing the number of fibers. By utilizing this hybrid approach, the accuracy of fiber number and orientation estimation in biological tissues is significantly improved, outperforming existing mixture models.","Electroencephalogram (EEG) is a non-invasive technology with high temporal resolution, widely used in Brain-Computer Interfaces (BCIs) for mental workload (MWL) classification. However, numerous EEG channels in current devices can make them bulky, uncomfortable, and time-consuming to operate in real-life scenarios. A Riemannian geometry approach has gained attention for channel selection to address this issue. In particular, Riemannian geometry employs covariance matrices of EEG signals to identify the optimal set of EEG channels, given a specific covariance estimator and desired channel number. However, previous studies have not thoroughly assessed the limitations of various covariance estimators, which may influence the analysis results. In this study, we aim to investigate the impact of different covariance estimators, namely Empirical Covariance (EC), Shrunk Covariance (SC), Ledoit-Wolf (LW), and Oracle Approximating Shrinkage (OAS), along with the influence of channel numbers on the process of EEG channel selection. We also examine the performance of selected channels using diverse deep learning models, namely Stacked Gated Recurrent Unit (GRU), Bidirectional Gated Recurrent Unit (BGRU), and BGRU-GRU models, using a publicly available MWL EEG dataset. Our findings show that although no universally optimal channel number exists, employing as few as four channels can achieve an accuracy of 0.940 (\u00b10.036), enhancing practicality for real-world applications. In addition, we discover that the BGRU model, when combined with OAS covariance estimators and a 32-channel configuration, demonstrates superior performance in MWL classification tasks compared to other estimator combinations. Indeed, this study provides insights into the effectiveness of various covariance estimators and the optimal channel subsets for highly accurate MWL classification. These findings can potentially advance the development of EEG-based BCI applications.","Highlights\n\u2022\nNine supervised machine-learning models were trained and compared.\n\u2022\nA hyperspectral data collection platform was developed for field data collection.\n\u2022\nEffectiveness of MCC and F1 scores were compared for imbalanced dataset.\n\u2022\nQuadratic discriminant classifier with F1 score 0.95 &amp; MCC 0.85 was chosen.\nAbstract\nWeed infestation and their management are a critical production challenge in agricultural fields. Palmer amaranth has created management challenges because it has multiple emergence pattern and has evolved resistant to nine unique herbicide sites of action. Effective Palmer amaranth detection and positive identification in field conditions will help to improve Palmer amaranth control. A field-based hyperspectral imaging system was developed to record Palmer amaranth in soybean fields. The data were pre-processed applying Savitzky-Golay 2nd derivative, Multiplicative Scatter Correction, and Standard Normal Variate in a forward feed manner. Recursive feature elimination, SelectFromModel, sequential forward selection, and backward elimination were used to select significant wavebands from the available 224 bands. Later, supervised machine-learning models were generated to classify soybean and Palmer amaranth using the selected wavebands. Matthew\u2019s correlation coefficient (MCC), F1 score, precision, and recall were considered as the most significant parameters to evaluate the models\u2019 performance. The highest result was obtained by quadratic discriminant analysis with a prediction accuracy of 93.95%, a precision of 90.30%, a recall of 90.29%, an F1 score of 0.95, and an MCC score of 0.85. The findings of this study showed that the combination of hyperspectral imaging and machine-learning is a potential technique for real-time weed detection in the open field condition.","Interpreting regulatory documents or building codes into computer-processable formats is essential for the intelligent design and construction of buildings and infrastructures. Although automated rule interpretation (ARI) methods have been investigated for years, most of them are highly dependent on the early and manual filtering of interpretable clauses from a building code. While few of them considered machine interpretability, which represents the potential to be transformed into a computer-processable format, from both clause- and document-level. Therefore, this research aims to propose a novel approach to automatically evaluate and enhance the machine interpretability of single clauses and building codes. First, a few categories are introduced to classify each clause in a building code considering the requirements for rule interpretation, and a dataset is developed for model training. Then, an efficient text classification model is developed based on a pretrained domain-specific language model and transfer learning techniques. Finally, a quantitative evaluation method is proposed to assess the overall interpretability of building codes. Experiments show that the proposed text classification algorithm outperforms the existing CNN- or RNN-based methods, by improving the F1-score from 72.16% to 93.60%. It is also illustrated that the proposed classification method can enhance downstream ARI methods with an improvement of 4%. Furthermore, analysis of more than 150 building codes in China showed that their average interpretability is only 34.40%, which implies that it is still difficult to fully transform an entire regulatory document into computer-processable formats. It is also argued that the interpretability of building codes should be further improved both from the human side (considering certain constraints when writing building codes) and the machine side (developing more powerful algorithms, tools, etc.).","Consumer preference prediction aims to predict consumers\u2019 future purchases based on their historical behavior-level data. Using machine learning algorithms, the prediction results provide evidence to conduct commercial activities and further improve consumer experiences. However, missing values and imbalanced class problems of consumer behavioral data always make machine learning algorithms ineffective. While several methods have been proposed to address missing data or imbalanced class problems, few works have considered the relationships among missing mechanisms, imputation algorithms, imbalanced class methods, and the effectiveness of classification algorithms that use impute data. In this study, we aim to propose an adaptive process for selecting the optimal combination of amputation, imputation, imbalance treatment, and classification based on classification performance. Our research extends the literature by showing significant interaction effects between 1) the amputation mechanism and imputation algorithms, 2) imputation and imbalance treatments, and 3) imbalance treatments and classification algorithms. Using three consumer behavioral datasets from the UCI Machine Learning Repository, we empirically show that, among different classification methods, the overall performance of Random Forest is better than that of Logit, SVM, or Decision Tree. Moreover, Logit, as the most widely used classification method, suffers most from imbalance issues in real-world datasets. Furthermore, Metacost is always the best imbalance treatment for different imputation techniques or missing value mechanisms.","Automated classification of skin lesions in dermoscopy images has the potential to significantly improve survival rates and reduce the risk of death for skin cancer patients. However, existing supervised learning models heavily depend on well-annotated dermoscopy training data, which is expensive and labor-intensive to obtain. This paper addresses this issue by proposing a semi-supervised framework called Noisy Consistent Pseudo Labeling (NCPL), which only utilizes less annotated images with many unlabeled raw data. The NCPL framework consists of two components: the Noisy-Consistent Sample Learning(NCSL) module to remove low-confidence images, and the Attentive Clustered Feature Integration (ACFI) module, incorporating an uncertainty-aware attention mechanism. Specifically, the NCSL module is introduced to filter and generate reliable pseudo-labels for unlabeled skin images, with excellent capability of removing noisy samples. Additionally, the ACFI module integrates high-dimensional representations of original lesion images in an attentive manner, assisted with the annotated data. By focusing the representative samples and removing noisy images, the NCPL approach performs outstanding experimental results, demonstrating the superiority of the NCPL framework in semi-supervised skin lesion classification task.","Handwritten Chinese character recognition has achieved high accuracy using deep neural networks (DNNs), but the structural recognition (which offers structural interpretation, e.g., stroke and radical composition) is still a challenge. Existing DNNs treat character image as a whole and perform classification end-to-end without perception of the structure. They need a large amount of training samples to guarantee high generalization accuracy. In this paper, we propose a method for structural recognition of handwritten Chinese characters based on a modified part capsule auto-encoder (PCAE), which explicitly considers the hierarchical part-whole relationship of characters, and leverages extracted structural information for character recognition. Our PCAE is improved based on stacked capsule auto-encoder (SCAE) so as to better extract strokes and perform classification. By the modified PCAE, the character image is firstly decomposed into primitives (stroke segments), with their shape and pose information decoupled. The transformed primitives are aggregated into higher-level parts (strokes) guided by prior knowledge extracted from writing rules. This process enhances interpretability and improves the discrimination ability of features. Experimental results on a large dataset demonstrate the effectiveness of our method in both Chinese character recognition and stroke extraction tasks.","Improper sorting of construction and demolition waste (CDW) leads to significant environmental and economic implications, including inefficient resource use and missed recycling opportunities. To address this, we developed a machine-learning-assisted procedure for recognizing CDW fragments using an RGB camera. Our approach uniquely leverages selected feature extraction, enhancing classification speed and accuracy. We employed three classifiers: convolutional neural network (CNN), gradient boosting (GB) decision trees, and multi-layer perception (MLP). Notably, our method\u2019s extraction of selected features for GB and MLP outperformed the traditional CNN in terms of speed and accuracy, especially for challenging samples with similar textures. Specifically, while convolution resulted in an overall accuracy of 85.9%, our innovative feature extraction approach yielded accuracies up to 92.3%. This study\u2019s findings have significant implications for the future of CDW management, offering a pathway for efficient and accurate waste sorting, fostering sustainable resource use, and reducing the environmental impact of CDW disposal. Supplementary materials, including datasets, codes, and models, are provided, promoting transparency and reproducibility.\nHighlights\n\u2022\nClassifiers were trained to recognize construction and demolition waste (CDW).\n\u2022\nCDW fragments were recognized from RGB images.\n\u2022\nFeatures were extracted for GB and MLP models; CNN employed convolution.\n\u2022\nGB and MLP outperformed CNN in terms of speed and accuracy.\n\u2022\nGB: 92.3% overall accuracy, MLP: 91.3%, and CNN: 85.9%.","Deep forest models offer a promising alternative to traditional deep neural networks by demanding fewer training samples and hyperparameters. However, existing deep forest fault diagnosis models encounter persistent challenges such as insufficient representation of multi-grained spatial information and redundancy of cascaded forest features. To address the above challenges, an enhanced deep forest method called random multi-grained fusion cascade forest (rgfc-Forest) is presented for fault diagnosis of electromechanical systems with limited training samples. First, a random multi-grained scanning module is designed to improve feature information learning. Subsequently, a feature fusion cascade forest module is constructed to improve the representativeness of features in multi-grained scanning and cascade forest delivery while ensuring data diversity. Finally, a decision tree self-growth strategy is combined to refine the classification capability of the high-level forest. To evaluate the effectiveness of our proposed method, we applied it to experimental data related to motor system and gearbox faults. Our results demonstrate significant improvements over existing methods: With just 20 samples per class, our method achieved an average accuracy of 84.41% for motor System Diagnosis. Similarly, for the gearbox system, we attained an impressive accuracy of up to 92.72% with the same limited dataset. These outcomes underscore the superior feature representation and fault classification capabilities of our approach compared to both benchmark deep forest models and mainstream deep learning methods when confronted with small training datasets.","Artificial intelligence (AI) technology has significant potential in Earth sciences, particularly in mineral identification for industrial exploration, geological mapping, and archaeological research. However, traditional methods are time-consuming, expensive, and complex. And existing mineral identification methods based on mineral photos face several critical challenges, including lack of consideration for natural image features captured in real environments, limitations of single-label classification which does not align with multi-mineral occurrences in nature, and growing computational complexity as the number of identifiable mineral labels increases. Therefore, this paper proposes an efficient mineral identification model based on multi-label image classification, focusing on natural environmental features. First, realistic feature datasets are created by simulating mineral photos in real environments. Then, the model uses the query-label (Query2Label) framework, with MaxViT-T (Multi-Axis Vision Transformer-Tiny) as the feature extraction network and the asymmetric loss function. Knowledge distillation is employed to improve identification accuracy while reducing computational complexity. The proposed model achieves an impressive average identification accuracy of 84.74% on a dataset of 495,756 mineral photos, surpassing existing models like ResNet-101, ML-GCN (Multi-Label Graph Convolutional Network), and SRN (Spatial Regularization Net). It maintains a lower parameter count and computational complexity. In the end, ablation experiments demonstrate the effectiveness of each optimization scheme.\nHighlights\n\u2022\nSelecting image noise and color shift methods for realistic mineral data simulation.\n\u2022\nProposing an enhanced two-stage image multi-Label identification model framework.\n\u2022\nApplying knowledge distillation for multi-label classification subtask decomposition.\n\u2022\nBeing with low computational complexity and high mineral identification accuracy.","Highlights\n\u2022\nHyperspectral images of 10,000 maize seeds from 20 varieties are collected.\n\u2022\nA self-supervised learning method for seed classification is proposed.\n\u2022\nThe proposed method performs well on the raw spectral data.\n\u2022\nSelf-supervised pre-trained model improves seed classification performance.\nAbstract\nRapid and non-destructive variety identification is essential for screening maize seeds for different end-uses such as food, feed, and breeding. Hyperspectral imaging (HSI) is one of the most commonly used techniques in such seed classification. Typically, after acquiring hyperspectral images of seeds, the spectral domain signals need to be preprocessed and a classifier need to be designed. The traditional method is to find a appropriate spectral preprocessing method through trial-and-error experiment, which is time-consuming, laborious and has high risk of misuse preprocessing. In view of this, this paper proposes a self-supervised learning method that includes pre-training and fine-tuning phases. In the pre-training phase, a model was trained on the unlabeled raw spectral data in an unsupervised manner to obtain general representations. In the fine-tuning phase, the pre-trained model was fine-tuned with the goal of the seed classification task and trained in a supervised manner on labeled spectral data. Experimental results showed that the proposed method did not rely on spectral preprocessing, and its performance was superior to other existing seed classification methods. In addition, the self-supervised pre-trained model significantly outperformed the non-pre-trained model in the downstream seed classification task, and obtained good generalization ability. Overall, this method combined with HSI for seed quality evaluation has broad application prospects.","For the machine learning-based prediction of the conversion from mild cognitive impairment to Alzheimer\u2019s disease, the collection of sufficient data to train a model is required, which involves a lot of time and expense. When data is not enough, combining public and in-house data may be appropriate by applying domain adaptation that alleviates inter-site heterogeneity. Existing methods simultaneously transform in-house and public data to represent them into a common feature space, and then train a classifier using labels in public data. However, this procedure causes the time- and cost-consuming re-training of classifier whenever in-house data changes, and also inheres the risk of information loss in public data. Motivated by this, we propose a method that only transforms in-house data while preserving public data, namely one-way domain adaptation. The proposed method represents in-house data similar with public data by matching the data distribution and the connectivity between brain regions with mean vectors and covariance matrices, respectively. Then, the pre-trained classifier in public data is applied to predict AD conversion for in-house data. The experiments, which use the Australian Imaging Biomarkers and Lifestyle Study of Aging and the Open Access Series of Imaging Studies as the in-house data and the Alzheimer's Disease Neuroimaging Initiative as the public data, show the effectiveness and efficiency of the proposed method, improving prediction performance about 34.8% on average without labels in the in-house datasets.","Highlights\n\u2022\nWe enhanced off-the-shelf ASR with classifiers to score verbal fluency tasks.\n\u2022\nMany novel scores utilizing timings of words can be calculated automatically.\n\u2022\nWe achieved high AUCs for the identification of valid words.\n\u2022\nMost automated scores correlate strongly with manual scores.\nAbstract\nSkip Objective Section\nObjective\nTo compare verbal fluency scores derived from manual transcriptions to those obtained using automatic speech recognition enhanced with machine learning classifiers.\nSkip Methods Section\nMethods\nUsing Amazon Web Services, we automatically transcribed verbal fluency recordings from 1400 individuals who performed both animal and letter F verbal fluency tasks. We manually adjusted timings and contents of the automatic transcriptions to obtain \u201cgold standard\u201d transcriptions. To make automatic scoring possible, we trained machine learning classifiers to discern between valid and invalid utterances. We then calculated and compared verbal fluency scores from the manual and automatic transcriptions.\nSkip Results Section\nResults\nFor both animal and letter fluency tasks, we achieved good separation of valid versus invalid utterances. Verbal fluency scores calculated based on automatic transcriptions showed high correlation with those calculated after manual correction.\nSkip Conclusion Section\nConclusion\nMany techniques for scoring verbal fluency word lists require accurate transcriptions with word timings. We show that machine learning methods can be applied to improve off-the-shelf ASR for this purpose. These automatically derived scores may be satisfactory for some applications. Low correlations among some of the scores indicate the need for improvement in automatic speech recognition before a fully automatic approach can be reliably implemented.","Over the past two decades, matrix-based or bilinear discriminant analysis (BLDA) methods have received much attention. However, it has been reported that the traditional vector-based regularized LDA (RLDA) is still quite competitive and could outperform BLDA on some benchmark datasets. A central question is whether the superiority of the vector-based RLDA would always hold for general matrix data, or is there any type of matrix data on which BLDA would perform better than RLDA? Actually, the reported comparisons are found to suffer from two limitations: (i) the comparisons are only limited to image data, and (ii) regularized RLDA is compared with non-regularized BLDA. In this paper, we break the two limitations and investigate the central question on another type of matrix data, namely multivariate time series (MTS) data. We propose a new two-parameter regularized BLDA (RBLDA) for MTS data classification. To choose the two parameters, we develop an efficient model selection algorithm. The newly proposed RBLDA enables us to perform a fair comparison between vector-based RLDA and matrix-based RBLDA. Experiments on a number of real MTS data sets are conducted to compare RBLDA with RLDA and evaluate the proposed algorithm. The results reveal that the superiority of the vector-based RLDA does not always hold for general matrix data, and RBLDA outperforms RLDA on MTS data. Moreover, the proposed model selection algorithm is efficient, and RBLDA can produce better visualization of MTS data than RLDA.\nHighlights\n\u2022\nA new regularized BLDA (RBLDA) is proposed for multivariate time series (MTS) classification.\n\u2022\nAn efficient model selection algorithm is developed for the proposed RBLDA.\n\u2022\nEmpirical results show that the proposed RBLDA generally performs better than RLDA on MTS data.\n\u2022\nRBLDA can produce better visualization of MTS data than RLDA.","As for the characteristics of the objects in the airtight package in the X-ray image, some prohibited items of the airtight package are difficult to be detected from the X-ray images with complex and overlapped backgrounds. In this article, the cooperative knowledge distillation method is used to enhance the prohibited items detection model in the X-ray image. To efficiently implement hard example mining, this article designs an Multi-task Classification Head (MCH) for teachers to provide prior knowledge of image-level and instance-level predictions. Different from the distillation method in which the students imitate the teacher, the algorithm in this article is implemented by the cooperation between teacher and student. In order to verify the effectiveness of this algorithm, a series of related experiments are carried out on PIDray and SIXray respectively. Experiments show that the algorithm improves the AP of State-of-the-Art dense object detectors (e.g., RetinaNet, ATSS, GFL, and TOOD) in SIXray by 1% \u223c 2%. Especially for prohibited items that are difficult to be found in X-ray images, the algorithm is more effective, and the dense object detectors can achieve a performance improvement of about AP of 2% on the Hidden subset of PIDray. The experiments demonstrate that the cooperative knowledge distillation algorithm proposed in this article can effectively enhance the performance of prohibited items detection, particularly showing more pronounced improvements in detecting hard examples.","The field of image classification faces significant challenges due to the scarcity of target samples, leading to model overfitting and difficult training. To address these issues, few-shot learning has emerged as a promising approach. However, current methods do not fully utilize the correlations among samples and external semantic information, resulting in poor recognition accuracy. To overcome these limitations, we propose a new few-shot classification method that incorporates both attributes and attention guided approach. The method leverages the attention mechanism to extract discriminative features from the images. By exploring regional correlations among samples, it assists in generating visual representations by utilizing predicted attribute features. As a result, accurate prototypes are generated. Extensive experiments were conducted on two attribute-labeled datasets, namely Caltech-UCSD Birds-200\u20132011(CUB) and SUN Attribute Database (SUN) Attribute Dataset. With the Resnet12 backbone, the method achieves remarkable accuracies of 79.95% and 89.34% for 1-shot and 5-shot, respectively, on the CUB dataset. Similarly, with the Conv4 backbone, the method achieves notable accuracies of 67.21% and 80.87% for 1-shot and 5-shot, respectively, on the SUN Attribute dataset. The achieved accuracies highlight the robustness and generalizability of our method, and show the capability of our method to accurately classify samples with limited training data, which is a significant advantage in real-world scenarios where labeled data are often scarce.","Recognizing plant species and disease is essential to practical applications, such as keeping biodiversity and obtaining a desired crop yield. This study aims to extend the recognition from known to unknown classes in the context of plants, termed Plant-relevant Open-Set Recognition (POSR). In this task, a trained model is required to either classify an input image into one of the known classes or an unknown class, even if the model is only trained with the images of known classes. To achieve this task, we propose a method to obtain a high-performance classifier with compact feature distributions for known classes. To have a high-performance classifier, a ViT model pre-trained in the PlantCLEF2022 dataset is transferred, following an observation that a plant-related source dataset is more beneficial to plant species and disease recognition than other commonly used datasets, such as ImageNet. To have compact feature distributions, we adopt additive margin Softmax loss (AM-Softmax) which brings the distance smaller between the features of the same known class and hence gives more spaces for the unknown class. Extensive experimental results suggest that our method outperforms current algorithms. To be more specific, our method obtains AUROC 93.685 and OSCR 93.256 on average on four public datasets, with an average accuracy of 99.295 on closed-set classification. We believe that our study will contribute to the community and, to fuel the field, our codes will be public https://github.com/xml94/POSR. .\nHighlights\n\u2022\nA plant-relevant versatile recognition, PVR, is explicitly proposed from a unified perspective for real-world applications.\n\u2022\nPVR is instantiated as POSR to perform known and unknown class recognition on plant species and disease.\n\u2022\nTo achieve POSR, a ViT model pre-trained in the PlantCLEF2022 dataset and AM-Softmax are leveraged.\n\u2022\nExtensive experiments are executed with non-trivial analysis, which suggests that the proposed method surpasses the current methods with clear margins.","With the rapid development of higher education in China, more and more archives are managed by the archives of colleges and universities. Therefore, many colleges and universities have equipped archives management software to manage archives by computer. However, at present, the mainstream archives management software for colleges and universities does not have the function of automatic classification of archives. In order to reduce the workload of college archives staff, this paper explores a text automatic classification method suitable for college archives. Classification tree algorithm is a method of organizing and storing information. The classification tree is used to classify data into various categories, such as customers and products. It also provides an effective way to search for data using keywords or phrases. Classification tree is very useful in document processing, because it can help us find documents with similar characteristics, such as customers with similar requirements, products with similar functions, etc. The working principle of classification tree algorithm is to decompose the problem into smaller subproblems, and then solve one subproblem at a time until all problems are completely solved.","The sequential three-way decision (S3WD) model, which merges three-way decisions and granular computing, is increasingly crucial in classification. The risk attitude to the decision process and result costs affects the decisive actions in the S3WD model. Furthermore, decision conflict arises when there is a discrepancy between coarse-grained and fine-grained definite decision-making for the same object, which can potentially impact decision accuracy. However, current studies show incomplete risk preference research and a lack of decision correction strategies to address decision conflict. To address the limitation, four sequential three-way classifiers (S3WCs) are proposed. First, three prominent distance functions are employed to compute similarity classes for condition probability estimation. Second, optimistic, pessimistic, and weighted compromise sequential three-way classifiers are established to reflect the risk preference for the two types of costs. Third, four precision differences in the S3WCs are defined from local and global perspectives. An S3WC with decision correction is presented to improve precision by judging precision differences in adjacent granularity levels and the entire granular structure. Finally, a series of experiments are conducted to thoroughly analyze the characteristics and applications of these S3WCs. The superior classification performance of the proposed models on diverse datasets is demonstrated.\nHighlights\n\u2022\nThree prominent distance functions are adopted to construct similarity classes for condition probability estimation.\n\u2022\nThree kinds of sequential three-way classifiers considering risk preference are proposed.\n\u2022\nLocal and global classification precision differences are defined to detect the classification precision changing status.\n\u2022\nThe sequential three-way classifier with a decision correction strategy is presented to improve the classification precision.","Using convolutional neural networks for 360\u00b0 images can induce sub-optimal performance due to distortions entailed by a planar projection. The distortion gets deteriorated when a rotation is applied to the 360\u00b0 image. Thus, many researches based on convolutions attempt to reduce the distortions to learn accurate representation. In contrast, we leverage the transformer architecture to solve image classification problems for 360\u00b0 images. Using the proposed transformer for 360\u00b0 images has two advantages. First, our method does not require the erroneous planar projection process by sampling pixels from the sphere surface. Second, our sampling method based on regular polyhedrons makes low rotation equivariance errors, because specific rotations can be reduced to permutations of faces. In experiments, we validate our network on two aspects, as follows. First, we show that using a transformer with highly uniform sampling methods can help reduce the distortion. Second, we demonstrate that the transformer architecture can achieve rotation equivariance on specific rotations. We compare our method to other state-of-the-art algorithms using the SPH-MNIST, SPH-CIFAR, and SUN360 datasets and show that our method is competitive with other methods.","Semi-supervised domain adaptation (SSDA) aims to bridge source and target domain distributions, with a small number of target labels available, achieving better classification performance than unsupervised domain adaptation (UDA). However, existing SSDA work fails to make full use of label information from both source and target domains for feature alignment across domains, resulting in label mismatch in the label space during model testing. This paper presents a novel SSDA approach, Inter-domain Mixup with Neighborhood Expansion (IDMNE), to tackle this issue. Firstly, we introduce a cross-domain feature alignment strategy, Inter-domain Mixup, that incorporates label information into model adaptation. Specifically, we employ sample-level and manifold-level data mixing to generate compatible training samples. These newly established samples, combined with reliable and actual label information, display diversity and compatibility across domains, while such extra supervision thus facilitates cross-domain feature alignment and mitigates label mismatch. Additionally, we utilize Neighborhood Expansion to leverage high-confidence pseudo-labeled samples in the target domain, diversifying the label information of the target domain and thereby further increasing the performance of the adaptation model. Accordingly, the proposed approach outperforms existing state-of-the-art methods, achieving significant accuracy improvements on popular SSDA benchmarks, including DomainNet, Office-Home, and Office-31.\nHighlights\n\u2022\nA novel algorithm called IDMNE is proposed for semi-supervised domain adaptation.\n\u2022\nInter-Domain Mixup mitigates label mismatch during feature alignment in adaptation.\n\u2022\nNeighborhood Expansion diversifies target label information, curbs prediction uncertainty.\n\u2022\nIntegrated framework surpasses state-of-the-art on three SSDA benchmark datasets.","Leveraging the enormous amounts of real-world data collected through Internet of Things (IoT) technologies, human activity recognition (HAR) has become a crucial component of numerous human-centric applications, with the aim of enhancing the quality of human life. While the recent advancements in deep learning have significantly improved HAR, the process of labeling data continues to remain a significant challenge due to the substantial costs associated with human annotation for supervised model training. Active learning (AL) addresses this issue by strategically selecting informative samples for labeling during model training, thereby enhancing model performance. Although numerous approaches have been proposed for sample selection, which consider aspects of uncertainty and representation, the difficulties in estimating uncertainty and exploiting distribution of high-dimensional data still pose a major issue. Our proposed deep learning-based active learning algorithm, called Multiclass Autoencoder-based Active Learning (MAAL), learns latent representation leveraging the capacity of Deep Support Vector Data Description (Deep SVDD). With the multiclass autoencoder which learns the normal characteristics of each activity class in the latent space, MAAL provides an informative sample selection for model training by establishing a link between the HAR model and the selection model. We evaluate our proposed MAAL using two publicly available datasets. The performance results demonstrate the improvements across the overall active learning rounds, achieving enhancements up to 3.23% accuracy and 3.67% in the F 1 score. Furthermore, numerical results and analysis of sample selection are presented to validate the effectiveness of the proposed MAAL compared to the alternative comparison methods.\nGraphical abstract\nDisplay Omitted\nHighlights\n\u2022\nWe present a deep learning-based active learning for an efficiently labeled dataset.\n\u2022\nThe proposed method extends the autoencoder with SVDD in a multiclass scheme.\n\u2022\nWe evaluate the proposed active learning method in the scenario of HAR applications.\n\u2022\nExperimental results show improvements in the performance of HAR with a smaller dataset.\n\u2022\nSelection of informative samples which is difficult for model to predict is validated.","Decision tree algorithm, because of its strong interpretability and high algorithm efficiency, is widely used in the field of pattern recognition and classification. When the number of data samples is small and there is uncertainty in the data, it is difficult for the traditional decision tree algorithm to fully mine the effective information in the data. In this paper, we use the Dempster\u2013Shafer framework to model data uncertainty and propose a hierarchical interval estimation method to improve decision tree algorithms. The proposed method constructs intervals through two methods of attribute boundary and mean square error estimation, which not only utilizes the characteristics of intervals to model the inaccuracy of data, but also constrains intervals from two aspects, narrowing the representation range of available information. By comparing with the classic decision tree algorithm and the decision tree algorithm based on single interval estimation, the proposed method can perform classification tasks robustly and accurately in different types of data under seven data sets.\nHighlights\n\u2022\nWe propose an evidential decision tree based on a reliability-based BPA generation method and hierarchical interval estimation.\n\u2022\nThe proposed method greatly reduces the influence of the labels of uncertain data on the classification accuracy of the decision tree and strengthens the robustness of the algorithm.\n\u2022\nComparison with fifteen different decision trees, the accuracy of our proposed method on seven datasets is higher than other methods especially in the face of with uncertain attributes and labels.","This paper presents an empirical analysis of theperformance of popular convolutional neural networks (CNNs) for identifying objects in real time video feeds. The most popular convolution neural networks for object detection and object category classification from images are Alex Nets, GoogLeNet, and ResNet50. A variety of image data sets are available to test the performance of different types of CNN\u2019s. The commonly found benchmark datasets for evaluating the performance of a convolutional neural network are anImageNet dataset, and CIFAR10, CIFAR100, and MNIST image data sets. This study focuses on analyzing the performance of three popular networks: Alex Net, GoogLeNet, and ResNet50. We have taken three most popular data sets ImageNet, CIFAR10, and CIFAR100 for our study, since, testing the performance of a network on a single data set does not reveal its true capability and limitations. It must be noted that videos are not used as a training dataset, they are used as testing datasets. Our analysis shows that GoogLeNet and ResNet50 are able to recognize objects with better precision compared to Alex Net. Moreover, theperformance of trained CNN\u2019s vary substantially across different categories of objects and we, therefore, will discuss the possible reasons for this.","Breast cancer became the major source of mortality between women. The accessibility of healthcare datasets and data analysis promote the researchers to apply study in extracting unknown pattern from healthcare datasets. The intention of this study is to design a prediction system that can predict the incidence of the breast cancer at early stage by analyzing smallest set of attributes that has been selected from the clinical dataset. Wisconsin breast cancer dataset (WBCD) have been used to conduct the proposed experiment. The potential of the proposed method is obtained using classification accuracy which was obtained by comparing actual to predicted values. The outcome confirms that the maximum classification accuracy (99.28%) is achieved for this study.","This study introduces a novel model designed to classify macrocauses of violent crimes. The model\u2019s practical application is demonstrated through its integration into the framework of the Natal Smart City Initiative in Brazil. Utilizing the Design Science methodology, the study details the model\u2019s development, its subsequent implementation through a machine learning pipeline, and its assessment employing four prominent classification techniques: Decision Trees, Logistic Regression, Random Forest, and XGBoost. XGBoost performed exceptionally well, achieving an average accuracy of 0.961791, an F1-Score of 0.961410, and an AUC of ROC curve of 0.994732. Accurate classification of criminal macrocauses is crucial for developing effective public safety policies. The proposed model can provide public safety institutions and criminal analysts with a valuable tool for better understanding aspects related to violent crime analysis in their cities. This can streamline the analysis and management process and provide more accurate information for decision-making. This study also has important implications for the emerging field of smart cities. By providing a tool to assist in decision-making and planning public safety strategies, this work contributes to the development of innovative, data-based, and theory-based solutions to address urban challenges.\nHighlights\n\u2022\nWe propose a model for classifying criminal macrocauses to help criminal analysts.\n\u2022\nWe compared Decision Trees, Random Forest, Logistic Regression, and XGBoost.\n\u2022\nXGBoost obtained the best results (0.96 accuracies) after statistical analysis.\n\u2022\nUsing feature engineering techniques and the SMOTE-NC to handle unbalanced data.","This study addresses challenges in land use and cover identification using remote sensing (RS) imagery, focusing on the Uppal region. By leveraging deep learning models, particularly an optimized ResNext-50 architecture, we aim to enhance efficiency and accuracy in classifying land features. Our approach integrates Landsat-8 and hyper-spectral satellite data, utilizing preprocessing techniques like dark subtraction, stacking, merging, and spectral enhancement. Principal Component Analysis (PCA) is applied to streamline high-dimensional feature sets obtained from pre-processed spectral data. We further employ hybrid NSCT-FDCT fusion for integrating Landsat-8 and hyperspectral images. The resulting fused image is fed into our classification process, utilizing the modified ResNext50 (Deep Learning Architecture) model with Reptile Search Optimization for weight link optimization. Notably, our proposed method achieves impressive outcomes: 97% accuracy, 96% sensitivity, 99% specificity, 3% error, 97% precision, and a 95% Matthew Correlation Coefficient. This demonstrates the efficacy of our approach in predicting diverse land covers within the Uppal region, showcasing the potential of Landsat-8 and Hyper-spectral data for accurate land use and cover identification.","Existing class incremental learning methods typically employ knowledge distillation to minimize discrepancies in model outputs. However, these methods are restricted by the mismatch between quondam knowledge and new data. To alleviate these issues, we introduce semantic alignment decouples the classification and distillation in different semantic spaces. The unmatched new data is regarded as out-of-distribution data on the old class distribution, and the corresponding pseudo-labels are attached to the new data using the original network. Intuitively, the pseudo-labels could be consistently preserved in the old semantic space. Moreover, we develop auxiliary self-supervised classifiers to learn more generalized representation, enabling a better stability-plastic trade-off. Furthermore, self-distillation is employed to refine self-supervised knowledge from auxiliary classifiers. Extensive experiments demonstrate that our method achieves the best performance on CIFAR100, ImageNet100, ImageNet, CUB200, and Stanford-Dogs120 datasets. Notably, our method outperforms existing methods by a substantial margin when only one old exemplar is stored per class, i.e., 11.34% and 21.46% improvement on CIFAR100 of 5 phases and 10 phases, respectively.","The QRS complex is the significant wave of electrocardiogram (ECG) and occurs during ventricular depolarization. If the synchronization problem (i.e., depolarization) occurs between endocardial cardiomyocytes and outer layers, then abnormal morphological changes in QRS complex take place which is possible in case of arrhythmia and ischemia. The proposed approach describes time domain measures of such alterations (i.e., the ratio of average rise &amp; fall amplitude and interval) of QRS complex along with the frequency domain feature i.e., peak frequency and power of mean QRS complexes. An improvised Difference Operation Method (DOM) (Yeh Yun-Chi et al., 2008) is implemented with added features like preprocessing techniques (e.g., baseline drifts and noise cancellation). The proposed methodology is evaluated with the standard databases, i.e., FANTASIA (healthy subjects), MIT-BIH Arrhythmia database (MITDB), and European ST-T database (EDB) respectively. Linear Discriminant Analysis (LDA) and decision tree are carried out for classification healthy, arrhythmic and ischemic subjects incorporating the time-frequency domain characteristics. The Naive Bayes\u2019 Classifiers also implemented where these two classes, i.e., ischemia and arrhythmia are visually distinguished by convergently spreading in different areas. Real-time hardware validation of this approach has also executed. The future scope of this approach is to be validated with ischemia-induced arrhythmia subjects for precise identification and classification.","Machine Learning (ML) and feature extraction techniques have shown a great potential in medical imaging field. This work presents an effective approach for the identification and classification of thyroid nodules. In the proposed model, various features are extracted using Grey Level Co-occurrence Matrix (GLCM), Local Binary Pattern (LBP) and intensity-based matrix. These features are fed to various ML classifiers like K-Nearest Neighbour (KNN), Decision-Tree (DT), Artificial Neural Network (ANN), Na\u00efve Bayes, Extreme Gradient Boosting (XGBoost), Random Forest (RF), Linear Regression (LR) and Support Vector Machine (SVM). From the result analysis, it can be observed that proposed Model-4 has performed better in comparison with the rest of seven proposed models with the reported literature. An improvement of 4% to 5% is seen in performance evaluation of model in comparison with reported literature.","Precise prediction of cancer survival is of paramount importance in aiding clinicians in formulating tailored treatment strategies. Such strategies have the potential to enhance the quality of life for individuals with cancer and lower the mortality rates associated with this disease. Recent research has underscored the significance of integrating data from various sources, including genomics and histopathologic images, as a pivotal step towards enhancing the accuracy of cancer survival prediction. Although these studies have achieved promising results, utilizing the consensus and complementarity of multimodal data for efficient learning of comprehensive multimodal representations poses an enormous challenge. In response to this challenge, we propose a Multimodal Disentangled Representation Learning (MMDR) method, aimed at acquiring modality-specific and shared representations, thus affording a comprehensive perspective of multimodal data. Empirical findings confirm that this approach yields notable enhancements in performance when compared to alternative methodologies.","Multi-instance learning (MIL), as a special version of classification, focuses on labeled sets (bags) consisting of unlabeled instances and has drawn accumulative attention due to its significant importance in practical applications. However, most existing MIL methods just utilize partial information (bags or instances) of MIL data to construct the kernel function, resulting in deteriorated classification performance of MIL. In this paper, we propose a Double Similarities weighted Multi-Instance Learning (DSMIL) kernel framework, which utilizes the similarities of Bag-to-Bag (B2B) and Instance-to-Bag (I2B). In the proposed kernel framework, the similarities of B2B and I2B could be derived from the prototypes distance of inter-bag and similarity matrix of intra-bag, respectively, based on the affinity propagation (AP) clustering of the bag. Meanwhile, we give theoretical proof of the validity of the designed kernel function. Experimental results on benchmark and semi-synthetic datasets show that our proposed method obtains competitive classification performance and achieves robustness to parameters and noise.\nHighlights\n\u2022\nA double similarities weighted MIL kernel is proposed and proved as a valid kernel.\n\u2022\nThe DSMIL integrates the similarities of B2B and I2B into the kernel function.\n\u2022\nThe DSMIL obtains competitive results on benchmark datasets and newsgroups dataset.","Identifying client needs to provide optimal services is crucial in tourist destination management. The events held in tourist destinations may help to meet those needs and thus contribute to tourist satisfaction. As with product management, the creation of hierarchical catalogs to classify those events can aid event management. The events that can be found on the internet are listed in dispersed, heterogeneous sources, which makes direct classification a difficult, time-consuming task. The main aim of this work is to create a novel process for automatically classifying an eclectic variety of tourist events using a hierarchical taxonomy, which can be applied to support tourist destination management. Leveraging data science methods such as CRISP-DM, supervised machine learning, and natural language processing techniques, the automatic classification process proposed here allows the creation of a normalized catalog across very different geographical regions. Therefore, we can build catalogs with consistent filters, allowing users to find events regardless of the event categories assigned at source, if any. This is very valuable for companies that offer this kind of information across multiple regions, such as airlines, travel agencies or hotel chains. Ultimately, this tool has the potential to revolutionize the way companies and end users interact with tourist events information.\nGraphical Abstract\nDisplay Omitted\nHighlights\n\u2022\nComputational techniques are used to classify tourism destination events.\n\u2022\nA Large Language Model (BERT) is used to get vectorial representations of events.\n\u2022\nA method to automatically classify events is proposed to easy the adoption of standards.\n\u2022\nThere is great scope for extending this methodology to other applications.","Interior style classification is an interesting problem which has potential applications both commercial and academic communities. This task aims to devise interior design styles automatically. Thus, interior designers will explore customers\u2019 tastes and then precisely provide suggestions for decor inspiration based on their preferences. Recently, Convolutional Neural Networks (CNNs) have been considered the de-facto standard in computer vision tasks. Therefore, several current works have tended to address interior style classification using CNN-based architectures. Moreover, transformer-based architectures and attention-based encoder\u2013decoder models have been proven successfully in computer vision and natural language processing tasks. Sequentially, more studies have been arguing the efficiency of combining CNN-based architectures and transformer-based architectures for normal image classification problems. In this project, we focus on finding an architecture network that is suitable for the interior style classification problem. We propose a robustness method to address interior style design classification, named ISC-DeIT. The proposed method is based on Data-efficient image transformer architectures and knowledge distillation, which can be trained on small datasets effectively. Especially, a proposed additional module is plugged to leverage learning feature representations for improving predictive accuracy. Experiments were carried out on a new curated dataset with five interior styles including Art-Decor, Hitech, Indochina, Industrial, and Scandinavian. Empirical results of ISC- DeiT indicated that the ability of prediction for interior style classification of the proposed method has been increased significantly, compared with other state-of-the-art methods.","Automatic classifier accuracy evaluation (ACAEval) on unlabeled test sets is critical for unseen real-world environments. The use of dataset-level regression on synthesized meta-datasets (comprised of many sample sets) has shown promising results for ACAEval. However, the existing meta-dataset for ACAEval is created using simple image transformations such as rotation and background substitution, which can make it difficult to ensure a reasonable distribution shift between the sample set and the test set. When the distribution shift is large, it becomes challenging to estimate the classifier accuracy on the test set using those sample sets. To ensure more robust ACAEval, this paper attempts to customize a meta-dataset in which each sample set has a reasonable distribution shift to the test set. An intra-class cycle-consistent adversarial learning (ICAL) method is introduced to transfer the style of a labeled training set to the style of the test set, by jointly considering the domain shift issue, the label flipping issue (the semantic information may be changed after style transformation), and the diversity of multiple sample sets in the meta-dataset. Experiments validate that under the same experimental setup, our method outperforms the existing ACAEval methods by a good margin, and achieves state-of-the-art performance on several standard benchmark datasets, including digit classification and natural image classification.\nHighlights\n\u2022\nTwo authors named Yan Huang in pinyin, one PostDoc (1st author), the other an Associate Prof (3rd author).\n\u2022\nPaper on ACAEval for classifier accuracy. Uses meta-dataset technique for unlabeled real-world data.\n\u2022\nCustomized meta-dataset for ACAEval to address distribution shift between samples and test set.\n\u2022\nOur sample set considers label flip issue. Introduces random indicator and FD margin loss.\n\u2022\nExperiments demonstrate our ACAEval matches existing methods and surpasses dataset-level regression.","In recent years, class-imbalanced learning has become an important branch of machine learning. Synthetic Minority Oversampling Technique (SMOTE) is known as a benchmark method to address imbalanced learning. Although SMOTE performs well on many data, it also has the drawback of generating noisy samples. There are many SMOTE variants to solve this problem. Specifically, these methods are hybrid sampling methods, that is, carrying out an undersampling stage after SMOTE to remove noisy samples. It requires a method that can accurately identify noise to provide reliable performance. In this paper, a hybrid re-sampling method based on SMOTE and a two-layer nearest neighbor classifier (SMOTE-kTLNN) is proposed. SMOTE-kTLNN recognition noise is realized by an Iterative-Partitioning Filter (IPF). Specifically, SMOTE is performed on the original data to balance the data, then the data is divided into n equal parts, establishing kTLNN on each part to predict the whole data. And noisy samples are removed according to the majority voting rule. In the last, the balanced data sets are used to train kNN, AdaBoost, and SVM to verify whether SMOTE-kTLNN is irrelevant to the classifier. The experiment results demonstrate that SMOTE-kTLNN performs better than the comparisons in 25 binary data sets, including Recall, AUC, F1-measure, and G-mean.","In image classification task, imbalanced dataset is a problem that often occurs. Batik pattern data also suffers this problem, mainly because of the poor quality of available images and rarity of certain patterns. In this research, we employed a novel ad- vanced augmentation and oversampling techniques on the imbalanced dataset to address this issue. This approach enhanced the diversity of the images, encompassing variations in color, contrast, wrinkles, and warps that may be present in batik garments. We employed two CNN models, DenseNet169 and VGG-16, along with three different training methods for our study. These methods included training without oversampling and advanced augmentation, training with oversampling, and training with both oversampling and advanced augmentation. The results showed that the best accuracy was achieved with DenseNet169 with our oversampled and augmented dataset, with an accuracy of 84.62%. Additionally, VGG-16 also performed well on said dataset, achieving an accuracy of 82.56%.Our results suggested that by using our oversampling &amp; advanced augmentation on the dataset,the model performance improved compared to plain data and oversampled data.","This paper features convolutional neural network (CNN) models on Clifford algebras applied to a medical image classification task, namely the diagnosis of acute lymphoblastic leukemia (ALL). ALL is a type of cancer identified by malformed lymphocytes, known as lymphoblasts, in the bloodstream. The image classification task aims to discriminate healthy cells from lymphoblasts. This work shows that CNNs featuring parameters in Clifford algebras significantly outperform real-valued networks of equivalent size in this application. Indeed, the real-valued and a Clifford CNN achieved an average accuracy of 94.60% and 97.02%, respectively, in the ALL-IDB dataset with a 50% train-test split. Moreover, we present smaller versions of Clifford CNNs with roughly 75% fewer parameters that yielded a 96.50% average accuracy. The results reported in this work are comparable to high-end models in the literature despite having several orders of magnitude fewer parameters.","This work constitutes the first approach for automatically classifying the surface that the voiding flow impacts in non-invasive sound uroflowmetry tests using machine learning. Often, the voiding flow impacts the toilet walls (traditionally made of ceramic) instead of the water in the toilet. This may cause a reduction in the strength of the recorded audio signal, leading to a decrease in the amplitude of the extracted envelope. As a result, just from analysing the envelope, it is impossible to tell if that reduction in the envelope amplitude is due to a reduction in the voiding flow or an impact on the toilet wall. In this work, we study the classification of sound uroflowmetry data in male subjects depending on the surface that the urine impacts within the toilet: the three classes are water, ceramic and silence (where silence refers to an interruption of the voiding flow). We explore three frequency bands to study the feasibility of removing the human-speech band (below 8 kHz) to preserve user privacy. Regarding the classification task, three machine learning algorithms were evaluated: the support vector machine, random forest and k-nearest neighbours. These algorithms obtained accuracies of 96%, 99.46% and 99.05%, respectively. The algorithms were trained on a novel dataset consisting of audio signals recorded in four standard Spanish toilets. The dataset consists of 6481 1-s audio signals labelled as silence, voiding on ceramics and voiding on water. The obtained results represent a step forward in evaluating sound uroflowmetry tests without requiring patients to always aim the voiding flow at the water. We open the door for future studies that attempt to estimate the flow parameters and reconstruct the signal envelope based on the surface that the urine hits in the toilet.","Diabetic retinopathy, a condition characterized by retinal damage and vision loss, is a prevalent complication of diabetes arising from elevated blood sugar levels. With a growing number of individuals affected, efficient and accurate diagnosis is crucial. This study aims to implement and compare the Local Binary Pattern (LBP) and Gray Level Co-occurrence Matrix (GLCM) feature extraction techniques, which have demonstrated success in prior research. The comparison will provide a comprehensive under- standing of the image features, extract relevant data, and improve the performance of the image analysis pipeline for diabetic retinopathy classification. The result showed that from three scenarios the best accuracy provided by Support Vector Machine with the accuracy score between 73% until 74%, however, other algorithm have little difference which the result on 73%.","The recognition and classification of wire melted marks is crucial in modern fire investigation. The existing technology mainly uses physical or chemical methods to deal with wire melted marks and draws conclusions through manual observation, or manually extracts the features and train classification model, both of which consume excessive manpower and resources. The research on automatic feature extraction and recognition of wire weld marks by artificial intelligence technology is still blank. Based on the data set of wire melted mark metallographic images provided by a city fire research institute, we proposed an algorithm to recognize the type of wire melted mark metallographic images based on artificial intelligence which can help fire fighters efficiently speculate the cause of fire. In the algorithm, the TransUnet network is used to segment the melted zone by semantic segmentation to extract the melted zone containing the main features, and the mIOU reaches 92.2%. Then, the features of wire melted mark are extracted based on the melted zone image. Finally, XGBoost is used for feature modeling for classification. The F1 Score of model is 82.9%.","Skewed class proportions in real-world datasets present a challenge for machine learning algorithms, as they have a tendency to correctly categorize the majority class while incorrectly classifying the minority class. Such classification disparities hold significant implications, particularly in predictive scenarios involving minority groups, where misclassifying minority instances could lead to adverse outcomes. To tackle this, class imbalance learning has gained attention, with the Synthetic Minority Oversampling Technique (SMOTE) being a notable approach that addresses class imbalance by generating synthetic instances for the minority class based on their feature space neighbors. Despite its effectiveness and simplicity, SMOTE is known to suffer from a noise propagation issue where noisy and uninformative samples are introduced. While various SMOTE variants, including hybrids with undersampling, have been developed to tackle this problem, identifying noisy samples in complex real-world datasets remains a challenge. To address this, our study introduces a new SMOTE-based hybrid approach called SMOTE-centroid displacement-based k-NN (SMOTE-CDNN). SMOTE-CDNN employs centroid displacement for class prediction, which is more robust against noisy data. After SMOTE is applied, noise instances are detected and removed for clearer decision boundaries if their labels predicted by our centroid displacement-based k-NN algorithm are different from the real ones. While our experiments on 24 imbalance datasets demonstrate the resilience and efficiency of our proposed algorithm, which outperforms state-of-art resampling algorithms with various classification models, we acknowledge the need for further investigation into specific dataset characteristics and classification scenarios to determine the generalizability of our approach.\nHighlights\n\u2022\nSkewed class proportions pose a challenge for machine learning.\n\u2022\nSMOTE is widely used but has a noise propagation issue.\n\u2022\nA novel hybrid variant, SMOTE-CDNN, uses k-NN with centroid displacement.\n\u2022\nIt detects and removes noise instances after SMOTE is applied.\n\u2022\nExperiments show its efficiency and outperformance of other algorithms.","With the exponential growth of various data interactions on network systems, network intrusions are also increasing. The emergence of edge computing technology brings a new solution to network security. However, due to the difficulty of processing massive and unbalanced data at the edge, higher accuracy requirements are necessary for deployed detection models. This paper proposes a multi-classification model for network intrusion detection based on reconstruction and feature matching. This model can be deployed on small-scale edge nodes, effectively identifying various attack behaviors through the utilization of reconstruction errors and adaptive scaling. Furthermore, we proposed a model transfer method based on feature matching to enhance the training and detection efficiency of multi-classification models under different data distribution conditions. The proposed model has been evaluated on the CICIDS2017 dataset in terms of accuracy, recall, precision and F1 score. The model demonstrates high accuracy for normal flows in the network, majority class attacks, and minority class attacks, achieving an overall multi-class accuracy of 99.81%, outperforming similar models. Furthermore, this model demonstrates faster convergence and training speed after feature matching, exhibiting better robustness and outstanding performance at the edge.","Ordinal classification of imbalanced datasets is a challenging problem that occurs in many real-world applications. The main challenge is to simultaneously consider the classes ordering and imbalanced distribution. Although the classic synthetic instances oversampling techniques can improve the identification of minority classes, they easily incur the damage of the classes ordering when the synthetic instances fall in non-adjacent classes regions. In this paper, we propose a powerful method for handling the imbalanced problem embedded in the ordinal classification, namely Iterative Minority oversampling technique for imbalanced Ordinal Classification (IMOC). Concretely, we first develop an iterative identification procedure to select the minority instance that is hardest to learn. Then, a weighted oversampling probability distribution that respects the ordinal nature is used to generate synthetic minority instances to balance the skewed distribution. Furthermore, two novel ensemble versions are developed to boost the capability of our proposed IMOC. In order to verify the effectiveness and robustness of our proposed methods, an extensive experimental study is carried out on a large number of datasets from real-world applications. The experimental results supported by proper statistical tests indicate that our proposed methods outperform state-of-the-art algorithms in terms of the most frequently used performance measures.","The Healthcare Analytics(HcA) is a process in which clinical data is analyzed and patient\u2019s treatment is performed. The treatment depends on the analysis of clinical data accumulated from Electronic Health Records (EHRs), pharmaceutical and research and development cost and claims of patient. Lung cancer is the most common among cancer disease and the foremost reason for deaths in both men and women. In this research work EHRs are analyzed and the survivability rate is predicted for lung cancer. Researchers apply Machine Learning Techniques (MLT)for predicting the survivability rate so that chemotherapy can be provided for cancer affected people. MLTare well accepted by doctors and work well in diagnosing and predicting cancer. An ensemble of Support Vector Machine (SVM), Naive Bayes (NBs)and classification trees (C4.5) can be used to evaluate patterns that are risk factors for lung cancer study. The North Central Cancer Treatment Group (NCCTG) lung cancer data set along with new patient data is used for evaluating the performance of support SVM, NBs and C4.5. The comparison isbased on accuracy, Area Under the Curve(AUC), Receiver Operating Characteristic (ROC) and the resultshows that C4.5 performs better in predicting lung cancer with the increase in training data set.","Data classification is an important and challenging issue encountered in many practical applications. The classifier based on Evidential Reasoning Rule (ER Rule) can well handle the uncertainty in classification and obtain competitive accuracy. However, there are many parameters to be optimized, and the computation cost of the usually adopted optimization strategy is relatively high. This fact weakens the application advantage of ER Rule classifier. To address this challenge, an asynchronous optimization approach is proposed. In the original ER Rule classifier, feature referential values and evidence weights are optimized synchronously; while in the proposed method, these two types of parameters are optimized separately based on their essential impacts on the classification results. For the optimization of feature referential values, the objective function measures the quality of belief matrix, which is the critical strategy to improve the computational efficiency. Three types of feasible objective functions are constructed. After obtaining optimal referential values, evidence weights are optimized based on the actual classification effect, and the required iterations decrease. Various experiments on 14 publicly available datasets verify the computational efficiency and classification performance of the proposed method.\nHighlights\n\u2022\nThe low computational efficiency of ER Rule-based classifier is addressed by optimizing its parameters asynchronously.\n\u2022\nThe objective function of feature referential values optimization measures the quality of belief matrix.\n\u2022\nThe effectiveness of the proposed method is validated by theoretical analysis and various experiments.","Drug repurposing, which involves using already approved drugs for new clinical targets, represents a cost-effective alternative to the development of new drugs. In this study, we introduce an innovative computational strategy, which uses Non-negative Matrix Tri-Factorization (NMTF) to generate vector embeddings of given sizes for drugs and drug targets; vector embeddings are then employed to generate predictions for drug repurposing using conventional classifiers, like random forest, logistic regression, and multi-layer perceptron.\nOur approach leverages the NMTF method within a new approach to classification, named two-tower architecture, which is effective in solving complex tasks, such as the optimal prediction of targets for already approved drugs. This approach produces robust models, with AUROC reaching 0.90, which outperform traditional NMTF. We evaluate our method in the context of Parkinson\u2019s Disease; within the newly revealed drug-target predictions, we highlight compounds that exhibit potential in mitigating neurodegeneration, thereby revealing a potentially useful drug in relationships with a well-identified target.","The fields of energy storage, photocatalysis, and sensors have undergone substantial technological advancements, which have led to the generation of vast amounts of data on electrochemical impedance (EIS). The interpretation of large amounts of EIS data is a challenging task since the analysis of EIS data requires multiple steps to get a suitable equivalent circuit. Recently, some progress has been made in the machine learning (ML) model for EIS classification. However, most of the ML models are performed as a \u201cblack box\u201d model, which provides only the classification result and lacks physical descriptor representation. Here, we apply variational autoencoders (VAE) to EIS data analysis, which includes classification, parameter prediction, and the visualization of physical descriptors. The VAE model performed well in the classification task, with an accuracy of 82.0%\u201392.4%. In the prediction task, VAE shows a high R-squared value on the Randles circuit. Additionally, the VAE model can map physical descriptors to the latent space, allowing the latent space to transform into a property space, which plays an important role in the optimization and exploration of novel materials research.\nGraphical abstract\nDisplay Omitted\nHighlights\n\u2022\nAutomatically analyze the EIS spectrum.\n\u2022\nVAE model design for classification and regression.\n\u2022\nThe high accuracy classifier of the EIS circuit model with accuracy of 82.0%\u201392.4%.\n\u2022\nThe use of VAE model for the exploration and visualization of EIS dataset.","Network modeling has proven to be an efficient tool for many interdisciplinary areas, including social, biological, transportation, and various other complex real-world systems. In addition, cellular automata (CA) are a formalism that has received significant attention in recent decades as a model for investigating patterns in the dynamic spatio-temporal behavior of these systems, based on local rules. Some studies investigate the use of cellular automata to analyze the dynamic behavior of networks and refer to them as network automata (NA). Recently, it has been demonstrated that NA is effective for network classification, as it employs a Time-Evolution Pattern (TEP) for feature extraction. However, the TEPs investigated in previous studies consist of binary values (states) that do not capture the intrinsic details of the analyzed network. Therefore, in this work, we propose alternative sources of information that can be used as descriptors for the classification task, which we refer as Density Time-Evolution Pattern (D-TEP) and State Density Time-Evolution Pattern (SD-TEP). We examine the density of alive neighbors of each node, which is a continuous value, and compute feature vectors based on histograms of TEPs. Our results demonstrate significant improvement over previous studies on five synthetic network datasets, as well as seven real datasets. Our proposed method is not only a promising approach for pattern recognition in networks, but also shows considerable potential for other types of data that can be transformed into network.\nHighlights\n\u2022\nA Network Classification Method based on network automata is proposed.\n\u2022\nConway\u2019s life cellular automata are used as Network Automata model.\n\u2022\nTime Evolution Patterns Extracted from Network Automata represents the networks.\n\u2022\nResults on real-world and synthetic networks outperforms previous methods.","Highlights\n\u2022\nOrigin and variety identification of Chrysanthemum were studied by NIR-HSI.\n\u2022\nAdvanced deep learning and visualization methods were utilized for classification.\n\u2022\nFew-shot Class-Incremental learning using the Replay training strategy was studied.\n\u2022\nGood performances were obtained by Few-shot Class-Incremental learning approach.\nAbstract\nChrysanthemum, a traditional Chinese medicine, possesses diverse pharmacological effects with a myriad of origins and varieties. Due to the difficulty of acquiring and modeling all Chrysanthemum varieties comprehensively, it becomes imperative to establish models based on the available samples in order to swiftly identify newly emerging Chrysanthemum categories from a limited dataset. In this study, hyperspectral imaging combined with deep learning was exploited for the classification of fourteen Chrysanthemum categories by origin and variety. Leveraging the convolutional neural network, the few-shot class-incremental learning (class-IL) method was applied to the detection of few-shot Chrysanthemum categories. By employing a Replay training strategy, the challenges associated with severely sample-limited and unbalanced classes can be effectively addressed. When incrementally expanding from four to fourteen categories, with each new category consisting of only 30 samples, the achieved accuracy on the test dataset reached 80.13 %. This remarkable performance exhibited a narrow margin of 15.75 % compared to conventional supervised learning, which utilized an incremental training sample size nearly 100 times larger. This approach consistently outperforms conventional supervised learning methods, thereby showcasing its remarkable scalability. It facilitates the practical implementation of few-shot learning and deep learning models, providing a substantiated framework to tackle real-world scenarios in various domains using hyperspectral imaging and related techniques.","Social media text can be classified in different ways, viz sentiment analysis, humour detection, hate speech detection and hope speech detection. Multitask learning (MTL) models built on Large Language Models (LLMs) eliminate the need to build separate models for each of these tasks. However, building MTL models by fully fine-tuning the LLM has limitations such as catastrophic forgetting and requiring complete retraining to add a new task. AdapterFusion was introduced to address these limitations. However, existing AdapterFusion techniques have not been experimented with code-mixed or code-switched text. Moreover, they only considered task-based AdapterFusion on monolingual LLMs. However, using monolingual LLMs is sub-optimal in classifying code-mixed or code-switched text. A better alternative is multilingual LLMs. In this paper, we present an MTL model that combines task AdapterFusion with language adapters on top of a multilingual LLM. We combine language adapters sequentially, in parallel, and as a fusion with task adapters to capture cross-lingual knowledge in code-mixed and code-switched text. We believe that this is the first research to introduce language-based AdapterFusion.","Convolutional neural network (CNN) and its variants have been widely applied to hyperspectral classification for their excellent ability to extract local features. However, as research on hyperspectral imaging (HSI) has progressed, CNNs have been proven to struggle in extracting and representing the sequential properties of spectral characteristics. Recently, some researchers have demonstrated the feasibility of transformer architecture in HSI classification due to its powerful ability to characterize spectral information. The lack of suitable pre-processing and optimization methods which are used for the transformer's application in HSI becomes a major limitation to model's performance. Therefore, to enhance model's performance and practicality, we propose an efficient transformer backbone for HSI classification, named Efficient-Spectralformer. In the proposed framework, we rethink the input form and design the Split Grouping Embedding (SGE) module that requires much less computational resources. Additionally, to maximize the use of attentional feature information from different layers, we have used the Multi-layer Feature Fusion (MFF) module is used to learn multi-layer spectral attention information. The experiments conducted on five HSI datasets demonstrate that the proposed method achieves better performance under much less memory usage (about 30% of the original on average) by comparing with the original Spectralformer.","Support vector machine (SVM) is widely recognized as an effective classification tool and has demonstrated superior performance in diverse applications. However, for large-scale pattern classification problems, it may require much memory and incur prohibitive computational costs. Motivated by this, we propose a new SVM model with novel generalized ramp loss (L R-SVM). The first-order optimality conditions for the non-convex and non-smooth L R-SVM are developed by the newly developed P-stationary point, based on which, the L R support vectors and working set of L R-SVM are defined, interestingly, which shows that all of the L R support vectors are on the two support hyperplanes under mild conditions. A fast proximal alternating direction method of multipliers with working set (L R-ADMM) is developed to handle L R-SVM and L R-ADMM has been demonstrated to achieve global convergence while maintaining a significantly low computational complexity. Numerical comparisons with nine leading solvers show that L R-ADMM demonstrates outstanding performance, particularly when applied to large-scale pattern classification problems with fewer support vectors, higher prediction accuracy and shorter computational time.\nHighlights\n\u2022\nThe new sparse and robust SVM model. We construct a new sparse and robust generalized ramp loss SVM.\n\u2022\nThe novel support vectors. To reduce the scale of the training set, we define the support vectors of generalized ramp loss SVM.\n\u2022\nThe efficient new algorithm. To solve the generalized ramp loss SVM, we design a new alternating direction method of multipliers with working set.\n\u2022\nHigh numerical performance. Numerical experiments show that the proposed algorithm has excellent performance.","Abstract. In the past decade, deep learning based methods have taken a dominant position in natural language processing (NLP). For almost all NLP tasks, deep learning based methods far surpassed traditional methods. Especially in the past five years, the development of deep learning methods has been particularly rapid. For example, the pre-training and fine-tuning paradigms represented by BERT have dominated the NLP field, while also driving the development of other fields such as computer vision. Nowadays pre-trained large language models (LLMs) such as GPT-3/ChatGPT further demonstrate the advantages of Transformer based deep learning methods, which can achieve good results across various problems without any specialized training. In spite of the remarkable success, their performances still underperform fine-tuned models in the task of text classification in some scenarios. Nevertheless, the LLMs are good generalist models. The goal we pursue is the deep learning methods with good generalization ability. In the case of limited computing resources and high performance requirements, the fine-tuned models remain the first choice. So how is the generalization ability of the fine-tuned models? In this paper, we explore the generalization of representative Chinese text classification methods based on deep learning. The experimental results indicate that Transformer based methods present good ability of generalization on two significant different Chinese datasets. In the current era of LLMs, this work can assist us in choosing more appropriate solutions for natural language processing tasks.","In this research, a method was developed for utilizing voice commands with programmable logic controllers (PLCs) and supervisory control and data acquisition (SCADA) systems, which are commonly utilized in industrial automation. This approach incorporates artificial intelligence to enable human\u2013machine interaction, aligning with the trends of Industry 4.0. A deep neural network was specifically designed for speech recognition, eliminating the need for reliance on any pre-existing speech-to-text engines. The objective was to create a model that is accurate and compact in size, making it suitable for embedded systems within industrial systems. To train the deep learning network, 21,600 sound files were generated. These files combined real factory noise with a synthetic dataset of human speech, forming a dataset comprising 60 different classes of voice commands. These commands encompassed actions like starting, stopping, and operating at various speeds for 10 motors controlled by the automation system. After applying the Mel-frequency cepstral coefficient (MFCC) to the voice commands, the resulting data was directly fed into the proposed network. The network achieved an impressive accuracy rate of 99.73%. Notably, the proposed network outperformed even networks several times its size.","One important aspect of human behavior understanding is the recognition and monitoring of daily activities. An accurate activity recognition system can improve the quality of life in many key areas. The multi-metric feature extraction and DeepForest classifier designed in this paper effectively had solved the problems of incomprehensive feature extraction and insufficient classifier accuracy.\nFirst, this paper extracts a total of 450 dimensional feature vectors using mean, variance, maximum, skewness, minimum, kurtosis, regression of independent variables and sample entropy as feature indicators, so that the features of the original vectors can be presented comprehensively. The dimensionality of the feature set is too large, so PCA is used to reduce the dimensionality of the high-dimensional feature vector. After studying the relationship between the dimensionality reduction and the cumulative contribution rate, it is concluded that the cumulative contribution rate reaches 90% when the dimensionality is 15, which can retain the original features better. Then DeepForest was used as the classifier, and the reduced-dimensional feature set was used as the sample set to divide the training set and test set by 3:1. The model was tested on the test set after training, and the classification accuracy of the test set was 98.202%. Six classifiers, GaussianNB, SVM, K-NN, XGBoost, RandomForest and DecisionTree, were selected as the control experimental group, of which only the RandomForest model reached 97%, while the rest of the control models did not achieve 95% effect, indicating that DeepForest was more accurate in classifying human activities.\nNext, the generalisation ability of the DeepForest classifier was assessed using Monte Carlo Cross Validation (MCCV), K-fold and its confusion matrix. The MCCV validation used 50% of the data as the training set and 30% of the data as the test set, and set the number of splits to 10, resulting in a mean accuracy of 98.227%. The size of K in the K-fold validation was determined to be 8 based on the number of people conducting the human activity experiment, and the final mean accuracy value obtained was 97.790%. The combined confusion matrix from the K-fold validation (which aggregates the confusion matrix for each classification result) was calculated, and the results showed that the highest accuracy reached 100% for A9 and A14 classifications, where A5 and A10, A15 and A10, A12 and A5 were more likely to be confused, with the highest error rate for A5 classification, which was 4.167%, and the rest of the activity classifications were better.","Neighborhood rough set theories are commonly used in global feature selection to achieve high performance in continuous data classification. However, selecting a single feature subset to represent the entire dataset may degrade the performance when there are intra-class dissimilarities among objects. Therefore, this paper proposes a novel feature-selection method, Granule-specific Feature Selection (GFS) to select local feature subsets for continuous data classification. The feature selection approach constitutes a novel feature selection algorithm and a novel feature evaluation function and uses existing approaches for granule identification and classification with some adjustments. The neighborhood rough set theories are used in granule (subclass) identification within each class when there are no subclass label information available in the training data, while an improved k-Nearest Neighbors approach is used in classification with granule-specific feature subsets. Experimental results show GFS outperforms most of the global, class-specific, and local feature selection baselines in terms of classification performance.\nHighlights\n\u2022\nGranule-specific feature selection using neighborhood rough set theories.\n\u2022\nFeature selection while managing intra-class dissimilarities of objects.\n\u2022\nHandle data uncertainty of continuous data when selecting features.\n\u2022\nImprove classification performance using granule-specific feature subsets.","This paper presents a novel framework that enables the generation of unbiased estimates for test loss using fewer labeled samples, effectively evaluating the predictive performance of classification models in data-limited applications. The framework\u2019s key innovation lies in developing an adaptive sampling distribution that iteratively identifies influential testing samples based on interactions between learner and evaluator agents. Notably, the adaptive distribution dynamically adjusts the evaluator agent\u2019s supervisory role by prioritizing inputs with discrepancies between the agents and considering the evaluator\u2019s uncertainty. Comprehensive experimental analyses on synthetic data and two sparse data sets from material extrusion additive manufacturing problems validate the framework\u2019s superiority over uniform and fixed sampling distributions. First, the proposed framework provides unbiased estimates of the test loss across various data sets, sampling ratios, and evaluator models. Second, the introduced adaptive sampling distribution significantly reduces the standard deviation of the test loss estimator compared to uniform sampling, achieving a 50% reduction for a 10% sampling ratio in the filament selection benchmark. Third, the framework demonstrates its efficacy in model selection to determine the optimal number of hidden units with a reduced number of test samples. Overall, this work offers a promising framework for evaluating classification models in applications where acquiring labeled data is time-consuming and resource-intensive, including materials science and engineering.","Internet traffic classification plays a key role in network visibility, Quality of Services (QoS), intrusion detection, Quality of Experience (QoE) and traffic-trend analyses. In order to improve privacy, integrity, confidentiality, and protocol obfuscation, the current traffic is based on encryption protocols, e.g., SSL/TLS. With the increased use of Machine-Learning (ML) and Deep-Learning (DL) models in the literature, comparison between different models and methods has become cumbersome and difficult due to a lack of a standardized framework. In this paper, we propose an open-source framework, named OSF-EIMTC, which can provide the full pipeline of the learning process and simulation reproducibility. From well-known datasets to extracting new and well-known features, it provides implementations of well-known ML and DL models (from the traffic classification literature) as well as experimental test-beds and their evaluation. By providing a standardized platform, OSF-EIMTC enables repeatable, reproducible, and accurate comparisons of both established and novel features and models. As part of our framework evaluation, we demonstrate the reproducibility of a variety of cases where the framework can be of use, utilizing multiple datasets, models, and feature sets. We show analyses of publicly available datasets and invite the community to participate in our open challenges using OSF-EIMTC, fostering collaborative advancements in encrypted traffic classification.","Accurate classification of magnetic resonance imaging (MRI) images of brain tumors is crucial for early diagnosis and effective treatment in clinical studies. In these studies, many models supported by artificial intelligence (AI) have been proposed as assistant systems for experts. In particular, state-of-the-art deep learning (DL) models that have proven themselves in different fields have been effectively used in the classification of brain MRI images. However, the low accuracy of multiple classification of these images still leads researchers to conduct different studies in this field. Especially there is a need to develop models that achieve high accuracy on original images, and it is believed that this need can be met not only by DL models but also by classical machine learning (ML) algorithms. However, it is critical to choose the hyperparameters correctly for the hybrid use of ML algorithms with DL models. This study proposes a powerful new hybrid method to perform multiple classifications of brain tumors with high accuracy. This method also uses a novel convolutional neural network (CNN) model for feature extraction, and ML algorithms are used for feature classification. In addition, nine state-of-the-art CNN models are used for CNN performance comparison. The Bayesian optimization algorithm is used to obtain the optimal hyperparameter values of ML algorithms. The results obtained from the experimental studies show that the proposed hybrid model achieved 97.15% mean classification accuracy and 97% recall, precision, and F1-score values. Other hybrid models, including DarkNet19-SVM, DarkNet53-SVM, DenseNet201-SVM, EfficientNetB0-SVM, InceptionV3-SVM, NasNetMobile-SVM, ResNet50-SVM, ResNet101-SVM, and Xception-SVM, achieved mean classification accuracies of 95.01%, 95.58%, 96.87%, 97.01%, 95.3%, 95.01%, 96.3%, 95.87%, and 96.23%, respectively. Additionally, the proposed hybrid model exhibited remarkable time efficiency, accomplishing the classification process in a mere 67 min. Conversely, the model that exhibited the lowest time efficiency was the InceptionV3, with a processing time of 370 min. In terms of computational complexity, the EfficientNetB0 model is the most efficient. Despite the higher computational complexity of the proposed CNN model compared to some other models, it achieves the second-best classification accuracy. These results show that the proposed method performs better than previous studies on the same dataset. Especially in the classification problem, the optimized ML algorithms were superior to CNN classifiers. Finally, except for one, the proposed CNN model achieved better classification accuracies than the state-of-the-art CNN models.","Artificial Intelligence (AI) classifier models based on Deep Neural Networks (DNN) have demonstrated superior performance in medical diagnostics. However, DNN models are regarded as \u201cblack boxes\u201d as they are not intrinsically interpretable and, thus, are reluctantly considered for deployment in healthcare and other safety-critical domains. In such domains explainability is considered a fundamental requisite to foster trust and acceptability of automatic decision-making processes based on data-driven machine learning models. To overcome this limitation, DNN models require additional and careful post-processing analysis and evaluation to generate suitable explainability of their predictions. This paper analyses a DNN model developed for predicting Alzheimer\u2019s Disease to generate and assess explainability analysis of the predictions based on feature importance scores computed using sensitivity analysis techniques. In this study, a high dimensional dataset was obtained from Magnetic Resonance Imaging of the brain for healthy subjects and for Alzheimer\u2019s Disease patients. The dataset was annotated with two labels, Alzheimer\u2019s Disease (AD) and Cognitively Normal (CN), which were used to build and test a DNN model for binary classification. Three Global Sensitivity Analysis (G-SA) methodologies (Sobol, Morris, and FAST) as well as the SHapley Additive exPlanations (SHAP) were used to compute feature importance scores. The results from these methods were evaluated for their usefulness to explain the classification behaviour of the DNN model. The feature importance scores from sensitivity analysis methods were assessed and combined based on similarity for robustness. The results indicated that features related to specific brain regions (e.g., the hippocampal sub-regions, the temporal horn of the lateral ventricle) can be considered very important in predicting Alzheimer\u2019s Disease. The findings are consistent with earlier results from the relevant specialised literature on Alzheimer\u2019s Disease. The proposed explainability approach can facilitate the adoption of black-box classifiers, such as DNN, in medical and other application domains.","Advancements in the rapidly evolving specialization of deep learning have aided in improving several natural language understanding tasks. Sentiment and emotion classification models have improved, but when it comes to fine-grained sentiment analysis, these models can perform better. Human sentiment in natural language is generally an intricate combination of emotions, which can sometimes be indeterminate, neutral, or ambiguous. In the case of fine-grained sentiment analysis, the sentiments can be very similar to each other and interconnected, e.g., anger and fear. Most deep learning systems try to solve the problem of fine-grained sentiment analysis as a classification problem. However, fine-grained sentiments might combine similar emotions with one primary emotion. Trying to solve the problem as a classification task can result in better performance on benchmarks but does not ensure a better understanding and representation of language. The proposed work explores applying neutrosophy for fine-grained sentiment analysis using large language models. Neutrosophy identifies neutralities and employs membership functions (neutral, positive, negative) to quantify an instance into Single Valued Neutrosophic Sets (SVNS). This paper introduces Refined Emotion Neutrosophic Sets (RENS) for emotions (with four emotions) and Refined Ekman\u2019s Emotion Neutrosophic Sets (REENS) with seven emotions. In this paper, refined neutrosophic sets with membership functions are employed for each sentiment across a given taxonomy and assigned their values using the Neutrosophic Iterative Neural Clustering (NINC) algorithm proposed in this paper. It facilitates not only classifying sentiments but also quantifying the presence of each sentiment present in a given sample. It aids in better understanding and representation of samples across multiple sentiments, as in fine-grained sentiment analysis, experiments are performed on the GoEmotions dataset. The proposed approach performs on par with cross-entropy deep learning classifiers and is reproducible across different pre-trained language models.\nHighlights\n\u2022\nRefined neutrosophic sets for fine-grained sentiment analysis.\n\u2022\nUsing classification models as powerful feature learners.\n\u2022\nNeutrosophic iterative neural clustering for feature segregation.\n\u2022\nExperimental analysis using BERT, MPNet, RoBERTa, ELECTRA and XLNet.\n\u2022\nNeutrosophic logic for sentiment representation and quantification.","Current person re-identification (ReID) methods heavily rely on well-annotated training data, and their performance suffers from significant degradation in the presence of noisy labels that are ubiquitous in real-life scenes. The reason is that noisy labels not only affect the prediction results of the classifier, but also impede feature refinement, making it difficult to distinguish between different person features. To address these issues, we propose an Adaptive Self-correction Classification (ASC) loss and an Adaptive Margin Self-correction Triplet (AMSTri) loss. Specifically, ASC loss helps the network to produce better predictions by balancing annotations and prediction labels, and pays more attention to the minority samples with the help of a focusing factor. On the other hand, the AMSTri loss introduces an adaptive margin that varies with sample features to accommodate complex data variations, and utilizes predicted labels to generate reliable triples for feature refinement. We then present an end-to-end adaptive self-correction joint training framework incorporating ASC loss and AMSTri loss to achieve a robust ReID model. Our comprehensive experiments demonstrate that the proposed framework outperforms most existing counterparts.","Aspect category sentiment analysis (ACSA) excels at identifying the aspect categories and corresponding sentiments involved in a sentence, regardless of whether the aspect terms are explicitly mentioned or not. However, current methods tend to overinflate the original data, resulting in the introduction of unnecessary information, and fail to capture the inter-task relationship sufficiently. This paper presents a new method termed the prompt-based joint model (PBJM) to address these complications. PBJM treats the sentiment polarity prediction as binary classification and leverages a natural language prompt template, a concise sentence that guides the model to perform aspect category identification subtask and curtails the need for data augmentation. The two subtasks are jointly trained in pre-trained language models (PLMs) to capture their correlation. Further, the attention mechanism for aspect categories enables the model to concentrate selectively on significant features such as phrases and words during the predictions. In addition, the verbalizer employs a set of parameters to balance the weight of each label word while projecting between the label space and the label words space. Through experiments on four datasets, our model demonstrated remarkable performance in detecting category-sentiment pairs.","While there have been extensive studies in the health assessment of the bearings using the vibration signal, most have focused on the constant operating conditions. The field, however, operates often under time-varying conditions as is the case of the automotive wheel bearing. In this study, a novel health indicator (HI) is proposed to address this problem based on the Isolation Forest algorithm, which was originally developed for anomaly detection. The method is advantageous in two aspects: the HI is not influenced by the type of operating conditions whether it is constant or time-varying. Only the data under normal condition are used to construct the HI without the need of run-to-failure data. The method is demonstrated by the three cases with different types of bearing and operating conditions ranging from constant to the highly variable conditions. As a result, monotonic trends are obtained for the HI in all cases, which may be useful for the prognostic monitoring. Furthermore, in comparison with the HI constructed by the run-to-failure data in the previous literature, it is found that the trends of HI agree reasonably well with each other, which supports the validity of the method.","A sequential learning framework for text categorization based on Meta-cognitive Neural Network (McNN) is presented in this paper. Initially text documents are pre-processed and represented in the form of Term Document Matrix (TDM). Since the TDM is of high dimension, to reduce it to lower dimension Regularized Locality Preserving Indexing (RLPI) is used. Further, to categorize the text document, Meta-cognitive Neural Network (McNN) classifier is employed. To measure the effectiveness of the proposed framework, various experiments are conducted on standard benchmark Reuters-21578 dataset and used leave one out cross validation technique to assess the performance. The proposed framework performance is investigated against two well known neural network based classifiers: MLP (Multi Layer Perceptron) and RBF-NN (Radial Basis Function-Neural Network). The experimental results reveals that the McNN classifier uses less number of training documents for learning and it has less true error rate than other two neural network classifiers.","Classifying ancient manuscripts based on their writing surfaces often becomes essential for palaeographic research, including writer identification, manuscript localization, date estimation, and, occasionally, forgery detection. Researchers continually perform corroborative tests to classify manuscripts based on physical materials. However, these tests, often performed on-site, require actual access to the manuscript objects. These procedures involve specific expertise in manuscript handling, a considerable amount of time, and cost. Additionally, any physical inspection can accidentally damage ancient manuscripts that already suffer degradation due to aging, natural corrosion, and damaging chemical treatments. Developing a technique to classify such documents using noninvasive techniques with only digital images can be extremely valuable and efficient. This study uses images from a famous historical collection, the Dead Sea Scrolls, to propose a novel method to classify the materials of the manuscripts. The proposed classifier uses the two-dimensional Fourier transform to identify patterns within the manuscript surfaces. Combining a binary classification system employing the transform with a majority voting process is adequate for this classification task. This initial study shows a classification percentage of up to 97% for a confined amount of manuscripts produced from either parchment or papyrus material. In the extended work, this study proposes a hierarchical k-means clustering method to group image fragments that are highly likely to originate from a similar source using color and texture features calculated on the image patches, achieving 77% and 68% for color and texture clustering with 100% accuracy on primary material classification. Finally, this study explores a convolutional neural network model in a self-supervised Siamese setup with a large number of images that obtains an accuracy of 85% on the pretext task and an accuracy of 66% on the goal task to classify the materials of the Dead Sea Scrolls images.","Cancer is a complicated illness that is caused by numerous gene mutations or deregulation of gene interactions. This study, on the other hand, proposes a unique method for cancer categorization. Cancer is the major reason of death in this era. Appropriate methods are required to diagnose it as early as possible so that accurate treatment should be started to save human lives. Heuristic Class Topper Optimization describes a vital role in the detection of cancer for classification. A large dataset of tumors has been taken and Naive Bayes classifier exported to categorize them. The Heuristic Class Topper Optimization method (HCTO) is planted to extract the features. The optimization technique is based on the intelligent learning of pupils in a classroom. Weak students are learning from the class topper. There are various class toppers based upon the number of sections. Thus, the characteristics are refined with the aid of HCTO. The HCTO algorithm is a novel artificial intelligence technology that is rapidly converging. The HCTO-NB technique is simple, less complex, accurate, and has a low error rate, all of which are important characteristics in cancer categorization. The recommended method\u2019s achieved parameters are accuracy of 97.6%, precision of 98.4, error rate decreased by 3% on 1000 iterations, and classification efficacy are all demonstrated. The results are also compared to the KNN classifier, which has been used to classify cancer by a number of studies in the past. Experiments on a range of datasets demonstrated that this novel method was more accurate and dependable by\u2009~\u200914% compared to KNN. The findings shows that the suggested approach is both rapid and accurate, making it a great alternative for cancer diagnosis in the real world. In this paper 4 types of cancers datasets with relevant features like age, Gender, Tumor size, Tumor area and smoker or non-smoker etc. have been used for real time validation.","Recently, introducing nonconvex loss functions in support vector machine (SVM) to improve the robustness against varies noises has been drawing much attention. In this paper, we first construct a new robust capped asymmetric elastic net (CaEN) loss function. Second, we describe a novel robust Huberized kernel-based (HK) loss function and theoretically demonstrate several important properties, such as smoothness, boundness and the trade-off between the standard least squares and the truncated least squares. Finally, we apply the CaEN loss and the HK loss into elastic net nonparallel hyperplane SVM (ENNHSVM) to develop a fused robust geometric nonparallel SVM (FRGNHSVM). The proposed FRGNHSVM not only inherits the advantages of ENNHSVM but also improves the robustness of classification problems. An efficient Pegasos-based DC (difference of convex functions) algorithm is implemented to solve the FRGNHSVM optimization problem. In comparison with four famous SVMs, including Lagrangian SVM, twin SVM, pinball SVM and C-loss twin SVM, experimental results on simulations and twelve UCI datasets show that the proposed FRGNHSVM can often improve more than 5% average prediction accuracy. Moreover, the performance of the high prediction accuracy of FRGHNSVM is more significant along with the ratio of label noise increasing, indicating its superiority in dealing with label-contaminated datasets.\nHighlights\n\u2022\nA novel FRGNHSVM is proposed for robust binary learning problems.\n\u2022\nFRGNHSVM is stable for re-sampling and robust to outliers.\n\u2022\nThe designed Pegasos-based DC algorithm is scalable and applicable to large data.\n\u2022\nExperimental results demonstrate the effectiveness of FRGNHSVM.","Semi-supervised learning (SSL) is a successful paradigm that can use unlabelled data to alleviate the labelling cost problem in supervised learning. However, the excellent performance brought by SSL does not transfer well to the task of class imbalance. The reason is that the class bias of pseudo-labelling further misleads the decision boundary. To solve this problem, we propose a new plug-and-play approach to handle the class imbalance problem based on a theoretical extension and analysis of distribution alignment. The method, called Basis Transformation Based Distribution Alignment (BTDA), efficiently aligns class distributions while taking into account inter-class relationships.BTDA implements the basis transformation through a learnable transfer matrix, thereby reducing the performance loss caused by pseudo-labelling biases. Extensive experiments show that our proposed BTDA approach can significantly improve performance in class imbalance tasks in terms of both accuracy and recall metrics when integrated with advanced SSL algorithms. Although the idea of BTDA is not complex, it can show advanced performance on datasets such as CIFAR and SVHN.","With rapid development of single-cell multi-modal sequencing technologies, more and more multi-omics data come into being and provide a unique opportunity for the identification of distinct cell types at the single-cell level. Therefore, it is important to integrate different modalities which are with high-dimensional features for boosting final multi-omics data classification performance. However, existing multi-omics data classification methods mainly focus on exploiting the complementary information of different modalities, while ignoring the learning confidence and cross-modal sample relationship during information fusion. In this paper, we propose a multi-omics data classification network via global and cross-modal feature aggregation, referred to as GCFANet. On one hand, considering that a large number of feature dimensions in different modalities could not contribute to final classification performance but disturb the discriminability of different samples, we propose a feature confidence learning mechanism to suppress some redundant features, as well as enhancing the expression of discriminative feature dimensions in each modality. On the other hand, in order to capture the inherent sample structure information implied in each modality, we design a graph convolutional network branch to learn the corresponding structure preserved feature representation. Then the modal-specific feature representations are concatenated and input to a transformer induced global and cross-modal feature aggregation module for learning consensus feature representation from different modalities. In addition, the consensus feature representation used for final classification is enhanced via a view-specific consistency preserved contrastive learning strategy. Extensive experiments on four multi-omics datasets are conducted to demonstrate the efficacy of the proposed GCFANet.\nHighlights\n\u2022\nA feature aggregation network is proposed for multi-omics classification.\n\u2022\nA contrastive learning strategy is designed for feature representation alignment.\n\u2022\nBoth complementary information and sample structure are used for consensus learning.\n\u2022\nThe proposed network is successfully used for drug response prediction.","Highlights\n\u2022\nGrading modeling and decision-making framework were proposed for yield estimation.\n\u2022\nNormalized weight decision-making strategy improved the estimation precision.\n\u2022\nDeep learning network ConvNeXt was compared for modeling performance.\n\u2022\nProposed framework performs precisely and robustly using UAV RGB images.\nAbstract\nRice yield estimation is of great significance for ensuring food security and breeding new varieties with high yield and good stress resistance. The popular yield estimation method is to combine unmanned aerial vehicle (UAV) images to extract vegetation index (VI) for multi-variable regression, whose application is always limited by expensive equipment and complex data processing. In this study, based on the deep learning network, ConvNeXt, a robust framework developed by grading modeling and normalized weight decision-making strategy was innovatively proposed to estimate the rice yield, only using RGB images collected by UAVs. The main results are: (1) the yield estimation performance of yield grading model was better than that of regression model, and R2, mean absolute error (MAE), and mean absolute percentage error (MAPE) on the test dataset using the grading model were 0.97, 410.94 kg/ha, and 3.96 % respectively; (2) the confidence scores of the grading model were adopted as the weights, which could correct the estimated yield of the misclassified samples and further reduce the estimated error. The normalized weight decision-making after optimization had obvious advantages, the performance of 2-weight strategy was the best, whose R2, MAE, and MAPE reached 0.98, 386.08 kg/ha, and 3.79 %, respectively; (3) the MAPE results of generalization assessment showed that the generalization of the grading (12.98 %) and regression (10.88 %) models was inferior to the proposed framework (8.71 %), which could reduce the MAPE on the generalization evaluation dataset to less than 10 %. Furthermore, the framework exhibited good adaptability when applied to the rice yield estimation of the new data in 2022, with an MAPE of 4.54 %. Considering both application potential and adaptability, this framework constructs a novel strategy and method for rice yield estimation using grading modeling and normalized weight decision-making strategy, which provides a reference for future real-time and precise yield estimation using UAV remote sensing.","Highlights\n\u2022\nPropose an improved random forest based on the improvement of decision trees.\n\u2022\nImprove the evaluation mechanism for the classification effect of decision trees.\n\u2022\nPropose a method for quantifying the diversity between decision trees.\n\u2022\nMultiple tests verify the superiority of the proposed improved random forest.\nAbstract\nRandom forest is one of the most widely used machine learning algorithms. Decision trees used to construct the random forest may have low classification accuracies or high correlations, which affects the comprehensive performance of the random forest. Aiming at these problems, the authors proposed an improved random forest based on the classification accuracy and correlation measurement of decision trees in this paper. Its main idea includes two parts, one is retaining the classification and regression trees (CARTs) with better classification effects, the other is reducing the correlations between the CARTs. Specifically, in the classification effect evaluation part, each CART was applied to make predictions on three reserved data sets, then the average classification accuracies were achieved, respectively. Thus, all the CARTs were sorted in descending order according to their achieved average classification accuracies. In the correlation measurement part, the improved dot product method was proposed to calculate the cosine similarity, i.e., the correlation, between CARTs in the feature space. By using the achieved average classification accuracy as reference, the grid search method was used to find the inner product threshold. On this basis, the CARTs with low average classification accuracy among CART pairs whose inner product values are higher than the inner product threshold were marked as deletable. The achieved average classification accuracies and correlations of CARTs were comprehensively considered, those with high correlation and weak classification effect were deleted, and those with better quality were retained to construct the random forest. Multiple experiments show that, the proposed improved random forest achieved higher average classification accuracy than the five random forests used for comparison, and the lead was stable. The G-means and out-of-bag data (OBD) score obtained by the proposed improved random forest were also higher than the five random forests, and the lead was more obvious. In addition, the test results of three non-parametric tests show that, there were significant diversities between the proposed improved random forest and the other five random forests. This effectively proves the superiority and practicability of the proposed improved random forest.","Existing intelligent classification methods could be inefficient to deal with the hybrid environments including hesitant fuzzy information and real numbers. With respect to this real issue, in this study, we propose some new intelligent methods to achieve deep learning and intelligent classification under this hybrid environment. To do this, we construct the hesitant fusion bidirectional recurrent neural network (HF-BiRNN) based on the hesitant fusion mechanism. Then, the twice-cycle mechanism is designed, which includes the extension mechanism and the decomposition-reorganization mechanism, to fully utilize the original data and optimize the classification results. Meanwhile, the overlap degree algorithm is constructed to filter the optimal outputs. After that, we further propose the hesitant expansion BiRNN (HE-BiRNN) by combining with the twice-cycle mechanism, overlap degree algorithm, and BiRNN. Lastly, these new methods are used to the problems of driving route classification and red wine quality assessment. The derived optimal results and comparison analysis fully show the effectiveness and feasibility of the new proposed mechanisms and developed models.","Pest control is essential for crop planting as crops are highly susceptible to pest damage. In general, pest recognition is a fundamental element of pest control. Previous works have used computer vision to achieve automatic pest recognition. However, only a few of them have focused on the open-world pest recognition problem. That is, most methods cannot process new pest categories without expensive network retraining. To fill the gap, this paper proposes an open-world pest image classifier based on two observations: (1) convolutional features learned from previous pest classes are generally applicable to new pest categories, and (2) removing fully-connected neural layers allows a deep network to be exempted from model fine-tuning in case of a new class. First, an optimized lightweight ResNet8-based matching network is developed as the image feature extractor, which saves computational resources. To prevent model collapse, the proposed ResNet8-based matching network is trained with the normalized temperature-scaled cross-entropy loss function instead of the triplet loss function. The trained ResNet8-based matching network is then used to compute similarities between support class prototypes and query image representations for the pest classification. Compared with the state-of-the-arts, the proposed method has achieved the highest 40-way 5-shot classification accuracy of 84 . 29 \u00b1 0 . 23 % with 14.18 frames per second on the D0 dataset. It is significantly superior to the ResNet12-based baseline. These suggest that the proposed method is a technically feasible solution to the open-world pest recognition problem. The Python code can be accessed at https://github.com/scau-gqw1993.\nHighlights\n\u2022\nThis work is, to our best knowledge, the first one on open-world pest classification.\n\u2022\nA lightweight matching network was obtained using ResNet8 and NT-Xent loss function.\n\u2022\nThe method achieved the best 40-way 5-shot accuracy of 84.29% on the D0 dataset.","The liver is a key organ in the human body that aids in the digestion of food, the elimination of toxins, and the storage of energy. Patients with liver disorders are on the rise all over the world. However, because the disorder's symptoms are unclear, it is difficult to diagnose it, which raises the disease's death rate. The study introduces novel fuzzy twin models for liver disease classification. In the first model, the membership is calculated based on the quadratic function called fuzzy twin kernel ridge regression-quadratic (FTKRR-Q). In the second model, we have calculated the fuzzy membership based on the centroid and named the model as fuzzy twin kernel ridge regression-centroid (FTKRR-C). For our research, the BUPA or liver disease dataset has been used from the UCI machine learning repository. Experimental results are compared with the twin support vector machine, kernel ridge regression classifier and twin kernel ridge regression classifier. The accuracy, sensitivity, F1-score, and Mathew's correlation coefficient are used to evaluate the suggested model's performance. Experiments are also carried out on some real-world benchmark datasets. The results reveal the applicability of the proposed models.","Classification of brain haemorrhage is a challenging task that needs to be solved to help advance medical treatment. Recently, it has been observed that efficient deep learning architectures have been developed to detect such bleeding accurately. The proposed system includes two different transfer learning strategies to train and fine-tune ImageNet pre-trained state-of-the-art architecture such as that VGG 16, Inception V3 and DenseNet121. The evaluation metrics have been calculated based on the performance analysis of the employed networks. Experimental results show that the modified fine-tuned Inception V3 performed well and achieved the highest test accuracy.","Skin cancer is the most prevalent type of cancer worldwide. Early detection is essential as it could be fatal at later stages. The classification of skin lesions is challenging since there are many variations, including changes in color, shape, size, high intra-class variation, and high inter-class similarity. In this paper, a unique class-wise attention method is proposed that considers each class equally while extracting additional discriminative information of skin lesions. The proposed attention mechanism is employed in a progressive manner to incorporate discriminative feature information from multiple scales. The proposed approach obtained competitive performance against more than 15 state-of-the-art methods including HAM1000 and ISIC 2019 leaderboard winners. The proposed method achieved 97.40% accuracy on the HAM10000 and 94.9% accuracy on the ISIC 2019 dataset.\nHighlights\n\u2022\nA novel attention based mechanism is proposed for skin lesions classification.\n\u2022\nThe problem of significant intra-class variance, high inter-class similarity, and class imbalance is addressed by a class-wise attention mechanism.\n\u2022\nThe important features for the classification are extracted without the burden of additional parameters using a progressive class-wise attention mechanism.\n\u2022\nThe final three layers of the baseline model are discarded and the global average pooling (GAP) and classification output layers are added to reduce the number of parameters, while maintaining performance.\n\u2022\nThe proposed network is robust, end-to-end trainable and has good generalization on unseen data.","The homophily assumption in graph theory posits that nodes with similar characteristics have a higher tendency to form connections. This principle has rendered Graph Neural Networks (GNNs) as vital tools for graph representation learning. However, many real-world graphs may exhibit a phenomenon often termed as neighbor class imbalance, which is characterized by frequent connections between dissimilar nodes, a scenario reflecting low homophily. Classical GNNs tend to overlook this issue, leading to a significant decline in performance. Prior research has attempted to address this challenge by employing high-order neighborhoods and filtering out dissimilar neighbors, yet they have paid little attention to homophily degree estimation and label utilization. In this work, we initially explore the performance of classical GNNs on a synthetic graph with varying homophily degrees, designated as SynG-N. Following this, we introduce a novel method, HLA-GNN, which integrates homophily degree estimation and label utilization to enhance classical GNNs. The degrees of homophily between node pairs are estimated using a limited set of ground-truth labels, which can be integrated into classic GNNs to guide the message aggregation process. Drawing on the label propagation algorithm, we combine the partially observed class labels to enhance the original feature space. Here, the observed class labels are randomly masked as a feature augmentation and training signal. Our experimental results on eight datasets with varying degrees of homophily underscore the effectiveness of our method. HLA-GNN achieves a 12.69%\u223c34.19% improvement on low-homophily graphs, while maintaining competitive results in homophilous settings.\nHighlights\n\u2022\nHLA-GNN assigns weights by homophily in latent space.\n\u2022\nHLA-GNN enhances feature space with label propagation algorithm.\n\u2022\nOur method Boosts low-homophily performance by 12.69% to 34.19%.","Previously, single classification models were mainly studied to classify human protein cell images, i.e., to identify a certain protein based on a set of different cells. However, a classifier can identify only one protein, in fact, a single cell usually consists of multiple proteins, and the proteins are not completely independent of each other. In this paper, we build a human protein cell classification model by multi-label learning. The logical relationship and distribution characteristics among the labels are analyzed to determine the different proteins contained in a set of different cells (i.e., containing multiple elements in the output space). In this paper, using human protein image data, we conducted comparison experiments on pre-trained Xception and InceptionResnet V2 to optimize the two models in terms of data augmentation, channel settings, and model structure. The results show that the Optimized InceptionResnet V2 model achieves high performance in the classification task. The final accuracy of the Optimized InceptionResnet V2 model we obtained reached 96.1%, which is a 2.82% improvement relative to that before the optimized model.","Intelligent Transportation Systems (ITS) have experienced significant growth over the past decade thanks to advances in control, communication, and information technology applied to vehicles, roads, and traffic control systems. Vehicle type classification plays a vital role in implementing ITS because of its ability to collect useful traffic information, enable future development of transport infrastructures, and increase human comfort. As a branch of machine learning, deep learning represents a frontier for artificial intelligence, which seeks to be closer to its primary goal. Deep learning is a powerful tool for classifying vehicle types because it can capture complex traffic data characteristics and learn from large amounts of data. This means that it can be used to accurately classify traffic data and generate valuable insights that can be used to improve traffic management. Researchers have successfully adopted these algorithms as a solution to propose optimal vehicle-type classification strategies. This paper highlights the role of deep learning algorithms in solving the vehicle type classification problem, reviewing the state-of-the-art approaches in this field.","Semi-supervised learning (SSL) addresses the lack of labeled data by exploiting large unlabeled data through pseudolabeling. However, in the extremely low-label regime, pseudo labels could be incorrect, a.k.a. the confirmation bias, and the pseudo labels will in turn harm the network training. Recent studies combined finetuning (FT) from pretrained weights with SSL to mitigate the challenges and claimed superior results in the low-label regime. In this work, we first show that the better pretrained weights brought in by FT account for the state-of-the-art performance, and importantly that they are universally helpful to off-the-shelf semi-supervised learners. We further argue that direct finetuning from pretrained weights is suboptimal due to covariate shift and propose a contrastive target pretraining step to adapt model weights towards target dataset. We carried out extensive experiments on both classification and segmentation tasks by doing target pretraining then followed by semi-supervised finetuning. The promising results validate the efficacy of target pretraining for SSL, in particular in the low-label regime.","In the semiconductor manufacturing process, analyzing the defect patterns on a wafer map is crucial for identifying the causes of the defects. The advent of convolutional neural networks (CNNs) has significantly increased the accuracy of automated wafer map pattern classification. Generally, the use of a larger training dataset results in higher classification accuracy. However, collecting a large number of wafer maps and labeling them with their defect categories is expensive and time-consuming. In this paper, we present an improved training method under data insufficiency for wafer map pattern classification. We apply supervised contrastive learning to train a CNN by exploiting the rotational-invariant characteristic of wafer map labeling. The CNN is trained by simultaneously minimizing two loss functions: classification loss and contrastive loss. The first loss function is to classify the rotational variants of wafer maps accurately. The second loss function is to align the representation vectors for the rotational variants of wafer maps with similar labels to be close to each other. Using two benchmark datasets, WM-811K and MixedWM38, we demonstrate that the proposed method enhances classification accuracy compared with existing methods, particularly when the training dataset is small.","This article presents a solution for Speech Emotion Recognition (SER) in multilingual setting using a hierarchical approach. The approach involves two levels, the first level identifies the gender of the speaker, while the second level predicts their emotional state. We evaluate the performance of three classifiers of increasing complexity: k-NN, transfer learning based on YAMNet, and Bidirectional Long Short-Term Memory neural networks. The models were trained, validated, and tested on a dataset that includes the big-six emotions and was collected from well-known SER datasets representing six different languages. Our results indicate that there are differences in classification accuracy when considering all data versus only female or male data, across all classifiers. Interestingly, prior knowledge of the speaker\u2019s gender can improve the overall classification performance.","Abstract\u2014The implementation of computational approaches for protein glycosylation site prediction is becoming popular since the experimental-validated glycosylation data became more abundant. Some of the data were found to be wrong after the experiment was again carried out with more sophisticated technology. To solve this issue, the latest state-of-the-art model trained the model based on a positive-unlabelled algorithm. The aim of this research is to explore the possibility of an approach applying a simple neural network algorithm and still achieve high classification performance. The model proposed in this research gave competitive results with fewer preprocessing steps. Increasing the accuracy of glycosylation site prediction can complement laboratory-based methods and is very useful for understanding the role of glycosylation.","Time series classification exists in widespread domains such as EEG/ECG classification, device anomaly detection, and speaker authentication. Although many methods have been proposed, efficient selection of intuitive temporal features to accurately classify time series remains challenging. Therefore, this paper presents TSC-RTF, a new time series classification method using random temporal features. First, to ensure the intuitiveness of the features, TSC-RTF selects subsequences containing important data points as candidates for intuitive temporal features. Then, TSC-RTF uses random sampling to reduce the number of candidates significantly. Next, TSC-RTF selects the final temporal features using a random forest to ensure the validity of the final temporal features. Finally, a deep learning classifier is trained by TSC-RTF to achieve high accuracy. The experimental results show that the proposed method can compete with the state-of-the-art methods.","With the emerging of new data collection ways, the features are incremental and accumulated gradually. Due to the expansion of feature spaces, it is more common that there are unknown biases between the distribution of training and testing datasets. It is known as the unknown data selection bias, which belongs to the learning scenario with non-i.i.d samples. The performance of traditional approaches, which need the i.i.d. assumption, will be aggravated seriously. How to design an algorithm to address the problem of data selection bias in this feature incremental scenario is crucial but rarely studied. In this paper, we propose a feature incremental classification algorithm with causality. Firstly, we embed the confounding variable balance algorithm in causal learning into the prediction modeling and utilize the logical regression algorithm with balancing regular terms as a baseline. Then, to satisfy the special requirement of feature increment, we design a new regularizer, which maintains the consistency of the regression coefficients between the data in the current and previous stages. It retains the correlation between the old features and labels. Finally, we propose the Multiple Balancing Logistic Regression model (MBRLR) to jointly optimize the balancing regularizer and weighted logistic regression model with multiple feature sets. We also present theoretical results to show that our proposed algorithm can make precise and stable predictions. Besides, the numerical results also demonstrate that our MBRLR algorithm is superior to other methods.\nHighlights\n\u2022\nWe have pioneered the use of causal inference and proposed a novel method approach, i.e., MBRLR, for achieving stable and accurate classification in the feature increase scenario.\n\u2022\nWe have improved the sample reweighting method by redesigning the loss function and regularizer which could be formulated as a convex optimization problem and could be easily solved.\n\u2022\nWe have proved that the MBRLR algorithm can make a stable classification for feature incremental data in the non-i.i.d case.","Emitter classification plays a crucial role in electronic support measurement systems. A data-driven model named Gaussian dynamic recurrent unit is proposed to accomplish the end-to-end emitter classification tasks, which is simplified based on the long short-term memory unit by using a Gaussian function to characterize the relationship between the input and the network state, and a learnable exponentially weighted average to update the states. The Gaussian function and the dynamic exponentially weighted average amplify the difference between the input signals from different classes. The parameter size and computational time of many emitter classification methods are measured. The results show that the model proposed in this paper has the highest parameter efficiency, and low computational and storage costs. The results on a real-world dataset show that the proposed model has the highest accuracy and stability. These advantages have important significance for deploying the model in practical applications.","Cross-domain sentiment analysis (CDSA) aims to learn transferable knowledge from the source domain to facilitate the sentiment polarity classification on the target domain of lacking labeled data. Currently, two types of unsupervised domain adaptation (UDA) methods are widely used in CDSA tasks. One employs the domain adversarial strategy to extract domain-invariant features, and the other utilizes the distance metric strategy to reduce domain distribution discrepancy. However, the fine-grained domain-specific information related to categories aligned between domains is not preserved, which suppresses the performance of target-domain classification. To overcome the mentioned problem, a unified Domain Adversarial Category-wise Alignment Network (DACAN) was proposed in this paper. An integrated network was constructed with progressive multi-level feature learning. Specifically, a feature extraction module was constructed with parameter sharing between two domains at low-level text feature extraction layers. The domain adversarial module was added to enable shared knowledge transfer by extracting domain-invariant information and by updating the shared parameters at the feature extraction layers. A category-wise alignment module was built to achieve local distribution alignment at the high dimension-level semantic layers guided by fine-grained category structure information. Meanwhile, joint constraint was established with domain-invariant constraint based on domain adversarial, and domain-consistency constraint based on category-wise alignment. Comprehensive experiments were conducted on two standard Amazon review datasets. The results show that DACAN outperforms other state-of-the-art UDA methods by 0.7% and 1.1% on the two- and three-category CDSA tasks, respectively. Also, better performance results are achieved with a synergistic UDA scheme compared with a single UDA scheme.","Plagiarism detection (PD) in natural language processing involves locating similar words in two distinct sources. The paper introduces a new approach to plagiarism detection utilizing bidirectional encoder representations from transformers (BERT)-generated embedding, an enhanced artificial bee colony (ABC) optimization algorithm for pre-training, and a training process based on reinforcement learning (RL). The BERT model can be incorporated into a subsequent task and meticulously refined to function as a model, enabling it to apprehend a variety of linguistic characteristics. Imbalanced classification is one of the fundamental obstacles to PD. To handle this predicament, we present a novel methodology utilizing RL, in which the problem is framed as a series of sequential decisions in which an agent receives a reward at each level for classifying a received instance. To address the disparity between classes, it is determined that the majority class will receive a lower reward than the minority class. We also focus on the training stage, which often utilizes gradient-based learning techniques like backpropagation (BP), leading to certain drawbacks such as sensitivity to initialization. In our proposed model, we utilize a mutual learning-based ABC (ML-ABC) approach that adjusts the food source with the most beneficial results for the candidate by considering a mutual learning factor that incorporates the initial weight. We evaluated the efficacy of our novel approach by contrasting its results with those of population-based techniques using three standard datasets, namely Stanford Natural Language Inference (SNLI), Microsoft Research Paraphrase Corpus (MSRP), and Semantic Evaluation Database (SemEval2014). Our model attained excellent results that outperformed state-of-the-art models. Optimal values for important parameters, including reward function are identified for the model based on experiments on the study dataset. Ablation studies that exclude the proposed ML-ABC and reinforcement learning from the model confirm the independent positive incremental impact of these components on model performance.\nHighlights\n\u2022\nBERT-based plagiarism detection with RL and ML-ABC.\n\u2022\nReward function in RL improves detection of minority plagiarism class.\n\u2022\nModel outperforms state-of-the-art on SNLI, MSRP, SemEval2014 datasets.\n\u2022\nAblation studies highlight the impact of ML-ABC and RL on performance.","Aspect-Level Sentiment Classification (ALSC) aims to assign specific sentiments to a sentence toward different aspects, which is one of the crucial challenges in the field of Natural Language Processing (NLP). Despite numerous approaches being proposed and obtaining prominent results, the majority of them focus on leveraging the relationships between the aspect and opinion words in a single instance while ignoring correlations with other instances, which will make models inevitably become trapped in local optima due to the absence of a global viewpoint. Instance representation derived from a single instance, on the one hand, the contained information is insufficient due to the lack of descriptions from other perspectives; on the other hand, its stored knowledge is redundant since the inability to filter extraneous content. To obtain a polished instance representation, we developed a Retrieval Contrastive Learning (RCL) framework to subtly extract intrinsic knowledge across instances. RCL consists of two modules: (a) obtaining retrieval instances by sparse retriever and dense retriever, and (b) extracting and learning the knowledge of the retrieval instances by using Contrastive Learning (CL). To demonstrate the superiority of RCL, five ALSC models are employed to conduct comprehensive experiments on three widely-known benchmarks. Compared with the baselines, ALSC models achieve substantial improvements when trained with RCL. Especially, ABSA-DeBERTa with RCL obtains new state-of-the-art results, which outperform the advanced methods by 0.92%, 0.23%, and 0.47% in terms of Macro F1 gains on Laptops, Restaurants, and Twitter, respectively.\nHighlights\n\u2022\nWe proposed RCL to enable ALSC models to generate polished representations.\n\u2022\nRCL has two modules: obtain retrieval instances and learn common features by CL.\n\u2022\nThe sparse and dense retrievers are used to obtain high-quality retrieval instances.\n\u2022\nThe ALSC model can be enhanced greatly by training with RCL.\n\u2022\nABSA-DeBERTa obtains new state-of-the-art results by being trained with RCL.","Highlights\n\u2022\nExploring the effect of locally weighted learning with machine learning model.\n\u2022\nGain ratio are used to select the most important environmental factor.\n\u2022\nLWL-RS-ADT appears as an accurate model in assessing landslide susceptibility.\nAbstract\nAssessing landslide susceptibility and predicting the possibility of landslide event is the foundation and prerequisite for emergency response and management of landslide disaster. The target of current paper is to propose five integration models based on integrating locally weighted learning (LWL) with a radial basis function classifier (RBF), Fisher's linear discriminant (FLDA), quadratic discriminant analysis (QDA), a Credal decision tree (CDT), an alternating decision tree (ADT) and random subspace (RS) and the performance of five integration models were compared for modeling landslide susceptibility. Yongxin County from China was employed as a study area, 364 landslide locations and fifteen environmental factors were applied. The results demonstrate that the proposed LWL-RS-ADT model is more reliable and stable than the other models. Among the fifteen environmental factors, NDVI, lithology, and altitude are the very significant factors in the six models. It is concluded that the proposed integration models provide an effective way to predict the susceptibility of landslides.","Fine-grained visual classification (FGVC) is a difficult task due to the challenges of discriminative feature learning. Most existing methods directly use the final output of the network which always contains the global feature with high-level semantic information. However, the differences between fine-grained images are reflected in subtle local regions which often appear in the front of the network. When the texture of the background and object are similar or the proportion of the background is too large, the prediction will be greatly affected. In order to solve the above problems, this paper proposes multi-granularity feature fusion module (MGFF) and two-stage classification based on Vision-Transformer (ViT). The former comprehensively represents images by fusing features of different granularities, thus avoiding the limitations of single-scale features. The latter leverages the ViT model to separate the object from the background at a very small cost, thereby improving the accuracy of the prediction. We conduct comprehensive experiments and achieves the best performance in two fine-grained tasks on CUB-200-2011 and NA-Birds.\nHighlights\n\u2022\nA multi-granularity feature fusion module is proposed to solve the limitations of single-scale features.\n\u2022\nA two-stage classification based on Vision-Transformer is proposed to reduce background interference on predictions. By leveraging the ViT model, the object can be separated from the background and the details can be enlarged.\n\u2022\nExtensive experiments prove the superiority of our model. The visualization results illustrate that our two-stage classification can accurately localize objects and facilitate correct predictions.","Early classification of longitudinal data remains an active area of research today. The complexity of these datasets and the high rates of missing data caused by irregular sampling present data-level challenges for the Early Longitudinal Data Classification (ELDC) problem. Coupled with the algorithmic challenge of optimising the opposing objectives of early classification (i.e., earliness and accuracy), ELDC becomes a non-trivial task. Inspired by the generative power and utility of the Generative Adversarial Network (GAN), we propose a novel context-conditional, longitudinal early classifier GAN (LEC-GAN). This model utilises informative missingness, static features, and earlier observations to improve the ELDC objective. It achieves this by incorporating ELDC as an auxiliary task within an imputation optimization process. Our experiments on several datasets demonstrate that LEC-GAN outperforms all relevant baselines in terms of F1 scores while increasing the earliness of prediction.","The histopathological analysis of a suspected region is critical for cancer diagnosis, treatment, and management. Histopathological diagnosis consists in analyzing the characteristics of the lesions using tissue sections stained with hematoxylin and eosin. Classification of digital tumor pathology images, called whole slide images (WSIs), is a great challenge since WSIs usually have huge resolutions while lacking localized annotations. Multiple instance learning (MIL) is a commonly used method applied to pathological image analysis. However, most MIL methods often focus only on the global representation of WSIs, ignoring whether the category labels play other roles in the model training besides being a supervision signal. In addition, feature confusion is also a problem that should be avoided for the analysis of WSIs with weakly supervised methods. To address these problems, we propose a novel algorithm of classifying WSI for cancer diagnosis. The proposed model, ProMIL, uses only slide-level labels rather than localized annotations for analysis. There are three innovations in this work. Firstly, we present the concept of class proxy which is the representation of the intrinsic feature of each category, and plays a key role in guiding the training of the model. Secondly, we design a novel WSI representation learning module that utilizes a multi-scale feature extraction strategy to represent each patch in a WSI and then aggregates these representations using an attention mechanism to encode the WSI. Thirdly, we design a metric-learning-based weakly supervised multiclass-classifier by measuring the similarity between each WSI embedding and class proxies. The proposed ProMIL can effectively alleviate the side effect of feature confusion, and carry intuitive interpretability and scalability. To evaluate the performance of ProMIL, we conduct a series of experiments on several datasets of WSIs with different types of cancer from open data sources. It can be observed from the experimental results that ProMIL outperforms most of the compared methods and achieves better performance on a various type of cancer image data for classification, thus suggesting the proposed method is suitable for classifying different categories of cancer rather than a specific kind of cancer. Therefore, it is expected to act as a general framework to be extended to more cancer diagnoses.\nHighlights\n\u2022\nThe class proxy is the representation of the intrinsic feature of each category.\n\u2022\nA multi-scale feature extraction strategy is proposed to represent each patch.\n\u2022\nEncoding the whole slide image by aggregating and gated-attention mechanism.\n\u2022\nWSI classification performs by metric-learning-based weakly supervised MIL.","Various ensemble machine learning techniques have been widely studied and implemented to construct the predictive models in different sciences, including bagging, boosting, and stacking. However, bagging and boosting concentrate on minimizing variance or bias, stacking techniques aimed at reducing both by identifying the optimal integration of base learners. Moreover, while most ensemble methods simply combine identical machine learning models, stacking utilizes a meta-machine learning model to combine different base learning models, aiming to enhance the overall accuracy of generalization. Therefore, this research showed the utilization of stacking, an ensemble approach, to develop mineral prospectivity models for Pb-Zn mineralization in the Varcheh District, west Iran. To end this, various exploration evidence layers, including geochemical data, remote sensing data, geological and tectonic controls were used to construct the stacking structure. In the following, a set of five base learners were applied, containing support vector regression (SVR) using RBF, linear and polynomial kernels, the K-nearest neighbor (KNN), and linear regression. Ridge, SVR-RBF and XGBoost were used as a meta-learner to integrate the outputs of basic learners. To measure how well each model performed, ROC, F1-score and Precision metrics was carried out. Moreover, compared to the separate algorithms, the stacking-based ensemble model showed a better prediction accuracy. The findings of this study demonstrated that the ensemble model based on stacking achieved a 95% prediction rate for Pb-Zn deposits, covering only 9% of the study area. As a result, this model holds promise as an effective tool for predicting mineral prospectivity in other study areas, regardless of whether they exhibit similar or different types of mineralization.","\u2014Graph convolutional network (GCN) has attracted much attention in the field of hyperspectral image classification for its excellent feature representation and convolution on arbitrarily structured non-Euclidean data. However, most state-of-the-art methods build a graph utilize the distance measure, which makes it challenging to fully characterize the complex relationship of hyperspectral remote sensing data. Moreover, the hyperspectral image usually has uncertainty introduced by the problems of the spectral variability and noise interference. This article uses fuzzy theory to optimize the GCN and thus solve the uncertainty problem in hyperspectral images, and presents a novel fuzzy graph convolutional network (F-GCN) for hyperspectral image classification. By calculating the fuzzy similarity of samples, a robust graph is first built rather than using the traditional Euclidean distance method, which allows a better representation of the complex relationship between hyperspectral remote sensing data. Furthermore, the proposed network introduces fuzzy layers into the model to cope with the ambiguity of the hyperspectral image. Finally, the classification results for three real-world hyperspectral data sets to show its feasibility and effectiveness in hyperspectral image classification.","In texture classification, local binary pattern (LBP) is currently one of the most widely-concerned feature encoding models. Most existing LBP-based texture classification methods are usually limited to single-kind texture features. In fact, an across-domain fusion of LBP features with other features, such as image contours, could be another potential path to promote texture classification. To enhance the feature modeling ability of LBP-based methods, this paper firstly designs a Cellular Neural Network (CellNN) with recurrent convolutions, initially trained by a simplified simulated-annealing algorithm, to extract informative image contours. For better reliability, a new three-channel contour extractor of deep CellNNs (i . e ., dCellNNs) is proposed. This extractor contains the initially-trained CellNNs of more than three layers, and it is further optimized by fine-tuning parameters. Moreover, a new weighted-base algorithm is designed to fulfill the fusion of the multi-scale texture features by LBPs and the contour features by dCellNNs to enhance feature representation. Finally, these enhanced features are concatenated together to generate the final multi-scale features of given texture image. On texture datasets KTH, Brodatz, OTC12 and UIUC, experiment results verify that the across-domain fusion of multi-scale LBPs and dCellNNs is efficient in capturing &amp; enhancing texture features. With moderate feature dimensionality and computational costs, it could improve texture classification, acquiring an obvious accuracy increase on previous state-of-the-art ones, e.g., a rise of 2.58% on KTH-TIPS2b, a rise of 3.11% on Brodatz, a rise of 0.71% on OTC12 and a rise of 0.42% on UIUC.\nHighlights\n\u2022\nA multi-scale across-domain feature fusion of textures and contours is proposed.\n\u2022\nThe first deep model of CellNN (dCellNNs) with two-stage training is proposed.\n\u2022\nThe three-channel texture contour extractor of dCellNNs is designed.\n\u2022\nA weighted-base fusing is designed for the across-domain fusion of features.","One of the significant challenges in Brain\u2013Computer Interface (BCI) is to develop a classifier that can decode users\u2019 mental states based on electroencephalogram (EEG) data collected from independent subjects. The focus of such subject-independent (SI) classification is justified because it can lead to BCIs that do not require a user-specific calibration process. In recent years, the emergence of deep neural networks (DNNs) has significantly improved the performance of EEG classification. Among various deep learning techniques, the training efficiency and the performance of Convolutional Neural Networks (CNNs), in particular, have led to several state-of-the-art architectures for accurate classification of EEG. Not surprisingly, the efforts to improve the performance of these architectures for EEG classification have been ramped up in recent years. In this regard, a trivial approach is to train and tune a large number of architectures and hyperparameters and hope to improve upon the existing results. In contrast with this ad hoc approach, here we put forward a systematic method inspired by the jackknife estimation to improve the performance of existing CNN architectures. Using EEGNet and ShallowConvNet as archetypical, our empirical results show that the proposed \u201cDelete-a-Subject Jackknife\u201d (DASJ) technique can potentially improve the performance of existing CNN architectures for SI classification of EEG.\nHighlights\n\u2022\nProposes DASJ-CNN which is a jackknife-inspired approach with Convolutional Neural Network (CNN) base classifiers.\n\u2022\nPresents an analytical motivation of the DASJ ensemble classification.\n\u2022\nEvaluates the effectiveness of the method using two state-of-the-art CNN architectures for EEG classification.\n\u2022\nDemonstrates the capability of the method to improve the performance of existing CNN architectures.","Hyperspectral image (HSI) classification has attracted significant interest among researchers owing to its diverse practical applications. Convolutional neural networks (CNNs) have been extensively utilized for HSI classification. However, the effectiveness of CNN-based approaches is constrained by the fixed size and structure of the convolutional kernels, as well as their incapacity to capture global features. Moreover, these networks are inadequate in modeling the sequential characteristics of data. Recently, a promising approach, window-based multi-head self-attention has emerged to address the limitations of CNNs and incorporate efficient sequence modeling capabilities. This paper introduces a novel method, multiscale 3D atrous convolution with a lightweight swin transformer (MACLST), that effectively combines the strengths of two networks to capture both local and global features at different scales in HSI classification. The MACLST is designed to process HSI cubes as input and employs a spectral\u2013spatial features extraction module based on multiscale 3D atrous convolution. This module involves parallel branches of 3D layers with varying atrous rates, enabling the extraction of features at multiple scales and resolutions. The extracted spectral\u2013spatial features are fused and passed to the lightweight Swin transformer module as linear embeddings. This module captures long-range dependencies and learns effective feature representations of HSI. To reduce computational complexity, the swin transformer module is simplified and consists of only two stages, offering a more efficient version of the original swin transformer. The proposed MACLST model is extensively evaluated on five widely used benchmark HSI datasets, and the experimental results validate its superiority over state-of-the-art approaches with an overall accuracy of 99.00%, 99.59%, 99.95%, 98.71%, and 94.98% on the Indian Pines, University of Pavia, Salinas Valley, Houston University 2013, and Houston University 2018 datasets, respectively.","Utilizing classical convolutional networks results in lackluster performance in certain classification tasks. To address this problem, recent solutions add extra layers or sub-networks to increase the classification performance of existing networks. More recent methods employ multiple networks coupled with varying learning strategies. However, these approaches demand larger memory and computational requirement due to additional layers, prohibiting usage in devices with limited computing power. In this paper, we propose an efficient convolutional block which minimizes the computational requirements of a network while maintaining information flow through concatenation and element-wise addition. We design a classification architecture, called Half-Append Half-Add Network (HAHANet), built using our efficient convolutional block. Our approach achieves state-of-the-art accuracy on several challenging fine-grained classification tasks. More importantly, HAHANet outperforms top networks while reducing parameter count by at most 54 times. Our code and trained models are publicly available at https://github.com/dlsucivi/HAHANet-PyTorch.","The problem of biometric person identification on the basis of component-based face recognition is considered. It is shown that the face recognition system can be represented as a hierarchically organized multilevel system, in which an ensemble of local classifiers forms \u201csoft\u201d decisions about the images of individual components of a face belonging to given classes. Then, based on the integration of these decisions, the final decision on whether the recognized face belongs to one of the given classes is formed. The problems of constructing a local classifier model, as well as choosing an integrator of intermediate solutions of local classifiers, are formulated and solved.","This work proposes a novel approach to object recognition, particularly for human faces, based on the principle of human cognition. The suggested approach can handle a dataset or problem with a large number of classes for classification more effectively. The model for the facial recognition-based object detection system was constructed using a combination of decision tree clustering based multi-level Backpropagation neural network classifier-TFMLBPNN-DTC and hybrid texture feature (ILMFD+GLCM) and applied on NS and ORL databases. This model produced the classification accuracy (\u00b1standard deviation) of 95.37 \u00b10.951877% and 90.83 \u00b1 1.374369% for single input and 96.58 \u00b10.5604582% and 91.50 \u00b1 2.850439% for group-based decision for NS and ORL database respectively. The better classification results encourage its application to other object recognition and classification issues. This work's basic idea also makes it easier to improve classification management for a wide range of classes.","Graph Neural Networks (GNNs) have emerged as one of the most prominent research areas in accomplishing machine learning tasks over graphical networks. GNNs are prominently used in performing tasks like semi-supervised node classification, link prediction, and community detection. GraphSAGE is one of the most recent GNN models which is being used to accomplish these tasks. A floor plan is an architectural design of a building that represents various floor compartments. In this paper, we represent floor plan(s) as graph(s). We characterize floor room (compartment) classification as a node classification problem. We propose a variation of the traditional GraphSAGE (sample and aggregation) algorithm: Centrality based GraphSAGE (CB-SAGE), which captures the structural properties of the network. We use the average clustering coefficient and average betweenness centrality to capture structure properties. We compute CB scores for all the floor plan graphs. Top 70% nodes (based on CB scores) are selected for the training. During the training, we append the betweenness centrality score of each node as an additional feature in the feature matrix for the embedding process. We conduct experiments on the House-GAN dataset, which contains 1,43,184 vectorized floor plan images. The proposed method outperforms the current state-of-art models in accomplishing the task of floor plan classification. We compare our results with the traditional machine learning approach (MLP) and other GNN-based methods. Our approach achieves an accuracy of 96.70%, which is significantly (approximately 16%) higher than other state-of-the-art methods.\nHighlights\n\u2022\nCB-SAGE: Achieves 96 . 70 % accuracy, outperforming state of the art methods.\n\u2022\nOur innovative GraphSAGE training method relies on CB scores, not random training.\n\u2022\nUtilized three different centrality measures for diverse floor plan insights.\n\u2022\nWe show the statistical significance of our CB-SAGE model compared to other methods.\n\u2022\nOur versatile framework is applicable to various domains\u2019 problem-solving.","With the development of internet technology, cloud computing is becoming increasingly popular, and it has a wide range of applications in various fields, such as mobile payments and the Internet of Things. The big data model is a very important and valuable set of useful and unique information. This article mainly introduces the establishment of an object-oriented architecture-based machine learning system classification model using big data analysis methods, as well as the use of neural network algorithms to construct machine learning system classification patterns. Through examples, a comparative experiment is conducted to verify the effectiveness of traditional manual annotation modeling methods combined with parallel processing. Its experimental results show that the model has high accuracy, with an accuracy rate above 92% and a recall rate above 94%, and its F1 value is infinitely close to 1, indicating that the average accuracy and precision of the model is very high.","Diabetes is considered as one of the deadliest and chronic diseases which causes an increase in blood sugar. Many complications occur if diabetes remains untreated and unidentified. The tedious identifying process results in visiting of a patient to a diagnostic center and consulting doctor. But the rise in machine learning approaches solves this critical problem. The motive of this study is to design a model which can prognosticate the likelihood of diabetes in patients with maximum accuracy. Therefore three machine learning classification algorithms namely Decision Tree, SVM and Naive Bayes are used in this experiment to detect diabetes at an early stage. Experiments are performed on Pima Indians Diabetes Database (PIDD) which is sourced from UCI machine learning repository. The performances of all the three algorithms are evaluated on various measures like Precision, Accuracy, F-Measure, and Recall. Accuracy is measured over correctly and incorrectly classified instances. Results obtained show Naive Bayes outperforms with the highest accuracy of 76.30% comparatively other algorithms. These results are verified using Receiver Operating Characteristic (ROC) curves in a proper and systematic manner.","For imbalanced data, classification efficiency degrades significantly due to the missing information for the positive class, and existing sampling schemes do not consider the distributions of samples. Additionally, the global parameters of fuzzy neighborhoods are set manually. These defects affect the effectiveness of classifier. To address these problems, we offer an adaptive fuzzy multi-neighborhood feature selection methodology with intercluster distance-based hybrid sampling for class-imbalanced data. First, the number of clusters can be defined in terms of the number of samples in the negative or positive class. The initial centers of the clusters are determined according to the number of clusters, and the dissimilarity and similarity measures are calculated by using the intercluster distances between samples. Then, the cluster center, fuzzy membership matrix, and intercluster distance are studied, and then the optimization objective function is designed. The hybrid sampling scheme can be used to combine the generated positive class samples and negative class samples and obtain a class-balanced system. Second, according to the sample distribution, the standard deviation and a set of adaptive fuzzy multi-neighborhood radii are designed. A fuzzy multi-neighborhood similarity relation is defined by introducing a Gaussian kernel model to obtain a fuzzy multi-neighborhood granule, and an improved fuzzy multi-neighborhood rough set model is provided. Uncertain measures of fuzzy neighborhood systems are evaluated by the positive region and dependency. Third, by integrating fuzzy dependence with fuzzy complementary condition entropy, fuzzy multi-neighborhood complementary mutual information is provided on two viewpoints of algebra and information. Finally, a heuristic feature subset selection methodology for imbalanced classification with hybrid sampling using fuzzy c-means clustering is studied to obtain this excellent set of features. Experiments on 26 imbalanced datasets show the effectiveness of our designed algorithm.\nHighlights\n\u25cf\nThe optimization objective function of hybrid sampling via intercluster distance is designed to get a class-balanced system.\n\u25cf\nA set of adaptive fuzzy multi-neighborhood radii is designed to study the fuzzy multi-neighborhood similarity relation.\n\u25cf\nAn adaptive fuzzy multi-neighborhood rough set model is provided to assess uncertain measures of fuzzy neighborhood systems.\n\u25cf\nFuzzy multi-neighborhood complementary mutual information is constructed from the two viewpoints of algebra and information.","Brain computer interface (BCI), has been one of the most popular domains in computing in the recent years. BCI is a pathway which allows communication between computers and the human brain. We acquire real time EEG data with the device, Neurosky Mindwave Mobile, which uses a single dry electrode. Experiment for acquisition of data is carried on 40 subjects (33 male and 7 female). Feature extraction of EEG signals are done by statistical measures such as mean, standard deviation, maximum and minimum amplitudes. In this paper we explore the approach of ensemble learning with classifiers such as random forest classifier to build a BCI model to predict mental states as concentration and meditation. Analysis and results of our proposed model shows an accuracy of 75% using the above methodologies. This model is further implemented in the field of Internet of Things (IoT), for the application of home automation.","Meta-learning is one of the important methods to solve the challenging few-shot learning setting by using previous knowledge and experience to guide the learning of new tasks. Model-agnostic meta-learning (MAML) is one of the most popular meta-learning algorithms, and many variants of MAML have appeared in recent years. However, the performance of this algorithm for few-shot classification falls behind some other algorithms working on this problem. Therefore, its generalization performance needs to be further explored and improved. In view of the generalization problem, we found that MAML always shares an initialization in the process of parameter update, ignoring the bias between different tasks, resulting in limited generalization performance. On the other hand, the sample diversity of meta-learning model is low, and shallow network training is generally used, so it is difficult to obtain good performance based on deep neural network models. Based on these problems, we propose a hybrid optimization meta-learning method based on data augmentation, initialization attenuation, and resolution increase, called Mix-MAML. Experimental results show that our method reaches 76.93% classification accuracy on mini-ImageNet with 100 \u00d7 100 resolution, and 83.62% classification accuracy on CIFAR-FS with 80 \u00d7 80 resolution in the 5-way 5-shot settings under ResNet12, which achieves comparable or even better performance than other algorithms in some standard few-shot learning benchmarks without changing MAML simplicity and model-agnostic.","Recent advance in linear support vector machine with the 0-1 soft-margin loss (L 0 / 1-SVM) shows the ability to solve the 0-1 loss problem directly. However, its theoretical and algorithmic requirements restrict us from directly extending the linear solving framework to its nonlinear kernel form. The lack of an explicit expression of the Lagrangian dual function of L 0 / 1-SVM is a major shortcoming among them. By applying the nonparametric Representation Theorem, we propose a nonlinear model for support vector machine with 0-1 soft-margin loss, called L 0 / 1-KSVM, which skillfully incorporates the kernel technique, and more importantly, follows the success in systematically solving its linear problem. The optimal condition is theoretically explored and a working set selection alternating direction method of multipliers (ADMM) algorithm is introduced to obtain its numerical solution. Furthermore, we first introduce a closed-form definition to the support vector (SV) of L 0 / 1-KSVM. Theoretically, we prove that all SVs of L 0 / 1-KSVM are only located on the parallel decision surfaces. The experimental results show that L 0 / 1-KSVM has much fewer SVs compared to its linear counterpart and the other six nonlinear benchmark SVM classifiers, while maintaining good prediction accuracy.","Data stream classification is of great significance to numerous real-world scenarios. Nevertheless, the prevalent data stream classification techniques are influenced by concept drift and demonstrate unreliability in non-stationary environments. Ensemble models are typically successful when they increase diversity among their members. Several ensembles that enhance diversity have been proposed in literatures. Regrettably, there is no established method to verify that cooperativity indeed improves performance. In response to this knowledge gap, we have developed an innovative ensemble learning framework driven by diversity and cooperativity, termed EDDC, to address the issue. EDDC first dynamically maintains multiple groups of classifiers, with primary classifier in each group chosen to enhance diversity. Next, cooperativity is employed to update groups and replace outdated members. Finally, when environment changes, EDDC adaptively selects either diversity or cooperativity as the strategy for predicting labeling of new instances, while also establishing an excellent performance guarantee. Through simulation experiments, we assessed the performance of EDDC and the benefits of cooperativity for enhancing prediction. The results demonstrated that EDDC is efficient and robust in most scenarios, particularly when dealing with gradual drift. Furthermore, EDDC maintains a competitive edge in terms of classification accuracy and other metrics.\nHighlights\n\u2022\nThis paper proposes an online ensemble model driven by diversity and cooperativity for data stream classification called EDDC.\n\u2022\nAn adaptive voting-strategy is proposed in this paper, which selects diversity or cooperativity as voting methods in ever-changing environment.\n\u2022\nAccording to simulation results, EDDC works superior to the-state-of-art ensembles, especially on gradual drifting stream.","Crop disease is a major threat to agricultural production. Reduced yield due to crop diseases can lead to immeasurable economic losses. Therefore, the detection and classification of crop diseases are of great significance. Based on AlexNet, VggNet, ResNet, and DenseNet, this paper presents an Ensemble Network improve the recognition accuracy. In this study, firstly training four CNN models by randomly sampling the dataset. Before that, these CNN were improved by batch normalization and global average pooling to accelerate convergence, designed a dynamic concatenated ReLU, an improved activation function on ReLU, to improved detection performance. We use focal loss to solve data imbalance. Then weighted voting is used to fuse the four CNN models. Finally, we verify that the EnsembleNet can effectively improve the recognition performance compared with a single network. Verified improvement in recognition performance on our datasets, obtained from PlantVillag.org, containing 4577 citrus leaf images of three categories. The maximum test accuracy in identifying citrus leaf diseases was as high as 93.58\n%\n. Compared with single network, our EnsembleNet can significantly performance. The experimental results showed that this method can be practically applied to the identification of citrus leaf diseases and provides a basis for the identification of other plant leaf diseases.","Deep learning-based incipient fault diagnostic techniques have achieved surprisingly well in wind turbines. Due to component failures, wind turbines must undergo active maintenance, substantially influencing revenue and power generation. Unfortunately, there are consistently uneven data distributions between samples with faults and those without faults, resulting in incorrect fault classification. Wind turbine fault classification has a significant data imbalance problem, compromising learning attention for majority and minority classes. Machine learning methodologies based on Generative Adversarial Networks (GAN), over-sampling, and under-sampling techniques for generating synthetic data have been widely employed to address the imbalance data problem. However, the traditional synthetic minority oversampling technique (SMOTE) accomplishes oversampling using linear interpolation between close minority class samples, which could be confusing, subpar, and indistinguishable from the majority class. This study suggests combining over and under-sampling using adaptive SMOTE and edited nearest neighbors (ASMOTE-ENN) that incorporate over-sampling with adaptive SMOTE and under-sampling with ENN to improve the quality of the generated samples. With this resampling technique, noise in an imbalanced dataset is reduced on three levels by using an adaptive nearest neighbor selection algorithm to find the nearest neighbors that are visible. Then use SMOTE to create samples that precisely fall into the minority class, and later use the ENN technique to eliminate instances that contribute to noise afterwards. Resampling data created by combining over- and under-sampling approaches to match the data distribution over all classes is the foundation of the suggested method\u2019s efficacy. A hybrid ensemble method is used for effective classification, including boosting, bagging, and stacking techniques. The original unbalanced and balanced data using the ASMOTE-ENN algorithm were classified using the proposed hybrid ensemble method. The classification results show that the proposed strategy is more accurate than a few imbalanced fault diagnosis techniques.\nHighlights\n\u2022\nTo gain complete knowledge, characteristics of SCADA data are obtained using data analysis.\n\u2022\nWe present the adaptive SMOTE-ENN algorithm-based data resampling technique to deal with imbalanced data.\n\u2022\nCreate a hybrid ensemble classification technique to identify faulty and no-fault events effectively.\n\u2022\nThe results demonstrate that the suggested method outperforms the most popular ML techniques.","Artificial neural networks are being used in many fields for different purposes. Medical diagnosis is one of the major purposes. In the field of medical, classification plays an important role as the main aim of the doctor is to classify whether a person is suffering from the disease or not. The objective of this paper is to evaluate neural network for the detection of alopecia in human beings and to find the accuracy. With the help of proposed model, the clinical experts will be able to get a second opinion that will help them to take proper decisions for diagnosing the presence of this disease in patients. This second opinion is crucial due to many factors while doing disease identification. These factors include increased population, environmental pollution, growing demands for proper medication, and less availability of medical experts to cope up with this increasing demand. Also, the dynamic nature of disease symptoms plays an important role in correct diagnosis of a certain disease. The proposed system uses a feedforward artificial neural network and backpropagation algorithm to classify patients with alopecia and without alopecia. The evaluation of the proposed system is done with the help of performance plot as well as regression plot. Experimental results show that the accuracy of proposed system is 91% which is reliable enough for a clinical expert to make his decisions.","Normal blood supply to the human brain may be marred by the presence of a clot inside the blood vessels. This clot structure called emboli inhibits normal blood flow to the brain. It is considered as one of the main sources of stroke. Presence of emboli in human\u2019s can be determined by the analysis of transcranial Doppler signal. Different signal processing and machine learning algorithms have been used for classifying the detected signal as an emboli, Doppler speckle, and an artifact. In this paper, we sought to make use of the wavelet transform based algorithm called Wavelet Scattering Transform, which is translation invariant and stable to deformations for classifying different Doppler signals. With its architectural resemblance to Convolutional Neural Network, Wavelet Scattering Transform works well on small datasets and subsequently was trained on a dataset consisting of 300 Doppler signals.\nTo check the effectiveness of extracted Scattering transform based features for Doppler signal classification, learning algorithms that included multi-class Support vector machine, k-nearest neighbor and Naive Bayes algorithms were trained. Comparative analysis was done with respect to the handcrafted Continuous wavelet transform features extracted from samples and Wavelet scattering with Support vector machine achieved an accuracy of 98.89%. Also, with set of extracted scattering coefficients, Gaussian process regression was performed and a regression model was trained on three different sets of scattering coefficients with zero order scattering coefficients providing least prediction loss of 34.95%.\nHighlights\n\u2022\nA white-box network Wavelet scattering transform is used for Doppler signal classification.\n\u2022\nWith architecture similarity to convolutional neural network, Wavelet scattering transform is well suited for small datasets.\n\u2022\nContinuous wavelet transform based handcrafted features are extracted for comparison with Wavelet scattering transform.\n\u2022\nRegression and classification results are presented for Doppler signal dataset.","Protein sequence classification is a crucial research field in bioinformatics, playing a vital role in facilitating functional annotation, structure prediction, and gaining a deeper understanding of protein function and interactions. With the rapid development of high-throughput sequencing technologies, a vast amount of unknown protein sequence data is being generated and accumulated, leading to an increasing demand for protein classification and annotation. Existing machine learning methods still have limitations in protein sequence classification, such as low accuracy and precision of classification models, rendering them less valuable in practical applications. Additionally, these models often lack strong generalization capabilities and cannot be widely applied to various types of proteins. Therefore, accurately classifying and predicting proteins remains a challenging task. In this study, we propose a protein sequence classifier called Multi-Laplacian Regularized Random Vector Functional Link (MLapRVFL). By incorporating Multi-Laplacian and L 2,1 \u2212 norm regularization terms into the basic Random Vector Functional Link (RVFL) method, we effectively improve the model's generalization performance, enhance the robustness and accuracy of the classification model. The experimental results on two commonly used datasets demonstrate that MLapRVFL outperforms popular machine learning methods and achieves superior predictive performance compared to previous studies. In conclusion, the proposed MLapRVFL method makes significant contributions to protein sequence prediction.\nHighlights\n\u2022\nMLapRVFL improves protein sequence prediction.\n\u2022\nEnhanced generalization and accuracy with Multi-Laplacian and L 2,1 \u2212 norm regularization terms.\n\u2022\nMLapRVFL outperforms in protein sequence classification.","Psychological changes in humans are the result of emotions which occur due to activities in daily life. To understand these changes in behavioral pattern, research on a ective computing has emerged. Emotions are an integral part of our daily lives, based on which in this paper an investigation have been made to analyze the impact of positive and negative emotions using Electroencephalogram (EEG). Three classes of emotions namely calm, anger and happiness have been studied. The EEG signals are recorded in real time from 10 subjects while watching di erent emotions video clips of 2 minutes each. Next, the fractal dimension feature has been extracted from raw EEG. To further detect emotional states, the extracted features have been classified using Support Vector Machine (SVM) with radial basis function (RBF) kernel with an average accuracy of 60%. The proposed methodology shows that emotions recognition is possible from EEG signals.","Amidst the critical role that high-quality labeled data plays in advancing machine learning, the persistence of noise within widely-used datasets remains a challenge. While noise learning has gained traction within machine learning, particularly in computer vision, its exploration in text and multimodal classification domains has lagged. Furthermore, a comprehensive comparison of noise learning techniques in text and multimodal classification has been lacking, partly due to variations in experimental noise settings across prior studies. Addressing these gaps, this research introduces a pioneering Multimodal Infusion Joint Training (MinJoT) framework featuring a novel co-regularized loss function that seamlessly integrates multimodal information during joint training. This framework notably excels in maintaining model robustness amidst noisy data environments. Adapting established noise learning methods from computer vision to text classification, the study conducts extensive experiments across five English and Chinese textual and multimodal datasets, involving four methods, five noise modes, and seven noise rates. Critically, this work challenges the implicit assumption that widely-used datasets are devoid of noise, revealing that these datasets indeed encompass noise levels ranging from 0.61% to 15.77% which is defined as intrinsic noise in this study. For the first time, the study investigates the impact of intrinsic noise on model performance, categorizing it into distinct levels of ambiguity. To facilitate accurate method comparison, a new dataset, Golden-Chnsenticorp (G-Chnsenticorp), is introduced, carefully crafted to be free of intrinsic noise. This research establishes the inaugural noise learning benchmark for text classification, while simultaneously pioneering the first noise learning framework tailored for multimodal sentiment classification. Through these contributions, the study advances the understanding of noise learning in text and multimodal contexts, providing a novel framework, benchmarks, and insights to propel the field forward.\nHighlights\n\u2022\nIntroducing MinJoT, a novel noise learning framework for multimodal classification.\n\u2022\nStudying intrinsic noise in text classification and its impacts.\n\u2022\nCreating G-Chnsenticorp, a noise-free dataset, for reliable research in this area.","The ability to classify images is dependent on having access to large labeled datasets and testing on data from the same domain of which the model was trained on. Classification becomes more challenging when dealing with new data from a different domain, where gathering and especially labeling a larger image dataset for retraining a classification model requires a labor-intensive human effort. Cross-domain classification frameworks were developed to handle this data domain shift problem by utilizing unsupervised image-to-image translation models to translate an input image from the unlabeled domain to the labeled domain. The problem with these unsupervised models lies in their unsupervised nature. For lack of annotations, it is not possible to use the traditional supervised metrics to evaluate these translation models to pick the best-saved checkpoint model. This paper introduces a new method called Domain-knowledge Inspired Pseudo Supervision (DIPS) which utilizes Gaussian Mixture Models and domain knowledge to generate pseudo annotations to enable the use of traditional supervised metrics. This method was designed specifically to support cross-domain classification applications contrary to other typically used metrics such as the Fr\u00e9chet Inception Distance (FID) which were designed to evaluate the model in terms of the quality of the generated image from a human-eye perspective. DIPS outperforms state-of-the-art GAN evaluation metrics when selecting the optimal saved checkpoint. Furthermore, DIPS showcases its robustness and interpretability by demonstrating a strong correlation with truly supervised metrics, highlighting its superiority over existing state-of-the-art alternatives The boiling crisis problem has been approached as a case study. The code and data to replicate the results can be found on the official GitHub-repository https://github.com/Hindawi91/DIPS. .\nHighlights\n\u2022\nDIPS utilizes domain knowledge and a GMM model to provide pseudo supervision.\n\u2022\nDIPS introduces a metric for evaluating UI2I models in cross-domain classification.\n\u2022\nDIPS outperforms SOTA unsupervised metrics such as the FID, KID, IS and MMD.\n\u2022\nDIPS correlates highly with true supervision, making it robust and explainable.\n\u2022\nDIPS real-world applicability is demonstrated through the boiling crisis problem.","Eosinophilic Esophagitis (EoE) is an allergic condition increasing in prevalence. To diagnose EoE, pathologists must find 15 or more eosinophils within a single high-power field (400X magnification). Determining whether or not a patient has EoE can be an arduous process and any medical imaging approaches used to assist diagnosis must consider both efficiency and precision. We propose an improvement of Adorno et al\u2019s approach for quantifying eosinphils using deep image segmentation. Our new approach leverages Monte Carlo Dropout, a common approach in deep learning to reduce overfitting, to provide uncertainty quantification on current deep learning models. The uncertainty can be visualized in an output image to evaluate model performance, provide insight to how deep learning algorithms function, and assist pathologists in identifying eosinophils.","In this paper, we address an issue of finding explainable clusters of class-uniform data in labeled datasets. The issue falls into the domain of interpretable supervised clustering. Unlike traditional clustering, supervised clustering aims at forming clusters of labeled data with high probability densities. We are particularly interested in finding clusters of data of a given class and describing the clusters with the set of comprehensive rules. We propose an iterative method to extract high-density clusters with the help of decision-tree-based classifiers as the most intuitive learning method, and discuss the method of node selection to maximize quality of identified groups.","Nature inspired algorithms have become popular for discovering classification rules due to their ability to effectively handle large and complex search spaces. However, nature inspired algorithms have to compute the fitness of individuals (candidate classification rules) over successive generations repeatedly. Each fitness computation requires scanning the training data. Since the database scan is computationally expensive operation, the execution time for nature inspired algorithms for discovering classification rules grows unreasonably faster for bulky datasets. This paper proposes a novel fitness computation framework for nature inspire algorithms by using a list structure. The indices of instances, covered by every possible attribute-value pair with respect to each class in the training data, are stored in the suggested list structure. The list is prepared only once in advance and stores all information to compute the fitness of any rule that may come up in the life time of the nature inspired algorithm. The existence of the pre-maintained list eliminates the need of scanning training data again and again for fitness computation. We have conducted experiments on 12 datasets from UCI machine learning repository. The results show that the suggested fitness computational framework brings significant speed gain without compromising predictive accuracy. Although, a Genetic Algorithm is used for classification rule discovery as the nature inspired algorithm in this paper, the fitness computation framework is general and can be used with any other nature inspired algorithm.","Although the high number of bands in hyperspectral remote sensing images increases their usefulness, it also causes some processing difficulty. In supervised classification, one problem is decreasing classification accuracy due to the insufficient training samples against the bands. A way to deal with this problem is the selection of appropriate bands by the metaheuristic methods. Because of the stochastic search, the selected bands differ in any implementation of a metaheuristic method. So, the results obtained from the classification of these different band subsets will also have some differences. In this study, a fusion-based approach has been proposed to improve the classification of hyperspectral remote sensing images by multiple implementations of a metaheuristic method for band selection. We have tested the proposed method using ten metaheuristic methods with different objective functions on four well-known datasets. The results show the proposed fusion-based approach successfully improves the classification accuracy in all experiments. The accuracy improvement varies depending on the metaheuristic method, the objective function, and the dataset and ranges from 0.4% to 15.7%. The proposed method improves the classification of complex datasets more and affects weaker objective functions considerably. The results also show the proposed method brings the accuracy of different metaheuristic methods close to each other and reduces the sensitivity of selecting the proper ones. Thus, an automated classification system can be obtained using a parameter-less method.\nHighlights\n\u2022\nA new fusion-based classification is proposed for hyperspectral images based on stochastic nature of metaheuristic methods.\n\u2022\nA fully automated classification system is developed to classify hyperspectral remote sensing images.\n\u2022\nMany Traditional and new metaheuristic methods include PSO, CSO, GWO, GEO, JSA, \u2026 are examined in the proposed framework.\n\u2022\nThe experiments are done in both case of filter and wrapper band selection with different objective function.","As pests can cause heavy crop losses, integrated pest management is a vital aspect of agriculture. In general, pest recognition is essential to the integrated pest management. Many studies have explored how to achieve automatic pest recognition using computer vision and artificial intelligence techniques. However, most existing methods did not consider the class ambiguity problem. That is, a pest image may belong to multiple possibly true categories, but only one possible class label is assigned to the pest image. To close the above gap, this study converted the conventional one-label pest classification task into a multi-label one. In detail, the state-of-the-art deep network, Swin Transformer, was first modified to enable the predicted scores of possible classes to approximate one simultaneously by replacing the fully connected soft-max layer with a sigmoid activation layer. Then, a two-stage supervised learning algorithm using the binary cross entropy loss and the novel soft binary cross entropy loss was designed to train the Swin-Transformer-based multi-label classification model with single-label images. Experiments on the IP102 image dataset showed that the proposed method obtained the highest F 1-score value of 60.83%. It outperformed the second-best one by a margin of 7.52%. In conclusion, the proposed method can tackle the pest class ambiguity problem on IP102 better.\nHighlights\n\u2022\nExploit the multi-label classification to tackle the pest class ambiguity problem.\n\u2022\nDevelop a modified Swin-Transformer-based multi-label classification model.\n\u2022\nPropose the soft binary cross entropy loss to train the model with single-label images.","This study conducts a thorough evaluation of deep learning architectures, pretraining methods, and finetuning approaches for mammogram classification for tissue density. No architecture was distinctly superior. However, models pretrained on ImageNet consistently surpassed those trained on custom mammogram datasets. Finetuning strategies played a crucial role in model performance. In particular, finetuning the entire model yielded better results. Investigation of confusion matrices revealed that most misclassifications occurred within a one-grade difference, but severe misclassifications were observed in certain configurations. While some architectures offered comparable performance, trade-offs between model performance and computational efficiency were observed, with convolutional neural networks showing faster inference times on CPUs compared to vision transformers.","Multiple instance learning (MIL) is a classic weakly supervised learning approach, in which samples are grouped into bags that may contain varying numbers of instances. A bag is designated as positive if it contains at least one positive instance; otherwise, it is considered negative. Previous studies have consistently assumed that the bag labels are completely known. In fact, labeling every bag can be extremely challenging or even unfeasible due to the exorbitant expenses in terms of time and labor. Fortunately, it is much easier to obtain the similarity confidence, which represents the probability of two bags sharing the same label. How to employ it in MIL is worthy of study. Inspired by the above study, we present the first attempt to investigate MIL from similarity-confidence bags. Therefore, this paper proposes a new framework for training bag-level classifiers that adheres to the principle of empirical risk minimization. Moreover, we theoretically derive a generalization error bound to guarantee model convergence. Finally, we implement risk correction to mitigate potential over-fitting problem and provide theoretical consistency. Numerical experiments on eight datasets further validate the effectiveness of the proposed bag-level classifier.\nHighlights\n\u2022\nWe first explore MIL from similarity-confidence bags that makes sense in many scenes.\n\u2022\nWe propose an unbiased estimator at bag-level that does not rely on loss or optimizer.\n\u2022\nWe introduce a method for estimating unknown prior probability.\n\u2022\nWe derive a theoretical error bound and a optimal parameter convergence rate.\n\u2022\nWe employ risk correction methods to mitigate over-fitting and derive the consistency.","Over the past two decades, an increasing number of large-scale structures have been built around the world. Constructing these structures has been a time consuming and highly expensive process. Thus, providing a structural health monitoring system to guarantee their proper functionality is important. In recent years, the advancement of technology and artificial intelligence methods based on signal processing and machine learning has attracted the attention of researchers. The challenges currently exist in the field of structural health monitoring to identify and classify damages to achieve high accuracy in a health-monitoring program. The presence of noise in measurement, various exciting load types, and varying environmental conditions cause difficulty in the practical identification and classification of damage in structures. Recent studies have employed finite element modeling to test the effectiveness of proposed methods for identifying damages in structures. However, detecting damage in real-world structures as mentioned above, presents unique difficulties, and the effectiveness of the proposed methods for damage detection in real-world structures remains uncertain. In order to improve the performance of damage detection methods and increase the accuracy of these methods as much as possible, the most important action is to identify damage sensitive data in the structure. The next challenge is to choose a high performance algorithm for damage identification and classification. One of the advanced algorithms, which has a very high ability to extract the desired features from the measured data, is the XGBoost algorithm. This algorithm has recently attracted the attention of researchers and has been used in different fields. So far, the ability of this algorithm has not been examined in the field of damage detection in order to extract desirable features. This article deals with the identification, classification, and severity of damages in the SMC benchmark bridge, which is an existing megastructure in the real world, as well as the IASC-ASCE benchmark structure, whose responses were taken under applied loads in the laboratory environment. First, using the XGBoost algorithm, the importance of the features extracted from the sensors' data is evaluated, and then the features, which are effective in the damage detection process, are selected. The results of this algorithm indicate that only by selecting 6 features from a large volume of data, the best performance can be achieved and selecting more does not help increase efficiency. In the next step, the Stacking method, which is a hybrid machine learning algorithm for damage classification, is evaluated and compared with some conventional machine learning algorithms that have been used in previous studies. The Stacking method stands out as the top performer with an average accuracy rate of 93.1%, leading to the conclusion that it is the most effective approach. Finally, by applying the presented algorithm to the two mentioned structures, its validation is appraised.","Neurodegenerative disorders like Alzheimer\u2019s disease (AD) are irreversible and show atrophies in the area of the cerebral cortex of brain. AD leads to loss of memory and other cognitive impairments. The AD subjects are evaluated based on magnetic resonance imaging scans. The data may have the problem of class imbalance, noise and outliers which is a great challenge for classification. Support vector machines and twin support vector machine-based classifiers may not effectively deal with these problems as both these models assume that all the samples are equally important for the separating hyperplane. To overcome these issues, we propose intuitionistic fuzzy least square twin support vector machine for class imbalance problems (IFLSTSVM) and class specific-IFLSTSVM (CS-IFLSTSVM). To minimize the effects of class imbalance, the samples are appropriately weighted to minimize their effect on the optimal hyperplane. Moreover, we use intuitionistic fuzzy scores to overcome the issues of noise and outliers. Intuitionistic fuzzy score values generate appropriate weights by considering both the distance of the samples from the class centroid as well as the heterogeneity of the samples. The proposed models IFLSTSVM and CS-IFLSTSVM are efficient as they need to solve a system of linear equations. In Alzheimer\u2019s disease diagnosis, the proposed IFLSTSVM and CS-IFLSTSVM models showed better performance in MCI_vs_AD and CN_vs_MCI cases, respectively. Moreover, the proposed models showed better performance in the diagnosis of breast cancer classification. The statistical analysis carried out over KEEL and UCI data leads to the superiority of the proposed models. The source code of the proposed model is available at https://github.com/mtanveer1/Diagnosis-of-Alzheimer-s-disease-via-Intuitionistic-fuzzy-least-squares-twin-SVM.\nHighlights\n\u2022\nTo handle the CIL issue, training data are assigned weights based on imbalance ratio.\n\u2022\nThe regularization term improves the generalization performance of proposed models.\n\u2022\nThe proposed models solve system of linear equations. Hence, it is very efficient.\n\u2022\nThe weights reduce the impact of noise/outliers in classifying imbalanced data.\n\u2022\nThe results on biomedical and UCI data show the superiority of proposed models.","The wide acceptance of decision tree classifiers lies with their fast performance and simple nature. The J48 group of decision tree classifiers are widely used for classification and decision-making process. In this paper, three popular J48 group classifiers, namely J48, J48Consolidated and J48Graft are evaluated using both binary and multi-class datasets across thirteen performance matrices, which is unique in its area. In order to come across a versatile classifier, the evaluated results of these classifiers are nourished to a prominent multi-criteria decision-making module called Technique for Order of Preference by Similarity to Ideal Solution (TOPSIS) for suitable rank allocation.","Recently, the prompt tuning technique, which incorporates prompts into the input of the pre-training language model (like BERT, GPT), has shown promise in improving the performance of language models when facing limited annotated data. However, the equivalence of template semantics in learning is not related to the effect of prompts and the prompt tuning often exhibits unstable performance, which is more severe in the domain of the scientific domain. To address this challenge, we propose to enhance prompt tuning using data augmentation with L2 regularization. Namely, pairing-wise training for the pair of the original and transformed data is performed. Our experiments on two scientific text datasets (ACL-ARC and SciCite) demonstrate that our proposed method significantly improves both accuracy and robustness. By using 1000 samples out of 1688 in the ACL-ARC training set, our method achieved an F1 score 3.33% higher than the same model trained on all 1688-sample data. In the SciCite dataset, our method surpassed the same model with labeled data reduced by over 93%. Our method is also proved to have high robustness, reaching F1 scores from 1% to 8% higher than those models without our method after the Probability Weighted Word Saliency attack.","This study introduces an ensemble methodology, namely, hybrid feature ranking and classifier aggregation (HyFraCa), to integrate ensemble feature selection and ensemble classification in a composite framework. The proposed HyFraCa is embedded in a multi-criteria decision-making (MCDM)-based scheme for feature ranking and classifier weighting, with an effective aggregation rule that yields a consensus feature ranking from ensembles of heterogeneous classifiers and feature selectors. Experimental evaluations on 20 public UCI datasets demonstrated the superiority of HyFraCa in producing a more accurate and generalizable classification compared with state-of-the-art benchmark ensemble methods. HyFraCa also provides robust and reliable consensus feature rankings, which are favorable for real-world classification problems in which feature interpretability is emphasized.","Recently meta-learning-based few-shot learning methods have been widely used for relation classification. Previous work reveals that meta-learning performs poorly in scenarios where the edge probability distribution of the target domain dataset appears to be significantly different from the source domain. In this paper, we enhance the meta-learning framework with high-dimensional semantic feature extraction and hyperplane projection metrics for meta-tasks. First, we enhance the focus of BERT on entity words by adding entity markers and vector pooling. After that, the high-dimensional semantic features of the support set are extracted and transformed into hyperplanes. Finally, we obtain the classification results by calculating the projection distance between the query sample and the hyperplane. In addition, we design a auxiliary function with a plane correction factor, which can better amplify the plane spacing and reduce the degree of category confusion, which is important for solving the problem of metric spatial loss. Experiments on two real-world few-shot datasets show that our model HPN is more effective in classifying few-shot relations in the same domain and domain-adapted scenarios. And HPN is more stable on NOTA tasks.\nHighlights\n\u2022\nA hyperplane projection network is proposed for few shot relation classification.\n\u2022\nPropose an effective entity encoder that extracts better features.\n\u2022\nFirst apply a novel hyperplane module to generate class-level feature.","Face shape classification plays a crucial role in various applications, including facial recognition and personalized beauty rec- ommendations like hairstyles. In some cases, it is tricky to establish one's genuine facial shape, making it difficult to further process this information. This paper attempts to harness the advantages of the Swin Transformer to classify face shapes with higher accuracy. We also compared the test results with augmentation to evaluate the improvement. Our proposed method obtained 86.34% accuracy with augmentation, which is decent for further processing in various applications that require face shape recognition.","Heart disease is a major health issue, and accurate diagnosis of irregular heartbeats and heart failure is crucial. Current diagnostic processes can be time-consuming, requiring significant effort from clinicians. An effective classifier, ADCGNet: Attention-based Dual Channel Gabor Network is proposed to address this challenge by accurately classifying anomalies. ADCGNet involves pre-processing every ECG beat into two-dimensional images using Analytical Morlet transform and then applying thirty-two Gabor filters and Sobel edge detection to enhance features. ADCGNet comprises three blocks, with the first block using dual channels to extract essential features in the images efficiently. The second block includes a multi-head attention mechanism to focus on relevant features, and the third block uses a SoftMax activation function to perform classification tasks. Extensive experiments with public datasets from PhysioNet, and comparison with several state-of-the-art classifiers indicate ADCGNet is superior. Specifically, ADCGNet achieved an accuracy of 99.17%, 98.98% in precision, a recall of 98.87%, an F1-score of 98.82% and AUC, 98.75% with optimal hyperparameters. Further, a GRAD-CAM visualization of activated areas on the test samples gives graphical insight into the performance of ADCGNet. The proposed ADCGNet classifier has promising potential for enhancing the diagnosis of heart disease, and we believe it will be of much interest to the medical community.","The Internet is a crucial way to share information in both personal and professional areas. Sentiment analysis attracts great interest in marketing, research, and business today. The instability faced by imbalanced datasets on sentiment analysis is examined in this research. Balancing the datasets using techniques based on under-sampling and over-sampling is examined to achieve more efficient classification results as the effects of using BERT as word embedding and ensemble learning methods for classification. The effects of the resampling training set algorithms on different deep learning classifiers were investigated using BERT as a word embedding model and Cohen's kappa, accuracy, ROC-AUC curve, and MCC as evaluation metrics with k-fold validation on three sentiment analysis datasets containing English, Arabic, and Moroccan Arabic Dialect texts. Also, we did those performance metrics for all models when scaling the dataset for training and testing, and we calculated the memory and the execution time for each model. Finally, we analyzed the National Office of Railways of Morocco (ONCF) customers' Facebook comments in Modern Standard Arabic (MSA) and MD to determine customer satisfaction as positive, negative, and neutral comments.","Hyperspectral facial data presents creative information, taken with Hyperspectral camera, as compared to the traditional camera and images. This facial spectral dataset is a rich source of information extraction and analysis over traditional single band data with constant wavelength of Electromagnetic Spectrum (EMS). Herein study, Hyperspectral Image (HSI) classification and recognition have done in Visible to Near Infrared (NIR) region using supervised classifiers: Maximum Likelihood, Minimum Distance and SAM (Spectral Angle Mapper). The experiment is performed on CMU Hyperspectral Face Datasets (HFDS) within the spectral range of 610 to 1050 nm (VIR-NIR), having 45 spectral bands using ENVI 4.8 image processing tool. Supervised classification is performed using conventional supervised classifiers (MLH, MDA and SAM) on pre-processed HSI, further compressed dataset has classified by PCAMLH, PCAMD and PCASAM classifiers.","With advancement in satellites and remote sensing technology, reflectance data are increasingly being used in agriculture. In this paper, the machine learning models have been explored with three distinct types of properties to classify the hydro-metrological rainfall parameter, Standardized Precipitation Index (SPI), and Vegetation Condition Index (VCI) to monitor the agriculture state of Rajasthan. These three distinct indexes are used to classify the geospatial Rainfall-SPI, Rainfall-VCI, and Rainfall-SPI-VCI models. The K-fold cross validation has been used to evaluate the robustness of the outperforming classification method. The outcome shows that DecisionTree and RandomForest classification model performs outstanding machine learning classification methods for vegetation with sensitivity of 0.798 and accuracy of 95.792% on test dataset with DecisionTree and of 0.796 and accuracy of 95.584% on testing dataset with RandomForest model."],"author":["Ankit and Saleena, Nabizath","Manivannan, Siyamalan","Liu, Jingyi and Li, Sheng","Ali, Muhammad and Zhu, Peimin and Jiang, Ren and Huolin, Ma and Ehsan, Muhsan and Hussain, Wakeel and Zhang, Hao and Ashraf, Umar and Ullaah, Jared","Sotero, Roberto Carlos and Sanchez-Bornot, Jose Miguel and Iturria-Medina, Yasser","Huang, Yingcheng and Xiao, Fuyuan","Mushava, Jonah and Murray, Michael","Zhang, Liu and Wei, Yaoguang and Liu, Jincun and Wu, Jianwei and An, Dong","Chen, Benwei and Zhang, Xianyong and Yang, Jilin","Zeng, Jie and Xiao, Fuyuan","Ozkaya, Suat Gokhan and Baygin, Mehmet and Barua, Prabal Datta and Tuncer, Turker and Dogan, Sengul and Chakraborty, Subrata and Acharya, U. Rajendra","Zhang, Bo and Ming, Zuheng and Liu, Yaqian and Feng, Wei and He, Liang and Zhao, Kaixing","Ritesh and Bhagvati, Chakravarthy","Shrivastava, Saurabh and Shukla, Sanyam and Khare, Nilay","NaN","Li, Jianqiang and Cheng, Wenxiu and Xu, Xi and Zhao, Linna and Liu, Suqin and Gao, Zhengkai and Ye, Caihua and You, Huanling","Erceg, Mirjana and Palamas, Georgios","Dharwada, Sriram and Tembhurne, Jitendra and Diwan, Tausif","Yuan, Yage and Wei, Jianan and Huang, Haisong and Jiao, Weidong and Wang, Jiaxin and Chen, Hualin","Mu, Tengxiao and Liang, Yaru and Liu, Lingzhi","Liu, Dan and Zhong, Shisheng and Lin, Lin and Zhao, Minghang and Fu, Xuyun and Liu, Xueyun","Li, Ximing and Wang, Bing and Wang, Yang and Wang, Meng","Raza, Rehan and Zulfiqar, Fatima and Khan, Muhammad Owais and Arif, Muhammad and Alvi, Atif and Iftikhar, Muhammad Aksam and Alam, Tanvir","Rajalakshmi, A. and Sridhar, S. S.","NaN","Wang, Rui and Huang, Weiguo and Zhang, Xiao and Wang, Jun and Ding, Chuancang and Shen, Changqing","Ashraf, Mudasir and Zaman, Majid and Ahmed, Muheet","Tripathi, Diwakar and Edla, Damodar Reddy and Kuppili, Venkatanareshbabu and Bablani, Annushree and Dharavath, Ramesh","Gyasi-Agyei, Amoakoh","Firdous, Naira and Din, Nusrat Mohi Ud and Assad, Assif","Zhu, Jianjian and Su, Zhongqing and Wang, Qingqing and Lan, Zifeng and Siu-fai Chan, Frankie and Han, Zhibin and Wang, Zhaokun and Wing-fai Wong, Sidney and Chi-fung Ngan, Andy","Deng, Jiakang and Xing, De and Chen, Cheng and Han, Yongguo and Chen, Jianqiang","Dai, Lulu and Han, Mingyue","NaN","Matorin, S. I. and Gul, S. V. and Shcherbinina, N. V.","Ren, He and Wang, Jun and Huang, Weiguo and Jiang, Xingxing and Zhu, Zhongkui","Hechen, Zhenzhe and Huang, Wei and Yin, Le and Xie, Wenjing and Zhao, Yixin","Nouri, Zahra and Kiani, Vahid and Fadishei, Hamid","Zhang, Zhuo and Wang, Hongfei and Jiang, Wen and Geng, Jie","Khatri, Sabita and Arora, Deepak and Kumar, Anil","Chen, Jianting and Ding, Ling and Yang, Yunxiao and Xiang, Yang","Li, Li and Han, Qihong and Li, Jiayao and Cui, Zhanqi","Qian, Wenbin and Xiong, Yinsong and Ding, Weiping and Huang, Jintao and Vong, Chi-Man","Mu, Qiaoxu and Zhang, Meng","Li, Jingbo and Yang, Guijun and Yang, Hao and Xu, Weimeng and Feng, Haikuan and Xu, Bo and Chen, Riqiang and Zhang, Chengjian and Wang, Han","Jhee, Jong Ho and Yeon, Jeongheun and Kwak, Yoonshin and Shin, Hyunjung","Wu, Hanrui and Li, Nuosi and Zhang, Jia and Chen, Sentao and Ng, Michael K. and Long, Jinyi","Sithungu, Siphesihle Philezwini and Ehlers, Elizabeth Marie","Chen, Xiangbo and Nishiyama, Masashi and Iwai, Yoshio","Azizi, Afshin and Zhang, Zhao and Rui, Zhaoyu and Li, Yunxi and Igathinathane, C. and Flores, Paulo and Mathew, Jithin and Pourreza, Alireza and Han, Xiongzhe and Zhang, Man","Hasan, Ali M. and Al-Waely, Noor K.N. and Aljobouri, Hadeel K. and Jalab, Hamid A. and Ibrahim, Rabha W. and Meziane, Farid","Saha, Sourajit and Saha, Nisha","Ding, Biyun and Zhang, Tao and Wang, Chao and Liu, Ganjun and Liang, Jinhua and Hu, Ruimin and Wu, Yulin and Guo, Difei","NaN","NaN","He, Xiaoxu","Sun, Zhigang and Wang, Guotao and Zhai, Guofu and Li, Pengfei and Liang, Qi and Zhang, Min","Nariswari, Rinda and Pudjihastuti, Herena","Putro, Nur Achmad Sulistyo and Avian, Cries and Prakosa, Setya Widyawan and Leu, Jenq-Shiou","Hong, Jung-Sik and Lee, Jeongeon and Sim, Min K.","Duan, Jicong and Gu, Yan and Yu, Hualong and Yang, Xibei and Gao, Shang","Kathuria, Charu and Mehrotra, Deepti and Misra, Navnit Kumar","Ke, Ting and Ge, Xuechun and Yin, Feifei and Zhang, Lidong and Zheng, Yaozong and Zhang, Chuanlei and Li, Jianrong and Wang, Bo and Wang, Wei","Zheng, Jian and Hu, Xin","Che, Xiaoya and Chen, Degang and Deng, Jiang and Mi, Jusheng","Fan, Caoyun and Chen, Wenqing and Tian, Jidong and Li, Yitian and He, Hao and Jin, Yaohui","Wu, Peishu and Wang, Zidong and Li, Han and Zeng, Nianyin","Jeune, Hayden and Pechan, Niklas and Reitsma, Sharn-Konet and Kempa-Liehr, Andreas W.","Mousa, Yehia and Taha, Radwa and Kaur, Ranpreet and Afifi, Shereen","He, Zhengxiang and Jia, Mingtao and Wang, Liguan","Wasi, Nesar Ahmad and Abulaish, Muhammad","Chandana Mani, R.K. and Kamalakannan, J.","Sun, Guoying and Cheng, Yanan and Zhang, Zhaoxin and Tong, Xiaojun and Chai, Tingting","Yao, Rujing and Wu, Ou","Edla, Damodar Reddy and Ansari, Md Fahim and Chaudhary, Nikhil and Dodia, Shubham","Lung, Rodica Ioana and Suciu, Mihai-Alexandru","Ojo, Akinlolu Oluwabusayo and Bouguila, Nizar","Xiang, Yu and Bai, Lei","Bahri, Shivani and Bahri, Pranav and Lal, Sangeeta","Zhong, Mingwei and Yi, Siqi and Fan, Jingmin and Zhang, Yikang and He, Guanglin and Cao, Yunfei and Feng, Lutao and Tan, Zhichao and Mo, Wenjun","Alcacer, Aleix and Martinez-Garcia, Marina and Epifanio, Irene","NaN","Bahala, Renante G. and Sagum, Ria A.","Ma, Shengjin and Yuan, Wang and Wang, Yiting and Tan, Xin and Zhang, Zhizhong and Ma, Lizhuang","Chen, Qiong and Huang, Tianlin and Liu, Qingfa","Zhang, Yu and Zuo, Xin and Zheng, Xuxu and Gao, Xiaoyong and Wang, Bo and Hu, Weiming","Wei, Jianan and Wang, Jiaxin and Huang, Haisong and Jiao, Weidong and Yuan, Yage and Chen, Hualin and Wu, Rui and Yi, Junhui","Liang, Xiayu and Gao, Ying and Xu, Shanrong","Elaraby, Nagwa and Barakat, Sherif and Rezk, Amira","NaN","Gou, Hongyuan and Zhang, Xianyong and Yang, Jilin and Lv, Zhiying","Abd-Alhalem, Samia M. and Marie, Hanaa Salem and El-Shafai, Walid and Altameem, Torki and Rathore, Rajkumar Singh and Hassan, Tarek M.","Yuan, Gaoteng and Lu, Lu and Zhou, Xiaofeng","Isnan, Mahmud and Hidayat, Alam Ahmad and Pardamean, Bens","T., Prabhavathy and Elumalai, Vinodh Kumar and E., Balaji","NaN","Wang, Ye and Wang, Yaxiong and Zhao, Guoshuai and Qian, Xueming","Shi, Jue and Chen, Xiaofang and Xie, Yongfang and Zhang, Hongliang and Cen, Lihui and Sun, Yubo","Liu, Hankai and Huang, Xianying and Liu, Xiaoyang","Zhang, Xiaoming and Yu, Lean","Su, Yawen","Wang, Hengbin and Ye, Zijing and Wang, Yan and Liu, Xueyi and Zhang, Xindan and Zhao, Yuanyuan and Li, Shaoming and Liu, Zhe and Zhang, Xiaodong","Kim, Hayeon and Cho, Myeongji and Son, Hyeon S.","Saremi, Mohammad and Amirani, Mehdi Chehel","Khan, Murtaza Ali and AlGhamdi, Mohammed","NaN","Panigrahi, Lipismita and Chandra, Tej Bahadur and Srivastava, Atul Kumar and Varshney, Neeraj and Singh, Kamred Udham and Mahato, Shambhu and Rajamohan, Vasudevan","Zhang, Yong and Jiang, Yuqing and Zhang, Qi and Liu, Da","Mojtahedi, Ramtin and Hamghalam, Mohammad and Jarnagin, William R. and Do, Richard K. G. and Simpson, Amber L.","Chen, Lu and Luo, Xinwei and Zhou, Hanlu","Yu, Ziru and Cui, Wei","Puri, Ashishi and Kumar, Sanjeev","Kingphai, Kunjira and Moshfeghi, Yashar","Graham Ram, Billy and Zhang, Yu and Costa, Cristiano and Raju Ahmed, Mohammed and Peters, Thomas and Jhala, Amit and Howatt, Kirk and Sun, Xin","Zheng, Zhe and Zhou, Yu-Cheng and Chen, Ke-Yin and Lu, Xin-Zheng and She, Zhong-Tian and Lin, Jia-Rui","Liu, Yahui and Li, Bin and Yang, Shuai and Li, Zhen","Zhu, Qi and Li, Sen and Li, Zhantao and Min, Xianjun and Li, Qian","Wu, Xin-Jian and Ao, Xiang and Zhang, Rui-Song and Liu, Cheng-Lin","NaN","Ming, Yuhang and Shao, Haidong and Cai, Baoping and Liu, Bin","Gao, Qi and Long, Teng and Zhou, Zhangbing","Zhang, Liu and Zhang, Shubin and Liu, Jincun and Wei, Yaoguang and An, Dong and Wu, Jianwei","Park, Sunghong and Son, Sang Joon and Park, Kanghee and Nam, Yonghyun and Shin, Hyunjung","Bushnell, Justin and Unverzagt, Frederick and Wadley, Virginia G. and Kennedy, Richard and Gaizo, John Del and Clark, David Glenn","Zhao, Jianhua and Liang, Haiye and Li, Shulan and Yang, Zhiji and Wang, Zhen","Wei, Yuanxi and liu, Yinan and Wang, Haibo","Wang, Ziquan and Li, Hui and Zhang, Zikai and Chen, Feng and Zhai, Jia","Meng, Yao and Xu, Mingle and Kim, Hyongsuk and Yoon, Sook and Jeong, Yongchae and Park, Dong Sun","Zhao, Ziye","Liang, Pei and Cao, Wanying and Hu, Junhua","Cho, Sungmin and Jung, Raehyuk and Kwon, Junseok","Li, Jichang and Li, Guanbin and Yu, Yizhou","Park, Hyunseo and Lee, Gyeong Ho and Han, Jaeseob and Choi, Jun Kyun","Gao, Bingjie and Zhou, Qianli and Deng, Yong","Sharma, Neha and Jain, Vibhor and Mishra, Anju","Kumari, Madhu and Singh, Vijendra","NaN","Aruna Sri, P. and Santhi, V.","Fu, Zhiling and Wang, Zhe and Xu, Xinlei and Yang, Mengping and Chi, Ziqiu and Ding, Weichao","Bhoi, Akash Kumar and Sherpa, Karma Sonam and Khandelwal, Bidita","Srivastava, Rajshree and Kumar, Pardeep","Xu, Ying and Liu, Honglei and Shi, Yi and Li, Ao and Wang, Minghui","Zhang, Jianan and Wu, Yongfei and Hao, Fang and Liu, Xueyu and Li, Ming and Zhou, Daoxiang and Zheng, Wen","NaN","Vo, Anh H. and Nguyen, Bao T.","Huang, Yan and Zhang, Zhang and Huang, Yan and Wu, Qiang and Huang, Han and Zhong, Yi and Wang, Liang","Sun, Pengfei and Wang, Zhiping and Jia, Liyan and Xu, Zhaohui","Filia, Beatrice Josephine and Lienardy, Filbert Fernandes and Laksana, I Kadek Perry Bagus and Jordan, Jayasidhi Ariyo and Siento, Joyceline Graciella and Honova, Shilvia Meidhi and Hasana, Silviya and Permonangan, Ivan Halim","Vieira, Guilherme and Valle, Marcos Eduardo and Lopes, Wilder","NaN","Makmur, Nathanael Matthew and Kwan, Felicia and Rana, Astrid Dewi and Kurniadi, Felix Indra","Shi, Wen and Zhao, Hong and Zhang, Haoran and Song, Lipei and Chen, Ke and Zhang, Bin","Wang, Alex X. and Chukova, Stefanka S. and Nguyen, Binh P.","Yang, Yue and Cheng, Jieren and Liu, Zhaowu and Li, Huimin and Xu, Ganglou","Wang, Ning and Zhang, Zhong-Liang and Luo, Xing-Gang","K R, Pradeep and N C, Naveen","Zhao, Ruirui and Sun, Jianbin and Tu, Li and Jiang, Jiang","Messa, Letizia and Testa, Carolina and Carelli, Stephana and Rey, Federica and Cereda, Cristina and Raimondi, Manuela Teresa and Ceri, Stefano and Pinoli, Pietro","Doonyapisut, Dulyawat and Kim, Byeongkyu and Kim, Jung Kyu and Lee, Eunseok and Chung, Chan-Hwa","Zielinski, Kallil M.C. and Ribas, Lucas C. and Machicao, Jeaneth and Bruno, Odemir M.","Cai, Zeyi and He, Mengyu and Li, Cheng and Qi, Hengnian and Bai, Ruibin and Yang, Jian and Zhang, Chu","Rathnayake, Himashi and Sumanapala, Janani and Rukshani, Raveesha and Ranathunga, Surangika","Huang, Weiliang and He, Wenxuan and Liao, Shuhong and Xu, Zhen and Yan, Jingwen","Wang, Huajun and Shao, Yuanhai","Zhou, Jun Yu and Fei, Chun Qing and Zou, Bing Guo","Aydogmus, Omur and Bingol, Mustafa Can and Boztas, Gullu and Tuncer, Turker","Chen, Junze","Sewwandi, Mahawaga Arachchige Nayomi Dulanjala and Li, Yuefeng and Zhang, Jinglan","Pourkamali-Anaraki, Farhad and Nasrin, Tahamina and Jensen, Robert E. and Peterson, Amy M. and Hansen, Christopher J.","Bader, Ofek and Lichy, Adi and Dvir, Amit and Dubin, Ran and Hajaj, Chen","Celik, Muhammed and Inik, Ozkan","Atmakuru, Akhila and Di Fatta, Giuseppe and Nicosia, Giuseppe and Varzandian, Ali and Badii, Atta","Sharma, Mayukh and Kandasamy, Ilanthenral and Vasantha, W.B.","Fu, Hui and Zhang, Ke and Wang, Jingyu","Ping, Zhichao and Sang, Guoming and Liu, Zhi and Zhang, Yijia","Sim, Jinwoo and Min, Jinhong and Kim, Seokgoo and Lee, Seok Woo and Choi, Joo-Ho","Revanasiddappa, M B and Harish, B S and Kumar, S V Aruna","Dhali, Maruf A. and Reynolds, Thomas and Alizadeh, Aylar Ziad and Nijdam, Stephan H. and Schomaker, Lambert","Kukreja, Sonia and Sabharwal, Munish and Katiyar, Alok and Gill, D. S.","Gao, Ruiyao and Qi, Kai and Yang, Hu","Ye, Jinhuang and Wu, Jiawei and Li, Zuoyong and Zheng, Xianghan","Zheng, Xiao and Wang, Minhui and Huang, Kai and Zhu, En","Yang, Rui and Zhou, Jun and Lu, Xiangyu and Shen, Jianxun and Chen, Huizhe and Chen, Mengyuan and He, Yong and Liu, Fei","Sun, Zhigang and Wang, Guotao and Li, Pengfei and Wang, Hui and Zhang, Min and Liang, Xiaowen","Zhou, Wei and Luo, Danxue","Guo, Qingwen and Wang, Chuntao and Xiao, Deqin and Huang, Qiong","Gupta, Deepak and Hazarika, Barenya Bikash and Borah, Parashjyoti","Ghosh, Arpita and Soni, Badal and Baruah, Ujwala","Naveed, Asim and Naqvi, Syed S. and Khan, Tariq M. and Razzak, Imran","Lv, Dingyang and Xu, Zhengjia and Zhang, Jinghui and Wang, Yuchen and Dong, Fang","Dong, Yumin and Che, Xuanxuan and Fu, Yanying and Liu, Hengrui and Sun, Lina","Wang, Xiaoying and Chen, Xiaohai and Zhang, Zhongwen and He, Haisheng","Xu, Xun and Liao, Jingyi and Cai, Lile and Nguyen, Manh Cuong and Lu, Kangkang and Zhang, Wanyue and Yazici, Yasin and Foo, Chuan Sheng","Bae, Youngjae and Kang, Seokho","Nicolini, Marco and Ntalampiras, Stavros","Mahesworo, Bharuno and Cenggoro, Tjeng Wawan and Lumbanraja, Favorisen Rosyking and Pardamean, Bens","Ji, Cun and Du, Mingsen and Wei, Yanxuan and Hu, Yupeng and Liu, Shijun and Pan, Li and Zheng, Xiangwei","Ni, Haotian and Gu, Shilin and Fan, Ruidong and Hou, Chenping","Liao, Yilin and Su, Rixin and Wang, Wenhai and Li, Haozhe and Wang, Hao and Liu, Zhaoran and Liu, Xinggao","Jia, Xibin and Li, Chen and Zeng, Meng and Wang, Luo and Mi, Qing","Xiong, Jiale and Yang, Jing and Yan, Lei and Awais, Muhammad and Khan, Abdullah Ayub and Alizadehsani, Roohallah and Acharya, U. Rajendra","Jian, Zhongquan and Li, Jiajian and Wu, Qingqiang and Yao, Junfeng","Hong, Haoyuan","Xu, Yang and Wu, Shanshan and Wang, Biqi and Yang, Ming and Wu, Zebin and Yao, Yazhou and Wei, Zhihui","Pingi, Sharon Torao and Nayak, Richi and Bashar, Md Abul","Li, Xiaoyu and Yang, Bei and Chen, Tiandong and Gao, Zheng and Huang, Mengjie","Hajihosseinlou, Mahsa and Maghsoudi, Abbas and Ghezelbash, Reza","Xu, Jindong and Li, Kang and Li, Ziyi and Chong, Qianpeng and Xing, Haihua and Xing, Qianguo and Ni, Mengying","Chang, Mingzhe and Ji, Luping and Zhu, Jiewen","Dolzhikova, Irina and Abibullaev, Berdakh and Zollanvari, Amin","Farooque, Ghulam and Liu, Qichao and Sargano, Allah Bux and Xiao, Liang","Antioquia, Arren Matthew C. and Cordel II, Macario O.","Opanasenko, V. M. and Fazilov, Sh.Kh. and Radjabov, S. S. and Kakharov, Sh.S.","Kumar, Upendra","Verma, Atul Kumar and Jadeja, Mahipal","Hu, Tao","Sisodia, Deepti and Sisodia, Dilip Singh","Sun, Lin and Li, Mengmeng and Ding, Weiping and Xu, Jiucheng","Edla, Damodar Reddy and Mangalorekar, Kunal and Dhavalikar, Gauri and Dodia, Shubham","Jia, Jinfang and Feng, Xiang and Yu, Huiqun","Liu, Ju and Huang, Ling-Wei and Shao, Yuan-Hai and Chen, Wei-Jie and Li, Chun-Na","Zhang, Kuangyan and Zhang, Tuyi and Liu, Sanmin","Li, Bo and Tang, Jinhong and Xie, Nengke","Chatterjee, Subhajit and Byun, Yung-Cheol","Kapoor, Ishita and Mishra, Anju","Lone, Ab Waheed and Aydin, Nizamettin","Gu, Xingyue and Ding, Yijie and Xiao, Pengfeng","Kaur, Barjinder and Singh, Dinesh and Roy, Partha Pratim","Liu, Bo and He, Lejian and Xie, Yuchen and Xiang, Yuejia and Zhu, Li and Ding, Weiping","Al-Hindawi, Firas and Rahman Siddiquee, Md Mahfuzur and Wu, Teresa and Hu, Han and Sun, Ying","Lin, Kevin and Brown, Donald and Syed, Sana and Greene, Adam","Kokash, Natallia and Makhnist, Leonid","Saroj and Vashishtha, Jyoti and Goyal, Pooja and Ahuja, Jyoti","Aghaee, Reza and Momeni, Mehdi and Moallem, Payman","Guo, Qingwen and Wang, Chuntao and Xiao, Deqin and Huang, Qiong","Wang, Kaier and Tikhonov, Aristarkh and Hill, Melissa and Litchfield, Lester","Zhang, Xuan and Xu, Yitian and Liu, Xuhua","Ahmadian, Vahid and Beheshti Aval, S. Bahram and Noori, Mohammad and Wang, Tianyu and Altabey, Wael A.","Ganaie, M.A. and Kumari, Anuradha and Girard, Anouck and Kasa-Vubu, Josephine and Tanveer, M.","Panigrahi, Ranjit and Borah, Samarjeet","Shi, Shijun and Hu, Kai and Xie, Jie and Guo, Ya and Wu, Huayi","Wang, Xuetao and He, Qiang and Jian, Wanwei and Meng, Haoyu and Zhang, Bailin and Jin, Huaizhi and Yang, Geng and Zhu, Lin and Wang, Linjing and Zhen, Xin","Wang, Wei and Wei, Xueguang and Wang, Bailing and Li, Yan and Xin, Guodong and Wei, Yuliang","Salim, Brigita Vanessa and Chyntia and Indrawan, Jason Orlando and Hidayat, Jessica and Matthew, Steven and Mangkang, Tesalonika Abigail Eikwine and Hasana, Silviya and Permonangan, Ivan Halim","Arhin, Joseph Roger and Zhang, Xiaoling and Coker, Kenneth and Agyemang, Isaac Osei and Attipoe, Wisdom Kwame and Sam, Francis and Adjei-Mensah, Isaac and Agyei, Emmanuel","Habbat, Nassera and Nouri, Hicham and Anoun, Houda and Hassouni, Larbi","Shwetank and Neeraj and Jitendra and Vikesh and Jain, Kamal","Goyal, Hemlata and Joshi, Nisheeth and Sharma, Chilka"],"cluster":[1,1,1,1,1,1,1,0,1,1,1,0,1,1,1,0,1,1,1,1,1,1,2,1,1,1,1,1,1,1,1,0,1,1,1,1,0,1,1,1,1,1,1,1,1,1,1,1,1,1,2,1,1,1,1,0,1,1,0,1,1,1,1,1,1,1,1,1,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,2,1,1,1,1,1,1,1,1,1,1,1,1,1,1,2,1,1,1,0,1,1,1,1,1,0,1,1,1,1,0,1,1,1,1,1,1,1,1,1,1,2,1,1,2,1,1,1,1,1,2,1,1,1,1,1,1,0,1,1,1,1,1,1,2,1,1,1,1,1,1,0,1,1,1,1,1,1,1,0,1,1,1,1,1,1,1,2,1,1,2,1,1,1,1,1,2,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,1,1,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,2,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,1,1,0,1],"labels":["C-1","C-1","C-1","C-1","C-1","C-1","C-1","C-0","C-1","C-1","C-1","C-0","C-1","C-1","C-1","C-0","C-1","C-1","C-1","C-1","C-1","C-1","C-2","C-1","C-1","C-1","C-1","C-1","C-1","C-1","C-1","C-0","C-1","C-1","C-1","C-1","C-0","C-1","C-1","C-1","C-1","C-1","C-1","C-1","C-1","C-1","C-1","C-1","C-1","C-1","C-2","C-1","C-1","C-1","C-1","C-0","C-1","C-1","C-0","C-1","C-1","C-1","C-1","C-1","C-1","C-1","C-1","C-1","C-0","C-1","C-1","C-1","C-1","C-1","C-1","C-1","C-1","C-1","C-1","C-1","C-1","C-1","C-1","C-1","C-1","C-1","C-1","C-1","C-1","C-1","C-1","C-2","C-1","C-1","C-1","C-1","C-1","C-1","C-1","C-1","C-1","C-1","C-1","C-1","C-1","C-1","C-2","C-1","C-1","C-1","C-0","C-1","C-1","C-1","C-1","C-1","C-0","C-1","C-1","C-1","C-1","C-0","C-1","C-1","C-1","C-1","C-1","C-1","C-1","C-1","C-1","C-1","C-2","C-1","C-1","C-2","C-1","C-1","C-1","C-1","C-1","C-2","C-1","C-1","C-1","C-1","C-1","C-1","C-0","C-1","C-1","C-1","C-1","C-1","C-1","C-2","C-1","C-1","C-1","C-1","C-1","C-1","C-0","C-1","C-1","C-1","C-1","C-1","C-1","C-1","C-0","C-1","C-1","C-1","C-1","C-1","C-1","C-1","C-2","C-1","C-1","C-2","C-1","C-1","C-1","C-1","C-1","C-2","C-0","C-1","C-1","C-1","C-1","C-1","C-1","C-1","C-1","C-1","C-1","C-1","C-1","C-1","C-1","C-1","C-1","C-1","C-1","C-0","C-1","C-1","C-0","C-0","C-1","C-1","C-1","C-1","C-1","C-1","C-1","C-1","C-1","C-1","C-1","C-1","C-1","C-1","C-1","C-1","C-2","C-1","C-1","C-1","C-1","C-1","C-1","C-1","C-1","C-1","C-1","C-1","C-1","C-1","C-1","C-0","C-1","C-1","C-0","C-1"],"publication_date":["2024-02-27","2024-02-27","2024-02-27","2024-02-27","2024-02-27","2024-02-27","2024-02-27","2024-02-27","2024-02-27","2024-02-27","2024-02-27","2024-02-04","2024-02-27","2024-02-27","2024-02-27","2024-02-01","2024-02-19","2024-02-06","2024-02-01","2024-02-22","2024-02-27","2024-02-13","2024-02-01","2024-02-01","2024-02-01","2024-02-01","2024-02-27","2024-02-27","2024-02-02","2024-02-01","2024-02-01","2024-02-07","2024-02-28","2024-02-27","2024-02-06","2024-02-27","2024-02-01","2024-02-09","2024-02-01","2024-02-27","2024-02-01","2024-02-27","2024-02-01","2024-02-07","2024-02-27","2024-02-15","2024-02-01","2024-02-28","2024-02-12","2024-02-27","2024-02-01","2024-02-27","2024-02-27","2024-02-27","2024-02-27","2024-02-27","2024-02-01","2024-02-27","2024-02-28","2024-02-27","2024-02-01","2024-02-27","2024-02-01","2024-02-27","2024-02-01","2024-02-27","2024-02-27","2024-02-12","2024-02-12","2024-02-01","2024-02-27","2024-02-14","2024-02-27","2024-02-03","2024-02-27","2024-02-16","2024-02-01","2024-02-04","2024-02-27","2024-02-27","2024-02-27","2024-02-27","2024-02-27","2024-02-07","2024-02-07","2024-02-27","2024-02-27","2024-02-27","2024-02-01","2024-02-27","2024-02-27","2024-02-01","2024-02-27","2024-02-27","2024-02-27","2024-02-03","2024-02-27","2024-02-27","2024-02-27","2024-02-27","2024-02-19","2024-02-01","2024-02-28","2024-02-15","2024-02-27","2024-02-01","2024-02-17","2024-02-27","2024-02-03","2024-02-01","2024-02-27","2024-02-27","2024-02-15","2024-02-27","2024-02-01","2024-02-01","2024-02-03","2024-02-04","2024-02-27","2024-02-27","2024-02-27","2024-02-27","2024-02-27","2024-02-01","2024-02-27","2024-02-01","2024-02-05","2024-02-27","2024-02-27","2024-02-01","2024-02-27","2024-02-01","2024-02-27","2024-02-01","2024-02-27","2024-02-27","2024-02-27","2024-02-14","2024-02-27","2024-02-27","2024-02-19","2024-02-28","2024-02-27","2024-02-27","2024-02-27","2024-02-01","2024-02-27","2024-02-27","2024-02-03","2024-02-16","2024-02-27","2024-02-27","2024-02-27","2024-02-03","2024-02-01","2024-02-27","2024-02-01","2024-02-28","2024-02-01","2024-02-01","2024-02-27","2024-02-01","2024-02-01","2024-02-01","2024-02-13","2024-02-27","2024-02-27","2024-02-27","2024-02-01","2024-02-27","2024-02-27","2024-02-15","2024-02-27","2024-02-27","2024-02-27","2024-02-27","2024-02-27","2024-02-22","2024-02-10","2024-02-01","2024-02-04","2024-02-01","2024-02-27","2024-02-01","2024-02-01","2024-02-27","2024-02-01","2024-02-01","2024-02-01","2024-02-01","2024-02-14","2024-02-14","2024-02-27","2024-02-27","2024-02-22","2024-02-27","2024-02-01","2024-02-01","2024-02-27","2024-02-27","2024-02-27","2024-02-01","2024-02-01","2024-02-01","2024-02-07","2024-02-27","2024-02-01","2024-02-01","2024-02-27","2024-02-27","2024-02-01","2024-02-12","2024-02-08","2024-02-27","2024-02-27","2024-02-27","2024-02-27","2024-02-01","2024-02-27","2024-02-01","2024-02-01","2024-02-27","2024-02-04","2024-02-27","2024-02-27","2024-02-01","2024-02-01","2024-02-27","2024-02-01","2024-02-01","2024-02-27","2024-02-15","2024-02-27","2024-02-27","2024-02-01","2024-02-12","2024-02-01","2024-02-01","2024-02-01","2024-02-27","2024-02-01","2024-02-27","2024-02-27","2024-02-27","2024-02-01","2024-02-01","2024-02-27","2024-02-27"],"title":["An Ensemble Classification System for Twitter Sentiment Analysis","Semi-supervised imbalanced classification of wafer bin map defects using a Dual-Head CNN","A dependency-based hybrid deep learning framework for target-dependent sentiment classification","Reservoir characterization through comprehensive modeling of elastic logs prediction in heterogeneous rocks using unsupervised clustering and class-based ensemble machine learning","Improving fMRI-based Autism Spectrum Disorder Classification with Random Walks-informed Feature Extraction and Selection","Fractal belief Jensen\u2013Shannon divergence-based multi-source information fusion for pattern classification","Flexible loss functions for binary classification in gradient-boosted decision trees: An application to credit scoring","A hyperspectral band selection method based on sparse band attention network for maize seed variety identification","Feature selections based on three improved condition entropies and one new similarity degree in interval-valued decision systems","A high order fractal-based Kullback\u2013Leibler divergence with application in classification","An automated earthquake classification model based on a new butterfly pattern using seismic signals","RsMmFormer: Multimodal Transformer Using Multiscale Self-attention for Remote Sensing Image Classification","Word Representations For Gender Classification Using Deep Learning","Support vector machine with eagle loss function","Class3Dp: A supervised classifier of vegetation species from point clouds","How to identify pollen like a palynologist: A prior knowledge-guided deep feature learning for real-world pollen classification","Towards Harmonious Coexistence: A Bioacoustic-Driven Animal-Computer Interaction System for Preventing Ship Collisions with North Atlantic Right Whales","An Optimal Weighted Ensemble of 3D CNNs for Early Diagnosis of Alzheimer\u2019s Disease","Review of resampling techniques for the treatment of imbalanced industrial data classification in equipment condition monitoring","Classification model based on improved K-means clustering algorithm","Feature-level SMOTE: Augmenting fault samples in learnable feature space for imbalanced fault diagnosis of gas turbines","Graph-based Text Classification by Contrastive Learning with Text-level Graph Augmentation","Lung-EffNet: Lung cancer classification using EfficientNet from CT-scan images","Classification of yoga, meditation, combined yoga\u2013meditation EEG signals using L-SVM, KNN, and MLP classifiers","Material handling machine activity recognition by context ensemble with gated recurrent units","Federated contrastive prototype learning: An efficient collaborative fault diagnosis method with data privacy","Using Ensemble StackingC Method and Base Classifiers to Ameliorate Prediction Accuracy of Pedagogical Data","Credit Scoring Model based on Weighted Voting and Cluster based Feature Selection","Detection of explosives in dustbins using deep transfer learning based multiclass classifiers","An imbalanced classification approach for establishment of cause-effect relationship between Heart-Failure and Pulmonary Embolism using Deep Reinforcement Learning","Surface quality prediction and quantitative evaluation of process parameter effects for 3D printing with transfer learning-enhanced gradient-boosting decision trees","FFANet: Dual Attention-Based Flow Field Aware Network for 3D Grid Classification and Segmentation","Robust Sentiment Classification Based on the Backdoor Adjustment","Fuzzy classification with distance-based depth prototypes: High-dimensional unsupervised and/or supervised problems","Three-Dimensional System-Object Classification for Prediction and Control Support","Domain-invariant feature fusion networks for semi-supervised generalization fault diagnosis","Dilated-Windows-based Vision Transformer with Efficient-Suppressive-self-attention for insect pests classification","Rarity updated ensemble with oversampling: An ensemble approach to classification of imbalanced data streams","A target intention recognition method based on information classification processing and information fusion","Enhancing Decision Tree Classification Accuracy through Genetically Programmed Attributes for Wart Treatment Method Identification","Active diversification of head-class features in bilateral-expert models for enhanced tail-class optimization in long-tailed classification","Two-step multi-view data classification based on dynamic Graph-ELM","Label correlations-based multi-label feature selection with label enhancement","Hybrid compression for LSTM-based encrypted traffic classification model","Orchard classification based on super-pixels and deep learning with sparse optical images","Multi-scale Heat Kernel Graph Network for Graph Classification","Collaborative contrastive learning for hypergraph node classification","SupervisedImmuneNet: Training Artificial Immune Networks using a Supervised Learning Approach for Improved Multi-Class Classification","Comparison of Simplified SE-ResNet and SE-DenseNet for Micro-Expression Classification","Comprehensive wheat lodging detection after initial lodging using UAV RGB images","Molecular subtypes classification of breast cancer in DCE-MRI using deep features","A Lightning fast approach to classify Bangla Handwritten Characters and Numerals using newly structured Deep Neural Network","Acoustic scene classification: A comprehensive survey","Fault detection and classification with the rebmix R package","A machine learning-based approach for flames classification in industrial Heavy Oil-Fire Boilers","Supervised spectral feature learning for fine-grained classification in small data set","Signal detection and material identification method for loose particles inside sealed relays based on fusion classification model","Support Vector Machine Method for Predicting Non-Linear Data","Revisiting the K-Fold Approach for a Stable Model on Amyotrophic Lateral Sclerosis Prediction Scheme using LSTM and Attention Mechanism","Concise rule induction algorithm based on one-sided maximum decision tree approach","ECC + +: An algorithm family based on ensemble of classifier chains for classifying imbalanced multi-label data","Predicting the protein structure using random forest approach","A general maximal margin hyper-sphere SVM for multi-class classification","irrelevant attribute resistance approach to binary classification for imbalanced data","Exploiting local label correlation from sample perspective for multi-label classification via three-way decision theory","Unlock the Potential of Counterfactually-Augmented Data in Out-Of-Distribution Generalization","KD-PAR: A knowledge distillation-based pedestrian attribute recognition model with multi-label mixed feature learning network","Spatial Variation Sequences for Remote Sensing Applications with Small Sample Sizes","Melanoma Classification Using Deep Learning","UACNet: A universal automatic classification network for microseismic signals regardless of waveform size and sampling rate","SKEDS \u2014 An external knowledge supported logistic regression approach for document-level sentiment classification","Modeling of Aquila Optimizer with Hybrid ResNet-DenseNet enabled Breast Cancer Classification on Histopathological Images","Text classification with improved word embedding and adaptive segmentation","A Taxonomy for Learning with Perturbation and Algorithms","Classification of Facial Expressions from EEG signals using Wavelet Packet Transform and SVM for Wheelchair Control Operations","An Evolutionary Approach to Feature Selection and Classification","A topic modeling and image classification framework: The Generalized Dirichlet variational autoencoder","Feature Fusion Gate: Improving Transformer Classifier Performance with Controlled Noise","A Novel approach of Sentiment Classification using Emoticons","Power transformer fault diagnosis based on a self-strengthening offline pre-training model","Ordinal classification for interval-valued data and interval-valued functional data","Pattern recognition based on statistical methods combined with machine learning in railway switches","Spider Plus: A Text Classifier for Research Article Components","Self-supervised Contrastive Feature Refinement for Few-Shot Class-Incremental Learning","SWRM: Similarity Window Reweighting and Margin for Long-Tailed Recognition","Improving metric-based few-shot learning with dynamically scaled softmax loss","Novel extended NI-MWMOTE-based fault diagnosis method for data-limited and noise-imbalanced scenarios","ASE: Anomaly scoring based ensemble learning for highly imbalanced datasets\u25aa","A generalized ensemble approach based on transfer learning for Braille character recognition","The verification of hen egg types by the classification of ultra-weak photon emission data","Three-way fusion measures and three-level feature selections based on neighborhood decision systems","Cervical cancer classification based on a bilinear convolutional neural network approach and random projection","Feature selection using a sinusoidal sequence combined with mutual information","Indonesian Agricultural-crops Classification Using Transfer Learning Model","Hand gesture classification framework leveraging the entropy features from sEMG signals and VMD augmented multi-class SVM","Multimodal Context-Aware Detection of Glioma Biomarkers Using MRI and WSI","Learning to complement: Relation complementation network for few-shot class-incremental learning","Hybrid-driven BRBCS-BOM with expert intervention and its application for abnormity recognition in electrolytic cell","Improve label embedding quality through global sensitive GAT for hierarchical text classification","Consumer credit risk assessment: A review from the state-of-the-art classification algorithms, data traits, and learning methods","A Natural Language Processing System for Text Classification Corpus Based on Machine Learning","Improving the crop classification performance by unlabeled remote sensing data","A Study of Classification Techniques Based on Spike Protein Sequences of MERS-CoV","Proposing a novel multi-label mapping approach for use in SVM-based multi-class classification problems","A customized deep learning-based framework for classification and analysis of social media posts to enhance the Hajj and Umrah services","Multi-instance learning with application to the profiling of multi-victim homicides","mBCCf: Multilevel Breast Cancer Classification Framework Using Radiomic Features","Multi-label learning based on instance correlation and feature redundancy","Leveraging Contrastive Learning with SimSiam for the Classification of Primary and Secondary Liver Cancers","A ship-radiated noise classification method based on domain knowledge embedding and attention mechanism","LSCA-net: A lightweight spectral convolution attention network for hyperspectral image processing","Hybrid Approach for Accurate Fiber Estimation in Brain Tissues: Mixture Model and Deep Learning Technique","On Channel Selection for EEG-Based Mental Workload Classification","Palmer amaranth identification using hyperspectral imaging and machine learning technologies in soybean field","A text classification-based approach for evaluating and enhancing the machine interpretability of building codes","Handling missing values and imbalanced classes in machine learning to predict consumer preference: Demonstrations and comparisons to prominent methods","Noisy-Consistent Pseudo Labeling Model for Semi-supervised Skin Lesion Classification","Structural Recognition of Handwritten Chinese Characters Using a Modified Part Capsule Auto-encoder","Machine-learning-assisted classification of construction and demolition waste fragments using computer vision: Convolution versus extraction of selected features\u25aa","rgfc-Forest: An enhanced deep forest method towards small-sample fault diagnosis of electromechanical system","Mineral identification based on natural feature-oriented image processing and multi-label image classification","Maize seed variety identification using hyperspectral imaging and self-supervised learning: A two-stage training approach without spectral preprocessing","In-house data adaptation to public data: Multisite MRI harmonization to predict Alzheimer\u2019s disease conversion","Post-processing automatic transcriptions with machine learning for verbal fluency scoring","Matrix-based vs. vector-based linear discriminant analysis: A comparison of regularized variants on multivariate time series data","Cooperative distillation with X-ray images classifiers for prohibited items detection","Attribute- and attention-guided few-shot classification","Known and unknown class recognition on plant species and diseases","Classification tree algorithm and its application in general archives management system","A sequential three-way classification model based on risk preference and decision correction\u25aa","Sampling based spherical transformer for 360 degree image classification","Inter-domain mixup for semi-supervised domain adaptation","Multiclass autoencoder-based active learning for sensor-based human activity recognition","HIE-EDT: Hierarchical interval estimation-based evidential decision tree","An Analysis Of Convolutional Neural Networks For Image Classification","Breast Cancer Prediction system","A criminal macrocause classification model: An enhancement for violent crime analysis considering an unbalanced dataset","The reptile optimized deep learning model for land cover classification of the uppal earth region in telangana state using satellite image fusion","Semantic alignment with self-supervision for class incremental learning","Ischemia and Arrhythmia Classification Using Time-Frequency Domain Features of QRS Complex","Performance comparison of various machine learning classifiers using fusion of LBP, intensity and GLCM feature extraction techniques for thyroid nodules classification","Cancer Survival Prediction by Multimodal Disentangled Representation Learning","Double similarities weighted multi-instance learning kernel and its application","Tourism destination events classifier based on artificial intelligence techniques","A framework-based transformer and knowledge distillation for interior style classification","Customized meta-dataset for automatic classifier accuracy evaluation","SMOTE-kTLNN: A hybrid re-sampling method based on SMOTE and a two-layer nearest neighbor classifier","Improving Batik Pattern Classification using CNN with Advanced Augmentation and Oversampling on Imbalanced Dataset","Clifford Convolutional Neural Networks for Lymphoblast Image Classification","Automatic classification of the physical surface in sound uroflowmetry using machine learning methods","Comparing Local Binary Pattern and Gray Level Co-occurrence Matrix for Feature Extraction in Diabetic Retinopathy Classification","Wire melted mark metallographic image recognition and classification based on semantic segmentation\u25aa","Synthetic minority oversampling using edited displacement-based k-nearest neighbors","A multi-classification detection model for imbalanced data in NIDS based on reconstruction and feature matching","Iterative minority oversampling and its ensemble for ordinal imbalanced datasets","Lung Cancer Survivability Prediction based on Performance Using Classification Techniques of Support Vector Machines, C4.5 and Naive Bayes Algorithms for Healthcare Analytics","Asynchronous optimization approach for evidential reasoning rule-based classifier","Leveraging Non-negative Matrix Tri-Factorization and Knowledge-Based Embeddings for Drug Repurposing: an Application to Parkinson's Disease","Deep generative learning for exploration in large electrochemical impedance dataset","A network classification method based on density time evolution patterns extracted from network automata","Identification of chrysanthemum using hyperspectral imaging based on few-shot class incremental learning","AdapterFusion-based multi-task learning for code-mixed and code-switched text classification","Efficient SpectralFormer for hyperspectral image classification","Fast generalized ramp loss support vector machine for pattern classification","A Case Study on the Generalization of Chinese Text Classification Methods based on Deep Learning","An automated voice command classification model based on an attention-deep convolutional neural network for industrial automation system","Classify Human Activities Based On Deepforest","Granule-specific feature selection for continuous data classification using neighborhood rough sets","Evaluation of classification models in limited data scenarios with application to additive manufacturing","OSF-EIMTC: An open-source framework for standardized encrypted internet traffic classification","Development of hybrid models based on deep learning and optimized machine learning algorithms for brain tumor Multi-Classification","Sensitivity Analysis for Feature Importance in Predicting Alzheimer\u2019s Disease","Emotion quantification and classification using the neutrosophic approach to deep learning","An adaptive self-correction joint training framework for person re-identification with noisy labels","Aspect category sentiment analysis based on prompt-based learning with attention mechanism","Construction of bearing health indicator under time-varying operating conditions based on Isolation Forest","Meta-cognitive Neural Network based Sequential Learning Framework for Text Categorization","Pattern Recognition Techniques in Image-Based Material Classification of Ancient Manuscripts","NaN","Fused robust geometric nonparallel hyperplane support vector machine for pattern classification","Rethinking Distribution Alignment for Inter-class Fairness","Global and cross-modal feature aggregation for multi-omics data classification and application on drug response prediction","A robust rice yield estimation framework developed by grading modeling and normalized weight decision-making strategy using UAV imaging technology","An improved random forest based on the classification accuracy and correlation measurement of decision trees","The hesitant fuzzy BiRNN based on twice-cycle mechanism and its intelligent applications","A lightweight open-world pest image classifier using ResNet8-based matching network and NT-Xent loss function","Fuzzy twin kernel ridge regression classifiers for liver disorder detection","Brain haemorrhage classification from CT scan images using fine-tuned transfer learning deep features","PCA: Progressive class-wise attention for skin lesions diagnosis","Imbalanced node classification with Graph Neural Networks: A unified approach leveraging homophily and label information","Classification of human protein cell images using deep neural networks","Vehicle type classification in intelligent transportation systems using deep learning","Revisiting pretraining for semi-supervised learning in the low-label regime","Supervised contrastive learning for wafer map pattern classification","Gender-Aware Speech Emotion Recognition in Multiple Languages","Standard Multi-Layer Perceptron on Positive - Unlabeled Glycosylation Site Dataset","Time series classification with random temporal features","Feature incremental learning with causality","Gaussian dynamic recurrent unit for emitter classification","An improved unified domain adversarial category-wise alignment network for unsupervised cross-domain sentiment classification","Efficient reinforcement learning-based method for plagiarism detection boosted by a population-based algorithm for pretraining weights","Retrieval Contrastive Learning for Aspect-Level Sentiment Classification","Landslide susceptibility assessment using locally weighted learning integrated with machine learning algorithms","Two-stage fine-grained image classification model based on multi-granularity feature fusion","Conditional Generative Adversarial Network for Early Classification of Longitudinal Datasets using an Imputation Approach","ProMIL: A weakly supervised multiple instance learning for whole slide image classification based on class proxy","Stacking: A novel data-driven ensemble machine learning strategy for prediction and mapping of Pb-Zn prospectivity in Varcheh district, west Iran","Fuzzy graph convolutional network for hyperspectral image classification","Multi-scale LBP fusion with the contours from deep CellNNs for texture classification","A Jackknife-Inspired Deep Learning Approach to Subject-Independent Classification of EEG","Swin transformer with multiscale 3D atrous convolution for hyperspectral image classification","HAHANet: Towards Accurate Image Classifiers with Less Parameters","Multilevel Face Recognition System","A Noval Approach for Object Recognition Using Decision Tree Clustering by Incorporating Multi-Level BPNN Classifiers and Hybrid Texture Features","CB-SAGE: A novel centrality based graph neural network for floor plan classification","Software Engineering Classification Model and Algorithm Based on Big Data Technology","Prediction of Diabetes using Classification Algorithms","Adaptive fuzzy multi-neighborhood feature selection with hybrid sampling and its application for class-imbalanced data","Classification of EEG data for human mental state analysis using Random Forest Classifier","Few-shot classification via efficient meta-learning with hybrid optimization","A nonlinear kernel SVM classifier via L 0 / 1 soft-margin loss with classification performance","A novel ensemble framework driven by diversity and cooperativity for non-stationary data stream classification","Ensemble of Deep Convolutional Network for Citrus Disease Classification Using Leaf Images","Highly imbalanced fault classification of wind turbines using data resampling and hybrid ensemble method approach","Automated Classification Method for Early Diagnosis of Alopecia Using Machine Learning","Wavelet Scattering Transform based Doppler signal classification","MLapRVFL: Protein sequence prediction based on Multi-Laplacian Regularized Random Vector Functional Link","EEG Based Emotion Classification Mechanism in BCI","MinJoT: Multimodal infusion Joint Training for noise learning in text and multimodal classification problems","Domain-knowledge Inspired Pseudo Supervision (DIPS) for unsupervised image-to-image translation models to support cross-domain classification","Uncertainty Quantification for Eosinophil Segmentation","Using Decision Trees for Interpretable Supervised Clustering","A Novel Fitness Computation Framework for Nature Inspired Classification Algorithms","A fusion-based approach to improve hyperspectral images\u2019 classification using metaheuristic band selection","A novel multi-label pest image classifier using the modified Swin Transformer and soft binary cross entropy loss","Evaluating Mammogram Image Classification: Impact of Model Architectures, Pretraining, and Finetuning","Multiple instance learning from similarity-confidence bags","Comparative study of a newly proposed machine learning classification to detect damage occurrence in structures","Diagnosis of Alzheimer\u2019s disease via Intuitionistic fuzzy least squares twin SVM","Rank Allocation to J48 Group of Decision Tree Classifiers using Binary and Multiclass Intrusion Detection Datasets","Robust scientific text classification using prompt tuning based on data augmentation with L2 regularization","Hybrid feature ranking and classifier aggregation based on multi-criteria decision-making","Hyperplane projection network for few-shot relation classification","Face Shape Classification Using Swin Transformer Model","ADCGNet: Attention-based dual channel Gabor network towards efficient detection and classification of electrocardiogram images","Sentiment analysis of imbalanced datasets using BERT and ensemble stacking for deep learning","Pixel Based Supervised Classification of Hyperspectral Face Images for Face Recognition","An Empirical Analysis of Geospatial Classification for Agriculture Monitoring"],"x":{"__ndarray__":"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","dtype":"float64","shape":[248]},"x_backup":{"__ndarray__":"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","dtype":"float64","shape":[248]},"y":{"__ndarray__":"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","dtype":"float64","shape":[248]},"y_backup":{"__ndarray__":"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","dtype":"float64","shape":[248]}},"selected":{"id":"1071","type":"Selection"},"selection_policy":{"id":"1072","type":"UnionRenderers"}},"id":"1011","type":"ColumnDataSource"},{"attributes":{},"id":"1072","type":"UnionRenderers"},{"attributes":{"high":2,"low":0,"palette":["#1f77b4","#aec7e8","#ff7f0e"]},"id":"1013","type":"LinearColorMapper"},{"attributes":{"children":[{"id":"1002","type":"Div"}]},"id":"1080","type":"Row"},{"attributes":{"text":"&lt;a href=\"clusters_last_half_year.html\" target=\"_blank\"&gt;&lt;button&gt;Last Half Year&lt;/button&gt;&lt;/a&gt;"},"id":"1004","type":"Div"},{"attributes":{"margin":[5,5,5,5],"sizing_mode":"stretch_width","style":{"font-family":"Helvetica Neue, Helvetica, Arial, sans-serif;","font-size":"1.1em"},"text":"&lt;h3&gt;Filter by Text:&lt;/h3&gt;&lt;p1&gt;Search keyword to filter out the plot. It will search abstracts, titles and authors. \n    Press enter when ready. Clear and press enter to reset the plot.&lt;/p1&gt;"},"id":"1007","type":"Div"},{"attributes":{"args":{"current_selection":{"id":"1064","type":"Div"},"source":{"id":"1011","type":"ColumnDataSource"}},"code":"\n        var titles = [];\n        var authors = [];\n        var abstracts = [];\n        var publicationDates = [];\n        var clusters = [];\n\n        cb_data.source.selected.indices.forEach(index =&gt; {\n            titles.push(source.data['title'][index]);\n            authors.push(source.data['author'][index]);\n            abstracts.push(source.data['abstract'][index]);\n            publicationDates.push(source.data['publication_date'][index]);\n            clusters.push(source.data['cluster'][index]);\n        });\n\n        var title = \"&lt;p1&gt;&lt;b&gt;Title:&lt;/b&gt; \" + (titles[0] ? titles[0].toString() : \"Not available.\") + \"&lt;br&gt;\";\n        var author = \"&lt;p1&gt;&lt;b&gt;Author:&lt;/b&gt; \" + (authors[0] ? authors[0].toString() : \"Not available.\") + \"&lt;br&gt;\";\n        var abstract = \"&lt;p1&gt;&lt;b&gt;Abstract:&lt;/b&gt; \" + abstracts[0].toString() + \"&lt;br&gt;\";\n        var publicationDate = \"&lt;p1&gt;&lt;b&gt;Publication Date:&lt;/b&gt; \" + publicationDates[0].toString() + \"&lt;/p1&gt;&lt;br&gt;\";\n        var cluster = \"&lt;p1&gt;&lt;b&gt;Cluster:&lt;/b&gt; \" + clusters[0].toString() + \"&lt;/p1&gt;\";\n\n        current_selection.text = title + author + abstract + publicationDate + cluster;\n        current_selection.change.emit();\n    "},"id":"1065","type":"CustomJS"},{"attributes":{"label":{"field":"labels"},"renderers":[{"id":"1052","type":"GlyphRenderer"}]},"id":"1061","type":"LegendItem"},{"attributes":{"data_source":{"id":"1011","type":"ColumnDataSource"},"glyph":{"id":"1050","type":"Scatter"},"hover_glyph":null,"muted_glyph":null,"nonselection_glyph":{"id":"1051","type":"Scatter"},"selection_glyph":null,"view":{"id":"1053","type":"CDSView"}},"id":"1052","type":"GlyphRenderer"},{"attributes":{"fill_color":{"field":"cluster","transform":{"id":"1013","type":"LinearColorMapper"}},"line_alpha":{"value":0.3},"size":{"units":"screen","value":5},"x":{"field":"x"},"y":{"field":"y"}},"id":"1050","type":"Scatter"},{"attributes":{"below":[{"id":"1025","type":"LinearAxis"}],"center":[{"id":"1029","type":"Grid"},{"id":"1034","type":"Grid"},{"id":"1060","type":"Legend"}],"left":[{"id":"1030","type":"LinearAxis"}],"margin":[5,5,5,5],"plot_height":500,"plot_width":500,"renderers":[{"id":"1052","type":"GlyphRenderer"}],"sizing_mode":"scale_both","title":{"id":"1015","type":"Title"},"toolbar":{"id":"1041","type":"Toolbar"},"toolbar_location":"above","x_range":{"id":"1017","type":"DataRange1d"},"x_scale":{"id":"1021","type":"LinearScale"},"y_range":{"id":"1019","type":"DataRange1d"},"y_scale":{"id":"1023","type":"LinearScale"}},"id":"1014","subtype":"Figure","type":"Plot"},{"attributes":{"sizing_mode":"stretch_width","style":{"font-family":"Helvetica Neue, Helvetica, Arial, sans-serif;","font-size":"1.1em"},"text":"&lt;h3&gt;Time Range:&lt;/h3&gt;&lt;p1&gt;Click on the button to change the time range of the plot.&lt;/p1&gt;"},"id":"1002","type":"Div"},{"attributes":{"ticker":{"id":"1026","type":"BasicTicker"}},"id":"1029","type":"Grid"},{"attributes":{"overlay":{"id":"1059","type":"BoxAnnotation"}},"id":"1037","type":"BoxZoomTool"},{"attributes":{},"id":"1026","type":"BasicTicker"},{"attributes":{"children":[{"id":"1003","type":"Div"},{"id":"1004","type":"Div"},{"id":"1005","type":"Div"}]},"id":"1081","type":"Row"},{"attributes":{"children":[{"id":"1078","type":"Row"},{"id":"1079","type":"Row"},{"id":"1080","type":"Row"},{"id":"1081","type":"Row"},{"id":"1082","type":"Row"},{"id":"1083","type":"Row"},{"id":"1084","type":"Row"},{"id":"1085","type":"Row"},{"id":"1086","type":"Row"}]},"id":"1087","type":"Column"},{"attributes":{"children":[{"id":"1008","type":"Div"},{"id":"1007","type":"Div"}]},"id":"1082","type":"Row"},{"attributes":{"fill_alpha":{"value":0.1},"fill_color":{"value":"#1f77b4"},"line_alpha":{"value":0.1},"line_color":{"value":"#1f77b4"},"size":{"units":"screen","value":5},"x":{"field":"x"},"y":{"field":"y"}},"id":"1051","type":"Scatter"},{"attributes":{"callback":null},"id":"1019","type":"DataRange1d"},{"attributes":{"source":{"id":"1011","type":"ColumnDataSource"}},"id":"1053","type":"CDSView"},{"attributes":{},"id":"1036","type":"WheelZoomTool"},{"attributes":{"callback":{"id":"1063","type":"CustomJS"},"end":3,"margin":[15,15,15,15],"sizing_mode":"stretch_width","start":0,"title":"Cluster #","value":3},"id":"1074","type":"Slider"},{"attributes":{"children":[{"id":"1064","type":"Div"}]},"id":"1086","type":"Row"},{"attributes":{"formatter":{"id":"1058","type":"BasicTickFormatter"},"ticker":{"id":"1026","type":"BasicTicker"}},"id":"1025","type":"LinearAxis"},{"attributes":{"background_fill_alpha":{"value":0.6},"items":[{"id":"1061","type":"LegendItem"}]},"id":"1060","type":"Legend"},{"attributes":{"children":[{"id":"1074","type":"Slider"},{"id":"1075","type":"TextInput"}]},"id":"1083","type":"Row"},{"attributes":{"text":"&lt;a href=\"clusters_last_year.html\" target=\"_blank\"&gt;&lt;button&gt;Last Year&lt;/button&gt;&lt;/a&gt;"},"id":"1005","type":"Div"},{"attributes":{},"id":"1023","type":"LinearScale"},{"attributes":{"sizing_mode":"stretch_width","style":{"font-family":"Helvetica Neue, Helvetica, Arial, sans-serif;","font-size":"1.1em"},"text":"&lt;h3&gt;Filter by the Clusters:&lt;/h3&gt;&lt;p1&gt;The slider below can be used to filter the target cluster. \nSimply slide the slider to the desired cluster number to display the plots that belong to that cluster. \nSlide back to the last cluster to show all the plots.&lt;/p1&gt;"},"id":"1008","type":"Div"},{"attributes":{"callback":{"id":"1063","type":"CustomJS"},"margin":[15,15,15,15],"sizing_mode":"scale_both","title":"Search:"},"id":"1075","type":"TextInput"},{"attributes":{},"id":"1056","type":"BasicTickFormatter"},{"attributes":{},"id":"1035","type":"PanTool"},{"attributes":{},"id":"1038","type":"ResetTool"},{"attributes":{},"id":"1058","type":"BasicTickFormatter"},{"attributes":{"margin":[5,5,5,5],"sizing_mode":"stretch_width","style":{"color":"#2e484c","font-family":"Julius Sans One, sans-serif;"},"text":"&lt;h1&gt;Clustering Literature on Supervised Learning by Classification (Last Month)&lt;/h1&gt;"},"id":"1076","type":"Div"},{"attributes":{"callback":null},"id":"1017","type":"DataRange1d"},{"attributes":{"active_drag":"auto","active_inspect":"auto","active_multi":null,"active_scroll":"auto","active_tap":"auto","tools":[{"id":"1012","type":"HoverTool"},{"id":"1035","type":"PanTool"},{"id":"1036","type":"WheelZoomTool"},{"id":"1037","type":"BoxZoomTool"},{"id":"1038","type":"ResetTool"},{"id":"1039","type":"SaveTool"},{"id":"1040","type":"TapTool"}]},"id":"1041","type":"Toolbar"},{"attributes":{"bottom_units":"screen","fill_alpha":{"value":0.5},"fill_color":{"value":"lightgrey"},"left_units":"screen","level":"overlay","line_alpha":{"value":1.0},"line_color":{"value":"black"},"line_dash":[4,4],"line_width":{"value":2},"render_mode":"css","right_units":"screen","top_units":"screen"},"id":"1059","type":"BoxAnnotation"},{"attributes":{"height":75,"margin":[20,20,20,20],"sizing_mode":"stretch_width","style":{"color":"#0269A4","font-family":"Helvetica Neue, Helvetica, Arial, sans-serif;","font-size":"1.1em"},"text":"Keywords: Slide to specific cluster to see the keywords."},"id":"1062","type":"Paragraph"},{"attributes":{"text":"Clustering of the ACM papers on Supervised Learning by Classification"},"id":"1015","type":"Title"},{"attributes":{},"id":"1071","type":"Selection"},{"attributes":{"children":[{"id":"1006","type":"Div"}]},"id":"1079","type":"Row"},{"attributes":{"children":[{"id":"1014","subtype":"Figure","type":"Plot"}]},"id":"1085","type":"Row"},{"attributes":{"dimension":1,"ticker":{"id":"1031","type":"BasicTicker"}},"id":"1034","type":"Grid"},{"attributes":{},"id":"1039","type":"SaveTool"},{"attributes":{"text":"&lt;a href=\"clusters_last_month.html\" target=\"_blank\"&gt;&lt;button&gt;Last Month&lt;/button&gt;&lt;/a&gt;"},"id":"1003","type":"Div"},{"attributes":{"children":[{"id":"1062","type":"Paragraph"}]},"id":"1084","type":"Row"},{"attributes":{"margin":[5,5,5,5],"sizing_mode":"stretch_width","style":{"font-family":"Helvetica Neue, Helvetica, Arial, sans-serif;","font-size":"1.1em"},"text":"Clustering of literature on supervised learning by classification from ACM Digital Library. \n    The dataset is extracted from &lt;a href=\"https://dl.acm.org/topic/ccs2012/10010147.10010257.10010258.10010259.10010263?expand=all&amp;startPage=\"&gt;here&lt;/a&gt;."},"id":"1006","type":"Div"},{"attributes":{"formatter":{"id":"1056","type":"BasicTickFormatter"},"ticker":{"id":"1031","type":"BasicTicker"}},"id":"1030","type":"LinearAxis"},{"attributes":{"callback":{"id":"1065","type":"CustomJS"}},"id":"1040","type":"TapTool"},{"attributes":{},"id":"1031","type":"BasicTicker"},{"attributes":{"margin":[20,20,20,20],"sizing_mode":"scale_both","style":{"color":"#BF0A30","font-family":"Helvetica Neue, Helvetica, Arial, sans-serif;","font-size":"1.1em"},"text":"Click on a plot to see the info about the article.","width":150},"id":"1064","type":"Div"},{"attributes":{"callback":null,"point_policy":"follow_mouse","tooltips":[["Title","@title"],["Author","@author"],["Abstract","@abstract{safe}"],["Publication Date","@publication_date"],["Cluster","@cluster"]]},"id":"1012","type":"HoverTool"}],"root_ids":["1087"]},"title":"Bokeh Application","version":"1.4.0"}}
        </script>
        <script type="text/javascript">
          (function() {
            var fn = function() {
              Bokeh.safely(function() {
                (function(root) {
                  function embed_document(root) {
                    
                  var docs_json = document.getElementById('1176').textContent;
                  var render_items = [{"docid":"51b4bb2a-fa7a-4669-baa2-0c956413ced0","roots":{"1087":"59d5eaae-bd4c-4d32-bc5e-3c56dfb706b9"}}];
                  root.Bokeh.embed.embed_items(docs_json, render_items);
                
                  }
                  if (root.Bokeh !== undefined) {
                    embed_document(root);
                  } else {
                    var attempts = 0;
                    var timer = setInterval(function(root) {
                      if (root.Bokeh !== undefined) {
                        clearInterval(timer);
                        embed_document(root);
                      } else {
                        attempts++;
                        if (attempts > 100) {
                          clearInterval(timer);
                          console.log("Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing");
                        }
                      }
                    }, 10, root)
                  }
                })(window);
              });
            };
            if (document.readyState != "loading") fn();
            else document.addEventListener("DOMContentLoaded", fn);
          })();
        </script>
    
  </body>
  
</html>