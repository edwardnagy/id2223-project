



<!DOCTYPE html>
<html lang="en">
  
  <head>
    
      <meta charset="utf-8">
      <title>Clustering papers on Supervised Learning by Classification</title>
      
      
        
          
        
        
          
        <script type="text/javascript" src="https://cdn.pydata.org/bokeh/release/bokeh-1.4.0.min.js"></script>
        <script type="text/javascript" src="https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.4.0.min.js"></script>
        <script type="text/javascript">
            Bokeh.set_log_level("info");
        </script>
        
      
      
    
  </head>
  
  
  <body>
    
      
        
          
          
            
              <div class="bk-root" id="8ee57938-c867-4570-88eb-ff975b2ca1b4" data-root-id="1087"></div>
            
          
        
      
      
        <script type="application/json" id="1176">
          {"946ecee8-9104-46f9-a7fd-10758e4f2034":{"roots":{"references":[{"attributes":{"margin":[20,20,20,20],"sizing_mode":"scale_both","style":{"color":"#BF0A30","font-family":"Helvetica Neue, Helvetica, Arial, sans-serif;","font-size":"1.1em"},"text":"Click on a plot to see the info about the article.","width":150},"id":"1064","type":"Div"},{"attributes":{},"id":"1036","type":"WheelZoomTool"},{"attributes":{},"id":"1057","type":"BasicTickFormatter"},{"attributes":{},"id":"1031","type":"BasicTicker"},{"attributes":{"children":[{"id":"1002","type":"Div"}]},"id":"1080","type":"Row"},{"attributes":{"bottom_units":"screen","fill_alpha":{"value":0.5},"fill_color":{"value":"lightgrey"},"left_units":"screen","level":"overlay","line_alpha":{"value":1.0},"line_color":{"value":"black"},"line_dash":[4,4],"line_width":{"value":2},"render_mode":"css","right_units":"screen","top_units":"screen"},"id":"1059","type":"BoxAnnotation"},{"attributes":{"callback":null},"id":"1017","type":"DataRange1d"},{"attributes":{"active_drag":"auto","active_inspect":"auto","active_multi":null,"active_scroll":"auto","active_tap":"auto","tools":[{"id":"1012","type":"HoverTool"},{"id":"1035","type":"PanTool"},{"id":"1036","type":"WheelZoomTool"},{"id":"1037","type":"BoxZoomTool"},{"id":"1038","type":"ResetTool"},{"id":"1039","type":"SaveTool"},{"id":"1040","type":"TapTool"}]},"id":"1041","type":"Toolbar"},{"attributes":{},"id":"1038","type":"ResetTool"},{"attributes":{"high":10,"low":0,"palette":["#1f77b4","#aec7e8","#ff7f0e","#ffbb78","#2ca02c","#98df8a","#d62728","#ff9896","#9467bd","#c5b0d5","#8c564b"]},"id":"1013","type":"LinearColorMapper"},{"attributes":{"children":[{"id":"1003","type":"Div"},{"id":"1004","type":"Div"},{"id":"1005","type":"Div"}]},"id":"1081","type":"Row"},{"attributes":{"margin":[5,5,5,5],"sizing_mode":"stretch_width","style":{"font-family":"Helvetica Neue, Helvetica, Arial, sans-serif;","font-size":"1.1em"},"text":"Clustering of literature on supervised learning by classification from ACM Digital Library. \n    The dataset is extracted from &lt;a href=\"https://dl.acm.org/topic/ccs2012/10010147.10010257.10010258.10010259.10010263?expand=all&amp;startPage=\"&gt;here&lt;/a&gt;."},"id":"1006","type":"Div"},{"attributes":{"formatter":{"id":"1055","type":"BasicTickFormatter"},"ticker":{"id":"1026","type":"BasicTicker"}},"id":"1025","type":"LinearAxis"},{"attributes":{"callback":null,"point_policy":"follow_mouse","tooltips":[["Title","@title"],["Author","@author"],["Abstract","@abstract{safe}"],["Publication Date","@publication_date"],["Cluster","@cluster"]]},"id":"1012","type":"HoverTool"},{"attributes":{},"id":"1071","type":"Selection"},{"attributes":{"args":{"current_selection":{"id":"1064","type":"Div"},"source":{"id":"1011","type":"ColumnDataSource"}},"code":"\n        var titles = [];\n        var authors = [];\n        var abstracts = [];\n        var publicationDates = [];\n        var clusters = [];\n\n        cb_data.source.selected.indices.forEach(index =&gt; {\n            titles.push(source.data['title'][index]);\n            authors.push(source.data['author'][index]);\n            abstracts.push(source.data['abstract'][index]);\n            publicationDates.push(source.data['publication_date'][index]);\n            clusters.push(source.data['cluster'][index]);\n        });\n\n        var title = \"&lt;p1&gt;&lt;b&gt;Title:&lt;/b&gt; \" + (titles[0] ? titles[0].toString() : \"Not available.\") + \"&lt;br&gt;\";\n        var author = \"&lt;p1&gt;&lt;b&gt;Author:&lt;/b&gt; \" + (authors[0] ? authors[0].toString() : \"Not available.\") + \"&lt;br&gt;\";\n        var abstract = \"&lt;p1&gt;&lt;b&gt;Abstract:&lt;/b&gt; \" + abstracts[0].toString() + \"&lt;br&gt;\";\n        var publicationDate = \"&lt;p1&gt;&lt;b&gt;Publication Date:&lt;/b&gt; \" + publicationDates[0].toString() + \"&lt;/p1&gt;&lt;br&gt;\";\n        var cluster = \"&lt;p1&gt;&lt;b&gt;Cluster:&lt;/b&gt; \" + clusters[0].toString() + \"&lt;/p1&gt;\";\n\n        current_selection.text = title + author + abstract + publicationDate + cluster;\n        current_selection.change.emit();\n    "},"id":"1065","type":"CustomJS"},{"attributes":{"callback":{"id":"1063","type":"CustomJS"},"end":11,"margin":[15,15,15,15],"sizing_mode":"stretch_width","start":0,"title":"Cluster #","value":11},"id":"1074","type":"Slider"},{"attributes":{"children":[{"id":"1006","type":"Div"}]},"id":"1079","type":"Row"},{"attributes":{"children":[{"id":"1078","type":"Row"},{"id":"1079","type":"Row"},{"id":"1080","type":"Row"},{"id":"1081","type":"Row"},{"id":"1082","type":"Row"},{"id":"1083","type":"Row"},{"id":"1084","type":"Row"},{"id":"1085","type":"Row"},{"id":"1086","type":"Row"}]},"id":"1087","type":"Column"},{"attributes":{"dimension":1,"ticker":{"id":"1031","type":"BasicTicker"}},"id":"1034","type":"Grid"},{"attributes":{"children":[{"id":"1064","type":"Div"}]},"id":"1086","type":"Row"},{"attributes":{"height":75,"margin":[20,20,20,20],"sizing_mode":"stretch_width","style":{"color":"#0269A4","font-family":"Helvetica Neue, Helvetica, Arial, sans-serif;","font-size":"1.1em"},"text":"Keywords: Slide to specific cluster to see the keywords."},"id":"1062","type":"Paragraph"},{"attributes":{"source":{"id":"1011","type":"ColumnDataSource"}},"id":"1053","type":"CDSView"},{"attributes":{"text":"&lt;a href=\"clusters_last_month.html\" target=\"_blank\"&gt;&lt;button&gt;Last Month&lt;/button&gt;&lt;/a&gt;"},"id":"1003","type":"Div"},{"attributes":{"callback":{"id":"1063","type":"CustomJS"},"margin":[15,15,15,15],"sizing_mode":"scale_both","title":"Search:"},"id":"1075","type":"TextInput"},{"attributes":{},"id":"1021","type":"LinearScale"},{"attributes":{"formatter":{"id":"1057","type":"BasicTickFormatter"},"ticker":{"id":"1031","type":"BasicTicker"}},"id":"1030","type":"LinearAxis"},{"attributes":{"children":[{"id":"1074","type":"Slider"},{"id":"1075","type":"TextInput"}]},"id":"1083","type":"Row"},{"attributes":{"children":[{"id":"1008","type":"Div"},{"id":"1007","type":"Div"}]},"id":"1082","type":"Row"},{"attributes":{"fill_color":{"field":"cluster","transform":{"id":"1013","type":"LinearColorMapper"}},"line_alpha":{"value":0.3},"size":{"units":"screen","value":5},"x":{"field":"x"},"y":{"field":"y"}},"id":"1050","type":"Scatter"},{"attributes":{"text":"&lt;a href=\"clusters_last_half_year.html\" target=\"_blank\"&gt;&lt;button&gt;Last Half Year&lt;/button&gt;&lt;/a&gt;"},"id":"1004","type":"Div"},{"attributes":{"text":"Clustering of the ACM papers on Supervised Learning by Classification"},"id":"1015","type":"Title"},{"attributes":{"label":{"field":"labels"},"renderers":[{"id":"1052","type":"GlyphRenderer"}]},"id":"1061","type":"LegendItem"},{"attributes":{"sizing_mode":"stretch_width","style":{"font-family":"Helvetica Neue, Helvetica, Arial, sans-serif;","font-size":"1.1em"},"text":"&lt;h3&gt;Time Range:&lt;/h3&gt;&lt;p1&gt;Click on the button to change the time range of the plot.&lt;/p1&gt;"},"id":"1002","type":"Div"},{"attributes":{"margin":[5,5,5,5],"sizing_mode":"stretch_width","style":{"font-family":"Helvetica Neue, Helvetica, Arial, sans-serif;","font-size":"1.1em"},"text":"&lt;h3&gt;Filter by Text:&lt;/h3&gt;&lt;p1&gt;Search keyword to filter out the plot. It will search abstracts, titles and authors. \n    Press enter when ready. Clear and press enter to reset the plot.&lt;/p1&gt;"},"id":"1007","type":"Div"},{"attributes":{"background_fill_alpha":{"value":0.6},"items":[{"id":"1061","type":"LegendItem"}]},"id":"1060","type":"Legend"},{"attributes":{},"id":"1026","type":"BasicTicker"},{"attributes":{"callback":{"id":"1065","type":"CustomJS"}},"id":"1040","type":"TapTool"},{"attributes":{"data_source":{"id":"1011","type":"ColumnDataSource"},"glyph":{"id":"1050","type":"Scatter"},"hover_glyph":null,"muted_glyph":null,"nonselection_glyph":{"id":"1051","type":"Scatter"},"selection_glyph":null,"view":{"id":"1053","type":"CDSView"}},"id":"1052","type":"GlyphRenderer"},{"attributes":{"ticker":{"id":"1026","type":"BasicTicker"}},"id":"1029","type":"Grid"},{"attributes":{"children":[{"id":"1076","type":"Div"}]},"id":"1078","type":"Row"},{"attributes":{"children":[{"id":"1014","subtype":"Figure","type":"Plot"}]},"id":"1085","type":"Row"},{"attributes":{"margin":[5,5,5,5],"sizing_mode":"stretch_width","style":{"color":"#2e484c","font-family":"Julius Sans One, sans-serif;"},"text":"&lt;h1&gt;Clustering Literature on Supervised Learning by Classification (Last Year)&lt;/h1&gt;"},"id":"1076","type":"Div"},{"attributes":{"args":{"out_text":{"id":"1062","type":"Paragraph"},"p":{"id":"1014","subtype":"Figure","type":"Plot"},"slider":{"id":"1074","type":"Slider"},"source":{"id":"1011","type":"ColumnDataSource"},"text":{"id":"1075","type":"TextInput"},"topics":["proposed, decision, tree, fairness, forest, problem, time, smote, bias, deep, minority, attributes, species, oversampling, research, robustness, breast, cancer, surface, scaling, signal, users, credit, convex, different, blood, support, categories, rule, algorithms, number","emotion, human, emotions, security, xgboost, communication, information, behavior, time, automatic, framework, number, risk, recognition, proposed, approaches, neural, social, process, facial, participants, different, use","different, proposed, unknown, known, specific, traffic, new, existing, novel, open, information, art, real, world","multi, minority, instance, space, framework, world, instances, decision, selection, problem, proposed, level, existing, high, process, study, research, significant, demonstrate, better","image, images, detection, proposed, fusion, information, cnn, different, multi, networks, order, deep, higher, disease, neural, vectors, function, type, significant, power, known","deep, graph, multi, view, fusion, novel, detection, proposed, confidence, representation, long, covid-, node, time, convolution, nodes, series, different, cnn, related, kernel, predictions, uncertainty, negative, eeg, positive, images, high, level","proposed, time, information, redundancy, problem, multi, subset, number, technique, different, existing, approaches, databases, reduction, selected, analysis, improve, world","recognition, time, face, shape, neural, language, temporal, space, binary, object, modality, deep, information, visual, activities, novel, proposed, parts, structure, art","sentiment, text, analysis, language, semantic, category, information, sentence, word, dependency, languages, graph, aspect, nlp, syntactic, neural, sentences, representation, detection, networks, researchers, convolutional, categories, term, classify, matrix, representations","unlabeled, generalization, information, pseudo, framework, source, novel, adaptation, domains, new, art, shift, available, compared, effectiveness, transfer, semi","ensemble, proposed, compared, technique, process, base, deep, hybrid, multi, efficient, best, techniques, crop, voting, problem, different, improved, real, benchmark, important, strategy, evaluated, world"]},"code":"\n\t\t\t\tvar key = text.value;\n\t\t\t\tkey = key.toLowerCase();\n\t\t\t\tvar cluster = slider.value;\n                var clusters_count = slider.end;\n                var data = source.data; \n                \n                \n                x = data['x'];\n                y = data['y'];\n                x_backup = data['x_backup'];\n                y_backup = data['y_backup'];\n                labels = data['cluster'];\n                abstract = data['abstract'];\n                title = data['title'];\n                author = data['author'];\n                if (cluster == clusters_count) {\n                    out_text.text = 'Keywords: Slide to specific cluster to see the keywords.';\n                    for (i = 0; i &lt; x.length; i++) {\n\t\t\t\t\t\tif(abstract[i].toLowerCase().includes(key) || \n                        (title[i] &amp;&amp; title[i].toLowerCase().includes(key)) ||\n                        (author[i] &amp;&amp; author[i].toLowerCase().includes(key))) {\n\t\t\t\t\t\t\tx[i] = x_backup[i];\n\t\t\t\t\t\t\ty[i] = y_backup[i];\n\t\t\t\t\t\t} else {\n\t\t\t\t\t\t\tx[i] = undefined;\n\t\t\t\t\t\t\ty[i] = undefined;\n\t\t\t\t\t\t}\n                    }\n                }\n                else {\n                    out_text.text = 'Keywords: ' + topics[Number(cluster)];\n                    for (i = 0; i &lt; x.length; i++) {\n                        if(labels[i] == cluster) {\n\t\t\t\t\t\t\tif(abstract[i].toLowerCase().includes(key)\n                            || (title[i] &amp;&amp; title[i].toLowerCase().includes(key))\n                            || (author[i] &amp;&amp; author[i].toLowerCase().includes(key))) {\n\t\t\t\t\t\t\t\tx[i] = x_backup[i];\n\t\t\t\t\t\t\t\ty[i] = y_backup[i];\n\t\t\t\t\t\t\t} else {\n\t\t\t\t\t\t\t\tx[i] = undefined;\n\t\t\t\t\t\t\t\ty[i] = undefined;\n\t\t\t\t\t\t\t}\n                        } else {\n                            x[i] = undefined;\n                            y[i] = undefined;\n                        }\n                    }\n                }\n            source.change.emit();\n            "},"id":"1063","type":"CustomJS"},{"attributes":{},"id":"1023","type":"LinearScale"},{"attributes":{},"id":"1072","type":"UnionRenderers"},{"attributes":{"children":[{"id":"1062","type":"Paragraph"}]},"id":"1084","type":"Row"},{"attributes":{"below":[{"id":"1025","type":"LinearAxis"}],"center":[{"id":"1029","type":"Grid"},{"id":"1034","type":"Grid"},{"id":"1060","type":"Legend"}],"left":[{"id":"1030","type":"LinearAxis"}],"margin":[5,5,5,5],"plot_height":500,"plot_width":500,"renderers":[{"id":"1052","type":"GlyphRenderer"}],"sizing_mode":"scale_both","title":{"id":"1015","type":"Title"},"toolbar":{"id":"1041","type":"Toolbar"},"toolbar_location":"above","x_range":{"id":"1017","type":"DataRange1d"},"x_scale":{"id":"1021","type":"LinearScale"},"y_range":{"id":"1019","type":"DataRange1d"},"y_scale":{"id":"1023","type":"LinearScale"}},"id":"1014","subtype":"Figure","type":"Plot"},{"attributes":{},"id":"1035","type":"PanTool"},{"attributes":{"callback":null},"id":"1019","type":"DataRange1d"},{"attributes":{"callback":null,"data":{"abstract":["A growing worldwide consensus agrees that a global energy transition to renewable energy sources is urgent to avoid the direst consequences of rapid climate change. This transition is a substantial challenge facing humanity, which requires cooperation and innovation across disciplines and nations. In this context, the precise estimation of the renewable potential of a given area is valuable for decision-makers. Cities will play an essential role in this transition through distributed photovoltaic generation as evidenced by the UN 11 th sustainable development goal. However, the complex nature of cities makes this estimation a difficult problem. Recently, several machine learning approaches have successfully contributed to different aspects of the urban photovoltaic potential estimation problem. In the present manuscript, these proposals are summarized, following a hierarchical framework usually described in the literature, including the latest available research. Input and target variables involved in the discussed works are reclassified using a novel categorization. This categorization highlights interesting trends in the field, for each sub-problem in the hierarchical approach, which allows the identification of knowledge gaps and possible future lines of research. The present work presents other unexplored avenues and lists concisely the techniques and variables used for each estimation problem, facilitating improvements on already explored techniques or innovation on not yet explored avenues.","Cross-domain arrhythmia classification (CAC) aims to transfer the model trained on a label-sufficient source domain to a label-scarce target domain. To the best of our knowledge, almost all existing CAC models focus on the unsupervised setting, where no labeled target samples are available. However, in most practical scenes, acquiring limited annotated target samples is feasible, which can provide reliable target semantic information for model learning directly. Consequently, we first propose a more realistic semi-supervised CAC setting, where only the source samples and extremely limited target samples are annotated. Most previous CAC models realize cross-domain learning by aligning the feature distributions of source and target domains coarsely and globally, where the semantic invariance within each class is not taken into consideration during the domain alignment process. Additionally, the semantic information contained in the feature space is not fully utilized for target pseudo label mining. To address the above two issues, a unified framework containing the semantic-aware feature alignment (SAFA) and prototype-based label propagation (PBLP) modules is proposed. In the proposed framework, SAFA and PBLP are complementary to each other. Specifically, SAFA provides more robust prototypes for PBLP by performing semantic-aware feature alignment, and PBLP offers more reliable target pseudo labels for more effective semantic-aware feature alignment learning. Comprehensive qualitative and quantitative experimental results on different benchmarks verify the effectiveness of the proposed method.\nHighlights\n\u2022\nThis paper introduces a realistic semi-supervised CAC scene.\n\u2022\nA unified framework is designed to address two issues of semi-supervised CAC.\n\u2022\nSAFA and PBLP are designed to address feature alignment and mine pseudo labels.\n\u2022\nExperiments on four ECG benchmarks demonstrate the effectiveness of our method.","The use of functional magnetic resonance imaging (fMRI) data in machine learning (ML)-based classification of autism spectrum disorder (ASD) has been a topic of increasing research interest in past years due to the noninvasiveness of the fMRI technique and its potential for providing valuable biomarkers. However, there are still controversies surrounding some fMRI data preprocessing steps, such as bandpass filtering and global signal regression (GSR). It still needs to be determined whether or how these preprocessing steps impact the classification accuracy of ML algorithms. This paper uses fMRI signals from the ABIDE-I dataset to train a long short-term memory (LSTM) network to classify subjects into ASD or healthy controls (HC). We considered 18 preprocessing pipelines comprising all combinations of with and without filtering, with and without global signal regression, and three different segment lengths of 1 min, 2 min, and 3 min. The best model was obtained when using a segment length of 2 min. Our results suggest that not filtering produces significantly higher classification accuracies than filtering, whereas there were no significant differences in classification accuracies when removing or not the global signal.","Highlights\n\u2022\nThe highlights of the article are given below for your kind perusal. Kindly, consider and forward my article for further processes.\n\u2022\nThis paper introduces an intelligent approach for detecting the faults of an ANPC inverter with NP connection for EVs.\n\u2022\nThis is the first work that utilizes an SVM for detecting the faulty phase and DNN for locating the faulty switch in the 3-level ANPC inverter of EV to enhance the performance.\n\u2022\nThe proposed technique integrates the advantages of both the classifiers for obtaining enhanced performance outcomes.\n\u2022\nFurthermore, on observing the outcomes, it's clear that the accuracy of the proposed scheme is 0.9222, whereas for all the other conventional methods it's less. This also accounts for a precise fault detection and localization.\nAbstract\nAn intelligent technique for detecting and localizing an inverter switch fault or phase fault of a Three-Level Active Neutral Point Clamped (ANPC) inverter is proposed in this research. Moreover, a 3L-ANPC inverter can gain the controllability of EV's power train and not need to be stalled even after the occurrence of the fault. Hence, an efficient fault diagnosis methodology is required to identify the type of phase fault by a Support Vector Machine (SVM), a machine learning model consisting of sets of labeled training data with regression and classification challenges. Finally, when the fault occurs, the location of the switch fault can be identified by a Deep Neural Network (DNN), which consists of layers of neurons between the input and output layers which fuses the feature extraction process with increased accuracy. Thus, the detection and localization of the open-circuit fault of the switches in the ANPC inverter help overcome all single faults, hence gaining its current controllability without stopping the vehicle. The accuracy of fault detection is improved in a precise manner. Finally, the performance of the proposed work is evaluated over other conventional models concerning varied metrics like the accuracy of identification and localization.","Gated spiking neural P (GSNP) model is a recently developed recurrent-like network, which is abstracted by nonlinear spiking mechanism of nonlinear spiking neural P systems. In this study, a modification of GSNP is combined with attention mechanism to develop a novel model for sentiment classification, called attention-enabled GSNP model or termed as AGSNP model. The AGSNP model has two channels that process content words and aspect item respectively, where two modified GSNPs are used to obtain dependencies between content words and between aspect words. Moreover, two attention components are used to establish semantic correlation between content words and aspect item. Comparative experiments on three real data sets and several baseline models are conducted to verify the effectiveness of the AGSNP model. The comparison results demonstrate that the AGSNP model is competent for aspect-level sentiment classification tasks.\nHighlights\n\u2022\nWe propose a modification of gated spiking neural P systems.\n\u2022\nWe propose a new model for sentiment classification, termed as AGSNP model.\n\u2022\nWe design a sentiment classification framework based on AGSNP.","Adversarial training is effective to train robust image classification models. To improve the robustness, existing approaches often use many propagations to generate adversarial examples, which have high time consumption. In this work, we propose an efficient adversarial training method with loss guided propagation (ATLGP) to accelerate the adversarial training process. ATLGP takes the loss value of generated adversarial examples as guidance to control the number of propagations for each training instance at different training stages, which decreases the computation while keeping the strength of generated adversarial examples. In this way, our method can achieve comparable robustness with less time than traditional training methods. It also has good generalization ability and can be easily combined with other efficient training methods. We conduct comprehensive experiments on CIFAR10 and MNIST, the standard datasets for several benchmarks. The experimental results show that ATLGP reduces 30% to 60% training time compared with other baseline methods while achieving similar robustness against various adversarial attacks. The combination of ATLGP and ATTA (an efficient adversarial training method) achieves superior acceleration potential when robustness meets high requirements. The statistical propagation in different training stages and ablation studies prove the effectiveness of applying loss guided propagation for each training instance. The acceleration technique can more easily extend adversarial training methods to large-scale datasets and more diverse model architectures such as vision transformers.\nHighlights\n\u2022\nWe propose an novel formulation to define the efficient adversarial training problem.\n\u2022\nATLGP considers the difference of adversarial attributes among training instance.\n\u2022\nExtensive experiments are conducted to prove the outstanding performance of ATLGP.","The study addresses the challenges of human action recognition and analysis in computer vision, with a focus on classifying Indian dance forms. The complexity of these dance styles, including variations in body postures and hand gestures, makes classification difficult. Deep learning models require large datasets for good performance, so standard data augmentation techniques are used to increase model generalizability. The study proposes the Indian Classical Dance Generative Adversarial Network (ICD-GAN) for augmentation and the quantum-based Convolutional Neural Network (QCNN) for classification. The research consists of three phases: traditional augmentation, GAN-based augmentation, and a combination of both. The proposed QCNN is introduced to reduce computational time. Different GAN variants DC-GAN, CGAN, MFCGAN are employed for augmentation, while transfer learning-based CNN models VGG-16, VGG-19, MobileNet-v2, ResNet-50, and new QCNN are implemented for classification. The study demonstrates that GAN-based augmentation outperforms traditional methods, and QCNN reduces computational complexity while improving prediction accuracy. The proposed method achieves a precision rate of 98.7% as validated through qualitative and quantitative analysis. It provides a more effective and efficient approach compared to existing methods for Indian dance form classification.","Aspect-based sentiment analysis (ABSA) aims to analyze the sentiment polarity of a given text towards several specific aspects. For implementing the ABSA, one way is to convert the original problem into a sentence semantic matching task, using pre-trained language models, such as BERT. However, for such a task, the intra- and inter-semantic relations among input sentence pairs are often not considered. Specifically, the semantic information and guidance of relations revealed in the labels, such as positive, negative and neutral, have not been completely exploited. To address this issue, we introduce a self-supervised sentence pair relation classification task and propose a model named Multi-level Semantic Relation-enhanced Learning Network (MSRL-Net) for ABSA. In MSRL-Net, after recasting the original ABSA task as a sentence semantic matching task, word dependency relations and word-sentence relations are utilized to enhance the word-level semantic representation for the sentence semantic matching task, while sentence semantic relations and sentence pairs relations are utilized to enhance the sentence-level semantic representation for sentence pair relation classification. Empirical experiments on SemEval 2014 Task 4, SemEval 2016 Task 5 and SentiHood show that MSRL-Net significantly outperforms other baselines such as BERT in terms of accuracy, Macro-F1 and AUC.\nHighlights\n\u2022\nA Multi-level Semantic Relation-enhanced Learning Network model is proposed.\n\u2022\nA new self-supervised relation classification task is introduced.\n\u2022\nFour types of relations at different levels are selectively leveraged.\n\u2022\nThe experiments on three public datasets demonstrate the effectiveness of our model.","Social media's explosive growth brings with it a variety of societal risks ranging from severely harmful issues such as dangerous organizations and child sexual exploitation to moderately harmful content like displays of aggression, borderline nudity to benign or distasteful contents like gross videos and baity content. In recent times, the multitude and magnitude of these harms is being further exacerbated with the advent of generative AI [5]. Meta is committed to ensuring that Facebook is a place where people feel empowered to communicate and we take our role seriously in keeping abuse off the platform [7]. In this talk, I will describe practical challenges and lessons learned from tackling bad experiences for users on Facebook, particularly in the subjective, borderline and low quality spectrum of harms using state of the art, scalable machine learning approaches to content understanding, user behavior understanding and personalized ranking.","Highlights\n\u2022\nAn estimate of the Bayes cost is proposed as the loss to train neural networks for ordinal classification of imbalanced data.\n\u2022\nThe network parameters, as well as the decision thresholds, are updated during training to minimize the Bayes cost.\n\u2022\nThe neural network architecture has a single neuron in the output layer (one-dimensional input space).\n\u2022\nBoth shallow networks and deep networks can be used.\n\u2022\nExperiments with real data show the accuracy and flexibility of the proposed method, specially in imbalanced problems.\nAbstract\nOrdinal classification of imbalanced data is a challenging problem that appears in many real world applications. The challenge is to simultaneously consider the order of the classes and the class imbalance, which can notably improve the performance metrics. The Bayesian formulation allows to deal with these two characteristics jointly: It takes into account the prior probability of each class and the decision costs, which can be used to include the imbalance and the ordinal information, respectively. We propose to use the Bayesian formulation to train neural networks, which have shown excellent results in many classification tasks. A loss function is proposed to train networks with a single neuron in the output layer and a threshold based decision rule. The loss is an estimate of the Bayesian classification cost, based on the Parzen windows estimator, which is fitted for a thresholded decision. Experiments with several real datasets show that the proposed method provides competitive results in different scenarios, due to its high flexibility to specify the relative importance of the errors in the classification of patterns of different classes, considering the order and independently of the probability of each class.","Existing graph neural networks (GNNs) associate nodes in networks with specific samples in datasets, and thus ignore the conceptual information hidden in object-attribute clusters in datasets. Besides, processing data without structural information is a problem for GNNs since structural information is the input of networks. In this paper, we aim to integrate conceptual information into the message passing of GNNs. To this end, concept lattice theory is fused into existing GNNs. A concept lattice is a powerful tool for describing generalization and specialization relations between formal concepts. And formal concepts, basic elements of concept lattices, effectively explain dependencies between features and samples. On this basis, we propose a new GNN framework induced by a concept lattice to overcome the intrinsic limitations of GNNs. And the novel GNN framework not only joins conceptual information into the message passing but also enables a GNN to process data with or without structural information. Furthermore, the proposed framework is validated under transductive and inductive learning conditions, respectively. The experimental results show that GNNs induced by concept lattices can handle the information hidden in datasets effectively and improve classification accuracies on most benchmark datasets over previous methods.","Human Multimodal Sentiment Analysis (MSA) is an attractive research that studies sentiment expressed from multiple heterogeneous modalities. While transformer-based methods have achieved great success, designing an effective \u201dco-attention\u201d model to associate text modality with nonverbal modalities remains challenging. There are two main problems: 1) the dominant role of the text in modalities is underutilization, and 2) the interaction between modalities is not sufficiently explored. This paper proposes a deep modular Co-Attention Shifting Network (CoASN) for MSA. A Cross-modal Modulation Module based on Co-attention (CMMC) and an Advanced Modality-mixing Adaptation Gate (AMAG) are constructed. The CMMC consists of the Text-guided Co-Attention (TCA) and Interior Transformer Encoder (ITE) units to capture inter-modal features and intra-modal features. With text modality as the core, the CMMC module aims to guide and promote the expression of emotion in nonverbal modalities, and the nonverbal modalities increase the richness of the text-based multimodal sentiment information. In addition, the AMAG module is introduced to explore the dynamical correlations among all modalities. Particularly, this efficient module first captures the nonverbal shifted representations and then combines them to calculate the shifted word embedding representations for the final MSA tasks. Extensive experiments on two commonly used datasets, CMU-MOSI and CMU-MOSEI, demonstrate that our proposed method is superior to the state-of-the-art performance.","Cardiac arrhythmias indicate cardiovascular disease which is the leading cause of mortality worldwide, and can be detected by an electrocardiogram (ECG). Automated deep learning methods have been developed to overcome the disadvantages of manual interpretation by medical experts. The performance of the networks strongly depends on hyperparameter optimization (HPO), and this NP-hard problem is suitable for metaheuristic (MH) methods. In this study, a novel method is proposed for the HPO of a convolutional neural network (CNN) arrhythmia classifier using an MH algorithm. The approach utilizes our variant of an MH method, named the memory-enhanced artificial hummingbird algorithm, which has an additional memory unit that stores the evaluations of the solutions and reduces the computation time significantly. The study also proposes a novel fitness function that considers both the accuracy rate and the total number of parameters of each candidate network. Experiments were conducted on raw ECG samples from the MIT-BIH arrhythmia database. The proposed method was compared with five other MH methods and achieved equal or outperforming results, with classification accuracy reaching 98.87%. The proposed method yielded promising results in finding a high-performing solution with relatively lower complexity.\nHighlights\n\u2022\nA novel memory-enhanced AHA-based automated CNN arrhythmia classifier is proposed.\n\u2022\nA new fitness function is developed to evaluate hyperparameters of candidate CNNs.\n\u2022\nThe proposed method is utilized to classify cardiac arrhythmia on raw ECG samples.\n\u2022\nThe obtained results are promising compared to other state-of-the-art methods.","The twin support vector machines (TWSVM) is a milestone in multi-plane classification with state-of-the-art performance on many classification problems. However, on large scale datasets, the learning speed of TWSVM is expensive. In this paper, we propose a fast twin support vector machines (FTWSVM). In our FTWSVM, a pair of hyperplanes are computed directly from the training dataset without numerical iterations. Experiments on several benchmark datasets show that our method can exhibit good generalization performance and fast learning compared to the fast support vector classifier (FSVC), which specializes in big data problems, and TWSVM, which generalizes well speed.","Despite its wide applications in criminal investigations and clinical communications with patients suffering from autism, automatic micro-expression recognition remains a challenging problem because of the lack of training data and imbalanced classes problems. In this study, we proposed a meta-learning-based multi-model fusion network (Meta-MMFNet) to solve the existing problems. The proposed method is based on the metric-based meta-learning pipeline, which is specifically designed for few-shot learning and is suitable for model-level fusion. The frame difference and optical flow features were fused, deep features were extracted from the fused feature, and finally in the meta-learning-based framework, weighted sum model fusion method was applied for micro-expression classification. Meta-MMFNet achieved better results than state-of-the-art methods on four datasets. The code is available at https://github.com/wenjgong/meta-fusion-based-method.","Enhancing the interpretability of AI techniques is paramount for increasing their acceptability, especially in highly interdisciplinary fields such as remote sensing, in which scientists and practitioners with diverse backgrounds work together to monitor the Earth\u2019s surface. In this context, counterfactual explanations are an emerging tool to characterize the behaviour of machine learning systems, by providing a post-hoc analysis of a given classification model. Focusing on the important task of land cover classification from remote sensing data, we propose a counterfactual explanation approach called CFE4SITS (CounterFactual Explanation for Satellite Image Time Series). One of its distinctive features over existing strategies is the lack of prior assumption on the targeted class for a given counterfactual explanation. This inherent flexibility allows for the automatic discovery of relationship between classes. To assess the quality of the proposed approach, we consider a real-world case study in which we aim to characterize the behavior of a ready-to-use land cover classifier. To this end, we compare CFE4SITS to recent time series counterfactual-based strategies and, subsequently, perform an in-depth analysis of its behaviour.","Graph Neural Networks (GNNs) have shown superior performance for semi-supervised learning of numerous web applications, such as classification on web services and pages, analysis of online social networks, and recommendation in e-commerce. The state of the art derives representations for all nodes in graphs following the same diffusion (message passing) model without discriminating their uniqueness. However, (i) labeled nodes involved in model training usually account for a small portion of graphs in the semi-supervised setting, and (ii) different nodes locate at different graph local contexts and it inevitably degrades the representation qualities if treating them undistinguishedly in diffusion.\nTo address the above issues, we develop NDM, a universal node-wise diffusion model, to capture the unique characteristics of each node in diffusion, by which NDM is able to yield high-quality node representations. In what follows, we customize NDM for semi-supervised learning and design the NIGCN model. In particular, NIGCN advances the efficiency significantly since it (i) produces representations for labeled nodes only and (ii) adopts well-designed neighbor sampling techniques tailored for node representation generation. Extensive experimental results on various types of web datasets, including citation, social and co-purchasing graphs, not only verify the state-of-the-art effectiveness of NIGCN but also strongly support the remarkable scalability of NIGCN. In particular, NIGCN completes representation generation and training within 10 seconds on the dataset with hundreds of millions of nodes and billions of edges, up to orders of magnitude speedups over the baselines, while achieving the highest F1-scores on classification.","Kernel functions are a key element in many machine learning methods to capture the similarity between data points. However, a considerable number of these functions do not meet all mathematical requirements to be a valid positive semi-definite kernel, a crucial precondition for kernel-based classifiers such as Support Vector Machines or Kernel Fisher Discriminant classifiers. In this paper, we propose a novel strategy employing a polar decomposition to effectively transform invalid kernel matrices to positive semi-definite matrices, while preserving the topological structure inherent to the data points. Utilizing polar decomposition allows the effective transformation of indefinite kernel matrices from Krein space to positive semi-definite matrices in Hilbert space, thereby providing an efficient out-of-sample extension for new unseen data and enhancing kernel method applicability across diverse classification tasks. We evaluate our approach on a variety of benchmark datasets and demonstrate its superiority over competitive methods.","Breast cancer is the most frequent cancer and the leading cause of death among females. Diagnosis mass from mammogram correctly can reduce the unnecessary biopsy to a large extent. In this paper, we present a novel mammogram classification method combining the Random Forest and the Locally Linear Embedding (LLE) dimensionality reduction algorithm for texture features. The proposed method consists of three stages. In the first stage, preprocessing is performed to enhance the contrast and suppress the noise of the ROI images. Then, the sixteen-dimensional texture features are extracted from Grey Level Co-occurrence Matrix (GLCM) as the input dataset of LLE and being mapped into a five-dimensional subspace. Finally, a Random Forest classifier is investigated for the mammogram classification and compared with the other four classifiers (SVM, KNN, Logistic Regression, MLPC). The experimental results show that the Random Forest classifier outperforms than the others, with an average accuracy of 92.87% and the AUC value of 0.99, that indicates that the combination of LLE algorithm and Random Forest classifier is a promising method for the mammogram classification.","Audio commands are a preferred communication medium to keep inspectors in the loop of civil infrastructure inspection performed by a semi-autonomous drone. To understand job-specific commands from a group of heterogeneous and dynamic inspectors, a model must be developed cost-effectively for the group and easily adapted when the group changes. This paper is motivated to build a multi-tasking deep learning model that possesses a Share\u2013Split\u2013Collaborate architecture. This architecture allows the two classification tasks to share the feature extractor and then split subject-specific and keyword-specific features intertwined in the extracted features through feature projection and collaborative training. A base model for a group of five authorized subjects is trained and tested on the inspection keyword dataset collected by this study. The model achieved a 95.3% or higher mean accuracy in classifying the keywords of any authorized inspectors. Its mean accuracy in speaker classification is 99.2%. Due to the richer keyword representations that the model learns from the pooled training data Adapting the base model to a new inspector requires only a little training data from that inspector Like five utterances per keyword. Using the speaker classification scores for inspector verification can achieve a success rate of at least 93.9% in verifying authorized inspectors and 76.1% in detecting unauthorized ones. Further The paper demonstrates the applicability of the proposed model to larger-size groups on a public dataset. This paper provides a solution to addressing challenges facing AI-assisted human\u2013robot interaction Including worker heterogeneity Worker dynamics And job heterogeneity.\nHighlights\n\u2022\nThe Share\u2013Split\u2013Collaborate multitask learning architecture is suitable for speaker-keyword classification.\n\u2022\nSubject-specific and phonetic-specific features intertwined in audio data can be disentangled.\n\u2022\nRich keyword representations are learned from multi-subject spoken command data.\n\u2022\nSmall data of new speakers are sufficient for adding new classes to the speaker classifier.\n\u2022\nSpeaker classification scores are also effective for the speaker verification.","Algorithmic bias often arises as a result of differential subgroup validity, in which predictive relationships vary across groups. For example, in toxic language detection, comments targeting different demographic groups can vary markedly across groups. In such settings, trained models can be dominated by the relationships that best fit the majority group, leading to disparate performance. We propose framing toxicity detection as multi-task learning (MTL), allowing a model to specialize on the relationships that are relevant to each demographic group while also leveraging shared properties across groups. With toxicity detection, each task corresponds to identifying toxicity against a particular demographic group. However, traditional MTL requires labels for all tasks to be present for every data point. To address this, we propose Conditional MTL (CondMTL), wherein only training examples relevant to the given demographic group are considered by the loss function. This lets us learn group specific representations in each branch which are not cross contaminated by irrelevant labels. Results on synthetic and real data show that using CondMTL improves predictive recall over various baselines in general and for the minority demographic group in particular, while having similar overall accuracy.","Skilled employees are the most important pillars of an organization. Despite this, most organizations face high attrition and turnover rates. While several machine learning models have been developed to analyze attrition and its causal factors, the interpretations of those models remain opaque. In this paper, we propose the HR-DSS approach, which stands for Human Resource (HR) Decision Support System, and uses explainable AI for employee attrition problems. The system is designed to assist HR departments in interpreting the predictions provided by machine learning models. In our experiments, we employ eight machine learning models to provide predictions. We further process the results achieved by the best-performing model by the SHAP explainability process and use the SHAP values to generate natural language explanations which can be valuable for HR. Furthermore, using \"What-if-analysis\", we aim to observe plausible causes for attrition of an individual employee. The results show that by adjusting the specific dominant features of each individual, employee attrition can turn into employee retention through informative business decisions.","Transfer Learning (TL) as an effective tool to solve the classification problem with few or no labeled training data usually confronts with the multiple source domains issue. Fusion of complementary knowledge in different source domains is expected to improve the classification accuracy in the target domain. For this purpose, we present a new Multi-source transfer learning method via Two-stage Weighted Fusion (MTWF). In MTWF, a credal classification rule is first developed to preserve the local imprecise information as much as possible because it could be difficult to commit some patterns to one precise class when using TL techniques. Then, multiple credal classification results characterized by mass function values produced by patterns in each source domain under different feature spaces are locally integrated to extract useful information in each source domain. The locally integrated credal classification results in different source domains are regarded as multiple sources of evidence for global combination using belief functions to make the class decision, and they are first discounted via the weights estimated by domain-consistency. Whereas, there exist some conflicts among these discounted credal classification results, so we proposed to further discount them via Belief Jensen\u2013Shannon (BJS) divergence for the small conflicts. MTWF was compared with a variety of advanced methods, and the experiment results demonstrate that MTWF can significantly improve the classification accuracy.","The aggregation of multiple opinions plays a crucial role in decision-making, such as in hiring and loan review, and in labeling data for supervised learning. Although majority voting and existing opinion aggregation models are effective for simple tasks, they are inappropriate for tasks without objectively true labels in which disagreements may occur. In particular, when voter attributes such as gender or race introduce bias into opinions, the aggregation results may vary depending on the composition of voter attributes. A balanced group of voters is desirable for fair aggregation results but may be difficult to prepare. In this study, we consider methods to achieve fair opinion aggregation based on voter attributes and evaluate the fairness of the aggregated results.\nTo this end, we consider an approach that combines opinion aggregation models such as majority voting and the Dawid and Skene model (D&amp;S model) with fairness options such as sample weighting. To evaluate the fairness of opinion aggregation, probabilistic soft labels are preferred over discrete class labels. First, we address the problem of soft label estimation without considering voter attributes and identify some issues with the D&amp;S model. To address these limitations, we propose a new Soft D&amp;S model with improved accuracy in estimating soft labels. Moreover, we evaluated the fairness of an opinion aggregation model, including Soft D&amp;S, in combination with different fairness options using synthetic and semi-synthetic data. The experimental results suggest that the combination of Soft D&amp;S and data splitting as a fairness option is effective for dense data, whereas weighted majority voting is effective for sparse data. These findings should prove particularly valuable in supporting decision-making by human and machine-learning models with balanced opinion aggregation.","An imbalanced learning problem is a challenge that has grabbed the interest of academics and industry. Class imbalance is a widespread problem in machine learning due to the unavailability of data in a specific category. Several under-sampling and oversampling methods handle class imbalance; however, finding the correct pattern heavily depends on the sub-sampling. This paper proposes a two-stage approach with a data sampling method where Mutually Disjoint Data Sets(MDS) created from majority samples and binded with minority samples. A set of heterogeneous machine learning algorithms were applied on these binded sub-samples to create ensemble models. MDS ensembles give importance to the minority samples with a double voting method. The proposed method has an improvement of 3.78, 13.78 percent of average recall on the train and test data compared to best-base model results","Image classification is one of the most important areas in computer vision. Hierarchical multi-label classification applies when a multi-class image classification problem is arranged into smaller ones based upon a hierarchy or taxonomy. Thus, hierarchical classification modes generally provide multiple class predictions on each instance, whereby these are expected to reflect the structure of image classes as related to one another. In this paper, we propose a multi-label capsule network (ML-CapsNet) for hierarchical classification. Our ML-CapsNet predicts multiple image classes based on a hierarchical class-label tree structure. To this end, we present a loss function that takes into account the multi-label predictions of the network. As a result, the training approach for our ML-CapsNet uses a coarse to fine paradigm while maintaining consistency with the structure in the classification levels in the label-hierarchy. We also perform experiments using widely available datasets and compare the model with alternatives elsewhere in the literature. In our experiments, our ML-CapsNet yields a margin of improvement with respect to these alternative methods.","Reordering in MT is a major challenge when translating between languages with different of sentence structures. In Phrase-based statistical machine translation (PBSMT) systems, syntactic pre-ordering is a commonly used pre-processing technique. This technique can be used to adjust the syntax of the source language to that of the target language by changing the word order of a source sentence prior to translation and solving to overcome a weakness of classical phrase-based translation systems: long distance reordering. In this paper, we propose a new pre-ordering approach by defining dependency-based features and using a neural network classifier for reordering the words in the source sentence into the same order in target sentence. Experiments on English-Vietnamese machine translation showed that our approach yielded a statistically significant improvement compared to our prior baseline phrase-based SMT system.","In this work, a semi-supervised hierarchical classifier based on local information (SSHC-BLI) is proposed. SSHC-BLI is a semi-supervised learning algorithm that can be applied to hierarchical classification, that is, it can handle labeled and unlabeled data in scenarios where the labels are arranged in a hierarchical structure. SSHC-BLI tries to pseudo-label each unlabeled instance using information of its nearest labeled instances. It uses a similarity function to determine whether the unlabeled instance is similar to its nearest labeled instances to assign it a label; if it is not, then it continues unlabeled. A heuristic similarity function of an instance with a set of instances was proposed to determine similitude. The method was tested in several datasets from functional genomics and compared against a hierarchical supervised classifier and two state of the art methods, showing in most cases superior performance, with statistical significance in accuracy and hierarchical F-measure.","This study is concerned with the design of a three-way classification mechanism realized through combing fuzzy decision trees and expressing uncertainty associated with classification results. The fuzzy decision tree used in this study is constructed through the generalization of the commonly used decision trees. The notion of three-way decision model first proposed for the interpretation of rules generated in rough set approximation has been widely used in many fields. This study proposes an efficient way to flag data with high level of uncertainty with the classification realized by fuzzy decision trees, which is the capability that commonly used fuzzy decision trees do not have. The data identified in this way are left to users\u2019 judgments or more advanced classification techniques. The developed mechanism is formed as a two-stage construct where a fuzzy decision tree is built by generalizing the Boolean classification boundaries of a pre-constructed decision tree using fuzzy sets and then determining the level of uncertainty to identify instances to be rejected due to a lack of sufficient confidence in their belongingness. The rejected instances that are difficult to process are classified as non-commitment cases and left to some further analyses. The rejection quality of the developed three-way classifier is quantified in terms of the classification accuracy and rejection coefficient. We also elaborate on striking a sound tradeoff between these two performance indicators. Experimental studies demonstrate that the developed mechanism could effectively improve the classification accuracy at the cost of a small proportion of the rejected instances and achieve better performance in comparison with other three-way decision models when generating a three-way decision output.\nHighlights\n\u2022\nThe aim is to form a three-way classifier based on fuzzy decision trees.\n\u2022\nThe developed mechanism could flag data with high level of uncertainty.\n\u2022\nThe developed mechanism could construct three regions with the aid of one threshold.","Text Classification (TC) is a fundamental task in the information retrieval community. Nowadays, the mainstay TC methods are built on the deep neural networks, which can learn much more discriminative text features than the traditional shallow learning methods. Among existing deep TC methods, the ones based on Graph Neural Network (GNN) have attracted more attention due to the superior performance. Technically, the GNN-based TC methods mainly transform the full training dataset to a graph of texts, however they often neglect the dependency between words, so as to miss potential semantic information of texts, which may be significant to exactly represent them. To solve the aforementioned problem, we generate graphs of words instead, so as to capture the dependency information of words. Specifically, each text is translated into a graph of words, where neighboring words are linked. We learn the node features of words by a GNN-like procedure and then aggregate them as the graph feature to represent the current text. To further improve the text representations, we suggest a contrastive learning regularization term. Specifically, we generate two augmented text graphs for each original text graph, we constrain the representations of the two augmented graphs from the same text close and the ones from different texts far away. We propose various techniques to generate the augmented graphs. Upon those ideas, we develop a novel deep TC model, namely Text-level Graph Networks with Contrastive Learning (TGNcl). We conduct a number of experiments to evaluate the proposed TGNcl model. The empirical results demonstrate that TGNcl can outperform the existing state-of-the-art TC models.","Highlights\n1\nA new ECOC encoding algorithm is proposed, which separates classes into two groups by maximizing the ratio of the inter-group distance to intra-group distance.\n2\nA novel Soft-Coded (SC) ECOC algorithm is proposed by softening the codewords of the ECOC codematrix by considering the subordination degree of each class to both groups.\n3\nA new measure coverage is designed to evaluate the subordination degrees of different classes to the positive and the negative groups, and the values are set as the elements of the codematrix.\n4\nA self-adaptive strategy is designed to adjust codewords, making them better fit the associated learners.\nAbstract\nError-Correcting Output Codes (ECOC) algorithms enable multiclass classification by reassigning multiple classes to the positive/negative group with the class reassignment schemes being recorded as binary/ternary hard-coded (HC) codematrices. Different classes tend to get diverse subordination degrees to the positive/negative group, providing clues to correct potential errors. However, the HC codematrices are unable to provide the information in the subordination degrees. In this paper, a Soft-Coded ECOC (SC-ECOC) scheme, namely, the Sequential Forward Floating Selection algorithm, is proposed by filling codematrices with real values instead of hard codes to improve classification performance. This algorithm divides multiple classes into two groups by maximizing the ratio of inter-group distance to intra-group distance. Then a new measure coverage is designed to evaluate the subordination degrees of different classes to both groups, which are set as the elements to form a codematrix. Furthermore, a self-adaptive strategy adjusts the value of each element to fit learners better. Experiments are carried out to verify the performance of our algorithm on various data sets, and results confirm that our algorithm can achieve more balanced results compared with the traditional HC ECOC algorithms. Besides, the values of soft codes correlate with the difficulty level of various classes to improve the multiclass classification ability.","Sentence classification is a significant task in natural language processing (NLP) and is applied in many fields. The syntactic and semantic properties of words and phrases often determine the success of sentence classification. Previous approaches based on sequential modeling mainly ignored the explicit syntactic structures in a sentence. In this paper, we propose a Syntax-Aware Transformer (SA-Trans), which integrates syntactic information in the transformer and obtains sentence embeddings by combining syntactic and semantic information. We evaluate our SA-Trans on four benchmark classification datasets (i.e., AG\u2019News, DBpedia, ARP, ARF), and the experimental results manifest that our SA-Trans model achieves competitive performance compared to the baseline models. Finally, the case study further demonstrates the importance of syntactic information for the classification task.","In pattern classification, the class imbalance problem always occurs when the number of observations in some classes is significantly different from that of other categories, which leads to the learning bias in the classifiers. One possible solution to this problem is to re-balance the training set by over-sampling the minority class. However, over-samplings always push the classification boundaries to the majority part, thus the recall increases while the precision decreases. To avoid this situation and better handle the class imbalance problem, this paper proposes a new over-sampling method, namely Subspace-based Minority Over-Sampling (abbr. SMO). This approach considers that each category of samples is formed by common and unique characteristics, and such characteristics can be extracted by subspace. To obtain the balanced data, the common part is over-sampled for more accurately depicting the minority, and the unique part can be expanded by some generative methods. The balanced data are obtained by restoring the generated products of the subspace to the original space. The experimental results demonstrate that the SMO has the ability to model complex data distributions and outperforms both classical and newly designed over-sampling algorithms. Also, SMO can be used to generate simple images, and the generation results of MNIST can be clearly identified by both human vision and machine vision.","Alzheimer's disease is a type of physical brain disorder. Once the disease has begun, it is unrealistic to stop the progression. However, advanced prediction can help to diminish the functions of proteins that influence the disease. Many authors used machine learning techniques to diagnose the disease at its early stage but failed to achieve accurate classification accuracy. So in this work, we attempted to build a model that is a combination of a deep neural network and a multi-layer feed-forward neural network that could accurately extract features from input and achieve the best result. The proposed hybrid model is implemented with a specific configuration for feature extraction and classification, and this model experiments on the MRI dataset collected from ADNI and Kaggle datasets. Experimental results show that an average proposed approach achieves 93.38% of accuracy, 98.5% of AUC 90.05% sensitivity, and 95.04% of specificity for multi-classification between AD, MCI, and CN.","Multimodal sentiment classification is a notable research field that aims to refine sentimental information and classify the sentiment tendency from sequential multimodal data. Most existing sentimental recognition algorithms explore multimodal fusion schemes that achieve good performance. However, there are two key challenges to overcome. First, it is essential to effectively extract inter- and intra-modality features prior to fusion, while simultaneously reducing ambiguity. The second challenge is how to learn modality-invariant representations that capture the underlying similarities. In this paper, we present a modality-invariant temporal learning technique and a new gated inter-modality attention mechanism to overcome these issues. For the first challenge, our proposed gated inter-modality attention mechanism performs modality interactions and filters inconsistencies from multiple modalities in an adaptive manner. We also use parallel structures to learn more comprehensive sentimental information in pairs (i.e., acoustic and visual). In addition, to address the second problem, we treat each modality as a multivariate Gaussian distribution (considering each timestamp as a single Gaussian distribution) and use the KL divergence to capture the implicit temporal distribution-level similarities. These strategies are helpful in reducing domain shifts between different modalities and extracting effective sequential modality-invariant representations. We have conducted experiments on several public datasets (i.e., YouTube and MOUD) and the results show that our proposed method outperforms the state-of-the-art multimodal sentiment categorization methods.\nHighlights\n\u2022\nProposing a modality-invariant temporal representation learning strategy.\n\u2022\nDesigning GIMA to adaptively filter inconsistencies between modalities.\n\u2022\nDesigning a Transformer-based parallel structure for modality-interactions.\n\u2022\nReaching the state-of-the-art for multimodal sentiment recognition on four datasets.","Highlights\n\u2022\nNew method for estimating ring profiles of timber boards based on optical scanning.\n\u2022\n1D CNNs are used to estimate the pith location and detect the surface annual rings.\n\u2022\nThe method was validated on a large number of board cross-sections.\n\u2022\nAverage ring width as an indicating property in machine strength grading of timber.\nAbstract\nIn softwood species, annual ring width correlates with various timber characteristics, including the density and modulus of elasticity along with bending and tensile strengths. Knowledge of annual ring profiles may contribute to more accurate machine strength grading of sawn timber. This paper proposes a fast and accurate method for automatic estimation of ring profiles along timber boards on the basis of optical scanning. The method utilizes two 1D convolutional neural networks to determine the pith location and detect the surface annual rings at multiple cross-sections along the scanned board. The automatically extracted rings and pith information can then be used to estimate the annual ring profile at each cross-section. The proposed method was validated on a large number of board cross-sections for which the pith locations and radial ring width profiles had been determined manually. The paper also investigates the potential of using the automatically estimated average ring width as an indicating property in machine strength grading of sawn timber. The results indicated that combining the automatically estimated ring width with other prediction variables can improve the accuracy of bending and tensile strength predictions, especially when the grading is based only on information extracted from optical and laser scanning data.","In recent years, the imbalanced classification problem has received much attention. SMOTE is one of the most popular methods to improve the performance of unbalanced data classification models. SMOTE changes the data distribution of unbalanced data sets by adding a few generated class samples, but the SMOTE algorithm has some limitations of its own, which may lead to problems such as the generated samples are noisy, the generated samples aggravate the boundary blurring, etc., which are especially obvious in the presence of samples with label noise. Granular-ball computing is an efficient, robust and scalable modeling method developed in the field of granular computing in recent years, and we can obtain clear decision boundaries by dividing data sets through granular-ball. Accordingly, this paper proposes a method, called Granular-ball SMOTE(GBSMOTE),to solve the above problems by first dividing the data set by granular-ball computing and then using SMOTE oversampling inside the granular-ball. The experimental results show the effectiveness of the proposed method, which is more prominent in the samples with label noise.","Ensemble methods are general techniques to improve the accuracy of any given learning algorithm. Boosting is a learning algorithm that builds the classifier ensembles incrementally. In this work we propose an improvement of the classical and inverse AdaBoost algorithms to deal with the problem of the presence of outliers in the data. We propose the Robust Alternating AdaBoost (RADA) algorithm that alternates between the classic and inverse AdaBoost to create a more stable algorithm. The RADA algorithm bounds the influence of the outliers to the empirical distribution, it detects and diminishes the empirical probability of \u201cbad\u201d samples, and it performs a more accurate classification under contaminated data.\nWe report the performance results using synthetic and real datasets, the latter obtained from a benchmark site.","Machine learning (ML) has become an important tool for the development of Industry 4.0. It assists the machining processes by monitoring and maintaining the conditions. Support vector machine (SVM) is one such algorithm of ML used to train and classify the data. The present work uses the SVM for predicting the surface roughness in the end milling of the low-carbon steel. The experiments were performed at nine different combinations of process parameters. Moreover, to monitor the cutting process online, the current drawn is measured using a current sensor. In this regard, a correlation between the current drawn and variation in surface roughness is reported. The average value of the surface roughness was predicted using the SVM at each combination. The results show that the SVM estimates the surface roughness with an approximate error of 0.4 %-10%. On the other hand, the surface roughness variation does not fit well with the current signals due to the variation in tool wear.","The capabilities of deep models are constantly mined for extraction and representation of features among text classification tasks. However, these models are sensitive to changes in input data, resulting in poor robustness. Meanwhile, the model lacks information interaction and weak representation ability. In this work, for feature extraction, a joint model that consists of a convolutional neural network, a bidirectional gated recurrent unit, and an attention mechanism is proposed. This new model can improve versatility and fully discover category information in text. For feature representation, a projector under the supervised contrastive learning method is introduced. The method can improve the representation of an encoder and realize aggregation of the same category. Considering the robustness of the PCRA, the gradient penalty is added to a contrastive loss function. Experiments are performed on four datasets to assess the proposed model (PCRA and PCRA-GP) using an accuracy metric. The experimental results show that our model is suitable for variable-length and bilingual texts. Compared with the baseline model, it remains competitive, and it reaches SOTA on the 20 Newsgroups dataset. Moreover, the performance of the model is evaluated under different hyperparameters to clarify its working mechanism.","In spite of recent advances in computer vision, the classic problem of offline handwritten signature verification still remains challenging. The signature verification task has a high intra-class variability because a given user often shows high variability between its samples. Besides, signature verification is harder in the presence of skilled forgeries. Recently, in order to tackle these challenges, the research community has investigated deep learning methods for learning feature representations of handwritten signatures. When mapping signatures to a feature space, it is desired to obtain dense clusters of signature\u2019s representations, in order to deal with intra-class variability. Besides, not only dense clusters are required but also a larger separation between different user\u2019s clusters in the feature space. Finally, it is also desired to move away feature representations of skilled forgeries in relation to the respective dense cluster of genuine representations. This last property is hard to achieve in the real-world scenario because skilled forgeries are not readily available during training. In this work, we hypothesize that such properties can be achieved by means of a multi-task framework for learning handwritten signature feature representations based on deep contrastive learning. The proposed framework is composed of two objective-specific tasks. The first task aims to map signature examples of the same user closer within the feature space, while separating the feature representations of signatures of different users. The second task aims to adjust the skilled forgeries representations by adopting contrastive losses with the ability to perform hard negative mining. Hard negatives are examples from different classes with some degree of similarity that can be applied for training. We evaluated models obtained with the proposed framework in terms of the equal error rate on GPDSsynthetic, CEDAR and MCYT-75 datasets in writer-dependent and writer-independent verification approaches. Using synthetic and real signature datasets, Friedman tests with Bonferroni\u2013Dunn post hoc tests were performed to compare the proposed multi-task contrastive models against the popular SigNet model as a baseline. Experiments demonstrated an statistically significant improvement in signature verification with a multi-task contrastive model based on the Triplet loss. Implementation of the method is available for download at https://github.com/tallesbrito/contrastive_sigver.\nHighlights\n\u2022\nWe propose a framework for the contrastive learning of signature representations.\n\u2022\nThe method uses similar signatures from different users to discriminate forgeries.\n\u2022\nModels following the proposed framework generalize well to 4 different datasets.\n\u2022\nExperiments show a statistically significant improvement to the SigNet model.\n\u2022\nWe found improvements for writer-dependent and writer-independent verification.","The smart world under Industry 4.0 is witnessing a notable spurt in sleep disorders and sleep-related issues in patients. Artificial intelligence and IoT are taking a giant leap in connecting sleep patients remotely with healthcare providers. The contemporary single-channel-based monitoring devices play a tremendous role in predicting sleep quality and related issues. Handcrafted feature extraction is a time-consuming job in machine learning-based automatic sleep classification. The proposed single-channel work uses Tsfresh to extract features from both the EEG channels (Pz-oz and Fpz-Cz) of the SEDFEx database individually to realise a single-channel EEG. The adopted mRMR feature selection approach selected 55 features from the extracted 787 features. A stacking ensemble classifier achieved 95%, 94%, 91%, and 88% accuracy using stratified 5-fold validation in 2, 3, 4, and 5 class classification employing healthy subjects data. The outcome of the experiments indicates that Tsfresh is an excellent tool to extract standard features from EEG signals.","Recent advances in technology and devices have caused a data explosion on the Internet and on our home PCs. This data is predominantly obtained in various modalities (text, image, video, etc.) and is essential for e-commerce websites. The products on these websites have both images and descriptions in text form, making them multimodal in nature. Earlier categorization and information retrieval methods focused mostly on a single modality. This study employs multimodal data for classification using neutrosophic fuzzy sets for uncertainty management for information retrieval tasks. This effort utilizes image and text data and, inspired by past techniques of embedding text over an image, attempts to classify the images using neutrosophic classification algorithms. For classification tasks, Neutrosophic Convolutional Neural Networks (NCNNs) are used to learn feature representations of the produced images. We demonstrate how a pipeline based on NCNN can be utilized to learn representations of the innovative fusion method. Traditional convolutional neural networks are vulnerable to unknown noisy conditions in the test phase, and as a result, their performance for the classification of noisy data declines. Comparing our method against individual sources on two large-scale multi-modal categorization datasets yielded good results. In addition, we have compared our method to two well-known multi-modal fusion methodologies, namely early fusion and late fusion.","Highlights\n\u2022\nPre-ictal phase detection for epileptic seizure.\n\u2022\nA novel training and feature selection procedure for ML classifiers.\n\u2022\nHybrid training with feature selection based GAs and information criteria.\n\u2022\nThe robust models in terms of balancing model complexity.\n\u2022\nIdentifying pre-ictal patterns with high accuracy and lower FP and FN.\nAbstract\nEpilepsy is the fourth most common neurological disorder, which affects the brain and brings out frequent seizures. They are bursts of electrical discharge that can cause a wide range of symptoms such as distraction or involuntary spasms involving the whole body. Preictal phase carries some important features related to seizures which can be found before seizure onset. Hence, this study introduces an efficient hybrid training procedure for machine learning (ML) classifiers that are able to classify Electroencephalogram (EEG) signals for the accurate detection of preictal phase. Essentially, the proposed approach consists of two stages: feature extraction and model estimation with feature selection. In this approach, while the feature extraction is executed by using wavelet transform, the model estimation is performed by hybrid ML classifiers. Essentially, this approach integrates the training mechanism with a novel feature subset and model selection procedure based on the Information Complexity Criteria (ICOMP) and Genetic Algorithms. For preictal phase detection application, the CHB-MIT Scalp EEG dataset was analyzed by both the proposed and traditional approaches. From the analysis results, it can be concluded that the hybrid ML classifiers not only produce robust models in the context of model information complexity, but also provide superior performance outputs than the classical approaches with respect to validity and reliability, over test datasets.","Hyperspectral image (HSI) provides rich spectral\u2013spatial information and the light detection and ranging (LiDAR) data reflect the elevation information, which can be jointly exploited for better land-cover classification. However, due to different imaging mechanisms, HSI and LiDAR data always present significant image difference, current pixel-wise feature fusion classification methods relying on concatenation or weighted fusion are not effective. To achieve accurate classification result, it is important to extract and fuse similar high-order semantic information and complementary discriminative information contained in multimodal data. In this paper, we propose a novel coupled adversarial learning based classification (CALC) method for fusion classification of HSI and LiDAR data. In specific, a coupled adversarial feature learning (CAFL) sub-network is first trained, to effectively learn the high-order semantic features from HSI and LiDAR data in an unsupervised manner. On one hand, the proposed CAFL sub-network establishes an adversarial game between dual generators and discriminators, so that the learnt features can preserve detail information in HSI and LiDAR data, respectively. On the other hand, by designing weight-sharing and linear fusion structure in the dual generators, we can simultaneously extract similar high-order semantic information and modal-specific complementary information. Meanwhile, a supervised multi-level feature fusion classification (MFFC) sub-network is trained, to further improve the classification performance via adaptive probability fusion strategy. In brief, the low-level, mid-level and high-level features learnt by the CAFL sub-network lead to multiple class estimation probabilities, which are then adaptively combined to generate a final accurate classification result. Both the CAFL and MFFC sub-networks are collaboratively trained by optimizing a designed joint loss function, which consists of unsupervised adversarial loss and supervised classification loss. Overall, by optimizing the joint loss function, the proposed CALC network is pushed to learn highly discriminative fusion features from multimodal data, leading to higher classification accuracies. Extensive experiments on three well-known HSI and LiDAR data sets demonstrate the superior classification performance by the proposed CALC method than several state-of-the-art methods. The source code of the proposed method will be made publicly available at https://github.com/Ding-Kexin/CALC.\nHighlights\n\u2022\nPropose a novel coupled adversarial learning method for multi-modal classification.\n\u2022\nFuse high-order semantic information and multi-modal complementary information.\n\u2022\nExplore effective multi-level feature fusion strategy to improve discrimination.\n\u2022\nDesign an end-to-end deep learning framework for two sub-network joint training.","Several machine learning applications, including genetics and fraud detection, suffer from incomplete label information. In such applications, a classifier can only train from positive and unlabeled (PU) examples in which the unlabeled data consist of both positive and negative examples. Despite a substantial presence of PU learning in the literature, few works have considered a class imbalance setting. Hence, we propose a novel two-step method that exploits anomaly detection to identify hidden positives within the unlabeled data. Our method allows the end-user to choose the anomaly detector depending on preference or domain knowledge. Moreover, we introduce Nearest-Neighbor Isolation Forest (NNIF), a novel semi-supervised anomaly detector based on the Isolation Forest. In contrast to unsupervised anomaly detectors, NNIF can utilize all available label information. Empirical analysis shows that our method generally outperforms, using NNIF as the anomaly detector, state-of-the-art PU learning methods for imbalanced data sets under different labeling mechanisms. Further experiments suggest that our two-step method shows strong robustness to wrong class prior estimates.","The recent pandemic has witnessed a parallel infodemic happening on social media platforms, leading to fear and anxiety within the population. Traditional machine learning (ML) frameworks for fake news detection are limited by the availability of data for training the model. By the time sufficient labeled datasets are available, the existing infodemic may itself come to an end. We propose a COVID-19 fake news detection framework using cross-domain classification techniques to achieve high levels of accuracy while reducing the waiting time for large training datasets to become available. We investigate the effectiveness of three approaches: Domain Adaptive Training, Transfer Learning, and Knowledge Distillation that reuse ML models from past infodemics to improve the accuracy in detecting COVID-19 fake news. Experiments with real-world datasets depict that Transfer Learning performs better than Domain Adaptive Training and Knowledge Distillation techniques.","For any digital application with document images such as retrieval, the classification of document images becomes an essential stage. Conventionally for the purpose, the full versions of the documents, that is the uncompressed document images make the input dataset, which poses a threat due to the big volume required to accommodate the full versions of the documents. Therefore, it would be novel, if the same classification task could be accomplished directly (with some partial decompression) with the compressed representation of documents in order to make the whole process computationally more efficient. In this research work, a novel deep learning model\u2014DWT-CompCNN\u2014is proposed for classification of documents that are compressed using High Throughput JPEG 2000 (HTJ2K) algorithm. The proposed DWT-CompCNN comprises of five convolutional layers with filter sizes of 16, 32, 64, 128, and 256 consecutively for each increasing layer to improve learning from the wavelet coefficients extracted from the compressed images. Experiments are performed on two benchmark datasets, Tobacco-3482 and RVL-CDIP, which demonstrate that the proposed model is time and space efficient, and also achieves a better classification accuracy in compressed domain.","Vertical federated learning (VFL) is an emerging paradigm for cross-silo organizations to build more accurate machine learning (ML) models. In this setting, multiple organizations (i.e., parties) hold the same set of samples with different features. However, different parties may have redundant or highly correlated features, leading to inefficient and ineffective VFL model training. Effective feature selection in VFL is therefore essential to mitigate such a problem and improve model effectiveness, as well as computation and communication efficiency. To this end, in this paper, we propose a federated feature selection framework, called FEAST, which leverages conditional mutual information (CMI) to select more informative features while having low redundancy. Furthermore, we design a communication-efficient method to reduce the information exchanged among the parties while protecting the parties' raw data. Extensive experiments on four real-world datasets demonstrate that the proposed framework achieves state-of-the-art performance in terms of accuracy, communication and computation costs.","Evolving classifiers and especially evolving fuzzy classifiers have been established as a prominent technique for addressing the recent demands in building classifiers in an incremental online manner, based on target labels typically provided by a single user. We present a framework for an interactive evolving multi-user fuzzy classifier system with advanced explainability and interpretability aspects (EFCS-MU-AEI). Multiple users may provide their label feedback based on which own users\u2019 classifiers are incrementally trained with evolving learning concepts. Its classification outputs are amalgamated by a specific ensembling scheme, respecting (i.) uncertainty in the class labels due to labeling ambiguities among the users and (ii.) different experience levels of the users as voting weights. A major focus thereby is concentrated on the explainability of classification outputs for the purpose to increase the quality (consistency and certainty) of the user (labeling) feedbacks. It is established to show reasons why certain decisions have been made and with which certainty levels and rule coverage degrees. The reasons are deduced from the most active rules, which are reduced in their length by a statistically-motivated instance-based feature importance level concept. Another major focus lies on the interpretability of extracted rules in order to represent understandable knowledge contained in the classification problem and especially to realize the labeling behaviors of different users for different parts of the feature space (= different sample groups). A specific incremental feature weighting technique, respecting label uncertainties from multiple users and sample forgetting weights (for handling drifts), as well as a fuzzy set merging process are proposed to aim for a high compactness and transparency of the rules.\nOur approach was evaluated based on a visual inspection scenario. It could be shown that the explanations of the classifier decisions in fact significantly improved the labeling behavior of three single users in terms of showing higher accumulated accuracy trends. Feature weights integration into the classifier updates could achieve transparent rules with final essential four features to describe the classification problem. Based on this description, it turned out in which ways, i.e. for which sample groups, the users with lower experience levels should be taught to improve their understanding about the process.\nHighlights\n\u2022\nEvolving multi-user classifier system fusioning users\u2019 feedback behaviors.\n\u2022\nAdvanced explainability aspects to increase understanding of classifier decisions.\n\u2022\nAdvanced interpretability aspects to guide users to an improved understanding.\n\u2022\nIntegration of a sample forgetting strategy to properly handle drifts in streams.\n\u2022\nEvaluation in an industrial application showing accuracy, decision reasons and rules.","Subtle changes in fine motor control and quantitative electroencephalography (qEEG) in patients with mild cognitive impairment (MCI) are important in screening for early dementia in primary care populations. In this study, an automated, non-invasive and rapid detection protocol for mild cognitive impairment based on handwriting kinetics and quantitative EEG analysis was proposed, and a classification model based on a dual fusion of feature and decision layers was designed for clinical decision-marking. Seventy-nine volunteers (39 healthy elderly controls and 40 patients with mild cognitive impairment) were recruited for this study, and the handwritten data and the EEG signals were performed using a tablet and MUSE under four designed handwriting tasks. Sixty-eight features were extracted from the EEG and handwriting parameters of each test. Features selected from both models were fused using a late feature fusion strategy with a weighted voting strategy for decision making, and classification accuracy was compared using three different classifiers under handwritten features, EEG features and fused features respectively. The results show that the dual fusion model can further improve the classification accuracy, with the highest classification accuracy for the combined features and the best classification result of 96.3% using SVM with RBF kernel as the base classifier. In addition, this not only supports the greater significance of multimodal data for differentiating MCI, but also tests the feasibility of using the portable EEG headband as a measure of EEG in patients with cognitive impairment.\nHighlights\n\u2022\nAn efficient measure based on handwriting and EEG data is presented for detection of MCI.\n\u2022\nA new integrated learning model for classification tasks with multiple test data.\n\u2022\nFeasibility of using combination of handwriting parameters and EEG features to classify MCI compared to any single modality.\n\u2022\nFeasibility of using handwriting kinetic parameters for screening of MCI and wearable EEG devices into the clinic.","Brain\u2013Computer Interfaces (BCI) systems based on electroencephalography (EEG) signals are experiencing a rapid development, counting with a number of methods, mainly from signal processing and machine learning areas. Although important results have been achieved, a robust performance is still a very challenging task, mainly considering high intra- and inter-subject variability in EEG data and long acquisition time intervals. Recently, Deep Learning methods, such as the Convolutional Neural Networks (CNNs), are being used in BCI systems in search of a performance improvement. However, the straightforward use of EEG data, without any processing step, may limit the full potential of 2D-kernels in CNNs. In light of this, in this work, we consider for classification with 2D-kernel-based CNNs the problem of encoding EEG data to images as a pre-processing stage, which includes the Gramian Angular Difference and Summation Fields, Markov Transition Fields and Recurrence Plots. Additionally, a comparative analysis using a selection of CNNs is performed. Results show a favorable performance for the proposed method, pointing towards a robust BCI system using cross-subject data, with short acquisition time interval.\nHighlights\n\u2022\nUse of image encoding methods as a preprocessing tool for EEG/SSVEP signals.\n\u2022\nUse of Convolutional Neural Networks for classification of image encoded EEG signals.\n\u2022\nComparative performance analysis with multiple imaging methods and neural networks.\n\u2022\nThe imaging methods allows robust classification performance for cross subject data.","Text classification is a fundamental task in Text Mining (TM) with applications ranging from spam detection to sentiment analysis. One of the current approaches to this task is Graph Neural Network (GNN), primarily used to deal with complex and unstructured data. However, the scalability of GNNs is a significant challenge when dealing with large-scale graphs. Multilevel optimization is prominent among the methods proposed to tackle the issues that arise in such a scenario. This approach uses a hierarchical coarsening technique to reduce a graph, then applies a target algorithm to the coarsest graph and projects the output back to the original graph. Here, we propose a novel approach for text classification using GNN. We build a bipartite graph from the input corpus and then apply the coarsening technique of the multilevel optimization to generate ten contracted graphs to analyze the GNN\u2019s performance, training time, and memory consumption as the graph is gradually reduced. Although we conducted experiments on text classification, we emphasize that the proposed method is not bound to a specific task and, thus, can be generalized to different problems modeled as bipartite graphs. Experiments on datasets from various domains and sizes show that our approach reduces memory consumption and training time without significantly losing performance.","The growing demand for sustainable development brings a series of information technologies to help agriculture production. Especially, the emergence of machine learning applications, a branch of artificial intelligence, has shown multiple breakthroughs which can enhance and revolutionize plant pathology approaches. In recent years, machine learning has been adopted for leaf disease classification in both academic research and industrial applications. Therefore, it is enormously beneficial for researchers, engineers, managers, and entrepreneurs to have a comprehensive view about the recent development of machine learning technologies and applications for leaf disease detection. This study will provide a survey in different aspects of the topic including data, techniques, and applications. The paper will start with publicly available datasets. After that, we summarize common machine learning techniques, including traditional (shallow) learning, deep learning, and augmented learning. Finally, we discuss related applications. This paper would provide useful resources for future study and application of machine learning for smart agriculture in general and leaf disease classification in particular.","The classification and detection of traffic status plays a vital role in the urban smart transportation system. The classification and mastery of the traffic status at different time periods and sections will help the traffic management department to optimize road management and implement rescue in real time. Travelers can follow the traffic conditions. We choose the best route to effectively improve travel efficiency and safety. However, due to factors such as weather, time of day, lighting, and sample labeling costs, the existing classification methods are insufficient in real time and detection accuracy to meet application requirements. In order to solve this problem, this article aims to effectively transfer and apply the pretrained model learned on large-scale image data sets to small-sample road traffic data sets. By sharing common visual features, model weight parameter migration, and fine-tuning, the road is finally optimized. Traffic conditions classification is based on Traffic-Net. Experiments show that the method in this article can not only obtain a prediction accuracy of more than 96% but also can effectively reduce the model training time and meet the needs of practical applications.","How can we accurately identify new memory workloads while classifying known memory workloads? Verifying DRAM (Dynamic Random Access Memory) using various workloads is an important task to guarantee the quality of DRAM. A crucial component in the process is open-set recognition which aims to detect new workloads not seen in the training phase. Despite its importance, however, existing open-set recognition methods are unsatisfactory in terms of accuracy since they fail to exploit the characteristics of workload sequences.\nIn this article, we propose Acorn, an accurate open-set recognition method capturing the characteristics of workload sequences. Acorn extracts two types of feature vectors to capture sequential patterns and spatial locality patterns in memory access. Acorn then uses the feature vectors to accurately classify a subsequence into one of the known classes or identify it as the unknown class. Experiments show that Acorn achieves state-of-the-art accuracy, giving up to 37% points higher unknown class detection accuracy while achieving comparable known class classification accuracy than existing methods.","The increasing use of machine learning in high-stakes domains \u2013 where people\u2019s livelihoods are impacted \u2013 creates an urgent need for interpretable, fair, and highly accurate algorithms. With these needs in mind, we propose a mixed integer optimization (MIO) framework for learning optimal classification trees \u2013 one of the most interpretable models \u2013 that can be augmented with arbitrary fairness constraints. In order to better quantify the \u201cprice of interpretability\u201d, we also propose a new measure of model interpretability called decision complexity that allows for comparisons across different classes of machine learning models. We benchmark our method against state-of-the-art approaches for fair classification on popular datasets; in doing so, we conduct one of the first comprehensive analyses of the trade-offs between interpretability, fairness, and predictive accuracy. Given a fixed disparity threshold, our method has a price of interpretability of about 4.2 percentage points in terms of out-of-sample accuracy compared to the best performing, complex models. However, our method consistently finds decisions with almost full parity, while other methods rarely do.","A model tree is a hybrid learning algorithm that integrates decision trees and embedding models, with simple structure and high interpretability. However, all the existing works on model trees neglect to estimate the reliability of the output. This estimate not only plays an important role in model selection but also provides an effective guide to the optimization of model tree performance. This work first introduces the output uncertainty of the embedding model into the model tree building process. Specifically, we model the tree based on an expanded post-pruning rule which introduces output uncertainty. At the same time, we include an error estimation term for the embedded model, in which the output uncertainty gives reverse guidance to the model performance. Besides, the proposed optimization rules can be extended as a supplementary condition to any existing post-pruning method. The uncertainty guided model tree is introduced and presented in detail by extending two post-pruning methods which are generally expected to have higher accuracy. Experiments on 12 benchmark data sets demonstrate the superiority of the proposed method.","State-of-the-art face recognition methods typically take the multi-classification pipeline and adopt the softmax-based loss for optimization. Although these methods have achieved great success, the softmax-based loss has its limitation from the perspective of open set classification: the multi-classification objective in the training phase does not strictly match the objective of open set classification testing. In this paper, we derive a new loss named global boundary CosFace (GB-CosFace). Our GB-CosFace introduces an adaptive global boundary to determine whether two face samples belong to the same identity so that the optimization objective is aligned with the testing process from the perspective of open set classification. Meanwhile, since the loss formulation is derived from the softmax-based loss, our GB-CosFace retains the excellent properties of the softmax-based loss, and CosFace is proved to be a special case of the proposed loss. We analyze and explain the proposed GB-CosFace geometrically. Comprehensive experiments on multiple face recognition benchmarks indicate that the proposed GB-CosFace outperforms current state-of-the-art face recognition losses in mainstream face recognition tasks. Compared to CosFace, our GB-CosFace improves 5.30%, 0.70%, and 0.36% at TAR@FAR=1e-6, 1e-5, 1e-4 on IJB-C benchmark.","The acquisition of Twitter by Elon Musk has spurred controversy and uncertainty among Twitter users. The move raised both praise and concerns, particularly regarding Musk's views on free speech. As a result, a large number of Twitter users have looked for alternatives to Twitter. Mastodon, a decentralized micro-blogging social network, has attracted the attention of many users and the general media. In this paper, we analyze the migration of 136,009 users from Twitter to Mastodon. We inspect the impact that this has on the wider Mastodon ecosystem, particularly in terms of user-driven pressure towards centralization. We further explore factors that influence users to migrate, highlighting the effect of users' social networks. Finally, we inspect the behavior of individual users, showing how they utilize both Twitter and Mastodon in parallel. We find a clear difference in the topics discussed on the two platforms. This leads us to build classifiers to explore if migration is predictable. Through feature analysis, we find that the content of tweets as well as the number of URLs, the number of likes, and the length of tweets are effective metrics for the prediction of user migration.","Ecosystems, settlements, and human lives are put at risk by forest fires every year. Several models proposed for the prediction of their occurrence and behavior have aimed at identifying their conditioning factors, risks, and post-effects. However, their application in other regions is impracticable or very difficult, due to the distinct geographic characteristics of the areas and the unavailability of data. This research is devoted to the prediction of both spread and behavior of wildfires at a specific time and/or in specific regions for helping fire management agencies minimize the damages caused. The Brazilian Federal District, inserted in the Cerrado biome, is the focus of the analyses, due to its large number of fire occurrences and reduced number of studies conducted on the region. A dataset was compiled from Brazilian governmental open data for the prediction of the wildfire behavior and used for the training of several Machine Learning models that consider the fire point of ignition to predict the areas that will be impacted. It includes observations on climate features from 5 monitoring stations and satellite data on fires that occurred over the past two decades and was enriched with other topographic, hydrographic, and anthropogenic features, such as urbanization index, distance to rivers/roads, and Normalized Difference Vegetation Index (NDVI). According to the results, AdaBoost model predicted the area affected by the wildfire with 91% accuracy, showing better performance than Random Forest (RF) 88%, Artificial Neural Network (ANN) 86%, and Support Vector Machine (SVM) 81%.","Tsetlin machine (TM) is a logic-based machine learning approach with the crucial advantages of being transparent and hardware-friendly. While TMs match or surpass deep learning accuracy for an increasing number of applications, large clause pools tend to produce clauses with many literals (long clauses). As such, they become less interpretable. Further, longer clauses increase the switching activity of the clause logic in hardware, consuming more power. This paper introduces a novel variant of TM learning - Clause Size Constrained TMs (CSC-TMs) - where one can set a soft constraint on the clause size. As soon as a clause includes more literals than the constraint allows, it starts expelling literals. Accordingly, oversized clauses only appear transiently. To evaluate CSC-TM, we conduct classification, clustering, and regression experiments on tabular data, natural language text, images, and board games. Our results show that CSC-TM maintains accuracy with up to 80 times fewer literals. Indeed, the accuracy increases with shorter clauses for TREC, IMDb, and BBC Sports. After the accuracy peaks, it drops gracefully as the clause size approaches a single literal. We finally analyze CSC-TM power consumption and derive new convergence properties.","Frauds using credit card are easily done by the fraudsters. Due to increase in fraud rates all over the world, various machine learning algorithms are being used by analysts and researchers to detect and analyze frauds in online transactions. However, training data set may have a few instances of one and more instances of another class in case of binary classification particular class which makes result biased. Hence, this paper sets an objective to give methodology which is able to detect fraud accurately in case for skewness of data. The proposed method compares different techniques to handle imbalance problem and chooses best approach out of these and uses XGBoost as classifier to predict whether transaction is fraudulent or not. The developed method is evaluated using European credit card fraud dataset and obtained better F1 score, recall and accuracy as 82.78%, 78.9% and 99.3% respectively as compared to other algorithms taken under study.","Shapelets are subsequences that are effective for classifying time-series instances. Learning shapelets by a continuous optimization has recently been studied to improve computational efficiency and classification performance. However, existing methods have employed predefined and fixed shapelet lengths during the continuous optimization, despite the fact that shapelets and their lengths are inherently interdependent and thus should be jointly optimized. To efficiently explore shapelets of high quality in terms of interpretability and inter-class separability, this study makes the shapelet lengths continuous and learnable. The proposed formulation jointly optimizes not only a binary classifier and shapelets but also shapelet lengths. The derived SGD optimization can be theoretically interpreted as improving the quality of shapelets in terms of shapelet closeness to the time series for target / off-target classes. We demonstrate improvements in area under the curve, total training time, and shapelet interpretability on UCR binary datasets.","Graphical abstract\nDisplay Omitted\nHighlights\n\u2022\nA new dry beans dataset has been prepared and availed publicly.\n\u2022\nThis dataset contains 33,064 images of 14 different types of dry beans.\n\u2022\nFor the first time, deep features has been applied to this problem.\n\u2022\nDry bean classification accuracy was achieved with 91.43% using SSA-ELM model.\n\u2022\nThe results demonstrate that deep feature based ELM models were useful.\nAbstract\nSince dry bean varieties have different qualities and economic values, their separation is of great importance in the field of agriculture. In recent years, the use of artificial intelligence-supported and image-based systems has become widespread for this process. This study aims to create a data set consisting of 14 classes in the detection of dry beans and to investigate the effectiveness of the hybrid structure of the extreme learning machine (ELM) model with GoogLeNet transfer learning on this dataset. At the same time, the salp swarm algorithm (SSA), which is one of the swarm intelligence algorithms, was used to test its applicability in ELM classifier by optimizing ELM parameters. The performance of these models was compared with ELM-based particle swarm optimization, harris hawks optimization, artificial bee colony, and traditional machine learning algorithms such as support vector machine and k-nearest neighbor. The suggested SSA-ELM model successfully classifies 14 different types of dry beans with a success rate of 91.43%. The comparable results demonstrate that the proposed hybrid model had better classification accuracy and performance metrics than traditional machine learning algorithms. In addition, it is seen that the use of image data, extraction of deep features, and classification with optimized ELM in the classification of dry beans have achieved comparable success in the literature.","Skip Abstract Section\nAbstract\nBreast cancer is a divergent and prominent cancer that is responsible for the morbidity and mortality of women throughout the world. This paper aims at early detection and accurate diagnosis of this fatal disease, which is one of the most important steps in breast cancer treatment. Therefore, various nested ensemble machine learning techniques are used to help doctors determine breast cancer at an early stage. The two-layer nested ensemble model has been proposed, which encompasses stacking and voting techniques to detect benign and malignant breast cancer tumors. A total of four two-layer nested ensemble models have been proposed. S(NaiveBayes)-V(3-Meta_Learner), S(BayesNet)-V(3-Meta_Learner), S(NaiveBayes)-V(4-Meta_Learner), and S(BayesNet)-V(4-Meta_Learner) have been designed to contain base learners and meta learners. The experiments have been conducted with the k-fold cross-validation technique for model evaluation. The proposed model is capable of classifying benign and malignant breast cancer tumors with 99.50% accuracy. The aforementioned four models have been compared with previous works in terms of classification accuracy, ROC, recall, precision, TP rate, FP rate, and F1 measure. The experiments showed that the proposed two-layer nested ensemble model S(BayesNet)-V(4-Meta_Learner) performed better than the other three models mentioned supra and competed with all the previously published works. This would help the scientific community and health practitioners diagnose breast cancer with early and accurate results.","Highlights\n\u2022\nConditional probability twin SVM (CPTWSVM) is presented to estimate the conditional probability function and the empirical risk of each class.\n\u2022\nCPTWSVM not only can measure the misclassification of the training samples for each class, but also returns the discriminant projections and distribution function of each class.\n\u2022\nCPTWSVM can eliminate the problems of inconsistent measurement in TWSVMs.\n\u2022\nCPTWSVM can be extended to multiclass classification and maintain the above properties.\n\u2022\nExperiments show that CPTWSVM has a better performance than some leading SVMs and TWSVMs communities.\nAbstract\nIn this paper, we estimate the conditional probability function by presenting a new twin SVM model (CPTWSVM) in binary and multiclass classification problems. The motivation of CPTWSVM is to implement the empirical risk minimization on training data, which is hard to realize in traditional twin SVMs. In each subproblem of CPTWSVM, it measures the empirical risk and outputs the corresponding probability estimate of each class, which eliminates the problems of inconsistent measurement in twin SVMs. Though an additional discriminant objective function is introduced, the optimization problem size of each subproblem is smaller than conditional probability SVM, and is solved by block decomposition algorithm efficiently. In addition, we extend CPTWSVM to multiclass classification by estimating the conditional probability of each class, and maintaining the above properties. Numerical experiments on benchmark and real application datasets demonstrate that CPTWSVM outputs the estimate of probability and the data projection well, resulting in better generalization ability than some leading TWSVMs communities, in terms of binary and multiclass classification.","Exposure to UV rays due to global warming can lead to sunburn and skin damage, ultimately resulting in skin cancer. Early prediction of this type of cancer is crucial. A detailed review in this paper explores various algorithms, including machine learning (ML) techniques as well as deep learning (DL) techniques. While deep learning strategies, particularly CNNs, are commonly employed for skin cancer identification and classification, there is also some usage of machine learning and hybrid approaches. These techniques have proven to be effective classifiers of skin lesions, offering promising results for early detection. The paper analyzes various researchers\u2019 reviews on skin cancer diagnosis to identify a suitable methodology for improving diagnostic accuracy. A publicly available dataset of dermoscopic images retrieved from the ISIC archive has been trained and evaluated. Performance analysis is done, considering metrics such as test and validation accuracy. The results indicate that the RF(random forest) algorithm outperforms other machine learning algorithms in both scenarios, with accuracies of 58.57% without augmentation and 87.32% with augmentation. MobileNetv2, ensemble of Dense Net and Inceptionv3 exhibit superior performance. During training without augmentation, MobileNetv2 achieves an accuracy of 88.81%, while the ensemble model achieves an accuracy of 88.80%. With augmentation techniques applied, the accuracies improved to 97.58% and 97.50%, respectively. Furthermore, experiment with a customized convolutional neural network (CNN) model was also conducted, varying the number of layers and applying various hyperparameter tuning methodologies. Suitable architectures, including a CNN with 7 layers and batch normalization, a CNN with 5 layers, and a CNN with 3 layers were identified. These models achieved accuracies of 77.92%, 97.72%, and 98.02% on the raw data and augmentation datasets, respectively. The experimental results suggest that these techniques hold promise for integration into clinical settings, and further research and validation are necessary. The results highlight the effectiveness of transfer learning models, in achieving high accuracy rates. The findings support the future adoption of these techniques in clinical practice, pending further research and validation.","Highlights\n\u2022\nA novel meta-learning based deep neural network SVM+ (ML-DSVM+) algorithm is proposed for CAD\n\u2022\nML-DSVM+ integrates the bi-channel DNNs and SVM+ into a unified framework for optimization\n\u2022\nA new coupled hinge loss is proposed to perform supervised bidirectional transfer\n\u2022\nML-DSVM+ can effectively alleviate the issues of class imbalance and overfitting\nAbstract\nTransfer learning (TL) can improve the performance of a single-modal medical imaging-based computer-aided diagnosis (CAD) by transferring knowledge from related imaging modalities. Support vector machine plus (SVM+) is a supervised TL classifier specially designed for TL between the paired data in the source and target domains with shared labels. In this work, a novel deep neural network (DNN) based SVM+ (DSVM+) algorithm is proposed for single-modal imaging-based CAD. DSVM+ integrates the bi-channel DNNs and SVM+ classifier into a unified framework to improve the performance of both feature representation and classification. In particular, a new coupled hinge loss function is developed to conduct bidirectional TL between the source and target domains, which further promotes knowledge transfer together with the feature representation under the guidance of shared labels. To alleviate the overfitting caused by the increased parameters in DNNs for limited training samples, the meta-learning based DSVM+ (ML-DSVM+) is further developed, which designs randomly selecting samples from the training data instead of other CAD tasks for meta-tasks. This sampling strategy also can avoid the issue of class imbalance. ML-DSVM+ is evaluated on three medical imaging datasets. It achieves the best results of 88.26\u00b11.40%, 90.45\u00b15.00%, and 87.63\u00b15.56% on accuracy, sensitivity and specificity, respectively, on the Bimodal Breast Ultrasound Image dataset, 90.00\u00b11.05%, 72.55\u00b13.87%, and 96.40\u00b12.26% of the corresponding indices on the Alzheimer's Disease Neuroimaging Initiative dataset, and 85.76\u00b13.12% of classification accuracy, 88.73\u00b17.22% of sensitivity, and 82.60\u00b11.56% of specificity for the Autism Brain Imaging Data Exchange dataset.","Online learning for multi-class classification is a well-studied topic in machine learning. The standard multi-class classification online learning setting assumes continuous availability of the ground-truth class labels. However, in many real-life applications, only partial feedback of the predicted label can be obtained and only the correctness of the prediction is available. Hence, knowledge of the correct label is missing in the case of erroneous predictions. In this case, learning may be slower and classifiers less accurate than in the full feedback scenario. Although several online learning algorithms with partial feedback have been proposed, real-world applications would still benefit from further performance improvement. In this paper, we exploit transfer learning to improve learning in the case of erroneous predictions. We propose the Partial Feedback Online Transfer Learning (PFOTL) algorithm, which uses learned knowledge from the source domain in addition to received partial feedback, and present analysis and a mistake bound for the algorithm. In our experimental results on four benchmark datasets, the proposed algorithm achieves higher online cumulative accuracy than the comparable state-of-the-art algorithms. Two potential applications of our work would be online recommender systems and privacy protection.\nHighlights\n\u2022\nWe propose a partial feedback online transfer learning (PFOTL) algorithm.\n\u2022\nWe utilize transfer learning to improve learning in the partial feedback scenario.\n\u2022\nWe theoretically analyze the mistake bound for the proposed PFOTL algorithm.\n\u2022\nExperiments on multiple datasets demonstrate the superior performance of PFOTL.","Given a large convolutional neural network (CNN) with hundreds of layers, when can the input data be correctly classified? How many layers does each image require? We propose an architecture with a mid-network classifier to classify certain images at earlier points in the model. When the network is very confident about an image, having high activations, then that individual image will be classified early. The number of computations and the average number of convolutions will be reduced if certain images can be classified earlier. In addition, the mid-network classification task is more difficult because fewer features have been extracted at earlier points in the network. Thus, the output and mid-network classifier will work together to correctly classify each image as fast as possible while preserving the accuracy. This proposed method has been implemented into well known computer vision architectures, like ResNet and GoogLeNet Inception. We have achieved large runtime improvements while limiting the accuracy degradation.","Automatic modulation classification (AMC) plays a vital role in modern communication systems, which can support wireless communication systems with limited spectrum resource. This paper proposes an AMC method, which integrates gated recurrent unit (GRU) and convolutional neural network (CNN) to utilize the complementary input features of received signals for spatiotemporal feature extraction and classification. Different from other state-of-the-art (SoA) frameworks, the proposed AMC classifier, named as fusion GRU deep learning neural network (FGDNN), aggregates firstly temporal features with GRUs and then extracts spatial features with CNNs. The GRUs can store temporal dynamic features, and facilitate to capture the characteristics of correlation and dependence among input features. The method is tested extensively with comparisons in order to verify its effectiveness. Experiment results show that the recognition rates of our method outperform other deep learning frameworks.","Federated learning (FL), a decentralized machine learning technique, enhances privacy by enabling multiple devices to collaboratively train a model without transferring data to a central server. FL is used in Human Activity Recognition (HAR) problems, where multiple users generating private wearable data share models with a server to learn a useful global model. However, FL may compromise data privacy through model information sharing during training. Moreover, it adheres to a one-size-fits-all approach toward data privacy, potentially neglecting varied user preferences in collaborative scenarios such as HAR. In response to these challenges, this paper presents a collaborative learning framework integrating differential privacy (DP) and FL, thus providing a tailored approach to privacy protection. While some existing works integrate DP and FL, they do not allow clients to have different privacy preferences. In this work, we introduce a framework that allows different clients to have different privacy preferences and hence more flexibility in terms of privacy. In our framework, DP adds individualized noise to individual clients\u2019 gradient updates for privacy. However, such noised updates can also be interpreted as an attack on the FL system. Defending against these attacks might result in excluding honest private clients altogether from training, posing a fairness concern. On the other hand, not having any defensive measures might allow malicious users to attack the system, posing a security issue. Thus, to address security and fairness, our framework incorporates a client selection strategy that protects the global model from malicious clients and provides fair model access to honest private clients. We have demonstrated the effectiveness of our system on a HAR dataset and provided insights into our framework\u2019s privacy, utility, and fairness.","Confront the problems of the semantic representation, classification accuracy and efficiency of text sentiment analysis methods need to be optimized, this paper proposes an optimized text sentiment classification model (Shap-PreBiNT) based on Transformer. In the text embedding stage, a Shap-Word model is devised to improve the ability of text semantic representation. The Shapley-value method is introduced to calculate the contribution weight of words in sentences, which fused with Word2Vec word vector. In the feature extraction stage, a bidirectional normalization layer is designed to regulate the feature distribution from multi-dimension. In the stage of network structure optimization, a pre-normalization structure is adopted to stabilize the gradient norm and accelerate the convergence rate. The experimental results demonstrate that the proposed model has a better performance by comparing with other related models. On the IMDB English dataset, the classification accuracy and F1-score reach 94.87% and 94.83%, which are 1.48% and 1.47% higher than Transformer. On the ChnSentiCorp Chinese dataset, our model achieves the highest accuracy and F1-Score, which are 91.82% and 91.66%, respectively, and increased by 2.43% and 2.51%.","Performance of Language Identification (LID) System using Gaussian Mixture Models (GMM) is limited by the convergence of Expectation Maximization (EM) algorithm to local maxima. In this paper an LID system is described using Gaussian Mixture Models for the extracted features which are then trained using Split and Merge Expectation Maximization Algorithm that improves the global convergence of EM algorithm. It improves the learning of mixture models which in turn gives better LID performance. A maximum likelihood classifier is used for classification or identifying a language. The superiority of the proposed method is tested for four languages","We investigate the geometric complexity of decision boundaries for robust training compared to standard training. By considering the local geometry of nearest neighbour sets, we study them in a model-agnostic way and theoretically derive a lower-bound\nR\n\u2217\n\u2208\nR\non the perturbation magnitude\n\u03b4\n\u2208\nR\nfor which robust training provably requires a geometrically more complex decision boundary than accurate training. We show that state-of-the-art robust models learn more complex decision boundaries than their non-robust counterparts, confirming previous hypotheses. Then, we compute\nR\n\u2217\nfor common image benchmarks and find that it also empirically serves as an upper bound over which label noise is introduced. We demonstrate for deep neural network classifiers that perturbation magnitudes\n\u03b4\n\u2265\nR\n\u2217\nlead to reduced robustness and generalization performance. Therefore,\nR\n\u2217\nbounds the maximum feasible perturbation magnitude for norm-bounded robust training and data augmentation. Finally, we show that\nR\n\u2217\n&lt;\n0.5\nR\nfor common benchmarks, where R is a distribution\u2019s minimum nearest neighbour distance. Thus, we improve previous work on determining a distribution\u2019s maximum robust radius.","With the development of new sequencing technologies, availability of genomic data has grown exponentially. Over the past decade, numerous studies have used genomic data to identify associations between genes and biological functions. While these studies have shown success in annotating genes with functions, they often assume that genes are completely annotated and fail to take into account that datasets are sparse and noisy. This work proposes a method to detect missing annotations in the context of hierarchical multi-label classification. More precisely, our method exploits the relations of functions, represented as a hierarchy, by computing probabilities based on the paths of functions in the hierarchy. By performing several experiments on a variety of rice (Oriza sativa Japonica), we showcase that the proposed method accurately detects missing annotations and yields superior results when compared to state-of-art methods from the literature.\nHighlights\n\u2022\nOne of the first works to address detection of missing annotations in HMC datasets.\n\u2022\nA novel state-of-art method based on post-processing predicted probabilities.\n\u2022\nAn experimental evaluation including 8 new datasets related to Oryza sativa Japonica.\n\u2022\nExploiting the hierarchy of functions helps to better identify missing gene functions.","Deep learning methods are criticized for the requirement of large set of labeled training data. Therefore, few-shot learning (FSL), which enables fast learning with only a few labeled examples, draws increasing attention. In FSL, the model is trained on base classes to learn the meta-knowledge, which is then applied to novel classes. The generalization from base classes to novel classes in FSL suffers from the class-level over-fitting problem. Generally, in traditional classification tasks, training samples and test samples are from the same class set, where over-fitting happens at the sample level. However, in FSL, the model is evaluated with classification tasks on novel classes. Thus, good fitting on base classes does not guarantee a good generalization to novel classes. In this paper, we reveal the class-level over-fitting problem in FSL and provide an explanation of the cause of this problem. Based on the explanation, we argue that simply scaling the logits of classifier during training can alleviate the class-level over-fitting problem, and analyze how scaling logits (SL) alleviates class-level over-fitting based on gradient back-propagation. Extensive experiments show that SL boosts the performance to the extent of 14% on four popular benchmark datasets. Further, SL also demonstrates its effectiveness on confidence calibration.","In this paper, we construct a novel least squares geometrical nonparallel hyperplane support vector machine (LS-GNHSVM) for binary classification. Unlike the standard TWSVM, LS-GNHSVM is a consistent model and can seek two nonparallel separating hyperplanes through one single quadratic programming problem. We also show that LS-GNHSVM is well-defined in the geometric view. Theoretical analysis is conducted to depict the properties of LS-GNHSVM. To the best of our knowledge, we first derive an upper bound, so-called violation tolerance upper bound (VTUB), to describe the relationship between the distance of any two violated samples against the equality constraints and the corresponding slack variables. Our experiments on synthetic, UCI benchmark and medical image datasets show that the proposed LS-GNHSVM can spend less time and perform better according to the average prediction accuracy, compared with several famous SVMs.\nHighlights\n\u2022\nWe propose a novel LS-GNHSVM, which is well-defined in the geometrical view.\n\u2022\nThe LS-GNHSVM is consistent and enjoys higher prediction accuracy.\n\u2022\nWe first formulate violation tolerance upper bound to depict the model properties.","Aging civil infrastructures are closely monitored by engineers for damage and critical defects. As the manual inspection of such large structures is costly and time-consuming, we are working towards fully automating the visual inspections to support the prioritization of maintenance activities. To that end we combine recent advances in drone technology and deep learning. Unfortunately, annotation costs are incredibly high as our proprietary civil engineering dataset must be annotated by highly trained engineers. Active learning is, therefore, a valuable tool to optimize the trade-off between model performance and annotation costs. Our use-case differs from the classical active learning setting as our dataset suffers from heavy class imbalance and consists of a much larger already labeled data pool than other active learning research. We present a novel method capable of operating in this challenging setting by replacing the traditional active learning acquisition function with an auxiliary binary discriminator. We experimentally show that our novel method outperforms the best-performing traditional active learning method (BALD) by 5% and 38% accuracy on CIFAR-10 and our proprietary dataset respectively.","In this paper, we introduce STEER to adapt learned semantic type extraction approaches to a new, unseen data lake. STEER provides a data programming framework for semantic labeling which is used to generate new labeled training data with minimal overhead. At its core, STEER comes with a novel training data generation procedure called Steered-Labeling that can generate high quality training data not only for non-numeric but also for numerical columns. With this generated training data STEER is able to fine-tune existing learned semantic type extraction models. We evaluate our approach on four different data lakes and show that we can significantly improve the performance of two different types of learned models across all data lakes.","Identification of exfoliated graphene flakes and classification of the thickness are important in the nanomanufacturing of advanced materials and devices. This paper presents a deep learning method to automatically identify and classify exfoliated graphene flakes on Si/SiO2 substrates from optical microscope images. The presented framework uses a hierarchical deep convolutional neural network that is capable of learning new images while preserving the knowledge from previous images. The deep learning model was trained and used to classify exfoliated graphene flakes into monolayer, bi-layer, tri-layer, four-to-six-layer, seven-to-ten-layer, and bulk categories. Compared with existing machine learning methods, the presented method showed high accuracy and efficiency as well as robustness to the background and resolution of images. The results indicated that the pixel-wise accuracy of the trained deep learning model was 99% in identifying and classifying exfoliated graphene flakes. This research will facilitate scaled-up manufacturing and characterization of graphene for advanced materials and devices.\nHighlights\n\u2022\nA machine intelligence approach is developed to classify exfoliated graphene flakes.\n\u2022\nA hierarchical deep convolutional neural network is applied for the classification task.\n\u2022\nThe neural network can learn from new images while preserving the learned knowledge.\n\u2022\nThe approach is robust to the background and resolution of microscopy images.\n\u2022\nAdvanced vision techniques are incorporated to handle highly imbalanced datasets.","In this paper, we propose a new general classification algorithm based on natural neighboring granular spheres, which has good noise immunity and self-adaptability. The old particle sphere generation algorithm is based on purity, and it can have good noise immunity and classification accuracy when the purity threshold is properly selected. However, when the quality of the data is unknown, the selection of the purity threshold will become a difficult problem. The old granular ball generaotr only considers the purity. When the purity reaches the purity threshold, even if the sample points inside the sphere should be divided into two balls, they will not continue to split, which may lead to a large difference between the actual distribution of the generated spheres and the sample points, and cause a negative impact on the next algorithm based on the sphere calculation. To address this situation, this paper proposes to continue the splitting of such balls by applying the natural neighbor-based clustering algorithm: LORE, which does not need parameter k, and thus can provide an effective basis for the granular ball generator.","Many existing image and text sentiment analysis methods only consider the interaction between image and text modalities, while ignoring the inconsistency and correlation of image and text data, to address this issue, an image and text aspect level multimodal sentiment analysis model using transformer and multi-layer attention interaction is proposed. Firstly, ResNet50 is used to extract image features, and RoBERTa-BiLSTM is used to extract text and aspect level features. Then, through the aspect direct interaction mechanism and deep attention interaction mechanism, multi-level fusion of aspect information and graphic information is carried out to remove text and images unrelated to the given aspect. The emotional representations of text data, image data, and aspect type sentiments are concatenated, fused, and fully connected. Finally, the designed sentiment classifier is used to achieve sentiment analysis in terms of images and texts. This effectively has improved the performance of sentiment discrimination in terms of graphics and text.","Skip BACKGROUND: Section\nBACKGROUND:\nWith the advent of 5G and the era of Big Data, the rapid development of medical information technology around the world, the massive application of electronic medical records and cases, and the digitization of medical equipment and instruments, a large amount of data has accumulated in the database system of hospitals, which includes clinical diagnosis data and hospital management data.\nSkip OBJECTIVE: Section\nOBJECTIVE:\nThis study aimed to examine the classification effects of different machine learning algorithms on medical datasets so as to better explore the value of machine learning methods in aiding medical diagnosis.\nSkip METHODS: Section\nMETHODS:\nThe classification datasets of four different medical fields in the University of California Irvine machine learning database were used as the research object. Also, six categories of classification models based on the Bayesian theorem idea, integrated learning idea, and rule-based and tree-based idea were constructed using the Weka platform.\nSkip RESULTS: Section\nRESULTS:\nThe between-group experiments showed that the Random Forest algorithm achieved the best results on the Indian liver disease patient dataset (ILPD), delivery cardiotocography (CADG), and lymphatic tractography (LYMP) datasets, followed by Bagging and partition and regression tree. In the within-group algorithm comparison experiments, the Bagging algorithm achieved better results than other algorithms based on the integration idea for 11 metrics on all datasets, mainly focusing on 2 binary datasets. Logit Boost had only 7 metrics with significant performance, and the best algorithm was Rotation Forest, with 28 metrics achieving optimal values. Among the algorithms based on tree ideas, the logistic model tree algorithm achieved optimal results on all metrics on the mammographic dataset (MAGR). The classification performance of BFTree, J48, and Random Tree was poor on each dataset. The best algorithm was Random Forest on the ILPD, CADG, and LYMP datasets with 27 metrics reaching the optimum.\nSkip CONCLUSION: Section\nCONCLUSION:\nMachine learning algorithms have good application value in disease prediction and can provide a reference basis for disease diagnosis.","Landslides are one of the world\u2019s most devastating and catastrophic natural disasters affecting human life and the economy. Many machine learning-based studies are reported on analyzing, classifying, and predicting Landslides, but there are countless avenues where these techniques must be developed to their full potential. This work proposes a deep convolutional neural network for classifying landslide data. The synthetic minority over-sampling method is employed on the dataset to address the class imbalance issue. A total of six shallow-learning algorithms and one deep-learning algorithm were used for baseline comparison. The proposed DCNN approach outperformed all the baselines chosen with an improvement of 2.1% in the f1-score. This study shows that deep learning would be better for building models capable of classifying landslides on real-world datasets.","Tables are widely used in documents because of their compact and structured representation of information. In particular, in scientific papers, tables can sum up novel discoveries and summarize experimental results, making the research comparable and easily understandable by scholars. Since the layout of tables is highly variable, it would be useful to interpret their content and classify them into categories. This could be helpful to directly extract information from scientific papers, for instance comparing performance of some models given their paper result tables. In this work, we address the classification of tables using a Graph Neural Network, exploiting the table structure for the message passing algorithm in use. We evaluate our model on a subset of the Tab2Know dataset. Since it contains few examples manually annotated, we propose data augmentation techniques directly on the table graph structures. We achieve promising preliminary results, proposing a data augmentation method suitable for graph-based table representation.","Social media is awash with hateful content, much of which is often veiled with linguistic and topical diversity. The benchmark datasets used for hate speech detection do not account for such divagation as they are predominantly compiled using hate lexicons. However, capturing hate signals becomes challenging in neutrally-seeded malicious content. Thus, designing models and datasets that mimic the real-world variability of hate warrants further investigation.\nTo this end, we present GOTHate, a large-scale code-mixed crowdsourced dataset of around 51k posts for hate speech detection from Twitter. GOTHate is neutrally seeded, encompassing different languages and topics. We conduct detailed comparisons of GOTHate with the existing hate speech datasets, highlighting its novelty. We benchmark it with 10 recent baselines. Our extensive empirical and benchmarking experiments suggest that GOTHate is hard to classify in a text-only setup. Thus, we investigate how adding endogenous signals enhances the hate speech detection task. We augment GOTHate with the user's timeline information and ego network, bringing the overall data source closer to the real-world setup for understanding hateful content. Our proposed solution HEN-mBERT is a modular, multilingual, mixture-of-experts model that enriches the linguistic subspace with latent endogenous signals from history, topology, and exemplars. HEN-mBERT transcends the best baseline by 2.5% and 5% in overall macro-F1 and hate class F1, respectively. Inspired by our experiments, in partnership with Wipro AI, we are developing a semi-automated pipeline to detect hateful content as a part of their mission to tackle online harm.","Immune repertoire classification, a typical multiple instance learning (MIL) problem, is a frontier research topic in computational biology that makes transformative contributions to new vaccines and immune therapies. However, the traditional instance-space MIL, directly assigning bag-level labels to instances, suffers from the massive amount of noisy labels and extremely low witness rate. In this work, we propose a noisy-label-learning formulation to solve the immune repertoire classification task. To remedy the inaccurate supervision of repertoire-level labels for a sequence-level classifier, we design a robust training strategy: The initial labels are smoothed to be asymmetric and are progressively corrected using the model's predictions throughout the training process. Furthermore, two models with the same architecture but different parameter initialization are co-trained simultaneously to remedy the known \"confirmation bias\" problem in the self-training-like schema. As a result, we obtain accurate sequence-level classification and, subsequently, repertoire-level classification. Experiments on the Cytomegalovirus (CMV) and Cancer datasets demonstrate our method's effectiveness and superior performance on sequence-level and repertoire-level tasks. Code available at https://github.com/TencentAILabHealthcare/NLL-IRC.","Most of the existing techniques for solving data imbalance problems are geared towards binary classification problems, hence a novel strategy capable of natively handling multi-class classification problems is required. Existing implementations mainly employ a one-versus-rest approach to support multi-class problems and this generalisation hinders its effectiveness in datasets with multiple minority classes. On the contrary, a one-versus-one approach avoids such generalisation and provides finer control over the balancing strategy. In this paper, we propose a novel SCALA algorithm capable of handling imbalanced data with multiple minority class labels with a multi-class output. We introduce a user-defined set of scaling factors which are then integrated with a one-versus-one balancing strategy. Our results show that SCALA demonstrated a significant improvement compared to ADASYN and SMOTE in model performance metrics used to validate balancing techniques. SCALA can balance these datasets without allowing minority classes to overshadow other minority classes. This preserves the information needed by the training algorithm to distinguish between the classes to a high precision.","Near infrared fluorescence optical imaging (NIR-FOI) is a relatively new imaging modality to diagnose arthritis in the hands. The acquired data has two spatial dimensions and one temporal dimension, which visualizes the time dependent distribution of an administered color agent. In accordance with previous work, we hypothesize that the distribution process allows a joint-wise classification into inflammatory affected and unaffected.\nIn this work, we present the first approach to objectively classify hand joint NIR-FOI image stacks by designing, training, and testing a neural network. Previously presented model architectures for spatio-temporal classification do not yield satisfying results when trained on NIR-FOI data. A recall value of 0.812 of the over- and a recall value of 0.652 of the underrepresented class is achieved, the model\u2019s robustness tested against small variations and its attention visualized in activation maps.\nEven though these results leave room for further improvement, they also indicate, that the model architecture can capture the latent features of the data. We are confident, that more available data will lead to a robust classification model and can support medical doctors in using NIR-FOI as a diagnostic tool for PsA.","In vehicle leasing industry which presents a great business opportunity, information completed by applicants was assessed and judged by leasing associates manually in most cases; therefore, assessment results would be affected by their personal experience of leasing associates and decisions would be further affected accordingly. There are few researches on applicant credit risk assessment due to not easy to obtain of vehicle leasing data. Further, the difficulty in vehicle leasing risk assessment is increased due to class imbalance problems in vehicle leasing data. In order to address such issue, a research on credit risk assessment in vehicle leasing industry was conducted in this study. The great disparity in the ratio of high risk and low risk data was addressed by applying synthetic minority over-sampling technique (SMOTE). Then, classification effect of risk assessment model was improved by applying logistic regression in a two-phase manner. In the section of empirical analysis, the feasibility and effectiveness of the approach proposed in this study was validated by using data of actual vehicle leasing application cases provided by a financial institution in Taiwan. It is found that the proposed approach provided a simple yet effective way to build a credit risk assessment model for companies that provide vehicle leasing.","Accurate integration of high-dimensional single-cell sequencing datasets is important for the construction of cell atlases and for the discovery of biomarkers. Because the performance of integration methods varies in different scenarios and on different datasets, it is important to provide end users with an automated system for the benchmarking and selection of the best integration among several alternatives. Here, we present a system that uses an ensemble of auditors, trained by supervised machine learning, which quantifies residual variability of integrated data and automatically selects the integration with the smallest difference between observed and expected batch effects. A rigorous and systematic validation was performed using 6 popular integration methods and 52 benchmark datasets. Algorithmic and data biases were uncovered and shortcomings of existing validation metrics were examined. Our results demonstrate the utility, validity, flexibility and consistency of the proposed approach.","Skip OBJECTIVE: Section\nOBJECTIVE:\nA representation of the sound recordings that are associated with the movement of the entire cardiac structure is termed the Phonocardiogram (PCG) signal. In diagnosing such diverse diseases of the heart, PCG signals are helpful. Nevertheless, as recording PCG signals are prone to several surrounding noises and other disturbing signals, it is a complex task. Thus, prior to being wielded for advanced processing, the PCG signal needs to be denoised. This work proposes an improved heart sound classification by utilizing two-stage Low pass filtering and Wavelet Threshold (WT) technique with subsequent Feature Extraction (FE) using Wavelet Scatter Transform and further classification utilizing the Cubic Polynomial Support Vector Machine (SVM) technique for CVD.\nSkip METHOD: Section\nMETHOD:\nA computer-aided diagnosis system for CVD detection centered on PCG signal analysis is offered in this work. Initially, by heavily filtering the signal, the raw PCG signals obtained using the database were pre-processed. Then, to remove redundant information and noise, it is denoised via the WT technique. From the denoised PCG, wavelet time scattering features were extracted. After that, by employing SVMs, these features were classified for pathology.\nSkip RESULTS: Section\nRESULTS:\nFor the analysis, the PCG signal obtained from the Physionet dataset was considered. Heavy low-pass filtering utilizing a Low-Pass Butterworth Filter (LPBF) is entailed in the pre-processing step. This removed 98% of the noise inherently present in the signal. Further, the signal strength was ameliorated by denoising it utilizing the WT technique. Promising results with maximum noise removal of up to 99% are exhibited by the method. From the PCG, Wavelet Scattering (WS) features were extracted, which were later wielded to categorize the PCG utilizing SVMs with 99.72% accuracy for different sounds.\nSkip DISCUSSION: Section\nDISCUSSION:\nThe Classification accuracies are analogized with other classification techniques present in the literature. This technique exhibited propitious outcomes with a 3% improvement in the F1 score when weighed against the top-notch techniques. The improvement in the metrics is attributed to the usage of the pre-processing stage comprising of Low-pass filter and WT method, WS Transform (WST), and SVMs.\nSkip CONCLUSION: Section\nCONCLUSION:\nThe superiority of the proposed technique is advocated by the comparative investigation with prevailing methodologies. The system revealed that Coronary Artery Disease (CAD) can be implemented with superior methods to achieve high accuracy.","Associative classification (AC) performs much better than other traditional classifiers. It generates a huge number of class association rules (CARs). Since users are interested in the subset of rules, constraints are introduced in the generation of CARs. Real-world databases are record-based in which data is continuously added which demands incremental mining. Hence, constraint class association rules (CCAR) is mined from incremental data. To limit the number of rules and to remove the duplicate rules, redundant rule pruning and duplicate rule pruning techniques are applied. To improve the accuracy of the classifier, the rule selection using principality metric has been applied and the classifier is constructed with rules possessing high principality. Then, classifier is evaluated using single rule and multiple rule prediction methods and the accuracy of the proposed classifier are measured. Experimental results show that the accuracy of the proposed classifier is relatively higher when compared to other algorithms.","Some speleothems, in particular stalagmites, are laminated at the visible and microscopic scale, with the latter visible using fluorescence microscopy (e.g., confocal laser scanning microscopy). These laminations can be used to supplement speleothem chronologies, although this process is laborious and lateral variations in lamination geometry and quality necessitate a detailed look over an entire scan as opposed to a simple one-dimensional transect. In order to assist this process, we develop a classification-based machine learning algorithm using an open-source machine learning package. This algorithm is optimized for stalagmites growing at 20\u2013100\u202f\u03bcm yr\u22121 and outputs a 2-dimensional layer density map which may aid in quantitatively interpreting past variations in speleothem growth rate. This algorithm requires user supervision and interpretation, as image artefacts and magnification settings may complicate model output.\nHighlights\n\u2022\nMachine learning algorithm is presented for counting of laminations in stalagmites.\n\u2022\nVarious densities of neural networks are explored along with transfer learning.\n\u2022\nTwo networks assess (1) the quality of laminations and (2) the lamination count.\n\u2022\nThis model helps visualize changes in growth rate along a growth axis.","Class-incremental continual learning is an important area of research, as static deep learning methods fail to adapt to changing tasks and data distributions. In previous works, promising results were achieved using replay and compressed replay techniques. In the field of regular replay, GDumb [23] achieved outstanding results but requires a large amount of memory. This problem can be addressed by compressed replay techniques. The goal of this work is to evaluate compressed replay in the pipeline of GDumb. We propose FETCH, a two-stage compression approach. First, the samples from the continual datastream are encoded by the early layers of a pre-trained neural network. Second, the samples are compressed before being stored in the episodic memory. Following GDumb, the remaining classification head is trained from scratch using only the decompressed samples from the reply memory. We evaluate FETCH in different scenarios and show that this approach can increase accuracy on CIFAR10 and CIFAR100. In our experiments, simple compression methods (e.g., quantization of tensors) outperform deep autoencoders. In the future, FETCH could serve as a baseline for benchmarking compressed replay learning in constrained memory scenarios.","The measures typically used to assess binary classification problems fail to incorporate the uncertainty inherent to many contexts into the results. We propose using a Bayesian model to express the uncertainty in binary classification problems. This study identified 10 previous studies that provided sufficient data to demonstrate the use of Bayesian analysis in Information Systems (IS) contexts with varying levels of uncertainty. The analysis and user study show that the addition of Bayesian analysis is most useful in high uncertainty contexts with a wide interval for positive predictive value. Such an interval will lead to high uncertainty, even with very certain sensitivity and specificity. The usefulness of Bayesian analysis in conditions of medium uncertainty depends on the context. In conditions of low uncertainty, Bayesian analysis does not add much value. The user study showed that presenting models with uncertainty changed researcher perception of which model performed the best with 18 of 21 researchers changing their opinion. We recommend that authors estimate the uncertainty in their models and provide confusion matrices and prevalence estimates in their results to enable Bayesian analysis as research in a domain matures.","Addressing fairness in lesion classification from dermatological images is crucial due to variations in how skin diseases manifest across skin tones. However, the absence of skin tone labels in public datasets hinders building a fair classifier. To date, such skin tone labels have been estimated prior to fairness analysis in independent studies using the Individual Typology Angle (ITA). Briefly, ITA calculates an angle based on pixels extracted from skin images taking into account the lightness and yellow-blue tints. These angles are then categorised into skin tones that are subsequently used to analyse fairness in skin cancer classification. In this work, we review and compare four ITA-based approaches of skin tone classification on the ISIC18 dataset, a common benchmark for assessing skin cancer classification fairness in the literature. Our analyses reveal a high disagreement among previously published studies demonstrating the risks of ITA-based skin tone estimation methods. Moreover, we investigate the causes of such large discrepancy among these approaches and find that the lack of diversity in the ISIC18 dataset limits its use as a testbed for fairness analysis. Finally, we recommend further research on robust ITA estimation and diverse dataset acquisition with skin tone annotation to facilitate conclusive fairness assessments of artificial intelligence tools in dermatology. Our code is available at https://github.com/tkalbl/RevisitingSkinToneFairness.","Recognizing threats in baggage X-ray scans is one of the most crucial tasks for ensuring safety in high-risk areas, including airports, shopping malls, and cargoes, radiograph. Due to the rise in terrorist activity, particularly in the previous two decades, the identification of baggage threats has received the most attention. Nevertheless, this process is time-consuming and restricted by the security officer\u2019s inspection capabilities. To overcome this, several frameworks based on deep learning have been suggested to effectively detect contraband items. However, these approaches primarily suffer from the issue of class imbalance, where prohibited objects are rarely seen in the real world compared to harmless baggage content. This paper proposes a novel classification network optimized with the novel compound balanced affinity loss function to address the class imbalance. This proposed loss function is based on the synergic integration of max-margin learning and the effective sample representation. The suggested method is tested on two datasets, COMPASS-XP and SIXray, where it outperforms the state-of-the-art in terms of F1-score by 2.55% and 2.52%, respectively. Also, the proposed approach has surpassed the existing frameworks by attaining accuracy of 89.16% and 70.31%, respectively. To the best of our knowledge, this is the first contour-driven classification framework injected with a compound loss function for highly imbalanced threat classification.","Self-labeled techniques are semi-supervised classification models that overcome the shortage of labeled samples via an iterative process. Most relevant proposals are inspired by boosting schemes to iteratively enlarge labeled data, but these methods are constrained by the number and distribution of the initial labeled data. Up to the present, the only exceptions which can solve the above problem are SEG-SSC, k-means-SSC and LC-SSC. However, SEG-SSC relies on too many parameters. Besides, it is hard to improve the distribution of the initial labeled data when the initial labeled set can not roughly represent the distribution of the original data. k-means-SSC and LC-SSC fail to significantly improve the number of the initial labeled data by a limited number of representative points. To address the above issues, this paper proposes a framework based on local cores and synthetic examples generation for self-labeled semi-supervised classification (LCSEG-SSC). First, a new method for finding local cores on labeled and unlabeled data is proposed to improve the distribution of the initial labeled data. Second, STOPF or active labeling is used to predict found local cores. Third, a new example generation technique is proposed to create synthetic labeled samples, intending to improve the number of the initial labeled data. After that, any self-labeled with boosting schemes can be executed on the improved labeled data effectively. Intensive experiments prove that LCSEG-SSC outperforms state-of-the-art methods, especially in a relatively low ratio of labeled data.","State-of-the-art hand prostheses differ from those of previous generations, in that more hand positions and programmable gestures are available [4]. As an example, consider multi-finger prostheses like the i-limbTM ultra from Touch Bionics or the BebionicTM Hand from RSL Steeper. Both prosthetic effectors are primarily controlled by myoelectric signals, derived with two or more cutaneously applied sensors, placed atop residual muscles. After preprocessing and classification of these signals, three to five different movement states or hand positions can be accurately distinguished. Zardoshti-Kermani et al. [5] show that the classification becomes increasingly difficult as the number of gestures grows, because decision spaces and feature clusters overlap [3]. Since static separation becomes increasingly difficult, Hudgins et al. [3] and Attenberger [1] used time-dependencies inherent to electromyographic (EMG) signals to improve classification.","Deep learning models have become increasingly prevalent in various domains, necessitating their deployment on resource-constrained devices. Quantization is a promising way to reduce the model complexity in that it keeps model architecture intact and enables the model to operate on specialized hardwares(e.g., NPU, DSP). Input resolution is also essential in making a trade-off between accuracy and computation.\nIn this paper, we conduct a joint analysis of input resolution and quantization precision on their influence on accuracy for three popular models: ResNet-18, ResNet-50, and MobileNet-V2. By exploring the combined configuration space, we found that better accuracy can be achieved by jointly optimizing the input resolution and quantization bit-width while maintaining the computational complexity.","Automatic image annotation consists on automatically labeling images, or image regions, with a pre-defined set of keywords, which are regarded as descriptors of the high-level semantics of the image. In supervised learning, a set of previously annotated images is required to train a classifier. Annotating a large quantity of images by hand is a tedious and time consuming process; so an alternative approach is to label manually a small subset of images, using the other ones under a semi-supervised approach. In this paper, a new semi-supervised ensemble of classifiers, called WSA, for automatic image annotation is proposed. WSA uses naive Bayes as its base classifier. A set of these is combined in a cascade based on the AdaBoost technique. However, when training the ensemble of Bayesian classifiers, it also considers the unlabeled images on each stage. These are annotated based on the classifier from the previous stage, and then used to train the next classifier. The unlabeled instances are weighted according to a confidence measure based on their predicted probability value; while the labeled instances are weighted according to the classifier error, as in standard AdaBoost. WSA has been evaluated with benchmark data sets, and 2 sets of images, with promising results.","Essay writing has become one of the most common learning tasks assigned to students enrolled in various courses at different educational levels, owing to the growing demand for future professionals to effectively communicate information to an audience and develop a written product (i.e. essay). Evaluating a written product requires scorers who manually examine the existence of rhetorical categories, which is a time-consuming task. Machine Learning (ML) approaches have the potential to alleviate this challenge. As a result, several attempts have been made in the literature to automate the identification of rhetorical categories using Rhetorical Structure Theory (RST). However, RST do not provide information regarding students\u2019 cognitive level, which motivates the use of Bloom\u2019s Taxonomy. Therefore, in this research we propose to: i) investigate the extent to which classification of rhetorical categories can be automated based on Bloom\u2019s taxonomy by comparing the traditional ML classifiers with the pre-trained language model BERT, ii) explore the associations between rhetorical categories and writing performance. Our results showed that BERT model outperformed the traditional ML-based classifiers with 18% better accuracy, indicating it can be used in future analytics tool. Moreover, we found a statistical difference between the associations of rhetorical categories in low-achiever, medium-achiever and high-achiever groups which implies that rhetorical categories can be predictive of writing performance.","The issue of imbalanced data in machine learning has gained significant attention in recent years. Imbalanced data, where one class has significantly fewer samples than others, can lead to poor performance for machine learning models, especially in detecting minority class samples. To address this problem, various resampling techniques have been proposed, including the popular SMOTE (Synthetic Minority Over-sampling TEchnique). However, SMOTE suffers from the overlapping problem and may misclassify samples near the separation boundaries. This paper presents a novel framework to optimise border-based-SMOTEs, including Borderline-SMOTE and SVM-SMOTE which were specifically developed to solve the problem of misclassifying border samples. The proposed method ensures that generated samples improve the decision boundaries and are free from overlapping issues. The proposed method is evaluated on synthetic and real-world datasets, and results demonstrate its effectiveness in enhancing the performance of machine learning models, particularly in classifying minority class samples.","Partial label learning (PLL) is a typical weakly supervised learning problem in which each instance is associated with a candidate label set, and among which only one is true. However, the assumption that the ground-truth label is always among the candidate label set would be unrealistic, as the reliability of the candidate label sets in real-world applications cannot be guaranteed by annotators. Therefore, a generalized PLL named Unreliable Partial Label Learning (UPLL) is proposed, in which the true label may not be in the candidate label set. Due to the challenges posed by unreliable labeling, previous PLL methods will experience a marked decline in performance when applied to UPLL. To address the issue, we propose a two-stage framework named Unreliable Partial Label Learning with Recursive Separation (UPLLRS). In the first stage, the self-adaptive recursive separation strategy is proposed to separate the training set into a reliable subset and an unreliable subset. In the second stage, a disambiguation strategy is employed to progressively identify the ground-truth labels in the reliable subset. Simultaneously, semi-supervised learning methods are adopted to extract valuable information from the unreliable subset. Our method demonstrates state-of-the-art performance as evidenced by experimental results, particularly in situations of high unreliability. Code and supplementary materials are available at https://github.com/dhiyu/UPLLRS.","In this paper, we present a prototype selection technique for imbalanced data, Fuzzy Rough Imbalanced Prototype Selection (FRIPS), to improve the quality of the artificial instances generated by the Synthetic Minority Over-sampling TEchnique (SMOTE). Using fuzzy rough set theory, the noise level of each instance is measured, and instances for which the noise level exceeds a certain threshold level are deleted. The threshold is determined using a wrapper approach that evaluates the training Area Under the Curve of candidate subsets. This proposal aims to clean noisy data before applying SMOTE, such that SMOTE can generate high quality artificial data.\nExperiments on artificial data show that FRIPS in combination with SMOTE outperforms state-of-the-art methods, and that it particularly performs well in the presence of noise.","Machine learning research relies to a large extent on experimental observations. The evaluation of classifiers is often carried out by empirical comparison with classifiers generated by different learning algorithms, allowing the identification of the best algorithm for the problem at hand. Nevertheless, previously to this evaluation, it is important to state if the classifiers have truly learned the domain class concepts, which can be done by comparing the classifiers\u2019 predictive measures with the ones from the baseline classifiers. A baseline classifier is the one constructed by a na\u00efve learning algorithm which only uses the class distribution of the dataset. However, finding na\u00efve classifiers in multi-label learning is not as straightforward as in single-label learning. This work proposes a simple way to find baseline multi-label classifiers. Three specific and one general na\u00efve multi-label classifiers are proposed to estimate the baseline values for multi-label predictive evaluation measures. Experimental results show the suitability of our proposal in revealing the learning power of multi-label learning algorithms.","When data is of an extraordinarily large size or physically stored in different locations, the distributed nearest neighbor (NN) classifier is an attractive tool for classification. We propose a novel distributed adaptive NN classifier for which the number of nearest neighbors is a tuning parameter stochastically chosen by a data-driven criterion. An early stopping rule is proposed when searching for the optimal tuning parameter, which not only speeds up the computation but also improves the finite sample performance of the proposed algorithm. Convergence rate of excess risk of the distributed adaptive NN classifier is investigated under various sub-sample size compositions. In particular, we show that when the sub-sample sizes are sufficiently large, the proposed classifier achieves the nearly optimal convergence rate. Effectiveness of the proposed approach is demonstrated through simulation studies as well as an empirical application to a real-world dataset.","As an important branch of weakly supervised learning, partial label learning (PLL) tackles the problem where each training instance is associated with a set of candidate labels, among only one is correct. Most existing PLL algorithms elaborately designed loss functions and update strategies to learn potential ground-truth labels among candidate labels with deep neural networks. However, these algorithms are susceptible to the cumulative error caused by noisy label propagation when updating label confidences, this will make the deep models tend to overfit the noisy labels, thereby achieving poor generation performance. To remedy this issue, we propose a general framework multi-class partial hinge loss (MPHL) for PLL, which can disambiguate the candidate labels by optimizing the margin between the maximum modeling output from partial labels and that from non-partial ones. More importantly, the partial hinge loss can adaptively optimize the separation hyperplane to reduce the influence of cumulative error. Meanwhile, we introduce graph laplacian regularization to full mine the relationship between candidate labels of similar instances to constrain the separation hyperplane to improve the robustness of disambiguation. Extensive experimental results demonstrate that the multi-class partial hinge loss significantly outperforms the state-of-the-art counterparts.","In this work, we try to address the two challenging problems in machine learning (ML) which are: (a) the need for large amounts of labeled images for training supervised classifiers and (b) the supervised classification for time series data. We formulate the problem as an image classification task by transforming time series into domain-specific 2D features such as the scalogram and recurrence plot (RP). Such domain-specific features provide additional information to the models in contrast to raw time series data and enable us to take advantage of powerful state-of-the-art image classifiers for learning the patterns from these textured images. However, the requirement for large amounts of labeled image data is a major drawback in image classification. Thus, to address this problem, we propose to develop a multimodal fusion-deep reinforcement learning (MMF-DRL) approach as an alternative technique to traditional supervised image classifiers for the classification of time series. The two modalities scalograms and RP are fused together to make a multimodal dataset which is then fed into various models for comparison between supervised and RL approaches for time series classification. Our approach produces better accuracy than the state-of-the-art ChronoNet while needing fewer training examples. We validated our MMF-DRL approach using two different physiological time series datasets. Our results show the merit of using multiple modalities and RL in achieving better classification performance than training on a single modality. Moreover, with our approach, we got the highest accuracy of 90.20% and 89.63% respectively for the two datasets with less training data in contrast to the state-of-the-art SL model ChronoNet which gave 87.62% and 88.02% accuracy respectively for the two datasets with higher training data.","The feature selection problem is a significant challenge in pattern recognition, especially for classification tasks. The quality of the selected features plays a critical role in building effective models, and poor-quality data can make this process more difficult. This work explores the use of association analysis in data mining to select meaningful features, addressing the issue of duplicated information in the selected features. A novel feature selection technique for text classification is proposed, based on frequent and correlated items. This method considers both relevance and feature interactions, using association as a metric to evaluate the relationship between the target and features. The technique was tested using the SMS spam collecting dataset from the UCI machine learning repository and compared with well-known feature selection methods. The results showed that the proposed technique effectively reduced redundant information while achieving high accuracy (95.155%) using only 6% of the features.","Federated learning encounters a critical challenge of data heterogeneity, adversely affecting the performance and convergence of the federated model. Various approaches have been proposed to address this issue, yet their effectiveness is still limited. Recent studies have revealed that the federated model suffers severe forgetting in local training, leading to global forgetting and performance degradation. Although the analysis provides valuable insights, a comprehensive understanding of the vulnerable classes and their impact factors is yet to be established. In this paper, we aim to bridge this gap by systematically analyzing the forgetting degree of each class during local training across different communication rounds. Our observations are: (1) Both missing and non-dominant classes suffer similar severe forgetting during local training, while dominant classes show improvement in performance. (2) When dynamically reducing the sample size of a dominant class, catastrophic forgetting occurs abruptly when the proportion of its samples is below a certain threshold, indicating that the local model struggles to leverage a few samples of a specific class effectively to prevent forgetting. Motivated by these findings, we propose a novel and straightforward algorithm called Federated Knowledge Anchor (FedKA). Assuming that all clients have a single shared sample for each class, the knowledge anchor is constructed before each local training stage by extracting shared samples for missing classes and randomly selecting one sample per class for non-dominant classes. The knowledge anchor is then utilized to correct the gradient of each mini-batch towards the direction of preserving the knowledge of the missing and non-dominant classes. Extensive experimental results demonstrate that our proposed FedKA achieves fast and stable convergence, significantly improving accuracy on popular benchmarks.","In this article different approximations of a local classifier algorithm are described and compared. The classification algorithm is composed by two different steps. The first one consists on the clustering of the input data by means of three different techniques, specifically a k-means algorithm, a Growing Neural Gas (GNG) and a Self-Organizing Map (SOM). The groups of data obtained are the input to the second step of the classifier, that is composed of a set of one-layer neural networks which aim is to fit a local model for each cluster. The three different approaches used in the first step are compared regarding several parameters such as its dependence on the initial state, the number of nodes employed and its performance. In order to carry out the comparative study, two artificial and three real benchmark data sets were employed.","Instance selection (IS) serves as a vital preprocessing step, particularly in addressing the complexities associated with high-dimensional problems. Its primary goal is the reduction of data instances, a process that involves eliminating irrelevant and superfluous data while maintaining a high level of classification accuracy. IS, as a strategic filtering mechanism, addresses these challenges by retaining essential instances and discarding hindering elements. This refinement process optimizes classification algorithms, enabling them to excel in handling extensive datasets. In this research, IS offers a promising avenue to strengthen the effectiveness of classification in various real-world applications.","This paper presents the design and implementation of FLIPS, a middleware system to manage data and participant heterogeneity in federated learning (FL) training workloads. In particular, we examine the benefits of label distribution clustering on participant selection in federated learning. FLIPS clusters parties involved in an FL training job based on the label distribution of their data apriori, and during FL training, ensures that each cluster is equitably represented in the participants selected. FLIPS can support the most common FL algorithms, including FedAvg, FedProx, FedDyn, FedOpt and FedYogi. To manage platform heterogeneity and dynamic resource availability, FLIPS incorporates a straggler management mechanism to handle changing capacities in distributed, smart community applications. Privacy of label distributions, clustering and participant selection is ensured through a trusted execution environment (TEE). Our comprehensive empirical evaluation compares FLIPS with random participant selection, as well as three other \"smart\" selection mechanisms -- Oort [51], TiFL [15] and gradient clustering [27] using four real-world datasets, two different non-IID distributions and three common FL algorithms (FedYogi, FedProx and FedAvg). We demonstrate that FLIPS significantly improves convergence, achieving higher accuracy by 17-20 percentage points with 20-60% lower communication costs, and these benefits endure in the presence of straggler participants.","Highlights\n\u2022\nThis paper presents a novel solution to address high computational cost of current Riemannian framework based methods.\n\u2022\nWe propose a novel representation of Block Diagonal Symmetric Positive Definite Matrix Lie Algebra (BDSPDMLA) for 2D shapes.\n\u2022\nA new classification algorithm is introduced to achieve ten thousand times increase in speed over manifold methods.\n\u2022\nExperimental results on five datasets demonstrate BDSPDMLA\u2019s effectiveness in computation times and classification accuracy.\nAbstract\nRiemannian manifold plays a vital role as a powerful mathematical tool in computer vision, with important applications in curved shape analysis and classification. Significant progress has recently been made by Riemannian framework based methods that achieved state-of-the-art classification accuracy and robustness. However, these Riemannian manifold and Lie group methods require a very high computational complexity and do not include a description of the shape regions. This paper presents a novel mathematical tool, called Block Diagonal Symmetric Positive Definite Matrix Lie Algebra (BDSPDMLA) to represent curves, which extends the existing Lie group representations to a compact yet informative Lie algebra representation. The proposed Lie algebra based method addresses the computational bottleneck problem of the Riemannian framework based methods. In addition, it allows the natural fusion of various regions information with curved shape features for a more discriminative shape description. Here the region information is represented by values of distance maps, local binary patterns (LBP) and image intensity. Extensive experiments on five publicly available databases demonstrate that the proposed Lie algebra based method can achieve a speed of over ten thousand times faster than the Riemannian manifold and Lie group based baseline methods, while obtaining comparable accuracies for 2D shape classification.","A new learning algorithm is introduced that can deal with incomplete data. The algorithm uses a multi-granulation ensemble of classifiers approach. Firstly, the missing attributes tree (MAT) was constructed according to the missing values of samples. Secondly, the incomplete dataset was projected into a group of data subsets based on MAT, those data subsets were used as the training sets for the neural network. Based on bagging algorithm, each data subset was used to generate a group of classifiers and then using classifier ensemble to get the final prediction on each data subset. Finally, we adopt the conditional entropy as the weighting parameter to overcome the precision insufficiency of dimension based algorithm. Numerical experiments show that our learning algorithm can reduce the influence of missing attributes for classification results, and it is superior in performance to algorithm compared.","Orthopedic implant identification is an important and necessary step prior to performing revision surgery of different joints. The inability to identify an implant can lead to significant surgical difficulties with consequent unfavorable outcomes. This paper proposes a novel framework to identify the make and model of seven (7) different total shoulder arthroplasty implants utilizing plain X-ray images and Artificial intelligence. The proposed work classified implants with an accuracy of 91.48% and with an AUC (Area under curve) of 0.9932 showing higher effectiveness in orthopedic implant identification. Further work is required to enhance and progress this work, with a goal of greater accuracy and fewer errors.","Semi-supervised learning frameworks usually adopt mutual learning approaches with multiple submodels to learn from different perspectives. Usually, a high threshold is used to filter out numerous low-confidence predictions for unlabeled data to avoid transferring erroneous pseudo-labels between the submodels. However, such filtering cannot fully exploit unlabeled data with low prediction confidence. In this study, we propose a mutual learning framework based on pseudo-negative labels to overcome this problem. Negative labels are those to which a corresponding data item does not belong. In each iteration, one submodel generates pseudo-negative labels for each data item, and the other submodel learns from these labels. The roles of the two submodels interchange after each iteration until convergence. The dual model can improve its prediction ability by reducing the prediction probability of the pseudo-negative labels. We also propose a mechanism for selecting a few pseudo-negative labels to be fed into the submodels. In experiments, our framework achieved state-of-the-art results on several main benchmarks. Specifically, with our framework, the error rates of the 13-layer CNN model were 9.35% and 7.94% for CIFAR-10 with 1000 and 4000 labels, respectively. In addition, our framework has an error rate of 0.81% for the non-augmented MNIST with only 20 labels, which is much smaller than that of the other approaches. Our approach also demonstrated a significant performance improvement in domain adaptation.\nHighlights\n\u2022\nWe propose a Dual Negative Label Learning framework for semi-supervised learning.\n\u2022\nWe propose a selection mechanism to select representative pseudo-negative labels.\n\u2022\nWe demonstrate the effectiveness of the proposed method experimentally on benchmarks.","Structural segmentations of brain MRI can be generated by propagating manually labelled atlas images from a repository to a query subject and combining them. This method has been shown to be robust, consistent and increasingly accurate with increasing numbers of classifiers. It outperforms standard atlas-based segmentation but suffers, however, from problems of scale when the number of atlases is large. For a large repository and a particular query subject, using a selection strategy to identify good classifiers is one way to address problems of scale. This work presents and compares different classifier selection strategies which are applied to a group of 275 subjects with manually labelled brain MR images. We approximate an upper limit for the accuracy or overlap that can be achieved for a particular structure in a given subject and compare this with the accuracy obtained using classifier selection. The accuracy of different classifier selection strategies are also rated against the distribution of overlaps generated by random groups of classifiers.","Binary classification models are ubiquitous, and reliably measuring their performance is critical for their proper usage. Ideally, the performance of supervised models is measured using high-quality labeled datasets that are sufficiently large and representative of the population. However, obtaining labels for all segments of the population can be difficult, and model performance typically varies across different segments of the population (e.g., in different countries). In this work, we present a novel methodology to estimate the performance of a binary classifier in segments of the population where labels are unavailable. The main idea is that if two segments are \"similar,'' then the performance of the classifier in these two segments would also be \"similar.'' Specifically, we define a way to measure similarity between segments, and propose a statistical model that describes the performance of the model in unlabeled segments as a function of the performance in labeled segments. With extensive numerical experiments on synthetic and real-world datasets, we demonstrate that the proposed method substantially improves over existing methods in both estimation accuracy and computational efficiency. We also showcase the application of our method on the Instagram Adult Classifier to improve the geographic coverage and usability of the model.","Since Bitcoin first appeared on the scene in 2009, cryptocurrencies have become a worldwide phenomenon as important decentralized financial assets. Their decentralized nature, however, leads to notable volatility against traditional fiat currencies, making the task of accurately forecasting the crypto-fiat exchange rate complex. In this study, we examine the various independent factors that affect the Bitcoin-Dollar exchange rate's volatility. To this end, we propose CoMForE, a multimodal AdaBoost-LSTM ensemble model, which not only utilizes historical trading data but also incorporates public sentiments from related tweets, public interest demonstrated by search volumes, and blockchain hash-rate data. Our developed model goes a step further by predicting fluctuations in the overall cryptocurrency value distribution, thus increasing its value for investment decision-making. We have subjected this method to extensive testing via comprehensive experiments, thereby validating the importance of multimodal combination over exclusive reliance on trading data. Further experiments show that our method significantly surpasses existing forecasting tools and methodologies, demonstrating a 19.29% improvement. This result underscores the influence of external independent factors on cryptocurrency volatility.","In this paper, a novel hybrid Taguchi-Grey-based method for feature subset selection is proposed. The two-level orthogonal array is employed in the proposed method to provide a well-organized and balanced comparison of two levels of each feature (i.e., the feature is selected for pattern classification or not) and interactions among all features in a specific classification problem. That is, this two-dimensional matrix is mainly used to reduce the feature subset evaluation efforts prior to the classification procedure. Accordingly, the grey-based nearest neighbor rule and the signal-to-noise ratio (SNR) are used to evaluate and optimize the features of the specific classification problem. In this manner, important and relevant features can be identified for pattern classification. Experiments performed on different application domains are reported to demonstrate the performance of the proposed hybrid Taguchi-Grey-based method. It can be easily seen that the proposed method yields superior performance and is helpful for improving the classification accuracy in pattern classification.","Geological disasters result in significant human and property losses. It is imperative to identify areas prone to geological disasters for prevention and monitoring purposes. Identifying disaster-prone areas can be approached as a machine-learning classification problem. Various factors such as elevation, slope, aspect, terrain undulation, vegetation coverage, landform, and soil moisture content can be utilized as input variables for the classification algorithm to determine the probability of landslide occurrence within a specific range. Previous research in this area has been limited and fails to encompass diverse geological environments. This paper addresses this gap by utilizing a dataset collected within a province, which exhibits ample diversity and coverage. The dataset incorporates both continuous and discrete variables, allowing for a comprehensive evaluation of different classification algorithms. The primary objective of this study is to identify the most suitable classification algorithm for those conditions. By comparing the accuracy, ROC curves, and training duration of AdaBoost, cart, gbdt, xgboost, and random forest on multiple test datasets, it was determined that random forest yielded the best performance. The findings of this research can serve as a valuable reference for future related studies.","The introduction of emojis (or emoticons) in social media platforms has given the users an increased potential for expression. We propose a novel method called Classification of Emojis using Siamese Network Architecture (CESNA) to learn emoji-based representations of resource-poor languages by jointly training them with resource-rich languages using a siamese network.\nCESNA model consists of twin Bi-directional Long Short-Term Memory Recurrent Neural Networks (Bi-LSTM RNN) with shared parameters joined by a contrastive loss function based on a similarity metric. The model learns the representations of resource-poor and resource-rich language in a common emoji space by using a similarity metric based on the emojis present in sentences from both languages. The model, hence, projects sentences with similar emojis closer to each other and the sentences with different emojis farther from one another. Experiments on large-scale Twitter datasets of resource-rich languages - English and Spanish and resource-poor languages - Hindi and Telugu reveal that CESNA outperforms the state-of-the-art emoji prediction approaches based on distributional semantics, semantic rules, lexicon lists and deep neural network representations without shared parameters.","The Combined Algorithm Selection and Hyperparameter Optimization problem, in short, CASH, seeks the most suitable classifiers and hyperparameters for the underlying classification problems. In current literature, the common approaches in dealing with CASH problem are conducted via search-based methods such as sequential model-based optimization (SMBO) along with various active tests. Different from current existing approaches, in this paper, we propose a new method by incorporating the so-called denoising autoencoder (DAE) approach into meta-learning (MtL) for automatic configuration (both algorithms and their hyperparameters) recommendation, which appears to be quite effective compared to standard search-based approaches. More specifically, we set up the configuration search space for CASH and produce the metadata, and generate the classification performance on a set of collected historical datasets. Then both encoder and decoder in the DAE system are trained with the masked metadata as inputs and the unmasked metadata as targets to extract the subtle latent variables of metadata and recover the unmasked inputs subsequently. Under our framework, the performance over the entire configuration space can be predicted effectively through two different settings, and the configuration with the highest predictive performance is thus recommended. The first recommendation approach is by inactivating some inputs and then to recover their entries via the trained encoder and decoder for new problems, while in the second approach, the relationship between the acquired latent variables and the meta-features of historical datasets via kernel multivariate multiple regression (MMR) is enacted, leading to the performance estimation of new datasets being pursued directly through MMR and the decoder of DAE without requiring any new configuration evaluations. An automatic classification configuration recommendation system, including 81 historical problems and 11 common classifiers with a total of 4983 configurations, is established to show the effectiveness of our proposed approach. The comparative results on 45 testing problems demonstrate that our proposed model has the superior recommendation capacity in terms of the baselines for existing MtL as well as other search-based approaches.","Explaining predictions made by inductive classifiers has become crucial with the rise of complex models acting more and more as black-boxes. Abductive explanations are one of the most popular types of explanations that are provided for the purpose. They highlight feature-values that are sufficient for making predictions. In the literature, they are generated by exploring the whole feature space, which is unreasonable in practice. This paper solves the problem by introducing explanation functions that generate abductive explanations from a sample of instances. It shows that such functions should be defined with great care since they cannot satisfy two desirable properties at the same time, namely existence of explanations for every individual decision (success) and correctness of explanations (coherence). The paper provides a parameterized family of argumentation-based explanation functions, each of which satisfies one of the two properties. It studies their formal properties and their experimental behaviour on different datasets.","The automatic detection of mental disorders has been mainly performed through binary classifiers trained on the behavioral data collected through an interview setup. Such classifiers are usually trained by keeping the data from the participants having the disorder of interest in the positive class while the data from all other participants are kept in the negative class. In practice, it is well known that some mental disorders have common symptoms. Thus, the behavioral data may carry a mixed bag of attributes related to multiple disorders. As a result, the negative class may carry attributes related to the mental disorder of interest. This data impurity may lead to sub-optimal training of the classifier for a mental disorder of interest. In this study, we investigate this hypothesis in the context of major depressive disorder (MDD) and post-traumatic stress disorder detection (PTSD). The results show that upon removal of such data impurity, the MDD detection performances are significantly improved. However, such improvement is not observed consistently for PTSD detection, which may attribute to PTSD being a subtype of MDD.","Human-centric artificial intelligence struggles to build automated procedures that recognize emotions which can be integrated in artificial systems, such as user interfaces or social robots. In this context, this paper researches on building an Emotion Multi-modal Aggregator (EMmA) that will rely on a collection of open-source single source emotion classification methods aggregated to produce an emotion prediction. Although extendable, tested solution takes a video clip and divides into its frames and audio. Then a collection of primary classifiers are applied to each source and their results are combined in a final classifier utilizing machine learning aggregator techniques. The aggregator techniques that have been put to the test were Random Forest and k-Nearest Neighbors which, with an accuracy of 80%, have demonstrated superior performance over primary classifiers on the selected dataset.","Highlights\n\u2022\nA new HPFR model is contributed to jointly perform classification and prediction of time-varying functional data.\n\u2022\nTwo independent heavy-tailed processes are used to yield robust inferences against both magnitude and shape outliers.\n\u2022\nThe new HPFR inherits the advantages of Gaussian process functional regression (GPFR). At the same time, it makes up for the non-robustness of GPFR.\n\u2022\nA highly efficient MCMC sampling scheme is implemented for the inferences of the HPFR, thanks to a hierarchical expression of the model structure.\nAbstract\nWe propose a heavy-tailed process functional regression to jointly perform classification and prediction of time-varying functional data. We use two independent scale mixtures of Gaussian Processes to respectively model random effects and random errors, yielding robust inferences against both magnitude and shape outliers. We classify random curves by posterior predictive probabilities of class labels and offer a weighted prediction of future curve trends. A Bayesian estimation procedure is implemented through an MCMC sampling algorithm. The performance of classification and prediction of the proposed model is evaluated using simulated studies and some real data sets.","Highlights\n\u2022\nA novel framework termed FL-Tran is proposed to solve the multi-label image classification task.\n\u2022\nA multi-scale fusion mechanism is designed to align high-level features and low-level features to learn multi-scale features.\n\u2022\nA spatial attention mechanism based on transformer encoder is developed to capture the salient object features in images.\n\u2022\nA feature enhancement and suppression mechanism is proposed to excavate various potential useful features through stage-by-stage suppressing the most salient feature in the feature maps.\n\u2022\nExperiments on three publicly available datasets validate the superior performance of the proposed FL-Tran model compared with the state-of-the-art methods.\nAbstract\nThe purpose of multi-label image classification task is to accurately assign a set of labels to the objects in images. Although promising results have been achieved, most of the existing methods cannot effectively learn multi-scale features, so it is difficult to identify small-scale objects from images. Besides, current attention-based methods tend to learn the most salient feature regions in images, but fail to excavate various potential useful features concealed by the most salient feature, thus limiting the further improvement of model performance. To address above issues, we propose a novel Feature Learning network based on Transformer to learn salient features and excavate potential useful features (FL-Tran). Specifically, in order to solve the problem that current methods are difficult to identify small-scale objects, we first present a novel multi-scale fusion module (MSFM) to align high-level features and low-level features to learn multi-scale features. Additionally, a spatial attention module (SAM) utilizing transformer encoder is introduced to capture salient object features in images to enhance the model performance. Furthermore, we devise a feature enhancement and suppression module (FESM) with the aim of excavating potential useful features concealed by the most salient features. By suppressing the most salient features obtained in current SAM layer, and then forcing subsequent SAM layer to excavate potential salient features in feature maps, FL-Tran model can learn various useful features more comprehensively. Extensive experiments on MS-COCO 2014, PASCAL VOC 2007, and NUS-WIDE datasets demonstrate that our proposed FL-Tran model outperforms current state-of-the-art methods.","Multi-source online transfer learning uses the tagged data from multiple source domains to enhance the classification performance of the target domain. For unbalanced data sets, a multi-source online transfer learning algorithm that can oversample in the feature spaces of the source domain and the target domain is proposed. The algorithm consists of two parts: oversampling multiple source domains and oversampling online target domains. In the oversampling phase of the source domain, oversampling is performed in the feature space of the support vector machine (SVM) to generate minority samples. New samples are obtained by amplifying the original Gram matrix through neighborhood information in the source domain feature space. In the oversampling phase of the online target domain, minority samples from the current batch search for k-nearest neighbors in the feature space from multiple batches that have already arrived, and use the generated new samples and the original samples in the current batch to train the target domain function together. The samples from the source domain and the target domain are mapped to the same feature space through the kernel function for oversampling, and the corresponding decision function is trained using the data from the source domain and the target domain with relatively balanced class distribution, so as to improve the overall performance of the algorithm. Comprehensive experiments were conducted on four real datasets, and compared to other baseline algorithms on the Office Home dataset, the accuracy improved by 0.0311 and the G-mean value improved by 0.0702.","Remote sensing image scene classification aims to commit the semantic labels according to the content of images. Convolutional Neural Network (CNN) is often used here to extract deep discriminative feature of remote sensing images for classification. In practice, CNN is usually trained by images in the Red Green Blue (RGB) color space. Whereas, CNN also can be trained by images in some other color spaces, e.g., Hue Saturation Value. The CNN models trained by images in diverse color spaces will perform differently because different color spaces often emphasize diverse color information. Thus, we present an Evidential Combination method with Multi-color Spaces (ECMS) to integrate the complementary information of different color spaces for classification performance improvement. In ECMS, labeled remote sensing images in the RGB color space are first converted into other color spaces, and then they are used to train CNN models, respectively. The soft classification results (of query images) yielded by these CNN models are combined by evidence theory. During fusion, the reliabilities/weights of these outputs of different CNN models are usually different, so they should not be equally treated for combination. In our approach, the weights are learnt by minimizing the mean squared error between the combination results and ground truth on labeled images. By doing this, weighted evidence combination of soft classification results is employed to make scene class decision. We conducted experiments on several datasets to verify the effectiveness of ECMS, and the results show ECMS can significantly improve classification accuracy compared with many existing methods.\nHighlights\n\u2022\nEvidential combination method integrates CNNs trained in different color spaces.\n\u2022\nThe optimal weights are learnt by minimizing the constructed objective function.\n\u2022\nDS rule is employed to combine classification results yielded by different CNNs.\n\u2022\nResults on benchmarks verify the effectiveness of new method.","Forests play a crucial role in sustaining life on earth by providing vital ecosystem services and supporting a wide range of species. The unprecedented increase in forest fires aka \u2018infernos\u2019 due to global warming i.e. rising temperatures and changing weather patterns, is quite alarming. Recently, machine learning and computer vision-based techniques are leveraged to proactively analyze forest fire events. To this end, we propose novel semi-supervised classification and segmentation techniques using autoencoders to analyse forest fires, that require significantly less labelling effort in contrast to the fully-supervised methods. In particular, semi-supervised classification of forest fire using Convolutional autoencoders is proposed. Further, Class Activation Map-based techniques and patch-wise extraction methods are envisaged for the segmentation task. Extensive experiments are carried out on two publicly available large datasets i.e. FLAME and Corsican datasets. The proposed models are found to be outperforming the state-of-the-art approaches.","We use XLM (Cross-lingual Language Model), a transformer-based model, to perform sentiment analysis on Kannada-English code-mixed texts. The model was fine-tuned for sentiment analysis using the KanCMD dataset. We assessed the model\u2019s performance on English-only and Kannada-only scripts. Also, Malayalam and Tamil datasets were used to evaluate the model. Our work shows that transformer-based architectures for sequential classification tasks, at least for sentiment analysis, perform better than traditional machine learning solutions for code-mixed data.","As a powerful paradigm, deep learning (DL) models have been used in many applications for classification tasks in images, text, and audio. Through DL models, we can learn task-driven features from big data. However, DL models are fully deterministic and cannot handle uncertain and imprecise data. DL models are often sensitive to noise in data and do not operate well in areas where data are vague. Moreover, when there is a large feature set or high-dimensional data with irrelevant and redundant features, the DL models\u2019 performance decreases in classification due to training with irrelevant features. To gain reliable results in such high-dimensional problems, DL models require a large amount of data which usually grows exponentially concerning the number of features. Data uncertainty problems and irrelevant features in DL models cause low performance in classification tasks. This paper proposes an optimized fuzzy deep learning (OFDL) model for data classification based on Non-Dominated Sorting Genetic Algorithm II (NSGA-II). OFDL utilizes optimization in the composition of DL and fuzzy learning via the NSGA-II in multi-modal learning. To achieve effective classification, OFDL first considers intelligent feature selection by finding the best trade-offs between two conflicting objective functions, minimizing the number of features, and maximizing the accuracy (maximizing weights of selected features). Next, to reach optimized backpropagation and fuzzy membership functions, OFDL utilizes Pareto optimal solutions for multi-objective optimization using NSGA-II based on their objective functions. Furthermore, the fusion layer in OFDL fused optimized views of DL and fuzzy learning that provides a high-level representation of inputs and optimum features for classification tasks where the data contain high uncertainties and noises. This functionality gives valuable attributes during classification since identifying and selecting appropriate features ensures prompt and correct class. Also, it provides deep insight into tackling the effect of ambiguous data and each feature\u2019s uncertainty in the classification tasks. The examination of OFDL reveals good performance in terms of F-measure, accuracy, recall, precision, and True Positive Rate (TPR) compared to fuzzy classifiers. Furthermore, OFDL has higher accuracy in classification tasks than earlier fuzzy DNN models.","With the advent of the information age, there are more and more text data on the Internet. As the most widely distributed information carrier with the largest amount of data, it is particularly important to use text classification technology to organize and manage massive data scientifically. In this paper, a semi-supervised ensemble learning algorithm Heterogeneous-training is proposed and applied to the field of text classification. Based on the Tri-training algorithm, the Heterogeneous-training algorithm improves the traditional Tri-training algorithm by using different classifiers, dynamically updating the probability threshold and adaptively editing data. A large number of experiments show that our method always outperforms Tri-training algorithm in text classification on benchmark text data sets.","Performance of text classification models tends to drop over time due to changes in data, which limits the lifetime of a pretrained model. Therefore an ability to predict a model\u2019s ability to persist over time can help design models that can be effectively used over a longer period of time. In this paper, we provide a thorough discussion into the problem, establish an evaluation setup for the task. We look at this problem from a practical perspective by assessing the ability of a wide range of language models and classification algorithms to persist over time, as well as how dataset characteristics can help predict the temporal stability of different models. We perform longitudinal classification experiments on three datasets spanning between 6 and 19 years, and involving diverse tasks and types of data. By splitting the longitudinal datasets into years, we perform a comprehensive set of experiments by training and testing across data that are different numbers of years apart from each other, both in the past and in the future. This enables a gradual investigation into the impact of the temporal gap between training and test sets on the classification performance, as well as measuring the extent of the persistence over time. Through experimenting with a range of language models and algorithms, we observe a consistent trend of performance drop over time, which however differs significantly across datasets; indeed, datasets whose domain is more closed and language is more stable, such as with book reviews, exhibit a less pronounced performance drop than open-domain social media datasets where language varies significantly more. We find that one can estimate how a model will retain its performance over time based on (i) how well the model performs over a restricted time period and its extrapolation to a longer time period, and (ii) the linguistic characteristics of the dataset, such as the familiarity score between subsets from different years. Findings from these experiments have important implications for the design of text classification models with the aim of preserving performance over time.\nHighlights\n\u2022\nWe shed light into the temporal persistence of existing language models.\n\u2022\nWe analyse when and why model performance drops over time, which informs when a model needs adapting.\n\u2022\nWe investigate the impact of classification model choice in cross-temporal performance.\n\u2022\nWe analyse the impact of the dataset properties on performance drop over time.\n\u2022\nWe assess the potential and limitations of contextual language models to improve temporal persistence.","Machine Learning has revolutionized the categorization of vast legal documents, minimizing costs and improving evaluations. However, conventional models struggle with unseen data categories in real-world scenarios, a challenge termed Open Set Classification. Our study tackles the issue faced by the Court of Justice in S\u00e3o Paulo, Brazil, to identify recurring lawsuit themes from texts, as manual sorting is inefficient. We introduce a method to enhance confidence in text classification using an open dataset by converting multiclass challenges into binary ones with four confidence tiers. By testing various techniques, we found that combining doc2vec with the Support Vector Machine classifier delivers trustworthy results and robust performance. Ultimately, our method offers an effective solution for classifying legal texts confronting Open Set Classification issues in the legal sector.","The Hierarchical Inference (HI) paradigm has recently emerged as an effective method for balancing inference accuracy, data processing, transmission throughput, and offloading cost. This approach proves particularly efficient in scenarios involving resource-constrained edge devices like micro controller units (MCUs), tasked with executing tinyML inference. Notably, it outperforms strategies such as local inference execution, inference offloading, and split inference (i.e., inference execution distributed between two endpoints). Building upon the HI paradigm, this work explores different techniques aimed at further optimizing inference task execution. We propose three distinct HI approaches and evaluate their utility for image classification.","Compared with relatively easy feature creation or generation in data analysis, manual data labeling needs a lot of time and effort in most cases. Even if automated data labeling seems to make it better in some cases, the labeling results still need to be checked and verified by manual. The High Dimension and Low Sample Size (HDLSS) data are therefore very common in data mining and machine learning. For classification problems with the HDLSS data, due to data piling and approximate equidistance between any two input points in high-dimension space, some traditional classifiers often give poor predictive performance. In this paper, we propose a Maximum Decentral Projection Margin Classifier (MDPMC) in the framework of a Support Vector Classifier (SVC). In the MDPMC model, the constraints of maximizing the projection distance between decentralized input points and their supporting hyperplane are integrated into the SVC model in addition to maximizing the margin of two supporting hyperplanes. On ten real HDLSS datasets, the experiment results show that the proposed MDPMC approach can deal well with data piling and approximate equidistance problems. Compared with SVC with Linear Kernel (SVC-LK) and Radial Basis Function Kernel (SVC-RBFK), Distance Weighted Discrimination (DWD), weighted DWD (wDWD), Distance-Weighted Support Vector Machine (DWSVM), Population-Guided Large Margin Classifier (PGLMC), and Data Maximum Dispersion Classifier (DMDC), MDPMC obtains better predictive accuracy and lower classification errors than the other seven classifiers on the HDLSS data.\nHighlights\n\u2022\nA novel MDPMC approach is proposed for HDLSS problems.\n\u2022\nMaximum decentral projection is added to the constraints of MDPMC.\n\u2022\nData piling in the HDLSS setting can be solved by the MDPMC model.\n\u2022\nThe MDPMC model deals well with the approximate equidistance in HDLSS data.\n\u2022\nOur proposed MDPMC outperforms the other seven classifiers for predictive performance.","In this paper, a novel Improved Invasive Weed Optimization-based Hierarchical Attention Network (IIWO-HAN) is developed to achieve text classification. As for huge datasets, automatic labelling is required to get useful insights. Thus, the text classification becomes very important. IIWO has the quality of randomness and imitating compatibility of weeds colony and HAN makes use of levelled document structure. Hence, these two are integrated together to achieve text classification. Then, the proposed method has been analysed based on five popular parameters namely, Accuracy, Precision, TPR, TNR and FNR. For this purpose, three datasets: Reuters dataset, 20-Newsgroup dataset and Self-created dataset have been utilized where Reuters dataset, 20- Newsgroup dataset are standard datasets and the Self-created dataset consists of 5000 documents comprising of abstracts taken from various reputed journals. Further, the proposed methodology has been compared with an existing Improved Sine Cosine Algorithm (ISCA). It is found that IIWO-HAN results in 81.3%, 86.5%, 84.1%, 89.4% and 12.4% for TPR, TNR, Accuracy, Precision and FNR respectively and achieves better performance as compared to existing ISCA which gives 78.1%, 79.4%, 79.4%, 75.6%, 17.8% as TPR, TNR, Accuracy, Precision and FNR respectively for Reuters dataset. For 20- Newsgroup dataset, IIWO- HAN provides the TPR, TNR, Accuracy, Precision and FNR of 87.5%, 88.4%, 87.1%, 91.4% and 12.4% respectively and 84.1%, 86.5%, 89.4%, 92.4% and 15.8% as TPR, TNR, Accuracy, Precision and FNR respectively for Self-created dataset. It is found that the Precision of 92.47% and Accuracy of 89.4% is achieved using self-created dataset at training data value of 90% which is clearly better than Precision of 77.6% and Accuracy of 82.3% for existing ISCA.","Imbalanced data has been the focus of ongoing classification research. It describes a scenario where the distribution of data samples is uneven, and one or more classes in the dataset are underrepresented as a result. When trained on such datasets, this mismatch has a negative impact on the performance of conventional learning models. The key problem is in finding appropriate samples for creating synthetic data, even though numerous strategies have been developed to overcome class imbalance during data pre-processing. In this study, we offer an efficient method for overcoming imbalance classification issues caused by oversampling called Informative Sample Selection (ISS). The main goal of ISS is to find useful samples from the minority class in the dataset that may be used to produce data that is synthetic. We conducted experiments on 22 imbalanced datasets to evaluate the performance of our suggested model. We assessed the performance of ISS in comparison to several cutting-edge techniques, including SMOTE, Borderline-SMOTE, ADASYN, safe-level SMOTE, and ROS. AUC and F-Measure were the evaluation measures employed in our study. The outcomes of our tests show that ISS works better than the current approaches, showing significant progress in tackling the challenges brought on by imbalanced data in classification.","Imbalanced data are widely available in the real world, and it is difficult to effectively identify the minority class instances in imbalanced data. Various imbalanced classification models have been proposed. However, these models neglect the data density and the location of instances which can be important factors affecting classification performance. To tackle this issue, this paper proposes a hybrid imbalanced classification model based on data density (HICD). In data-level, the density-based resampling method is presented. The data partition Algorithm is given, which divides the data space into five regions based on the data density. The corresponding subsets are generated by sampling from the divided regions to improve the recognition of different classes of instances. In algorithm-level, we construct the corresponding ensemble models for different classes of instances. In addition, the model selection Algorithm is presented. On this basis, an appropriate model is selected for each instance based on its distribution. The performance of the proposed HICD was evaluated on 18 imbalanced datasets in the real world in terms of recall, the area under the roc curve (AUC), and G-mean. The experimental results validate that our method has better performance than other competitive algorithms in imbalanced classification.","Cardiovascular Diseases (CVDs), or heart diseases, are one of the top-ranking causes of death worldwide. About 1 in every 4 deaths are related to heart diseases, which are broadly classified as various types of abnormal heart conditions. However, diagnosis of CVDs is a time-consuming process in which data obtained from various clinical tests are manually analyzed. Therefore, new approaches for automating the detection of such irregularities in human heart conditions should be developed to provide medical practitioners with faster analysis via reducing the time of obtaining a diagnosis and enhancing results. Electronic Health Records(EHRs) are often utilized to discover useful data patterns that help improve the prediction of machine learning algorithms. Specifically, Machine Learning contributes significantly to solving issues like predictions in various domains, such as healthcare. Considering the abundance of available clinical data, there is a need to leverage such information for the betterment of humankind. In this work, a predictive model is proposed for heart disease prediction based on the stacking of various classifiers in two levels(Base level and Meta level). Various heterogeneous learners are combined to produce the strong model outcome. The model obtained 92% accuracy in prediction with a precision score of 92.6%, sensitivity of 92.6%, and specificity of 91%. The performance of the model was evaluated using various metrics, including accuracy, precision, recall, F1-scores, and area under the ROC curve values.","Pests and insects are the main hazards to crops, and they have a significant negative impact on both human health and wealth. Identification and classification of different types of crop insects accurately is an important task to avoid these losses. A manual examination is a quite sluggish and less efficient way of accomplishing this task while computer vision technology can significantly assist in this issue. A CNN-based deep learning model can classify insects and pests efficiently. In this paper, an ensemble-based model has been proposed using transfer learning. The experimental setup comprises pre-trained models like VGG16, VGG19, and ResNetv50 with a voting classifier ensemble technique. These pre-trained models are used to train the training dataset in a parallel pipeline model which is united with Ensemble Voting Classifier to generate the final prediction over the input sample. The benchmark IP102 dataset, having more than 75000 samples over 102 classes, is used to train and evaluate the model's performance. Results show that the proposed ensemble model achieves an accuracy of 82.5%, which is significant improvement over the recent state-of-the-art models proposed over the same selected dataset, hence strongly supporting the efficacy of the model.","Architectural floor plan is an essential document to share the building information among designers, and engineers. Automatic floor plan image analysis is useful to extract various information from the floor plan. Wall segmentation is an important step in floor plan image analysis. However, few research works have been conducted for automatic wall recognition in an architectural floor plan. In this paper, a convolution neural network, namely WallNet, is proposed to recognize the multi-class walls. The WallNet consists of an encoder and a decoder. The encoder captures low-level features, as well as multiscale contextual information. Based on these extracted feature maps, the walls are detected. The proposed network is applied to recognize five different classes of walls: solid-wall, dot-wall, diagonal-wall, hollow-wall and gray-wall. The experimental results show that the proposed architecture can obtain a mean average precision of 72%, which is superior compared to the state-of-the-art techniques.","The search for oil and gas reserves is an ongoing challenge. The possibility of oil and natural gas reserves running out in Indonesia in less than 25 years must also be considered by the industry. Unfortunately, the process of exploration and development of hydrocarbon fields still has its challenges, one of which is knowing the characteristics of hydrocarbon reservoirs. One of these characteristics that should be analyzed is rock facies. This could be done by using core data, well logs, analog studies, and others. However, this method is very expensive and time-consuming. Currently, the machine learning approach is the most popular solution for the classification of rock facies. However, a common issue with this method is the missing values due to recording errors in the field. Meanwhile, a machine learning system requires accurate data to form a good model. Therefore, we propose the use of a simple imputation method to solve the local mean imputation method by considering the uniqueness of each class before performing the imputation process. We also implemented several popular boosting methods in machine learning such as comparing Extreme Gradient Boosting and Light Gradient Boosting with several machine learning algorithms that are often used in solving facies classification problems such as Random Forest, Decision Tree, Support Vector Machine, and K-Nearest Neighbor. According to our results, the Local Mean imputation method improved the model's performance, where the perceived increase was between 1-10% and the model with the best performance was generated by the Random Forest method.","Learning from noisy labels is a challenge that arises in many real-world applications where training data can contain incorrect or corrupted labels. When fine-tuning language models with noisy labels, models can easily overfit the label noise, leading to decreased performance. Most existing methods for learning from noisy labels use static input features for denoising, but these methods are limited by the information they can provide on true label distributions and can result in biased or incorrect predictions. In this work, we propose the Dynamics-Enhanced Generative Model (DyGen), which uses dynamic patterns in the embedding space during the fine-tuning process of language models to improve noisy label predictions. DyGen uses the variational auto-encoding framework to infer the posterior distributions of true labels from noisy labels and training dynamics. Additionally, a co-regularization mechanism is used to minimize the impact of potentially noisy labels and priors. DyGen demonstrates an average accuracy improvement of 3.10% on two synthetic noise datasets and 1.48% on three real-world noise datasets compared to the previous state-of-the-art. Extensive experiments and analyses show the effectiveness of each component in DyGen. Our code is available for reproducibility on GitHub.","Stress detection is important for ensuring overall mental well-being of an individual. Literature suggests several approaches for prediction or classification of stress. However, the performance of these approaches varies a lot across subjects and tasks. Moreover, perception of stress is highly subjective and hence it is difficult to create a generic model/devices for prediction of stress. In this study, we have proposed an approach for creating a generic stress prediction model by combining the knowledge and variety from multiple public datasets containing galvanic skin response (GSR) data recorded during different context and activities. Most significant features are selected from these recorded signals and a voting based approach was finally adopted to develop a model for predicting mental stress. Proposed model has been validated using test data as well as a set of completely unseen data collected in our lab. We achieved an average classification accuracy of 89% (F-score 0.87) for test data and similar performance for completely unseen data as well. Results show that the proposed model outperforms the training models created using individual datasets. In addition, our model is created using skin response data recorded using off-the-shelf devices. Thus, our proposed model with selected feature set can be used for monitoring stress in real life scenarios and to create mass-market stress prediction products.","Automatic analysis of bioacoustic signals is a fundamental tool to evaluate the vitality of our planet. Frogs and bees, for instance, may act like biological sensors providing information about environmental changes. This task is fundamental for ecological monitoring and includes many challenges, such as nonuniform signal length processing, degraded target signal due to environmental noise, and the scarcity of labeled samples for training machine learning systems. To tackle these challenges, we present a bioacoustic signal classifier equipped with a discriminative mechanism to extract useful features for analysis and classification efficiently. The proposed classifier does not require a large amount of training data and handles nonuniform signal length natively. Unlike current bioacoustic recognition methods, which are task-oriented, the proposed model relies on transforming the input signals into vector subspaces generated by applying Singular Spectrum Analysis (SSA). Then, a subspace is designed to expose discriminative features. The proposed model shares end-to-end capabilities, which are desirable in modern machine learning systems. This formulation provides a segmentation-free and noise-tolerant approach to represent and classify bioacoustic signals and a highly compact signal descriptor inherited from SSA. The validity of the proposed method is verified using three challenging bioacoustic datasets containing anuran, bee, and mosquito species. Experimental results on three bioacoustic datasets have shown the competitive performance of the proposed method compared to commonly employed methods for bioacoustics signal classification in terms of accuracy. The code developed for this research can be found in the following repository: http://github.com/bernardo-gatto/DSSC.","In the healthcare industry, developing an efficient diagnostic system to classify liver cancer cells is a very perplexing and arduous task. Recently, several studies demonstrate that deep ensemble classifiers can achieve better predictive accuracy than individual deep learning classifiers. The deep ensemble learners exploit more than one individual deep learner to achieve better classification results and improved generalization performance. When implementing an ensemble learning (multiple classifier) approach, the selection of the optimum learners from a crew is a critical issue and an effective learner assortment strategy is used to achieve better results. Several researchers have applied different approaches (e.g., rule-based algorithms, evolutionary computing, simulated annealing, etc.) to determine the optimal learners that can increase the performance of the diagnostic system. This work proposes a new classifier selection strategy to construct an ensemble called a contribution-based iterative base learner removal algorithm (CIBRA). The proposed algorithm finds out the best subset of individual learners by considering both prediction accuracy and diversity. The proposed CIBRA enables each base learner in a pool to have multiple chances to partake in an iteration of selection. CIBRA drops the classifiers only if they have no residual opportunities. This procedure is reiterated till no learner in the crew has any remaining possibility to partake in the selection round. In this study, we test various decision synthesis techniques to increase the performance of the ensemble classifier. To assess the performance of CIBRA, 8 standard cancer databases are exploited. Extensive simulation results divulge that two base classifiers are enough to classify liver cancer cells from hematoxylin and eosin (H&amp;E) scans successfully. Based on the results obtained from this study, we construct an ensemble classifier using Dropout Extreme Learning Machine (DrpXLM) and Enhanced Convolutional Block Attention Modules (ECBAM) based residual network to classify liver cancer images. Besides, CIBRA generates better results when it operates with average probability as the decision synthesis technique.","The last decades have witnessed significant progress in machine learning with applications in different safety-critical domains, such as medical, law, education, and transportation. In high-stake domains, machine learning predictions have far-reaching consequences on the end-users. With the aim of applying machine learning for societal goods, there have been increasing efforts to regulate machine learning by imposing interpretability, fairness, robustness, privacy, etc. in predictions. Towards responsible and trustworthy machine learning, we propose two research themes in our dissertation research: interpretability and fairness of machine learning classifiers. In particular, we design algorithms to learn interpretable rule-based classifiers, formally verify fairness, and explain the sources of unfairness. Prior approaches to these problems are often limited by scalability, accuracy, or both. To overcome these limitations, we closely integrate automated reasoning and formal methods with fairness and interpretability to develop scalable and accurate solutions.","Breast cancer is one of the leading killers of women around the world. Finding compounds with good bioactivity, metabolic dynamics and safety, including Absorption, Distribution, Metabolism, Excretion and Toxicity (ADMET for short), is a long and challenging task in breast cancer therapy. In the paper, molecular descriptor data of compounds was analyzed by the ensemble learning algorithm, and important features were selected for the development and validation of ADMET classification models. The overall process includes data cleaning, data splitting to training and testing sets, feature selection and classification model evaluation. A Two-Level Stacking Algorithm (TLSA) based on ensemble learning is proposed for ADMET classification. Various performance measures like classification accuracy, precision, recall, confusion matrix, F1-score, Receiver Operating Characteristic (ROC) curves and the Area Under the ROC Curves (AUC) are reported to show the superiority of the proposed method as compared to different classifiers. The experimental results show that the second level algorithm for TLSA utilizes Logistic Regression is better than other classifiers for the properties of Absorption, Distribution and Excretion, with accuracy of 94.6037%, 94.9410% and 88.1956% respectively. For the properties Metabolism and Toxicity, the second level algorithm utilizes Support Vector Machine to achieve the best classification performance, with accuracy of 88.8702% and 96.7960% respectively. The results show that the proposed approach works well with the classification of compound properties and can be a good alternative for the well-known machine learning program.\nHighlights\n\u2022\nA novel TLSA model for ADMET classification of compounds is proposed.\n\u2022\nThe proposed method improves the accuracy of classification results.\n\u2022\nApplying a wide range of evaluation metrics to ensure the model\u2019s reliability.\n\u2022\nMost important molecular descriptors of compounds were selected using RF.","Writer identification based on handwriting recognition is considered one of the most common research areas in pattern recognition and biometrics. It has attracted much attention in recent decades due to the urgent need to develop biometric systems for many security applications. In this paper, Deep Writer Identification Network (DeepWINet), an effective deep Convolutional Neural Network (CNN) for writer identification, is proposed. The proposed model is evaluated in two different ways. In the first scenario, DeepWINet\u2019s CNN activation features, computed from the connected components of the writing, are passed to a customized nearest neighbor classifier for writer identification. In the second scenario, DeepWINet is evaluated as an end-to-end CNN network where the predicted results are averaged using an efficient strategy, Score Averaging Component-Decision Combiner. The proposed approach achieves competitive or the highest State-Of-The-Art performance on eight challenging handwritten databases with different languages.","For some named entities in the Chinese finance domain that are long, with difficult to delineate boundaries and diverse forms of expression, we propose a method based on pretrained language models for named entity recognition with enhanced features. First, the method considers entity boundary delineation and entity classification as two separate tasks and learns enhanced Chinese character features by introducing a gating-based multi-channel attention mechanism to delineate financial entity boundaries on the basis of a pretrained language model. Then, the boundary demarcation results are input into the pretrained language model in the form of mask units for data enhancement. Subsequently, document-level entity-based enhancement features are introduced to construct a finance entity classification model. We experimentally identified the best-performing Chinese pretrained language model from several state-of-the-art models and then embedded it into our method to compare against other benchmark models. The experimental results showed that our model is superior to other benchmark models on the named entity recognition task in the finance domain.","The high impedance faults (HIFs) detection in the micro-grid is very difficult for over-current protection owing to the less fault magnitude. As this is a practical problem in the distribution feeder, the detection of HIFs plays a vital role in industry 4.0. A dual-tree complex wavelet transform(DT-CWT) and data mining approaches have been used for fault detection and fault classification. For the extraction of wavelet features by DT-CWT, the residual voltage is used as input and fed to various data mining approaches to separate the HIF events and the created confusion matrix gives the best results. The correctness of the method implemented is investigated with other data mining approaches like support vector machine(SVM), k-nearest neighbor(KNN), and ensemble classifiers. As correlated to further methods, the proposed technique is 100% accurate. To execute the proposed method, the interconnection of DG with a wind-integrated micro-grid system is designed using Power System Computer-Aided Design (PSCAD) software.","Emotions are a vital and fundamental part of our existence. Whatever we do, say, or do not say somehow reflects our feelings, however not immediately. To comprehend human\u2019s most fundamental behaviour, we must examine these feelings using emotional data. According to the extensive literature review, categorising speech text into multiple classes is now undergoing extensive investigation. The application of this research is very limited in local and regional languages such as Hindi. This study focuses on text emotion analysis, specifically for the Hindi language. In our study, BHAAV Dataset is used, which consists of 20,304 sentences, where every other sentence has been manually annotated into one of the five emotion categories (Anger, Suspense, Joy, Sad, Neutral). Comparison of multiple machine learning and deep learning techniques with word embedding is used to demonstrate accuracy. And then, the trained model is used to predict the emotions of Hindi text. The best performance were observed in case of mBERT model with loss- 0.1689 ,balanced_accuracy- 93.88%, recall- 93.44%, auc- 99.55% and precision- 94.39 % on training data, while loss- 0.3073, balanced_accuracy- 91.84%, recall- 91.74%, auc- 98.46% and precision- 92.01% on testing data.","Most traditional methods for classifying marine ship targets using trajectory data rely on manual feature extraction, and it is difficult to consider the influence of spatial and temporal features on the classification results. In this paper, we propose a combination of a convolutional neural network and bidirectional gate recurrent unit (CNN-BiGRU) to classify ship targets using automatic identification system (AIS) data. First, AIS data are pre-processed to obtain valid ship trajectory segments, and basic information of the trajectory points, such as the speed, heading, and time, are used to construct the input feature vectors for the CNN and BiGRU, respectively. Second, the best CNN is trained and combined with BiGRU to obtain the CNN-BiGRU model. The combined model then simultaneously mines the spatio-temporal features contained in the AIS data and fuses the learned deep-level features. Finally, a fully connected layer is used to obtain the classification results of the ship targets. Compared with traditional machine learning algorithms and single deep learning models, the classification method in this study only requires the construction of simple input feature vectors and has different degrees of improvement in the four evaluation indexes of classification accuracy, precision, recall, and f-score, indicating that this method can effectively combine spatio-temporal features to improve the classification effect.","Deep neural networks have shown promising results on a wide variety of tasks using large-scale and well-annotated training datasets. However, data collected from real-world applications can suffer from two prevalent biases, i.e., long-tailed class distribution and label noise. Previous efforts on long-tailed learning and label-noise learning can only address a single type of data bias, leading to a severe deterioration of their performance. In this paper, we propose a distance-based sample selection algorithm called Stochastic Feature Averaging (SFA), which fits a Gaussian using the exponential running average of class centroids to capture uncertainty in representation space due to label noise and data scarcity. With SFA, we detect noisy samples based on their distances to class centroids sampled from this Gaussian distribution. Based on the identified clean samples, we then propose to train an auxiliary balanced classifier to improve the generalization for the minority class and facilitate the update of Gaussian parameters. Extensive experimental results show that SFA can enhance the performance of existing methods on both simulated and real-world datasets. Further, we propose to combine SFA with the sample-selection approach, distribution-robust, and noise-robust loss functions, resulting in significant improvement in performance over the baselines. Our code is available at https://github.com/HotanLee/SFA.","Sentiment classification aims to predict the sentiment label for a given text. Recently, several research efforts have been devoted to incorporate matching clues between text words and class labels into the learning process of text representation. However, these methods heavily rely on the availability of label content. Moreover, they simply capture the label-specific signals to measure each word\u2019s contribution by either implicitly employing a learnable label representation or explicitly leveraging the interaction between text words and labels via the interaction mechanism. To deal with these issues, in this paper, we propose a novel framework called Label-Guided Dual-view Sentiment Classifier (LGDSC). We first introduce a new strategy for generating an effective label description and then design a novel Dual-Channel Label-guided Attention Network (DLAN) to learn a text representation via two different channels. DLAN will be further leveraged to learn label-guided text representations from two different views. Extensive experimental results on four real-world datasets demonstrate that LGDSC consistently outperforms the state-of-the-art baseline methods.\nHighlights\n\u2022\nPropose an inverse label entropy based strategy for generating effective label descriptions.\n\u2022\nDesign a dual-channel label-guided attention network to learn text representation via two different channels.\n\u2022\nExtensive experiments conducted on four widely used datasets demonstrate the effectiveness of the proposed approach.","The deployment of photovoltaic (PV) cells as a renewable energy resource has been boosted recently, which enhanced the need to develop an automatic and swift fault detection system for PV cells. Prior to isolation for repair or replacement, it is critical to judge the level of the fault that occurred in the PV cell. The aim of this research study is the fault-level grading of PV cells employing deep neural network models. The experiment is carried out using a publically available dataset of 2,624 electroluminescence images of PV cells, which are labeled with four distinct defect probabilities defined as the defect levels. The deep architectures of the classical artificial neural networks are developed while employing hand-crafted texture features extracted from the EL image data. Moreover, optimized architectures of the convolutional neural network are developed with a specific emphasis on lightweight models for real-time processing. The experiments are performed for two-way binary classification and multiclass classification. For the first binary categorization, the proposed CNN model outperformed the state-of-the-art solution with a margin of 1.3% in accuracy with a significant 50% less computational complexity. In the second binary classification task, the CPU-based proposed model outperformed the GPU-based solution with a margin of 0.9% accuracy with an 8\u00d7 lighter architecture. Finally, the multiclass categorization of PV cells is performed and the state-of-the-art results with 83.5% accuracy are achieved. The proposed models offer a lightweight, efficient, and computationally cheaper CPU-based solution for the real-time fault-level categorization of PV cells.","Labeling images for visual segmentation is a time-consuming task which can be costly, particularly in application domains where labels have to be provided by specialized expert annotators, such as civil engineering. In this paper, we propose to use attribution methods to harness the valuable interactions between expert annotators and the data to be annotated in the case of defect segmentation for visual inspection of civil infrastructures. Concretely, a classifier is trained to detect defects and coupled with an attribution-based method and adversarial climbing to generate and refine segmentation masks corresponding to the classification outputs. These are used within an assisted labeling framework where the annotators can interact with them as proposal segmentation masks by deciding to accept, reject or modify them, and interactions are logged as weak labels to further refine the classifier. Applied on a real-world dataset resulting from the automated visual inspection of bridges, our proposed method is able to save more than 50% of annotators\u2019 time when compared to manual annotation of defects.","Highlights\n\u2022\nMultilabel Prototype Generation for efficient k-Nearest Neighbour classification.\n\u2022\nFour multiclass Prototype Generation methods are adapted to the multilabel space.\n\u2022\nEvaluation with twelve corpora, three noise scenarios, and different classifiers.\n\u2022\nProposals significantly improve performance and reduction rates of reference strategies.\n\u2022\nNovel adaptations proposed also show significant noise reduction capabilities.\nAbstract\nPrototype Generation (PG) methods are typically considered for improving the efficiency of the k-Nearest Neighbour (kNN) classifier when tackling high-size corpora. Such approaches aim at generating a reduced version of the corpus without decreasing the classification performance when compared to the initial set. Despite their large application in multiclass scenarios, very few works have addressed the proposal of PG methods for the multilabel space. In this regard, this work presents the novel adaptation of four multiclass PG strategies to the multilabel case. These proposals are evaluated with three multilabel kNN-based classifiers, 12 corpora comprising a varied range of domains and corpus sizes, and different noise scenarios artificially induced in the data. The results obtained show that the proposed adaptations are capable of significantly improving\u2014both in terms of efficiency and classification performance\u2014the only reference multilabel PG work in the literature as well as the case in which no PG method is applied, also presenting statistically superior robustness in noisy scenarios. Moreover, these novel PG strategies allow prioritising either the efficiency or efficacy criteria through its configuration depending on the target scenario, hence covering a wide area in the solution space not previously filled by other works.","In this paper, we examine several methods of acquiring Czech data for automated fact-checking, which is a task commonly modeled as a classification of textual claim veracity w.r.t. a corpus of trusted ground truths. We attempt to collect sets of data in form of a factual claim, evidence within the ground truth corpus, and its veracity label (supported, refuted or not enough info). As a first attempt, we generate a Czech version of the large-scale FEVER dataset built on top of Wikipedia corpus. We take a hybrid approach of machine translation and document alignment; the approach and the tools we provide can be easily applied to other languages. We discuss its weaknesses, propose a future strategy for their mitigation and publish the 127k resulting translations, as well as a version of such dataset reliably applicable for the Natural Language Inference task\u2014the CsFEVER-NLI. Furthermore, we collect a novel dataset of 3,097 claims, which is annotated using the corpus of 2.2 M articles of Czech News Agency. We present an extended dataset annotation methodology based on the FEVER approach, and, as the underlying corpus is proprietary, we also publish a standalone version of the dataset for the task of Natural Language Inference we call CTKFactsNLI. We analyze both acquired datasets for spurious cues\u2014annotation patterns leading to model overfitting. CTKFacts is further examined for inter-annotator agreement, thoroughly cleaned, and a typology of common annotator errors is extracted. Finally, we provide baseline models for all stages of the fact-checking pipeline and publish the NLI datasets, as well as our annotation platform and other experimental data.","Image classification is one of the most fundamental tasks in Computer Vision. In practical applications, the datasets are usually not as abundant as those in the laboratory and simulation, which is always called as Data Hungry. How to extract the information of data more completely and effectively is very important. Therefore, an Adaptive Data Augmentation Framework based on the tensor T-product Operator is proposed in this paper, to triple one image data to be trained and gain the result from all these three images together with only &lt;0.1% increase in the number of parameters. At the same time, this framework serves the functions of column image embedding and global feature intersection, enabling the model to obtain information in not only spatial but frequency domain, and thus improving the prediction accuracy of the model. Numerical experiments have been designed for several models, and the results demonstrate the effectiveness of this adaptive framework. Numerical experiments show that our data augmentation framework can improve the performance of original neural network model by 2%, which provides competitive results to state-of-the-art methods.","Early treatment of depression has been proven to be very effective. However, diagnosing depression remains a challenge with current diagnostic tools. Research has begun exploring digital and mobile modalities for depression screening. In our research, we focus on screening for moderate depression with the typed and transcribed responses in the StudentSADD (Student Suicidal Ideation and Depression Detection) dataset. Our modeling approach involves comparing the performance of five BERT (Bidirectional Encoder Representations from Transformers) variants with and without fine-tuning layers. Our results indicate that typed responses are more useful than transcribed responses when screening for depression. In particular, the less computationally expensive BERT variants with both fine-tuning layers performed best for the typed responses. The highest classifier balanced accuracy was 0.63. Our research can help inform the future development of essential digital mental illness screening models.","Highlights\n\u2022\nIn-field classification of the asymptomatic biotrophic phase of potato late blight.\n\u2022\nProximal hyperspectral images collected in multiple days with 20 potato genotypes.\n\u2022\nA new 3D convolutional deep learning architecture PLB-2D-3D-A is proposed.\n\u2022\nImportant wavelengths are determined for early PLB disease recognition in fields.\nAbstract\nEffective detection of potato late blight (PLB) is an essential aspect of potato cultivation. However, it is a challenge to detect late blight in asymptomatic biotrophic phase in fields with conventional imaging approaches because of the lack of visual symptoms in the canopy. Hyperspectral imaging can capture spectral signals from a wide range of wavelengths also outside the visual wavelengths. Here, we propose a deep learning classification architecture for hyperspectral images by combining 2D convolutional neural network (2D-CNN) and 3D-CNN with deep cooperative attention networks (PLB-2D-3D-A). First, 2D-CNN and 3D-CNN are used to extract rich spectral space features, and then the attention mechanism AttentionBlock and SE-ResNet are used to emphasize the salient features in the feature maps and increase the generalization ability of the model. The dataset is built with 15,360 images (64x64x204), cropped from 240 raw images captured in an experimental field with over 20 potato genotypes. The accuracy in the test dataset of 2000 images reached 0.739 in the full band and 0.790 in the specific bands (492 nm, 519 nm, 560 nm, 592 nm, 717 nm and 765 nm). This study shows an encouraging result for classification of the asymptomatic biotrophic phase of PLB disease with deep learning and proximal hyperspectral imaging.","This paper presents our solution for the 2nd COVID-19 Severity Detection Competition. This task aims to distinguish the Mild, Moderate, Severe, and Critical grades in COVID-19 chest CT images. In our approach, we devise a novel infection-aware 3D Contrastive Mixup Classification network for severity grading. Specifically, we train two segmentation networks to first extract the lung region and then the inner lesion region. The lesion segmentation mask serves as complementary information for the original CT slices. To relieve the issue of imbalanced data distribution, we further improve the advanced Contrastive Mixup Classification network by weighted cross-entropy loss. On the COVID-19 severity detection leaderboard, our approach won the first place with a Macro F1 Score of 51.76%. It significantly outperforms the baseline method by over 11.46%.","Over the past few decades, a lot of new neural network architectures and deep learning (DL)-based models have been developed to tackle problems more efficiently, rapidly, and accurately. For classification problems, it is typical to utilize fully connected layers as the network head. These dense layers used in such architectures have always remained the same \u2013 they use a linear transformation function that is a sum of the product of output vectors with weight vectors, and a trainable linear bias. In this study, we explore a different mechanism for the computation of a neuron\u2019s output. By adding a new feature, involving a product of higher order output vectors with their respective weight vectors, we transform the conventional linear function to higher order functions, involving powers over two. We compare and analyze the results obtained from six different transformation functions in terms of training and validation accuracies, on a custom neural network architecture, and with two benchmark datasets for image classification (CIFAR-10 and CIFAR-100). While the dense layers perform better in all epochs with the new functions, the best performance is observed with a quadratic transformation function. Although the final accuracy achieved by the existing and new models remain the same, initial convergence to higher accuracies is always much faster in the proposed approach, thus significantly reducing the computational time and the computational resources required. This model can improve the performance of every DL architecture that uses a dense layer, with remarkably higher improvement in larger architectures that incorporate a very high number of parameters and output classes.","The robustness of graph classification models plays an essential role in providing highly reliable applications. Previous studies along this line primarily focus on seeking the stability of the model in terms of overall data metrics (e.g., accuracy) when facing data perturbations, such as removing edges. Empirically, we find that these graph classification models also suffer from semantic bias and confidence collapse issues, which substantially hinder their applicability in real-world scenarios. To address these issues, we present MGRL, a multi-view representation learning model for graph classification tasks that achieves robust results. Firstly, we proposes an instance-view consistency representation learning method, which utilizes multigranularity contrastive learning technique to perform semantic constraints on instance representations at both the node and graph levels, thus alleviating the semantic bias issue. Secondly, we proposes a class-view discriminative representation learning method, which employs the prototype-driven class distance optimization technique to adjust intra- and interclass distances, thereby mitigating the confidence collapse issue. Finally, extensive experiments and visualizations on eight benchmark dataset demonstrate the effectiveness of MGRL.","Highlights\n\u2022\nProposed Weighted Visibility Graph based Elastic Net Sparse Classification.\n\u2022\nProposed Sequential Machine Learning Analysis with Stacked Denoising Autoencoder.\n\u2022\nProposed Riemannian Alliance based Tangent Space Mapping Transfer Learning.\n\u2022\nProposed a BAG deep learning model.\n\u2022\nProposed works are implemented for three music datasets.\nAbstract\nWith the rapid development of high-tech multimedia technologies, many musical resource assets are available online and it has always triggered an interest in the classification of different music genres. Detecting a set of music belonging to a similar genre is the main intention of the music recommendation playlist. With the help of machine learning, transfer learning and deep learning concepts, a robust music classifier is necessary so that the unlabelled music can be easily tagged and thereby the users experience of using media players with music files can be improved. The existing approaches in the past decade has various shortcomings due to the manual extraction of features followed by traditional machine learning classification techniques affecting the classification accuracy to a great extent along with its drawback to not perform well on multiclass classification problems and its inability to deal with huge data size. In this work, five interesting and novel approaches are proposed for music genre classification such as the proposed Weighted Visibility Graph based Elastic Net Sparse Classifier (WVG-ELNSC), the proposed classification using sequential machine learning analysis with Stacked Denoising Autoencoder (SDA) classifier, the proposed Riemannian Alliance based Tangent Space Mapping (RA-TSM) transfer learning techniques, classification using Transfer Support Vector Machine (TSVM) algorithm, and finally the proposed deep learning classifier with Bidirectional Long Short-Term Memory (BiLSTM) cum Attention model with Graphical Convolution Network (GCN) termed as BAG deep learning model is used here. The experiments are done for three music datasets such as GTZAN, ISMIR 2004 and MagnaTagATune datasets and a relatively higher classification accuracy of 93.51% is obtained when the proposed deep learning BAG model is utilized.","Highlights\n\u2022\nFormulating a new open-set task requires spotting and cognizing novel characters.\n\u2022\nProposing a new framework that handles novel characters without retraining.\n\u2022\nProposing a fast rectification technique for text recognition.\nAbstract\nScene text recognition is a popular research topic which is also extensively utilized in the industry. Although many methods have achieved satisfactory performance for the close-set text recognition challenges, these methods lose feasibility in open-set scenarios, where collecting data or retraining models for novel characters could yield a high cost. For example, annotating samples for foreign languages can be expensive, whereas retraining the model each time when a \u201cnovel\u201d character is discovered from historical documents costs both time and resources. In this paper, we introduce and formulate a new open-set text recognition task which demands the capability to spot and recognize novel characters without retraining. A label-to-prototype learning framework is also proposed as a baseline for the new task. Specifically, the framework introduces a generalizable label-to-prototype mapping function to build prototypes (class centers) for both seen and unseen classes. An open-set predictor is then utilized to recognize or reject samples according to the prototypes. The implementation of rejection capability over out-of-set characters allows automatic spotting of unknown characters in the incoming data stream. Extensive experiments show that our method achieves promising performance on a variety of zero-shot, close-set, and open-set text recognition datasets.","The sentiment analysis of user reviews on social networking is one among the fundamental process carried out by online business organizations to improve the quality of their products and retain the customers and thereby lift the monetary benefit. Although numerous analysis models exist, still there is space for improving the performance and accuracy of informal text data-based classification models. In this examination, we conduct a comparative assessment of the effectiveness of unigram feature set extricated utilizing n-gram technique with three ensemble methods namely Extremely Randomized Tree, Voting, and Bagging classifier based on the following five baseline classifiers Random Forest, Na\u00efve Bayes, K-NearestNeighbor, Ridge Classifier, and Support Vector Machine to identify polarity from mobile product reviews. Among the three ensemble methods, the Extremely Randomized Tree technique has better outcomes with the accuracy of 98% for positive and 85% for negative cases, with an overall accuracy of 96.8%. The error rate of all the three ensemble classifiers is also under 0.5% which uncovers that ensemble classifiers performs better compared to individual classifiers.","Pose-based approaches for sign language recognition provide light-weight and fast models that can be adopted in real-time applications. This paper presented a framework for isolated Arabic sign language (ArSL) recognition using hand and face keypoints. We employed MediaPipe pose estimator for extracting the keypoints of sign gestures in the video stream. Using the extracted keypoints, three models were proposed for sign language recognition, Long-Term Short Memory (LSTM), Temporal Convolution Networks (TCN) and Transformer based models. Moreover, we investigated the importance of non-manual features for sign language recognition systems and the obtained results showed that combining hand and face keypoints boosted the recognition accuracy by around \\(4\\% \\) compared with only hand keypoints. The proposed models were evaluated on Arabic and Argentinian sign languages. Using the KArSL-100 dataset, the proposed pose-based Transformer achieved the highest accuracy of \\(99.74\\% \\) and \\(68.2\\% \\) in signer-dependent and independent modes, respectively. Additionally, the Transformer was evaluated on the LSA64 dataset and obtained an accuracy of \\(98.25\\% \\) and \\(91.09\\% \\) in signer-dependent and independent modes, respectively. Consequently, the pose-based Transformer outperformed the state-of-the-art techniques on both datasets using keypoints from the signer\u2019s hands and face.","The primary research objective of this study is to develop an algorithm pipeline for recognizing human locomotion activities using multimodal sensor data from smartphones, while minimizing prediction errors due to data differences between individuals. The multimodal sensor data provided for the 2023 SHL recognition challenge comprises three types of motion data and two types of radio sensor data. Our team, \u2018HELP,\u2019 presents an approach that aligns all the multimodal data to derive a form of vector composed of 106 features, and then blends predictions from multiple learning models which are trained using different number of feature vectors. The proposed neural network models, trained solely on data from a specific individual, yield F1 scores of up to 0.8 in recognizing the locomotion activities of other users. Through post-processing operations, including the ensemble of multiple learning models, it is expected to achieve a performance improvement of 10% or greater in terms of F1 score.","In pattern recognition and data mining a data set is named skewed or imbalanced if it contains a large number of objects of certain type and a very small number of objects of the opposite type. The imbalance in data sets represents a challenging problem for most classification methods, this is because the generalization power achieved for classic classifiers is not good for skewed data sets. Many real data sets are imbalanced, so the development of new methods to face this problem is necessary. The SVM classifier has an exceptional performance for data sets that are not skewed, however for imbalanced sets the optimal separating hyper plane is not enough to achieve acceptable results. In this paper a novel method that improves the performance of SVM for skewed data sets is presented. The proposed method works by exciting the support vectors and displacing the separating hyper plane towards majority class. According to the results obtained in experiments with different skewed data sets, the method enhances not only the accuracy but also the sensitivity of SVM classifier on this kind of data sets.","Parkinson\u2019s disease (PD) is the second most common neurodegenerative disorder, as reported by the World Health Organization (WHO). In this paper, we propose a direct three-Class PD classification using two different modalities, namely, MRI and DTI. The three classes used for classification are PD, Scans Without Evidence of Dopamine Deficit (SWEDD) and Healthy Control (HC). We use white matter (WM) and gray matter (GM) from the MRI and fractional anisotropy (FA) and mean diffusivity (MD) from the DTI to achieve our goal. We train four separate CNNs on the above four types of data. At the decision level, the outputs of the four CNN models are fused with an optimal weighted average fusion technique. We achieve an accuracy of 95.53% for the direct three-class classification of PD, HC and SWEDD on the publicly available PPMI database. Extensive comparisons including a series of ablation studies clearly demonstrate the effectiveness of our proposed solution.","Researchers have been aware that emotion is not one-hot encoded in emotion-relevant classification tasks, and multiple emotions can coexist in a given sentence. Recently, several works have focused on leveraging a distribution label or a grayscale label of emotions in the classification model, which can enhance the one-hot label with additional information, such as the intensity of other emotions and the correlation between emotions. Such an approach has been proven effective in alleviating the overfitting problem and improving the model robustness by introducing a distribution learning component in the objective function. However, the effect of distribution learning cannot be fully unfolded as it can reduce the model\u2019s discriminative ability within similar emotion categories. For example, \u201cSad\u201d and \u201cFear\u201d are both negative emotions. To address such a problem, we proposed a novel emotion extension scheme in the prior work (Li, Chen, Xie, Li, and Tao, 2021). The prior work incorporated fine-grained emotion concepts to build an extended label space, where a mapping function between coarse-grained emotion categories and fine-grained emotion concepts was identified. For example, sentences labeled \u201cJoy\u201d can convey various emotions such as enjoy, free, and leisure. The model can further benefit from the extended space by extracting dependency within fine-grained emotions when yielding predictions in the original label space. The prior work has shown that it is more apt to apply distribution learning in the extended label space than in the original space. A novel sparse connection method, i.e., Leaky Dropout, is proposed in this paper to refine the dependency-extraction step, which further improves the classification performance. In addition to the multiclass emotion classification task, we extensively experimented on sentiment analysis and multilabel emotion prediction tasks to investigate the effectiveness and generality of the label extension schema.\nHighlights\n\u2022\nIdentify the association between emotion categories and fine-grained emotion concepts.\n\u2022\nSuggest a novel emotion label extension schema.\n\u2022\nDevice a classification framework incorporating distribution learning.\n\u2022\nDevice a novel sparse connection mechanism, Leaky Dropout.","Real-world applications of artificial intelligence that can potentially harm human beings should be able to express uncertainty about the made predictions. Probabilistic deep learning (DL) methods (e.g., variational inference [VI], VI last layer [VI-LL], Monte-Carlo [MC] dropout, stochastic weight averaging - Gaussian [SWA-G], and deep ensembles) can produce a predictive uncertainty but require expensive MC sampling techniques. Therefore, we evaluated if the probabilistic DL methods are uncertain when making incorrect predictions for an open-source driver intention recognition dataset and if a surrogate DL model can reproduce the uncertainty estimates. We found that all probabilistic DL methods are significantly more uncertain when making incorrect predictions at test time, but there are still instances where the models are very certain but completely incorrect. The surrogate DL models trained on the MC dropout and VI uncertainty estimates were capable of reproducing a significantly higher uncertainty estimate when making incorrect predictions.","Social and political polarization has become a dramatically intensifying force that is having a huge impact on political discourse, public policies and electoral outcomes in the 21st century. Twitter is a social media platform that mirrors to a large extent the sociological notion of public opinion, and has notably fueled these polarization dynamics worldwide. A proper understanding of how different issues become polarized in Twitter and their interrelationship is therefore crucial for the development of effective policies and governance strategies in our democracies. This paper introduces TwiSP, a framework for analyzing polarization on controversial topics in Twitter. TwiSP utilizes a combination of two cutting-edge machine learning techniques: stance detection for identifying attitudes and perspectives and BERTopic for topic modeling. The outcome of TwiSP is a visual tree-like representation of all tweets related to conflicting topics (rooted in a particular topic), contrasting their relationship using different colors to denote the degree of polarization. As a case study, we show how the TwiSP framework can be used for analyzing polarized issues in the context of the COVID-19 vaccine, exploring the resulting degree of polarization and the key topics driving it. The results reveal the diversity of opinions and the presence of highly polarized clusters in social media discussions. We contend that the TwiSP framework provides a novel and valuable tool for decision makers, helping them to recognize contentious issues behind the dynamics of polarization and ultimately identifying potential opportunities for bridging divides.","Imbalanced data classification is a challenging problem frequently encountered in many real-world applications. Traditional classification algorithms are generally designed to maximize overall accuracy; therefore, their effectiveness tends to be impeded by imbalanced data. Similar to other traditional classifiers, naive Bayes (NB) sometimes fails at predicting minority instances owing to its sensitivity to class distribution. To cope with this challenge, we proposed RankOptAUC NB (RNB), a novel attribute weighting method for the NB. In the proposed method, learning a weighted NB classifier was formulated as a nonlinear optimization problem with the objective of maximizing the area under the ROC (AUC). The optimization formulation enabled the RNB method to select important variables by simply adding a regularization term to the objective function. We also provided theoretical evidence that, based on the AUC metric, the proposed method improved the performance of a weighted NB classifier. The results of numerical experiments conducted using 30 real-world datasets proved that the proposed scheme successfully determined the optimal attribute weights for imbalanced data classification.\nHighlights\n\u2022\nA novel weighted naive Bayes (NB) for imbalanced data classification was proposed.\n\u2022\nLearning a weighted NB classifier was formulated as a nonlinear optimization problem.\n\u2022\nArea under ROC curve (AUC) was incorporated into the objective function.\n\u2022\nThe proposed method can select important attributes.","Cost-sensitive learning (CSL), which has gained widespread attention in class imbalance learning (CIL), can be implemented either by tuning penalty parameters or by designing new loss functions. In this paper, we propose a cost-sensitive learning method with a modified Stein loss function (CSMS) and a robust CSMS (RCSMS). Specifically, CSMS is flexible, as it realizes CSL from above two aspects simultaneously. In contrast, RCSMS merely achieves CSL by tuning penalty parameters, but the adopted loss function makes it insensitive to noise. To our best knowledge, it is the first time for Stein loss function derived from statistics to be applied in machine learning, which not only offers two alternative class imbalance solutions but also provides a novel idea for the design of loss functions in CIL. The mini-batch stochastic sub-gradient descent (MBGD) approach is employed to optimize CSMS and RCSMS. Meanwhile, the Rademacher complexity is used to analyze their generalization error bounds. Extensive experiments profoundly confirm the superiority of both models over benchmarks.","In recent years, object detection has been widely used in various fields such as face detection, remote sensing image detection and pedestrian detection. Due to the complex environment in the actual scene, we need to fully obtain the feature information in the image to improve the accuracy of object detection. This paper proposes an object detection algorithm based on coordinate attention and contextual feature enhancement. We design a multi-scale attention feature pyramid network, which first uses multi-branch atrous convolution to capture multi-scale context information, and then fuses the coordinate attention mechanism to embed location information into channel attention, and finally uses a bidirectional feature pyramid structure to effectively fuse high-level features and low-level features. We also adopt the GIoU loss function to further improve the accuracy of object detection. The experimental results show that the proposed method has certain advantages compared with other detection algorithms in the PASCAL VOC datasets.","Federated learning (FL) is an emerging machine learning paradigm that allows multiple parties to train a shared model collaboratively in a privacy-preserving manner. Existing horizontal FL methods generally assume that the FL server and clients hold the same model structure. However, due to system heterogeneity and the need for personalization, enabling clients to hold models with diverse structures has become an important direction. Existing model-heterogeneous FL approaches often require publicly available datasets and incur high communication and/or computational costs, which limit their performances. To address these limitations, we propose a simple but effective Federated Global prediction Header (FedGH) approach. It is a communication and computation-efficient model-heterogeneous FL framework which trains a shared generalized global prediction header with representations extracted by heterogeneous extractors for clients' models at the FL server. The trained generalized global prediction header learns from different clients. The acquired global knowledge is then transferred to clients to substitute each client's local prediction header. We derive the non-convex convergence rate of FedGH. Extensive experiments on two real-world datasets demonstrate that FedGH achieves significantly more advantageous performance in both model-homogeneous and -heterogeneous FL scenarios compared to seven state-of-the-art personalized FL models, beating the best-performing baseline by up to 8.87% (for model-homogeneous FL) and 1.83% (for model-heterogeneous FL) in terms of average test accuracy, while saving up to 85.53% of communication overhead.","The predictive performance of machine learning models tends to deteriorate in the presence of class imbalance. Multiple strategies have been proposed to address this issue. A popular strategy consists of oversampling the minority class. Classic approaches such as SMOTE utilize techniques like nearest neighbor search and linear interpolation, which can pose difficulties when dealing with datasets that have a large number of dimensions and intricate data distributions. As a way to create synthetic examples in the minority class, Generative Adversarial Networks (GANs) have been suggested as an alternative technique due to their ability to simulate complex data distributions. However, most GAN-based oversampling methods tend to ignore data uncertainty. In this paper, we propose a novel GAN-based oversampling method using evidence theory. An auxiliary evidential classifier is incorporated in the GAN architecture in order to guide the training process of the generative model. The objective is to push GAN to generate minority objects at the borderline of the minority class, near difficult-to-classify objects. Through extensive analysis, we demonstrate that the proposed approach provides better performance, compared to other popular methods.","The enhanced digitalization in the manufacturing sector is claimed to facilitate the generation or the use of the existing process data incorporating the production variations and offers a significant increase in the productivity and efficiency of a system. Moreover, manufacturing companies possess substantial knowledge while designing a product and manufacturing procedures. The primary requirement is to link and organize all the information sources related to the operation design and production. This research is concerned with the reuse of machining knowledge for existing and new parts having similarities in geometric features and operational conditions. The proposed methodology starts by extracting each machining operation's geometric information and cutting parameters using industrial part programs in the numerical control (NC) simulator VERICUT. The removed material between two consecutive operations is obtained through mesh comparison in the simulator to analyze the feature interactions. A deep learning approach based on 3D convolutional neural networks (CNN) is applied to classify similar geometries to reuse the process design knowledge by creating a library of operations. The proposed approach is implemented on actual machining data, and the results demonstrate the effectiveness of the proposed solution. The obtained knowledge clusters in the operations library assist in making propositions related to operational parameters for similar geometric features during the process planning phase reducing the planning and designing time of operations.","Individuals and businesses are frequently seen engaging in a fraud scheme, which results in the loss of funds, rights, and assets. This paper aims to provide an empirical analysis and study of a supervised learning technique, decision trees (DT), on a credit card transaction dataset as a benchmark. The proposed approach can be employed to reduce FPs and FNs through supervised machine learning (ML) technique. The fraud detection system also uses historical data to construct a training set and then analyses it to identify fraudulent activity. In this study, suspicious activity is detected using the decision tree classifier. The obtained results showed that FPs and FNs in an online transaction or activity can be reduced to a large extent using DT classification. This method achieved a promising result in terms of performance measures such as precision (99.7%), accuracy (92.25%), recall (81.49%), and F1_score (86.47%) respectively. As a result, the use of ML classifiers can reduce FPs and FNs, increasing customer satisfaction in an online transaction.","Recently, there are lots of literature on improving the robustness of SVM by constructing nonconvex functions, but they seldom theoretically study the robust property of the constructed functions. In this paper, based on our recent work, we present a novel capped asymmetric elastic net (CaEN) loss and equip it with the SVM as CaENSVM. We derive the influence function of the estimators of the CaENSVM to theoretically explain the robustness of the proposed method. Our results can be easily extended to other similar nonconvex loss functions. We further show that the influence function of the CaENSVM is bounded, so that the robustness of the CaENSVM can be theoretically explained. Other theoretical analysis demonstrates that the CaENSVM satisfies the Bayes rule and the corresponding generalization error bound based on Rademacher complexity guarantees its good generalization capability. Since CaEN loss is concave, we implement an efficient DC procedure based on the stochastic gradient descent algorithm (Pegasos) to solve the optimization problem. A host of experiments are conducted to verify the effectiveness of our proposed CaENSVM model.","In supervised classification, the object selection or instance selection is an important task, mainly for instance-based classifiers since through this process the time in training and classification stages could be reduced. In this work, we propose a new mixed data object selection method based on clustering and border objects. We carried out an experimental comparison between our method and other object selection methods using some mixed data classifiers.","Making each modality in multi-modal data contribute is of vital importance to learning a versatile multi-modal model. Existing methods, however, are often dominated by one or few of modalities during model training, resulting in sub-optimal performance. In this article, we refer to this problem as modality bias and attempt to study it in the context of multi-modal classification systematically and comprehensively. After stepping into several empirical analyses, we recognize that one modality affects the model prediction more just because this modality has a spurious correlation with instance labels. To primarily facilitate the evaluation on the modality bias problem, we construct two datasets, respectively, for the colored digit recognition and video action recognition tasks in line with the Out-of-Distribution (OoD) protocol. Collaborating with the benchmarks in the visual question answering task, we empirically justify the performance degradation of the existing methods on these OoD datasets, which serves as evidence to justify the modality bias learning. In addition, to overcome this problem, we propose a plug-and-play loss function method, whereby the feature space for each label is adaptively learned according to the training set statistics. Thereafter, we apply this method on 10 baselines in total to test its effectiveness. From the results on four datasets regarding the above three tasks, our method yields remarkable performance improvements compared with the baselines, demonstrating its superiority on reducing the modality bias problem.","Feature selection for multi-label classification tasks has attracted attention from the machine learning domain. The current algorithms transform a multi-label learning task to several binary single-label tasks, and then compute the average score of the features across all single-label tasks. Few research discusses the effect in averaging the scores. To this end, we discuss multi-label feature selection in the framework of fuzzy rough sets. We define a novel dependency functions with three fusion methods if the fuzzy lower approximation of each label has been calculated. A forward greedy algorithm is constructed to reduce the redundancy of the selected features. Numerical experiments validate the performance of the proposed method.","Information-theoretic measures have been commonly applied to evaluate the relevance and redundancy in multi-label feature selection. However, the current multi-label feature selection methods based on information-theoretic measures neglect the dynamic changes in the relevance of selected features and candidate features. Furthermore, they also do not fully consider the influence of label redundancy on the relevance of candidate features. In this paper, we first propose a new feature relevance term named Dynamic Correlation Change (DCC), which uses two conditional mutual information terms to evaluate the dynamic changes in the relevance of selected features and candidate features. We then introduce a new label redundancy term named Label Redundancy with Interaction Information (LRII), which more accurately quantifies the influence of label redundancy on the relevance of candidate features. On this basis, we design a new multi-label feature selection method, called Maximum Dynamic Correlation Change and Minimum Label Redundancy (MDCCMLR), by combining DCC and LRII. Finally, we conduct extensive experiments in order to verify the performance of our method by comparing it with some state-of-the-art multi-label feature selection methods based on information-theoretic measures in terms of six evaluation metrics. The experimental results show that the MDCCMLR method outperforms the other comparison methods on all six evaluation metrics.","NAVTEX is a crucial marine safety information broadcasting system for ensuring the safe navigation of ships, which plays a significant role in ship safety. However, the current manual reading and subject classification of NAVTEX suffer from low efficiency and accuracy. To enhance the processing efficiency of maritime safety information (MSI) and promote information and communication, achieving automated MSI classification with high confidence in the accuracy of the results becomes imperative. In the context of machine learning, this study proposes an adaptive weight TFIDF method to address the aforementioned challenges. The primary objective is to optimize the weights of keywords with prominent classification features in NAVTEX. Experimental results demonstrate that the adaptive weight-based TFIDF algorithm significantly improves the classification outcomes for NAVTEX. By enhancing the accuracy and efficiency of MSI classification, this approach facilitates the automation of NAVTEX analysis and promotes the reliability of the generated classification results.","In natural scene classification, it is common that one natural scene image may belong to multiple categories concurrently; as a result, multi-label learning has become a research hotspot. Despite recent rapid developments in multi-label learning, the increasing amount of high-dimensional data poses great challenges\u2014such as redundant features and high computational costs\u2014to conventional multi-label learning models. Most contemporary strategies for dealing with this issue depend on forcing feature learning into multi-label models. Notably, however, these approaches seldom pay attention to the label correlation and propagation in the feature subspace. To address this issue, we introduce an alternative multi-label feature learning solution that incorporates both labeled and unlabeled information. Unlike existing multi-label learning models , which rely on clean and trustworthy training datasets, we argue that in semi-supervised learning scenarios, the unlabeled data can be easily corrupted by noise or outliers, which causes the model performance to degrade. We next extract the label correlation and propagate the label information to discover the noise or outliers. Subsequently, our model adaptively searches the optimal feature subspace to reduce the influence of redundant features for high-dimensional data. The effectiveness of the proposed model is demonstrated by experimental observations on artificial and real-world datasets.","Infection with a virus can lead to a range of illnesses in humans, including cancer. When viruses infect a host, they may disrupt normal host function and cause deadly diseases. Understanding complicated viral illnesses requires novel viral genome prediction. Since many of the sequences in assembled contigs from human samples are not identical to known genomes, many assembled contigs are labeled \u201cunknown\u201d by conventional alignments. In this study, sequences from 19 metagenomic investigations were used to create the model proposed here, and these sequences were examined and classified using BLAST. We implemented k-mer counting and the bag-of-words technique using CountVectorizer. As far as we are aware, this work represents the first framework that combines natural language processing (NLP) along with traditional ML classification approaches on raw metagenomic contigs to automatically identify viruses in a variety of human biospecimens. The suggested models are general rather than specialized to a particular viral family. Since the proposed methodology is precise and simple, we may incorporate it into computer-aided diagnosis (CAD) systems to make day-to-day hospital activities easier. In the last stage, binary classification of deoxyribonucleic acid (DNA) with normal and viral genomes was performed using traditional ML classifiers. Using the KNN classifier, the suggested model achieved 98.6% classification accuracy along with 98.5% precision, 98.6% recall, 0.984 F1 score, 0.896 Matthews correlation coefficient, 0.895 kappa, 0.97 classification success index and detection rate of 98.6% for the prediction of viral genomes in DNA. Compared to previously developed ML techniques, the model achieved a significantly greater performance for viral genome prediction.","Enduring stress can have negative impacts on human health and behavior. Widely used wearable devices are promising for assessing, monitoring and potentially alleviating high stress in daily life. Although numerous automatic stress recognition studies have been carried out in the laboratory environment with high accuracy, the performance of daily life studies is still far away from what the literature has in laboratory environments. Since the physiological signals obtained from these devices are time-series data, Recursive Neural Network (RNN) based classifiers promise better results than other machine learning methods. However, the performance of RNN-based classifiers has not been extensively evaluated (i.e., with several variants and different application techniques) for detecting daily life stress yet. They could be combined with CNN architectures, applied to raw data or handcrafted features. In this study, we created different RNN architecture variants and explored their performance for recognizing daily life stress to guide researchers in the field.","Highlights\n\u2022\nNovel efficient deep learning based object classification methods are proposed.\n\u2022\nThe proposed methods aim to exploit the advantages of multiscale wavelets, sparse coding as well as neural networks.\n\u2022\nA multi-branch wavelet neural network architecture is designed.\n\u2022\nThe proposed multi-branch architecture allows us to produce multiple sparse codes at different directional wavelet subbands that are efficiently exploited during the classification stage.\nAbstract\nRecent advances in acquisition and display technologies have led to an enormous amount of visual data, which requires appropriate storage and management tools. One of the fundamental needs is the design of efficient image classification and recognition solutions. In this paper, we propose a wavelet neural network approach for sparse representation-based object classification. The proposed approach aims to exploit the advantages of sparse coding, multi-scale wavelet representation as well as neural networks. More precisely, a wavelet transform is firstly applied to the image datasets. The generated approximation and detail wavelet subbands are then fed into a multi-branch neural network architecture. This architecture produces multiple sparse codes that are efficiently combined during the classification stage. Extensive experiments, carried out on various types of standard object datasets, have shown the efficiency of the proposed method compared to the existing sparse coding and deep learning-based methods.","Pollen classification is considered an important task in palynology. In the Netherlands, two genera of the Urticaceae family, named Parietaria and Urtica, have high morphological similarities but induce allergy at a very different level. Therefore, distinction between these two genera is very important. Within this group, the pollen of Urtica membranacea is the only species that can be recognized easily under the microscope. For the research presented in this study, we built a dataset from 6472 pollen images and our aim was to find the best possible classifier on this dataset by analysing different classification methods, both machine learning and deep learning-based methods. For machine learning-based methods, we measured both texture and moment features based on images from the pollen grains. Varied feature selection techniques, classifiers as well as a hierarchical strategy were implemented for pollen classification. For deep learning-based methods, we compared the performance of six popular Convolutional Neural Networks: AlexNet, VGG16, VGG19, MobileNet V1, MobileNet V2 and ResNet50. Results show that compared with flat classification models, a hierarchical strategy yielded the highest accuracy with 94.5% among machine learning-based methods. Among deep learning-based methods, ResNet50 achieved an accuracy of 99.4%, slightly outperforming the other neural networks investigated. In addition, we investigated the influence on performance by changing the size of image datasets to 1000 and 500 images, respectively. Results demonstrated that on smaller datasets, ResNet50 still achieved the best classification performance. An ablation study was implemented to help understanding why the deep learning-based methods outperformed the other models investigated. Using Urticaceae pollen as an example, our research provides a strategy of selecting a classification model for pollen datasets with highly similar pollen grains to support palynologists and could potentially be applied to other image classification tasks.","Detecting fraud is a challenging task as fraud coexists with the latest in technology. The problem to detect the fraud is that the dataset is unbalanced where non-fraudulent class heavily dominates the fraudulent class. In this work, we considered the fraud detection problem as unbalanced data classification problem and proposed a model based on hybrid sampling technique, which is a combination of random under-sampling and over-sampling using SMOTE. Here, SMOTE is used to widen the data region corresponding to minority samples and random under-sampling of majority class is used for balancing the class distribution. The value difference metric (VDM) is used as distance measure while doing SMOTE. We conducted the experiments with classifiers namely k-NN, Radial Basis Function networks, C4.5 and Naive Bayes with varied levels of SMOTE on insurance fraud dataset. For evaluating the learned classifiers, we have chosen fraud catching rate, non-fraud catching rate in addition to overall accuracy of the classifier as performance measures. Results indicate that our approach produces high predictions against fraud and non-fraud classes.","Methods to classify objects into two or more classes are at the core of various disciplines. When a set of objects with their true classes is available, a supervised classifier can be trained and employed to decide if, for example, a new patient has cancer or not. The choice of performance measure is critical in deciding which supervised method to use in any particular classification problem. Different measures can lead to very different choices, so the measure should match the objectives. Many performance measures have been developed, and one of them is the F-measure, the harmonic mean of precision and recall. Originally proposed in information retrieval, the F-measure has gained increasing interest in the context of classification. However, the rationale underlying this measure appears weak, and unlike other measures, it does not have a representational meaning. The use of the harmonic mean also has little theoretical justification. The F-measure also stresses one class, which seems inappropriate for general classification problems. We provide a history of the F-measure and its use in computational disciplines, describe its properties, and discuss criticism about the F-Measure. We conclude with alternatives to the F-measure, and recommendations of how to use it effectively.","The Human Mobility Signature Identification (HuMID) problem aims at determining whether the incoming trajectories were generated by a claimed agent from the historical movement trajectories of a set of individual human agents such as pedestrians and taxi drivers. The HuMID problem is significant, and its solutions have a wide range of real-world applications, such as criminal identification for police departments, risk assessment for auto insurance providers, driver verification in ride-sharing services, and so on. Though Deep neural networks (DNN) based HuMID models on spatial-temporal mobility fingerprint similarity demonstrate remarkable performance in effectively identifying human agents' mobility signatures, it is vulnerable to adversarial attacks as other DNN-based models. Therefore, in this paper, we propose a Spatial-Temporal iterative Fast Gradient Sign Method with L0 regularization - ST-iFGSM - to detect the vulnerability and enhance the robustness of HuMID models. Extensive experiments with real-world taxi trajectory data demonstrate the efficiency and effectiveness of our ST-iFGSM algorithm. We tested our method on both the ST-SiameseNet and an LSTM-based HuMID classification model. It shows that ST-iFGSM can generate successful attacks to fool the HuMID models with only a few steps of attack in a small portion of the trajectories. The generated attacks can be used as augmented data to update and improve the HuMID model accuracy significantly from 47.36% to 76.18% on testing samples after the attack(86.25% on the original testing samples).","In this paper we summarize the contributions of participants to the fifth Sussex-Huawei Locomotion-Transportation (SHL) Recognition Challenge organized at the HASCA Workshop of UbiComp/ISWC 2023. The goal of this machine learning/data science challenge is to recognize eight locomotion and transportation activities (Still, Walk, Run, Bike, Bus, Car, Train, Subway) from the motion (accelerometer, gyroscope, magnetometer) and GPS (GPS location, GPS reception) sensor data of a smartphone in a user-independent manner. The training data of a \u201ctrain\u201d user is available from smartphones placed at four body positions (Hand, Torso, Bag and Hips). The testing data originates from \u201ctest\u201d users with a smartphone placed at one, but unknown, body position. We introduce the dataset used in the challenge and the protocol of the competition. We present a meta-analysis of the contributions from 15 submissions, their approaches, the software tools used, computational cost and the achieved results. The challenge evaluates the recognition performance by comparing predicted to ground-truth labels at every 10 milliseconds, but puts no constraints on the maximum decision window length. Overall, five submissions achieved F1 scores above 90%, three between 80% and 90%, two between 70% and 80%, three between 50% and 70%, and two below 50%. While the task this year is facing the technical challenges of sensor unavailability, irregular sampling, and sensor diversity, the overall performance based on GPS and motion sensors is better than previous years (e.g. the best performance reported in SHL 2020, 2021 and 2023 are 88.5%, 75.4% and 96.0%, respectively). This is possibly due to the complementary between the GPS and motion sensors and also the removal of constraints on the decision window length. Finally, we present a baseline implementation to help understand the contribution of each sensor modality to the recognition task.","With the recent surge of interest in machine learning, Positive and Unlabeled learning (PU learning) has also attracted much attention of scholars. A key bottleneck for addressing PU classification is the absence of training negative data, and thus many popular approaches belonging to the \u201ctwo-step\u201d strategy have been proposed. However, almost none of the existing two-step methods can thoroughly learn the feature information of samples, which makes the extracted negative samples unreliable and easily leads to undesirable results. Therefore, in this paper, we propose a two-phase projective dictionary pair learning (TPDPL) method for PU learning. The first phase of TPDPL determines reliable negatives by exploiting the reconstruction residuals and the second phase trains the DPL-based classifier with the extracted reliable negative and original positive samples to perform classification. Our experimental results demonstrate that the TPDPL approach can achieve highly competitive classification performance when compared with conventional and state-of-the-art PU learning algorithms. More importantly, due to the special dictionary pair learning framework, the computational complexity of TPDPL is extraordinarily low.","The destruction of archaeological sites and the loss of archaeological landscapes remains a global concern as populations and urban areas continue to expand. Archaeological sites are not only significant to local communities, national identities, and modern tourist economies but also provide critical knowledge of past sociocultural interactions, settlement patterns, human-environment relationships, and risk mitigation strategies. While archaeological landscapes and site destruction have remained outside of traditional land use land cover change (LULCC) studies, they are a form of urban and agricultural land use. By conceptualizing archaeological site destruction within land change science, this study provides an innovative approach for assessing \u201cwhat's left\u201d of historically surveyed archaeological landscapes. Using a Random Forest algorithm and Landsat satellite data, this study quantifies archaeological site destruction attributed to LULCC in Peru's lower Moche Valley between 1985 and 2020. More than 400 archaeological sites previously recorded during the Chan Chan-Moche Valley Project (CCMVP, 1969\u20131974) are analyzed. Results indicate that less than a quarter of the original CCMVP sites remain on the landscape. The primary drivers of LULCC in the lower Moche Valley include population growth, migration, and government policies, while secondary drivers include heritage values. Positioning archaeological survey data within land change science and integrating machine learning techniques can benefit historic survey reassessments globally and provides significant knowledge of archaeological site destruction and the socioeconomic conditions that underly dynamic landscape changes.","Named entity recognition is a fundamental task in natural language processing and has been widely studied. The construction of a recognizer requires training data that contains annotated named entities. However, it is expensive to construct such training data for low-resource domains. In this paper, we propose a recognizer that uses not only training data but also a domain specific dictionary that is available and easy to use. Our recognizer first uses character-based distributed representations to classify words into categories in the dictionary. The recognizer then uses the output of the classification as an additional feature. We conducted experiments to recognize named entities in recipe text and report the results to demonstrate the performance of our method.","Dynamic early exiting has been proven to improve the inference speed of the pre-trained language model like BERT. However, all samples must go through all consecutive layers before early exiting and more complex samples usually go through more layers, which still exists redundant computation. In this paper, we propose a novel dynamic early exiting combined with layer skipping for BERT inference named SmartBERT, which adds a skipping gate and an exiting operator into each layer of BERT. SmartBERT can adaptively skip some layers and adaptively choose whether to exit. Besides, we propose cross-layer contrastive learning and combine it into our training phases to boost the intermediate layers and classifiers which would be beneficial for early exiting. To keep the consistent usage of skipping gates between training and inference phases, we propose a hard weight mechanism during training phase. We conduct experiments on eight classification datasets of the GLUE benchmark. Experimental results show that SmartBERT achieves 2-3\u00d7 computation reduction with minimal accuracy drops compared with BERT and our method outperforms previous methods in both efficiency and accuracy. Moreover, in some complex datasets like RTE and WNLI, we prove that the early exiting based on entropy hardly works, and the skipping mechanism is essential for reducing computation. Our codes are available at: https://github.com/HuBoren99/SmartBert.","It is common to observe significant heterogeneity in clustered data across scientific fields. Cluster-wise conditional distributions are widely used to explore variations and relationships within and among clusters. This paper aims to capture such heterogeneity by employing cluster-wise finite mixture models. To address the heterogeneity among clusters, we introduce latent group structure and incorporate heterogeneous mixing proportions across different groups, accommodating the diverse characteristics observed in the data. The specific number of groups and their membership are unknown. To identify the latent group structure, we employ concave penalty functions to the pairwise differences of the preliminary consistent estimators for the mixing proportions. This approach enables the automatic division of clusters into finite subgroups. Theoretical results demonstrate that as the number of clusters and cluster sizes tend to infinity, the true latent group structure can be recovered with probability close to one, and the post-classification estimators exhibit oracle efficiency. We support our proposed approach\u2019s performance and applicability through extensive simulations and analysis of basic consumption expenditure among urban households in China.","The AAA pattern, i.e. the Arrangement, Action, and Assertion, is a common and nature layout to create a test case. Following this pattern in test cases may benefit comprehension, debugging, and maintenance. The AAA structure of real-life test cases may not be explicit due to its high complexity. Manually labeling AAA statements in test cases is tedious. Thus, an automated approach for labeling AAA statements in existing test cases could benefit new developers and projects that practice collective code ownership and test driven development.\nThis study contributes an automatic approach based on machine learning models. The \u201csecret sauce\u201d of this approach is a set of three learning features that are based on the semantic, syntax, and context information in test cases, derived from the manual tagging process. Thus, our approach mimics how developers may manually tag the AAA pattern of a test case. We assess the precision, recall, and F-1 score of our approach based on 449 test cases, containing about 16,612 statements, across 4 Apache open source projects. For achieving the best performance in our approach, we explore the usage of six machine learning models; the contribution of the SMOTE data balancing technique; the comparison of the three learning features; and the comparison of five different methods for calculating the semantic feature. The results show our approach is able to identify Arrangement, Action, and Assertion statements with a precision upwards of 92%, and recall up to 74%. Our experiments also provide empirical insights regarding how to best leverage machine learning for software engineering tasks.","Continual learning is an emerging research branch of deep learning, which aims to learn a model for a series of tasks continually without forgetting knowledge obtained from previous tasks. Despite receiving a lot of attention in the research community, temporal-based continual learning techniques are still underutilized. In this paper, we address the problem of temporal-based continual learning by allowing a model to continuously learn on temporal data. To solve the catastrophic forgetting problem of learning temporal data in task incremental scenarios, in this research, we propose a novel method based on attentive recurrent neural networks, called Temporal Teacher Distillation (TTD). TTD solves the catastrophic forgetting problem in an attentive recurrent neural network based on three hypotheses, namely Rotation Hypothesis, Redundant Hypothesis, and Recover Hypothesis. Rotation Hypothesis and Redundant hypotheses could cause the attention shift phenomenon, which degrades the model performance on the learned tasks. Moreover, not considering the Recover Hypothesis increases extra memory usage in continuously training different tasks. Therefore, the proposed TTD based on the above hypotheses complements the inadequacy of the existing methods for temporal-based continual learning. For evaluating the performance of our proposed method in task incremental setting, we use a public dataset, WIreless Sensor Data Mining (WISDM), and a synthetic dataset, Split-QuickDraw-100. According to experimental results, the proposed TTD significantly outperforms state-of-the-art methods by up to 14.6% and 45.1% in terms of accuracy and forgetting measures, respectively. To the best of our knowledge, this is the first work that studies continual learning in real-world incremental categories for temporal data classification with attentive recurrent neural networks and provides the proper application-oriented scenario.","This work presents the results of comparison text representations used for short text classification with SVM and neural network when challenged with imbalanced data. We analyze both direct and indirect methods for selecting the proper category and improve them with various representation techniques. As a baseline, we set up a BOW method and then use more sophisticated approaches: word embeddings and transformer-based. The study were done on a dataset from a legal domain where the task was to select the topic of the discussion with the layer. The experiments indicate that fine-tuned pre-trained BERT model for this task gives the best results.","The application of Train of EMU failures Detection System (TEDS) ensures the running safety of EMU. In order to reduce the manual workload and meet the requirements of fine classification monitoring, we researched the automatic classification technology, designed and implemented the corresponding methods, established the TEDS classification chart of all EMU trains at headquarters, and put forward the automatic calculation operation algorithm using multi-source data. Designed the generation scheme of local chart at the monitoring center, the system application function of automatic dispatching is developed, and the classification monitoring based on train running diagram was realized. The application shows that the method proposed in this paper have reduced the manual workload of work effectively, and the capability of abnormal detection has been enhanced, laid the technical foundation for the effective operation monitoring of EMU.","This paper proposes a novel approach for classifier ensemble by employing the concepts of multi-criteria decision-making (MCDM) and aggregation operators. In this framework, a heterogeneous ensemble process has been incorporated where we consider varied set of classifiers to train the model. Each considered classifier is trained on the training data and a score correspondent to it is generated by utilizing the MCDM process. Subsequently, during the training phase, the priority is generated among the classifiers. For the testing phase, these prioritized classifiers are combined using prioritized aggregation operator. The priority order determined during the training phase is used to ensemble the classifiers during the testing phase. The proposed method is tested on UCI benchmark datasets and outperforms existing state-of-the-art methods.","Voice signals are the essential input source for applications based on human and computer interaction technology. Gender identification through voice signals is one of the most challenging tasks. For voice signal based analysis, deep learning algorithms provide an alternative to traditional and conventional algorithms for classification. To identify the gender through voice signals of female, male and \u2018first-time\u2019 transgender, the deep learning algorithm is used to improve the robustness of the identification model with the Mel Frequency Cepstrum Coefficients (MFCC) as a feature of the voice signals. This article presents the identification accuracy of gender with the help of recorded live voice signals. The voice samples of the third gender are recorded in the Hindi language. These Hindi language voice samples of transgender are very low resources and are unavailable at any recognized sources. The simulation results do not depend on the duration of the signals and are text independent. The recurrent neural network \u2013 Bidirectional Long Short-term Memory (RNN \u2013 BiLSTM) algorithm has been simulated on the recorded voice signals. The simulation outcome is compared with the earlier reported results in the literature. The gender-wise average accuracy of the proposed model is achieved as 91.44%, 94.94%, and 96.11% for males, females, and transgender, respectively, using voice signals. The identification accuracy of transgender is high in comparison to other genders. On the other hand, the average accuracy of the proposed model is obtained as 94.16%.","The task of assigning internationally accepted commodity codes (aka HS codes) to traded goods is a critical function of customs offices. Like court decisions made by judges, this task follows the doctrine of precedent and can be nontrivial even for experienced officers. Together with the Korea Customs Service (KCS), we propose a first-ever explainable decision supporting model that suggests the most likely subheadings (i.e., the first six digits) of the HS code. The model also provides reasoning for its suggestion in the form of a document that is interpretable by customs officers. We evaluated the model using 5,000 cases that recently received a classification request. The results showed that the top-3 suggestions made by our model had an accuracy of 93.9% when classifying 925 challenging subheadings. A user study with 32 customs experts further confirmed that our algorithmic suggestions accompanied by explainable reasonings, can substantially reduce the time and effort taken by customs officers for classification reviews.","This paper proposes a real-time voice activity detection (VAD) system that utilizes a compressed convolutional neural network (CNN) model. On general-purpose computers, the system is capable of accurately classifying the presence of speech in audio with low latency. Whereas, when implemented on small devices, the system is showing higher latency, which is presumably an indication of high-load computations in the preprocessing steps. The results of the evaluation indicate that the proposed VAD system is an improvement over the existing solutions, in terms of reducing the model size and improving the level of accuracy among different evaluation metrics. Furthermore, the proposed VAD system offers an extension of the applicability by training the CNN model on a different and more diverse data set. Moreover, the proposed architecture is capable of being compressed to approximately one-eleventh of the size, facilitating eventual deployment on small devices. In contrast to existing closed VAD solutions, the entire pipeline of the proposed VAD system is developed in Python and made available as open source, ensuring the verifiability and accessibility of the work.","Natural Language Processing (NLP) is presently among the hottest scientific fields with an enormous growth rate of the relevant research. Sentiment analysis is a popular NLP problem that aims at the automatic identification of the polarity in user reviews, tweets, blog posts, comments, forum discussions and so on. Unfortunately, the natural sparseness of text, along with its intimate high dimensionality renders the direct application of machine/deep learning models problematic. For this reason, the relevant literature contains a wealth of state-of-the-art dimensionality reduction methods that confront these issues. In this paper, we conduct an experimental study on the effects of dimensionality reduction in the area of sentiment classification. More specifically, we consider multiple feature selection and feature extraction techniques and we investigate their impact on the effectiveness and the efficiency of seven state-of-the-art classifiers. The experimental evaluation includes accuracy and execution time measurements on four benchmark datasets with various degrees of reduction aggressiveness. The results indicate that, in most cases, dimensionality reduction has indeed a beneficial impact on the running times, whereas the accuracy sacrifices are usually small. However, we also indicate several exceptions where this observation is not valid. These exceptions are appropriately highlighted and discussed.","Breast cancer is the primary cause of women's death due to cancer; if detected in the early stage, it is a curable disease. Machine learning classification techniques are helpful in breast cancer detection. The research aims to investigate the averaged-perceptron machine-learning classifier performance on the Wisconsin original breast cancer dataset (WBC); the work has focused on two points; first, does the averaged-perceptron classifier has the quality to gain a higher accuracy than the other classifiers? Second, does it help to reduce false-negative or false-positive breast cancer predictions? The averaged-perceptron model recorded an accuracy score of 0.984 with zero false-negative predictions. The investigation has also signified the effect of threshold on false-negative or false-positive prediction. Applying the averaged-perceptron classifier in a computer-aided-diagnosis system can improve breast cancer recognition accuracy with zero false-positive or false-negative forecasts.","Sentiment analysis (also called opinion mining) is one of the widely used research fields of natural language processing. E-commerce service providers use this technique to analyze the sentiment of a product or a service in texts, posts, and comments. In particular, the service providers and users want to understand the sentiment on product aspect categories rather than the overall sentiment of a product. These aspect categories encounter the class imbalance problem. Therefore, the BERT (Bidirectional Encoder Representation from Transformers) based fine-tuning model is presented to deal with the imbalanced aspect categorization task. Specifically, this paper studies various data sampling techniques such as stratified random sampling (SRS), random undersampling (RUS), and random oversampling (ROS) for reducing the class imbalance problem. Empirically, the results show that the proposed BERT fine-tuning model with the SRS technique achieves better results. In particular, the model achieves 96.21% for the validation and 96.47% for testing using the news aggregator data. Similarly, the SMS spam collection data achieves 99.20% for the validation and 99.10% for testing.","Multi-view learning (MVL) is a rapidly evolving direction in the field of machine learning. Despite the positive results, most algorithms that combine multi-view learning with twin support vector machines (TSVM) focus on the traditional machine learning domain. No method has been accomplished for combining MVL, TSVM, and deep learning. In this paper, we propose two novel multi-view deep models to solve the multiclass classification problem, namely deep multi-view twin support vector machines (DMvTSVM) based on deep neural network (DNN) and auto-encoder (AE) network. They find two non-parallel hyperplanes such that each hyperplane is as close to its own class as possible while being as far away from the other class as possible. Meanwhile, we apply similarity regularization to the output of the Deep TSVM classifier for each view to learn consensus information between views, and use this to refine the joint weights of the deep model and TSVM. Finally, the novel models employ the o n e \u2212 v s \u2212 r e s t strategy to allow the DMvTSVM classifier to solve the multiclass classification problems. In the experiments, the proposed methods are compared with existing state-of-the-art algorithms to prove their effectiveness.\nHighlights\n\u2022\nThe first combination of twin support vector machines (TSVM) and deep neural networks.\n\u2022\nNesting of more advanced auto encoder networks.\n\u2022\nIncorporating multi-view learning capabilities.\n\u2022\nMutual negotiation and simultaneous learning between deep neural networks and TSVM.\n\u2022\nThe experiments prove the effectiveness of the algorithm in this paper.","The class imbalance learning (CIL) problem indicates when one class have very low proportions of samples (minority class) compared to the other class (majority class). Even though kernel ridge regression (KRR) shows high generalization ability at a considerably quicker learning speed than conventional machine learning algorithms, it fails to achieve an excellent result for CIL problems. To address this inherent limitation of KRR, a novel affinity-based fuzzy KRR (AFKRR) is proposed for dealing with the binary CIL problem. In AFKRR, an affinity-based fuzzy membership value is linked to each training sample. The affinity of the majority class datapoint is measured using the support vector data description (SVDD) trained by the majority class datapoints. The classification ability of the proposed AFKRR is calculated using the area under the receiver optimal characteristics curve (AUC), F-measure and Geometric mean. AFKRR\u2019s performance is compared with the support vector machine (SVM), affinity and class probability-based fuzzy SVM (ACFSVM), entropy-based fuzzy SVM (EFSVM), improved density weighted least squares SVM (IDLSSVM-CIL), KRR and intuitionistic fuzzy KRR (IFKRR) models on a few real-world as well as a few artificial imbalanced datasets. Experimental outcomes reveal the usability and efficacy of the proposed AFKRR model.","A very common practice to speed up instance based classifiers is to reduce the size of their training set, that is, replace it by a condensing set, hoping that their accuracy will not worsen. This can be achieved by applying a Prototype Selection or Generation algorithm, also referred to as a Data Reduction Technique. Most of these techniques cannot be applied on multi-label problems, where an instance may belong to more than one classes. Reduction through Homogeneous Clustering (RHC) and Reduction by Space Partitioning (RSP3) are parameter-free single-label Prototype Generation algorithms. Both are based on recursive data partitioning procedures that identify homogeneous clusters of training data, which they replace by their representatives. This paper proposes variations of these algorithms for multi-label training datasets. The proposed methods generate multi-label prototypes and inherit all the desirable properties of their single-label versions. They consider clusters that contain instances that share at least one common label as homogeneous clusters. It is shown via an experimental study based on nine multi-label datasets that the proposed algorithms achieve good reduction rates without negatively affecting classification accuracy.","Bibliographic references in scholarly documents are integral to discourse in humanities disciplines. While prior work has focused on reference extraction and parsing from these documents, little research has investigated the classification of footnotes containing bibliographic citations and author commentary using supervised machine learning methodologies. Using an historiographic dataset drawn from the JSTOR humanities archive, we train and compare the performance of a suite of single and hybrid machine learning classifiers on a novel, previously unexplored reference classification task in archival document analysis. Moreover, as a part of this analysis, we investigate the feasibility of using the grammar of these scholarly footnotes as training features for our machine learning models. In our work we compare the performance of traditional features previously used in reference mining and these novel, grammatical features inspired by natural language processing techniques. Our work demonstrates the superiority of hybrid models for classification of scholarly footnotes containing historiographic bibliographic references, the transferability of features from reference extraction to this research problem, and the viability of training machine learning models for this task utilizing novel, grammatical feature sets.","One of the main challenges in the industry is having trained and efficient operators in manufacturing lines. Smart adaptive guidance systems are developed that offer assistance to the operator during assembly. Depending on the operator\u2019s level of execution, the system should be able to serve a different guidance response. This paper investigates the assessment and classification of the operator\u2019s functional state using observed task execution times. Five different classifiers are studied for operator functional state classification on task execution time series. The experiments are based on an industry case and the ground truth is provided by an expert rule-based system. Three classification scenarios are defined that segment the problem on the level of the task, the individual, or the team. Furthermore, the investigation includes the evaluation of four distinct window-size configurations. The examination of how these scenarios and window-sizes influence the studied dataset across diverse classifiers reveals that achieving enhanced accuracy necessitates a larger input dimension. In this context, Convolutional Neural Networks predominantly exhibit superior performance compared to alternative classifiers. Careful attention needs to be paid to performance over classes and skills, but results confirm the validity of the approach for data-driven operator functional state classification.","Human-computer interaction and the creation of humanoid robots both depend heavily on emotions. By integrating the concept of emotion understanding, intelligent software systems become more effective and intuitive in resembling human-human interactions. Typically, we combine factors like intonation (speech), facial expression (visual modality), and word content (text). All possible multimodal combinations must be taken into consideration to process emotions appropriately. Among multimodal approaches, the use of human audio samples for emotion processing is given more weight than the use of facial expressions. To accomplish accurate categorization, analyzing massive volumes of real-time data has become more necessary. Machine Learning (ML) models that operate in a distributed fashion are crucial, given the size and complexity of the problem under study. In this respect, we propose a distributed ensemble model for vocal cue-based emotion classification. Three base ML models that work in a distributed manner were used. According to the findings, the ensemble model proposed differentiates between the seven fundamental emotions with reasonable accuracy. The proposed distributed ensemble model performed better than existing ML models on TESS, SAVEE, and RAVDESS, achieving 86% accuracy on the unified dataset.","Events are the core element of information in descriptive corpus. Many progresses have been made in Event Detection (ED) to detection and extraction of key events information from massive unstructured texts, However, it is still a challenge to detect event information from data with unavoidable noisy labels. A robust Joint-training Graph Convolution Networks (JT-GCN) model is proposed to meet the challenge of ED tasks with noisy labels in this paper. Specifically, we first employ two Graph Convolution Networks with Edge Enhancement (EE-GCN) to make predictions simultaneously. A joint loss combining the detection loss and the contrast loss from two networks is then calculated for training. Meanwhile, a small-loss selection mechanism is introduced to mitigate the impact of mislabeled samples in networks training process. These two networks gradually reach an agreement on the ED tasks as joint-training progresses. Corrupted data with label noise are generated from the benchmark dataset ACE2005. Experiments on ED tasks has been conducted with symmetry label noise on different level. The experimental results show that the proposed model is robust to the impact of label noise and superior to present models for ED tasks.","Classification tasks are of great importance in machine learning. However, class imbalance is a universal problem that needs to be solved in classification and can greatly affect the performance of machine learning classifiers. Developing from the basic belief rule base (BRB) system, the hierarchical belief rule-based system can integrate expert knowledge and has the potential to alleviate the negative effect of class imbalance. To utilize BRB to solve the imbalanced multi-classification task and avoid the combinational explosion problem, a novel hierarchical BRB structure based on the extreme gradient boosting (XGBoost) feature selection method, abbreviated as HFS-BRB is proposed in this paper in order to deal with any number of classes. In the hierarchical BRB structure, there is one main-BRB in the first level and several sub-BRBs in the second level. The XGBoost technique is used for feature selection in the modelling process of each abovementioned BRB model. The output of the main BRB represents the approximated classification between confusable classes. Then, these samples were transmitted to a certain sub-BRB for binary classification to make a precise prediction. Thus, a multi-classification problem can be transformed into several binary classification problems. The class imbalance is alleviated. To test the effectiveness of the proposed method, seven classical benchmark problems for imbalanced classification and a real asteroid orbit classification were performed.","Microscopy analysis of sputum images for bacilli screening is a common method used for both diagnosis and therapy monitoring of tuberculosis (TB). Nonetheless, it is a challenging procedure, since sputum examination is time-consuming and needs highly competent personnel to provide accurate results which are important for clinical decision-making. In addition, manual fluorescence microscopy examination of sputum samples for tuberculosis diagnosis and treatment monitoring is a subjective operation. In this work, we automate the process of examining fields of view (FOVs) of TB bacteria in order to determine the lipid content, and bacterial length and width. We propose a modified version of the UNet model to rapidly localise potential bacteria inside a FOV. We introduce a novel method that uses Fourier descriptors to exclude contours that do not belong to the class of bacteria, hence minimising the amount of false positives. Finally, we propose a new feature as a means of extracting a representation fed into a support vector multi-regressor in order to estimate the length and width of each bacterium. Using a real-world data corpus, the proposed method i) outperformed previous methods, and ii) estimated the cell length and width with a root mean square error of less than 0.01%.","Mixup is an efficient data augmentation technique, which improves generalization by interpolating random examples. While numerous approaches have been developed for Mixup in the Euclidean and in the hyperbolic space, they do not fully use the intrinsic properties of the examples, i.e., they manually set the geometry (Euclidean or hyperbolic) based on the overall dataset, which may be sub-optimal since each example may require a different geometry. We propose DynaMix, a framework that automatically selects an example-specific geometry and performs Mixup between the different geometries to improve training dynamics and generalization. Through extensive experiments in image and text modalities we show that DynaMix outperforms state-of-the-art methods over six downstream applications. We find that DynaMix is more useful in low-resource and semi-supervised settings likely because it displays a probabilistic view of the geometry.","The Bio-Basis Function Neural Network (BBFNN) is a successful neural network architecture for peptide classification. However, the selection of a subset of peptides for a parsimonious network structure is always a difficult process. We present a Sparse Bayesian Bio-Kernel Network in which a minimal set of representative peptides can be selected automatically. We also introduce per-residue weighting to the Bio-Kernel to improve accuracy and identify patterns for biological activity. The new network is shown to outperform the original BBFNN on various datasets, covering different biological activities such as as enzymatic and post-translational-modification, and generates simple, interpretable models.","A game theoretic flavoured decision tree is designed for multi-class classification. Node data is split by using a game between sub-nodes that try to minimize their entropy. The splitting parameter is approximated by a naive approach that explores the deviations of players that can improve payoffs by unilateral deviations in order to imitate the behavior of the Nash equilibrium of the game. The potential of the approach is illustrated by comparing its performance with other decision tree-based approaches on a set of synthetic data.","As a hot spot these years, cross-domain sentiment classification aims to learn a reliable classifier using labeled data from a source domain and evaluate the classifier on a target domain. In this vein, most approaches utilized domain adaptation that maps data from different domains into a common feature space. To further improve the model performance, several methods targeted to mine domain-specific information were proposed. However, most of them only utilized a limited part of domain-specific information. In this study, we first develop a method of extracting domain-specific words based on the topic information derived from topic models. Then, we propose a Topic Driven Adaptive Network (TDAN) for cross-domain sentiment classification. The network consists of two sub-networks: a semantics attention network and a domain-specific word attention network, the structures of which are based on transformers. These sub-networks take different forms of input and their outputs are fused as the feature vector. Experiments validate the effectiveness of our TDAN on sentiment classification across domains. Case studies also indicate that topic models have the potential to add value to cross-domain sentiment classification by discovering interpretable and low-dimensional subspaces.","As in the traditional single-label classification, the feature selection plays an important role in the multi-label classification. This paper presents a multi-label feature selection algorithm MLFS which consists of two steps. The first step employs the mutual information to complete the local feature selection. Based on the result of local selection, GA algorithm is adopted to select the global optimal feature subset and the correlations among the labels are considered. Compared with other multi-label feature selection algorithms, MLFS exploits the label correlation to improve the performance. The experiments on two multi-label datasets demonstrate that the proposed method has been proved to be a promising multi-label feature selection method.","Multi-label learning (MLL) usually requires assigning multiple relevant labels to each instance. While a fully supervised MLL dataset needs a large amount of labeling effort, using complementary labels can help alleviate this burden. However, current approaches to learning from complementary labels are mainly designed for multi-class learning and assume that each instance has a single relevant label. This means that these approaches cannot be easily applied to MLL when only complementary labels are provided, where the number of relevant labels is unknown and can vary across instances. In this paper, we first propose the unbiased risk estimator for the multi-labeled complementary label learning (MLCLL) problem. We also provide an estimation error bound to ensure the convergence of the empirical risk estimator. In some cases, the unbiased estimator may give unbounded gradients for certain loss functions and result in overfitting. To mitigate this problem, we improve the risk estimator by minimizing a proper loss function, which has been shown to improve gradient updates. Our experimental results demonstrate the effectiveness of the proposed approach on various datasets.","Some traditional Indian art forms enjoy widespread popularity across the world. One of the most prominent among these is the Madhubani style. This art form\u2019s rich heritage and beauty enthrall the connoisseurs and continue to inspire new designs catering to the changing tastes of prevalent fashion. Preservation of these traditional art forms is the need of the hour. Modern technological advances can be utilized with great advantage for this purpose. Since a database of Madhubani art forms was hitherto unavailable, an attempt is made in this work to create one from scratch. Five different classes of Madhubani art, i.e., Bharni, Godna, Kachni, Kohbar, and Tantrik, are identified, and the collected images are annotated with these classes. Classification of the art images is attempted using the handcrafted Local Binary Pattern (LBP) texture descriptors and state-of-the-art Convolutional Neural Networks (CNNs). The Transfer Learning approach with CNNs is employed to classify the designs. An attempt is made to obtain a better classification accuracy than the one provided by standard CNNs. Towards this end, the current work proposes a fusion of features extracted from several deep CNNs, decision fusion-based classification based on averaging prediction score (FAVG), and maximum vote score (FMAX). The proposed method\u2019s performance is tested on our Madhubani art dataset and compared against several standard pre-trained CNNs available in the literature. The proposed approaches provide significantly higher classification accuracy for Madhubani art patterns, with decision fusion based on averaging prediction score (FAVG) approach being the best. The maximum accuracy, specificity, and error rate scores are 98.82%, 99.72%, and 1.18%, respectively. This is the first such attempt, and the excellent results motivate further work to develop content-based image retrieval tools and evolutionary design-based tools for automating the development of new designs. These endeavors are expected to go a long way in preserving precious art heritage and fostering its rapid growth in the world market. The dataset will be made publically available for further experimentation.\nHighlights\n\u2022\nA New Madhubani art forms database is created from scratch.\n\u2022\nClassification into different Madhubani styles is attempted.\n\u2022\nTransfer Learning is utilized with Feature Fusion and Decision Fusion\n\u2022\n98.82% accuracy is obtained, beating other approaches.\n\u2022\nStatistical non-parametric Wilcoxon rank-sum test is used for validation.","Event classification (EC) aims to assign the event labels to unlabeled sentences and tends to struggle in real-world applications when only a few annotated samples are available. Previous studies have mainly focused on using meta-learning to overcome the low-resource problem where label data from other tasks are still required for model learning and selection. Accordingly, prompt learning-based approaches are proposed to address the low-resource issue. However, such approaches generally ignore task-specific information and adopt demonstration learning for fine-tuning, which fails to leverage the most informative examples for training and hurts performance. Thus, we propose a taxonomy-aware prompt learning framework TaxonPrompt that trains the language model with samples from easy to hard by imitating the human curricula, which effectively alleviates the classification bottleneck caused by insufficient data. We first design an event prompt generation (EPG) for automatically generating task-specific templates using sentences, labels, and trigger words. Then, we propose a Fisher information-based demonstration filtering (FDF) to dynamically select the most informative support examples for each query to train the model. We have conducted extensive experiments on two EC datasets: FewEvent and RAMS. The experimental results demonstrate the superiority of the proposed model over state-of-the-art baselines. In particular, our approach works well in the scenario of an extremely small number of available task resources and therefore constitutes a solution for few-shot event classification.\nHighlights\n\u2022\nWe designed a taxonomy-aware event classification framework for overcoming the classification bottleneck brought by insufficient data volume.\n\u2022\nWe apply a prompt-based method for few-shot event classification, which does not require abundantly seen classes.\n\u2022\nWe design a task-specific template generation strategy that can take all input factors into consideration.\n\u2022\nWe propose an FIM-based demonstration strategy for selecting informative training samples, which enable more efficient training.\n\u2022\nOur proposal can consistently outperform the baselines on two datasets against the state-of-the-art baselines.","Improving the explainability of Computer Vision models based on Deep Learning has recently become a compelling problem, ensuring reliable predictions to the end-user and enabling more fine-grained classifications. Recently, Concept Bottleneck models have been proposed for images classification, partitioning the problem in two stages and thereby defining a hierarchy of concepts. So far, however, little work has been done to investigate the applicability of this approach to other datasets with higher intra-class variability and ambiguity, and to discuss its flexibility to tasks different from whole-images classification. In this work we develop and discuss a Concept Bottleneck model for images segmentation, objects fine classification and tracking, and compare it to more classical methods based on Mask R-CNN and images similarity algorithms. All our models are trained and tested on a dataset comprised of pictures of fridges filled with various objects, however the method can be applied to any fine classification task. The proposed model makes full use of the hierarchy in concepts, exploiting the relationships between different categories at the same hierarchical level and relying on a novel method for handling multi-labels classifications. We show that the performance on fine classification is on par with a regular Mask R-CNN, but with a significant increase in explainability and in handling classes confusion. New explainable metrics are proposed to quantitatively evaluate the increase in explainability. We also demonstrate the effectiveness of the derived Concept Bottleneck features on related tasks, i.e., the tracking of objects between consecutive pictures in a sequence. The code is released as open source and available at https://opensource.silicon-austria.com/pittinof/hierarchical-concept-bottleneck.","Alzheimer\u2019s disease (AD) is a neurodegenerative disease characterized by cognitive and behavioral impairment that significantly interferes with social and occupational functioning. Mild cognitive impairment (MCI) is a relatively broad clinical condition involving a slight memory deficit, which in many cases represents a transitional state between a cognitively normal (CN) condition and AD. Structural magnetic resonance (sMR) imaging has been widely used in studies related to AD because it provides images with excellent anatomical details and information about structural and contrast changes induced by the disease in the brain. Many published studies restrict their analysis to a few particular regions of the brain and search for structural changes caused by the disease. Recent studies start looking for new AD biomarkers using multiple brain regions and focusing on subtle texture changes in the image. Therefore, this study proposes a new technique for MR image classification in AD diagnosis using graph kernels constructed from texture features extracted from sMR images. In our method, we first segment the MR brain images into multiple regions with the FreeSurfer. Then, we extract 22 texture features using three methods and define the graph-node attributes as the probability distributions of the extracted features. Next, for each texture feature, we build a graph and define its edge weights as the distances between pairs of node attributes using three distance metrics. After that, we use a threshold-based approach for graph edges removal and create the graph-kernels matrices. Finally, we perform image classification using Support Vector Machines (SVMs) with two graph-kernels. Results of our method have shown better performances for the CN\u00d7AD (AUC = 0.92) and CN\u00d7MCI (AUC = 0.81) classifications, and worse for the MCI\u00d7AD case (AUC = 0.78). This trend is consistent with other published results and makes sense if we consider the concept of Alzheimer\u2019s disease continuum from pathophysiological, biomarker and clinical perspectives. Besides allowing the use of different texture attributes for the diagnosis of Alzheimer\u2019s, our method uses the graph-kernel approach to represent texture features from different regions of the brain image, which considerably facilitates the image classification task via SVMs. Our results were promising when compared to the state-of-the-art in graph-based AD classification.\nHighlights\n\u2022\nAlzheimer\u2019s disease classification based on graph kernel Support Vector Machines.\n\u2022\nEvaluation of image texture patterns of multiple brain regions in Alzheimer.\n\u2022\nAssessment of three distance metrics to measure node attributes differences.\n\u2022\nThreshold based method for graph edge removal to obtain discriminative graphs.\n\u2022\nClassification results comparable to other proposed graph-based models.","A world of healthcare possibilities has been opened with the development of the Internet of Medical Things and related machine learning, deep learning, and artificial intelligence approaches. It has a broad range of uses: when linked to the Internet, common medical equipment and sensors may gather important data; deep learning and artificial intelligence algorithms use this data to understand symptoms and patterns and allow remote healthcare. There are a large number of people affected by thyroid disorders across the world. The ultrasound-based thyroid nodule detection using traditional methods increased the burden on the expertise. Therefore, alternate methods are required to overcome this problem. In order to facilitate early thyroid disorder detection, this research aims to offer an IoT-based ensemble learning framework. In the proposed ensemble model, three pre-trained models DeiT, Mixer-MLP and Swin Transformer, are used for feature extraction. The mRMR technique is used for relevant feature selection. A total of 24 machine learning models have been trained, and weighted average ensemble learning is employed using the Improved Jaya optimization algorithm and Coronavirus Herd Immunity optimization algorithm. The ensemble model with the improved Jaya optimization algorithm achieved excellent results. The best value for accuracy, precision, sensitivity, specificity, F2-score and ROC-AUC score are 92.83%, 87.76%, 97.66%, 88.89%, 0.9551 and 0.9357, respectively. The main focus of this research is to increase the specificity. A poor value of specificity can lead to a high false positive rate. This situation can increase anxiety and emotionally weaken the patient. The proposed ensemble model with the Improved Jaya optimization algorithm outperformed state-of-the-art techniques and can assist medical experts.","Traditional federated classification methods, even those designed for non-IID clients, assume that each client annotates its local data with respect to the same universal class set. In this paper, we focus on a more general yet practical setting, non-identical client class sets, where clients focus on their own (different or even non-overlapping) class sets and seek a global model that works for the union of these classes. If one views classification as finding the best match between representations produced by data/label encoder, such heterogeneity in client class sets poses a new significant challenge-local encoders at different clients may operate in different and even independent latent spaces, making it hard to aggregate at the server. We propose a novel framework, FedAlign1, to align the latent spaces across clients from both label and data perspectives. From a label perspective, we leverage the expressive natural language class names as a common ground for label encoders to anchor class representations and guide the data encoder learning across clients. From a data perspective, during local training, we regard the global class representations as anchors and leverage the data points that are close/far enough to the anchors of locally-unaware classes to align the data encoders across clients. Our theoretical analysis of the generalization performance and extensive experiments on four real-world datasets of different tasks confirm that FedAlign outperforms various state-of-the-art (non-IID) federated classification methods.","In this paper a left ventricle (LV) contour detection method is described. The method works from an approximate contour defined by anatomical landmarks extracted using Support Vector Machine (SVM) classifiers. The LV contour approximation is used as an initialization step for the deformable model algorithm. An optimization method based on a gradient descend algorithm is used to obtain the optimal contour that provides a minimum energy value. Both classifier and edge detection method performances have been validated. The error is determined as the difference between the shape estimated by the algorithm and the shape traced by an expert.","In this article, we investigate the effects on authorship identification tasks (including authorship verification, closed-set authorship attribution, and closed-set and open-set same-author verification) of a fundamental shift in how to conceive the vectorial representations of documents that are given as input to a supervised learner. In \u201cclassic\u201d authorship analysis, a feature vector represents a document, the value of a feature represents (an increasing function of) the relative frequency of the feature in the document, and the class label represents the author of the document. We instead investigate the situation in which a feature vector represents an unordered pair of documents, the value of a feature represents the absolute difference in the relative frequencies (or increasing functions thereof) of the feature in the two documents, and the class label indicates whether the two documents are from the same author or not. This latter (learner-independent) type of representation has been occasionally used before, but has never been studied systematically. We argue that it is advantageous, and that, in some cases (e.g., authorship verification), it provides a much larger quantity of information to the training process than the standard representation. The experiments that we carry out on several publicly available datasets (among which one that we here make available for the first time) show that feature vectors representing pairs of documents (that we here call Diff-Vectors) bring about systematic improvements in the effectiveness of authorship identification tasks, and especially so when training data are scarce (as it is often the case in real-life authorship identification scenarios). Our experiments tackle same-author verification, authorship verification, and closed-set authorship attribution; while DVs are naturally geared for solving the 1st, we also provide two novel methods for solving the 2nd and 3rd that use a solver for the 1st as a building block. The code to reproduce our experiments is open-source and available online.1","Interoperability issue is a significant problem in Building Information Modeling (BIM). Object type, as a kind of critical semantic information needed in multiple BIM applications like scan-to-BIM and code compliance checking, also suffers when exchanging BIM data or creating models using software of other domains. It can be supplemented using deep learning. Current deep learning methods mainly learn from the shape information of BIM objects for classification, leaving relational information inherent in the BIM context unused. To address this issue, we introduce a two-branch geometric-relational deep learning framework. It boosts previous geometric classification methods with relational information. We also present a BIM object dataset\u2014IFCNet++, which contains both geometric and relational information about the objects. Experiments show that our framework can be flexibly adapted to different geometric methods and relational features do act as a bonus to general geometric learning methods, obviously improving their classification performance, thus reducing the manual labor of checking models and improving the practical value of enriched BIM models.","Predicting students\u2019 academic performance is a critical research area, yet imbalanced educational datasets, characterized by unequal academic-level representation, present challenges for classifiers. While prior research has addressed the imbalance in binary-class datasets, this study focuses on multi-class datasets. A comparison of ten resampling methods (SMOTE, Adasyn, Distance SMOTE, BorderLineSMOTE, KmeansSMOTE, SVMSMOTE, LN SMOTE, MWSMOTE, Safe Level SMOTE, and SMOTETomek) is conducted alongside nine classification models: K-Nearest Neighbors (KNN), Linear Discriminant Analysis (LDA), Quadratic Discriminant Analysis (QDA), Support Vector Machine (SVM), Logistic Regression (LR), Extra Tree (ET), Random Forest (RT), Extreme Gradient Boosting (XGB), and Ada Boost (AdaB). Following a rigorous evaluation, including hyperparameter tuning and 10 fold cross-validations, KNN with SmoteTomek attains the highest accuracy of 83.7%, as demonstrated through an ablation study. These results emphasize SMOTETomek\u2019s effectiveness in mitigating class imbalance in educational datasets and highlight KNN\u2019s potential as an educational data mining classifier.","The use of automatic systems for medical image classification has revolutionized the diagnosis of a high number of diseases. These alternatives, which are usually based on artificial intelligence (AI), provide a helpful tool for clinicians, eliminating the inter and intra-observer variability that the diagnostic process entails. Convolutional Neural Network (CNNs) have proved to be an excellent option for this purpose, demonstrating a large performance in a wide range of contexts. However, it is also extremely important to quantify the reliability of the model\u2019s predictions in order to guarantee the confidence in the classification. In this work, we propose a multi-level ensemble classification system based on a Bayesian Deep Learning approach in order to maximize performance while providing the uncertainty of each classification decision. This tool combines the information extracted from different architectures by weighting their results according to the uncertainty of their predictions. Performance is evaluated in a wide range of real scenarios: in the first one, the aim is to differentiate between different pulmonary pathologies: controls vs bacterial pneumonia vs viral pneumonia. A two-level decision tree is employed to divide the 3-class classification into two binary classifications, yielding an accuracy of 98.19%. In the second context, performance is assessed for the diagnosis of Parkinson\u2019s disease, leading to an accuracy of 95.31%. The reduced preprocessing needed for obtaining this high performance, in addition to the information provided about the reliability of the predictions evidence the applicability of the system to be used as an aid for clinicians.\nHighlights\n\u2022\nWe present an ensemble of multi-scale Bayesian deep learning approach.\n\u2022\nThe Bayesian nature quantifies the uncertainty of each classifier\u2019s decision.\n\u2022\nUncertainty weighs the contribution of the different members of the ensemble.\n\u2022\nOur system demonstrates a large performance in classification of 2D/3D images.\n\u2022\nIt also identifies informative patterns regardless of their size and shape.","Data used in particle physics analyses have an imbalanced nature in which the events of interest are rare due to the broad background. These events can be identified from bulk by intensive computational studies including application of sophisticated analysis techniques. Classification algorithms provided by supervised machine learning (ML) approaches can be utilized to interpret skewed particle dataset as an alternative to the classic techniques even for multi particle state analysis. In this study, the ground state of the bottomonium (\u03a5(1 S)) and its excited states (\u03a5(2 S) and \u03a5(3 S)) were studied by application of multiclass classification approach based on random forest classifier (RFC) which is a novel ML approach example in particle analysis with implementation of resampling techniques for preprocessing dataset and modification of the weighting strategy. For this purpose, five widely used oversampling and two hybrid strategies, using over and under resampling together, were adjusted to RFC. Moreover, class weights applied RFC, weighted random forest (WRF), was used in the analysis. Due to the data structure, performance of the applied models was evaluated by the derivatives of confusion matrix. It is revealed that hybrid techniques implemented in RFC is suitable for handling highly imbalanced classes. G-mean and BAcc scores of upsilon states presented that with SMOTETomek strategy the model exhibited highest classification achievement, around 90%, with high sensitivity implying the success of the application on multiclass classification.","To evaluate the robustness of non-classifier models, we propose probabilistic local equivalence, based on the notion of randomized smoothing, as a way to quantitatively evaluate the robustness of an arbitrary function. For a given function\nf\n, probabilistic local equivalence evaluates whether, when sampling a normally-distributed point\nx\n\u2032\nin a neighborhood of a point\nx\n, there is a probability\n&gt;\n0.5\nthat\nf\n(\nx\n\u2032\n)\nis equivalent to\nf\n(\nx\n)\n, according to a user-defined notion of equivalence. We use probabilistic local equivalence to evaluate the effect of data augmentation methods for improving robustness, including adversarial training, on a model\u2019s performance. We also use probabilistic local equivalence to evaluate the effect on robustness of model architecture, number of parameters, pre-training, quantization, and other model properties.","Ensemble learning has been shown to be an effective approach to solve multi-label classification problem. However, most existing ensemble learning methods do not consider the difference between unseen instances, and existing methods that consider such difference fail to effectively explore label correlation, which limits their performance. To address these issues, we propose a novel method named MLDE (Multi-Label classification with Dynamic Ensemble learning). In MLDE, the most competent ensemble of base classifiers is selected and combined to predict each unseen instance. To make dynamic selection specific to multi-label problem and achieve better performance, we combine classification accuracy and ranking loss to serve as the competence measurement for the base classifiers. Specifically, classification accuracy is decomposable to multiple labels and distinguishes the ability difference of a classifier with respect to different labels, while ranking loss focuses on the overall performance of a classifier on the label set and thus fully considers the correlation between multiple labels. Extensive experiments on 24 publicly available datasets demonstrate that MLDE outperforms the state-of-the-art methods.","Highlights\n\u2022\nWe proposed two fusion strategies to promote the accuracy of pig cough recognition.\n\u2022\nClassifier fusion was proven to outperform feature fusion.\n\u2022\nThe proposed method achieved an accuracy of 99.20%.\nAbstract\nThe recognition of pig cough sounds is an effective way to monitor pig respiratory diseases which seriously affect healthy pig breeding. Due to the complexity of the pig-housing environment, achieving high precision cough recognition by relying only on a single feature or classifier is challenging. Therefore, in this study we investigated two fusion strategies, namely feature fusion and classifier fusion, to boost classification accuracy. For feature fusion, we improved the previously proposed feature fusion algorithm and selected better acoustic and image features for fusion. We also proposed a novel classifier fusion algorithm. In the algorithm, the support vector machine (SVM) classifiers trained by the acoustic features and deep features were fused by soft voting for pig cough prediction. The sound data collected in the pig barn were used to validate the proposed methods. Our methods achieved a substantial classification rate of 97.47% and 99.20% for feature fusion and classifier fusion, respectively. The results demonstrate that our proposed fusion strategies can significantly improve the recognition accuracy of pig cough sounds.","Deep neural networks (DNNs) and decision trees (DTs) are both state-of-the-art classifiers. DNNs perform well due to their representational learning capabilities, while DTs are computationally efficient as they perform inference along one route (root-to-leaf) that is dependent on the input data. In this paper, we present DecisioNet (DN), a binary-tree structured neural network. We propose a systematic way to convert an existing DNN into a DN to create a lightweight version of the original model. DecisioNet takes the best of both worlds - it uses neural modules to perform representational learning and utilizes its tree structure to perform only a portion of the computations. We evaluate various DN architectures, along with their corresponding baseline models on the FashionMNIST, CIFAR10, and CIFAR100 datasets. We show that the DN variants achieve similar accuracy while significantly reducing the computational cost of the original network.","Most of the flight accident data have uneven distribution of categories. When the traditional classifier is applied to this data, it will pay less attention to the minority class data. Synthetic Minority Over-sampling Technique (SMOTE), and its improvements are well-known methods to address this imbalance problem at the data level. However, traditional algorithms still have the problems in blurring the boundary of positive and negative classes and changing the distribution of original data. In order to overcome these problems and accurately predict flight accidents, a new Clustered Biased Borderline SMOTE(CBB-SMOTE) is proposed for Quick Access Recorder (QAR) Go-Around data. It generates more obvious positive and negative class boundaries by using K-means for boundary minority class data and safety minority class data respectively, and maintains the original data distribution to the greatest extent through a biased oversampling method. Experiments were carried out on a group of QAR Go-Around data. The data set is balanced by CBB-SMOTE, SMOTE, Cluster-SMOTE algorithm respectively, and the random forest algorithm is used to predict the new data set. The experimental results show that CBB-SMOTE outperforms the SMOTE in terms of G-means value, Recall and AUC.","Skip BACKGROUND: Section\nBACKGROUND:\nAutomatic recognition of a person\u2019s gender as well as his or her unilateral load state are issues that are often analyzed and utilized by a wide range of applications. For years, scientists have recognized human gait patterns for purposes connected to medical diagnoses, rehabilitation, sport, or biometrics.\nSkip OBJECTIVE: Section\nOBJECTIVE:\nThe present paper makes use of ground reaction forces (GRF) generated during human gait to recognize gender or the unilateral load state of a walking person as well as the combination of both of those characteristics.\nSkip METHODS: Section\nMETHODS:\nTo solve the above-stated problem parameters calculated on the basis of all GRF components such as mean, variance, standard deviation of data, peak-to-peak amplitude, skewness, kurtosis, and Hurst exponent as well as leading classification algorithms including kNN, artificial neural networks, decision trees, and random forests, were utilized. Data were collected by means of Kistler\u2019s force plates during a study carried out at the Bialystok University of Technology on a sample of 214 people with a total of 7,316 recorded gait cycles.\nSkip RESULTS: Section\nRESULTS:\nThe best results were obtained with the use of the kNN classifier which recognized the gender of the participant with an accuracy of 99.37%, the unilateral load state with an accuracy reaching 95.74%, and the combination of those two states with an accuracy of 95.31% which, when compared to results achieved by other authors are some of the most accurate.\nSkip CONCLUSION: Section\nCONCLUSION:\nThe study has shown that the given set of parameters in combination with the kNN classifying algorithm allows for an effective automatic recognition of a person\u2019s gender as well as the presence of an asymmetrical load in the form of a hand-carried briefcase. The presented method can be used as a first stage in biometrics systems.","Unexploded ordnance (UXO) dumped in water reservoirs pose a serious environmental and human safety hazard. Various ways of economically solving this problem are being sought. One of them is the use of machine learning methods for the automatic classification of dangerous objects based on the recorded signals. The paper presents the preliminary results on the use of machine learning methods applied to raw magnetometry data generated in a virtual environment based on the concept of a digital twin. This introduces a different approach to a standard approach, which is based on the inverse problem, where the signals are mapped to the magnetic dipole model. Conducted research points out that the highest performance can be obtained with neural networks, and a direct classification based on the raw signals allows to achieve accuracy of up to 93% when no remanent magnetization is present.","Sign Language Recognition (SLR) is a challenging task that aims to bridge the communication gap between the deaf and hearing communities. In recent years, deep learning-based approaches have shown promising results in SLR. However, the lack of interpretability remains a significant challenge. In this paper, we seek to understand which hand and pose MediaPipe Landmarks are deemed the most important for prediction as estimated by a Transformer model. We propose to embed a learnable array of parameters into the model that performs an element-wise multiplication of the inputs. This learned array highlights the most informative input features that contributed to solve the recognition task. Resulting in a human-interpretable vector that lets us interpret the model predictions. We evaluate our approach on public datasets called WLASL100 (SRL) and IPNHand (gesture recognition). We believe that the insights gained in this way could be exploited for the development of more efficient SLR pipelines.","A method for dealing the boundary region in three-way decision theory is proposed. In the three-way decision theory, all the elements are divided into three regions: positive region, negative region and boundary region. Positive region makes a decision of acceptance, negative region makes a decision of rejection. They can generate certain rules. However, boundary region makes a decision of abstaining. They generate uncertain rule. In classification, we always do with the boundary region. In this paper, we propose a method based on tri-training algorithm to reduce the boundary region. In the tri-training algorithm, we build up three classifiers based on three-way decision. We divide all the data into three parts randomly, aiming to keep the three classifiers different. We adopt a voting mechanism to label test samples. Experiments have shown that in most cases, tri-training algorithm is not only benefit for reducing boundary regions but also for improving classification precision. We also find some rules about the parameters alpha and beta how to affect boundary regions and classification precision.","In this research project, we used the financial texts published by the Federal Open Market Committee (FOMC), known as the FOMC Minutes, for sentiment analysis. The pre-trained FinBERT model, a state-of-the-art transformer-based model trained for NLP tasks in finance, was utilized for that. The focus of this research has been on improving the predictive performance of complex financial sentences, as our problem analysis has shown that such sentences pose a significant challenge to existing models. To accomplish this objective the original FinBERT model was fine-tuned for domain-specific sentiment analysis. A strategy, referred to as Sentiment Focus (SF) was utilized to reduce the complexity of sentences, making them more amenable to accurate sentiment predictions.\nTo evaluate the efficacy of our method, we curated a manually labeled test dataset comprising 1375 entries. The results demonstrated an overall improvement of in accuracy when using SF-enhanced fine-tuned FinBERT over the original FinBERT model. In cases of complex sentences containing conjunctions like but, while, and though with contradicting sentiments, our fine-tuned model outperformed the original FinBERT by a margin of .","Data irregularities, such as small disjuncts, class skew and imbalance, and outliers significantly affect the performance of classifiers. In this paper, we focus on identifying small disjuncts, which hitherto, has been addressed mainly by rule-based or inductive algorithms. Small disjuncts have been identified as distribution-based irregularities which provide significant learning, although they cover a subset of examples in the training set, which may be considered as being rare. Such samples are more error-prone than large disjuncts. Eliminating small disjuncts by removal or pruning is seen to affect the learning of the classifier adversely. Widely used non-rule-based learning algorithms like SVM, kNN, Logistic Regression, and Neural networks perform poorly in the presence of small disjuncts in the dataset. In this paper, a novel Sequential Ellipsoidal Partitioning method is proposed to identify small disjuncts in the dataset. This method is a supervised classifier that iteratively partitions the dataset into Minimum Volume Ellipsoids that contain points of the same label; this is performed based on the idea of Reduced Convex Hulls. By allowing an ellipsoid that contains points of one label to contain a few points of the other, such small disjuncts may be identified. As we discuss, the proposed technique is agnostic of underlying data distributions and is applicable as a supervised classifier when the datasets are highly skewed and imbalanced even. We demonstrate the performance of the approach using a few publicly available datasets.","Automatic credit scoring, a crucial risk management tool for banks and financial institutes, has attracted much attention in the past few decades. As such, various approaches have been developed to accurately and efficiently estimate defaults in loan applicants and seamlessly improve and facilitate decision-making in the lending process. However, the imbalanced nature of credit scoring datasets, as well as the heterogeneous nature of features in credit scoring task pose many challenges in developing and implementing effective credit scoring models, targeting the generalization power of classification models on unseen data. To mitigate these challenges, in this paper, we propose the Bagging Supervised Autoencoder Classifier (BSAC). BSAC is a learning model which simultaneously leverages the superior power of supervised autoencoders and representation learning in classification, as well as the Bagging mechanism to handle the irregularities in feature space. Supervised autoencoder has been exploited to learn an optimal latent space from heterogeneous features and perform classification on top of the learned latent space. In particular, the Bagging mechanism has been employed in the learning process to construct various samples of original data to tackle the problem that arises from imbalanced data and irregularities of features in latent space. Extensive experiments on various real-world and benchmark datasets validate the superiority and robustness of the proposed method in predicting the outcome of loan applications.\nHighlights\n\u2022\nA novel credit scoring model using representation, ensemble, and multi-task learning.\n\u2022\nIn BSAC, the learned representations are guided by the label information of samples.\n\u2022\nBSAC outperforms state-of-art baseline models in credit scoring imbalanced data.\n\u2022\nBSAC performs significantly better than the best base classifier in the pool.\n\u2022\nThe model shows a balanced performance in classifying positive and negative samples.","Detecting stance of online posts is a crucial task to understand online content and trends. Existing approaches augment models with complex linguistic features, target-dependent properties, or increase complexity with attention-based modules or pipeline-based architectures. In this work, we propose a simpler multi-task learning framework with auxiliary tasks of subjectivity and sentiment classification. We also analyze the effect of regularization against inconsistent outputs. Our simple model achieves competitive performance with the state of the art in micro-F1 metric and surpasses existing approaches in macro-F1 metric across targets. We are able to show that multi-tasking with a simple architecture is indeed useful for the task of stance classification.","Covid-19 is a serious disease caused by the Sars-CoV-2 virus that has been first reported in China at late 2019 and has rapidly spread around the world. As the virus affects mostly the lungs, chest X-rays are one of the safest and most accessible ways of diagnosing the infection. In this paper, we propose the use of an approach for detecting Covid-19 in chest X-ray images through the extraction and classification of local and global percolation-based features. The method was applied in two datasets: one containing 2,002 segmented samples split into two classes (Covid-19 and Healthy); and another containing 1,125 non-segmented samples split into three classes (Covid-19, Healthy and Pneumonia). The 48 obtained percolation features were given as input to six different classifiers and then AUC and accuracy values were evaluated. We employed the 10-fold cross-validation method and evaluated the lesion sub-types with binary and multiclass classification using the Hermite Polynomial classifier, which had never been employed in this context. This classifier provided the best overall results when compared to other five machine learning algorithms. These results based in the association of percolation features and Hermite polynomial can contribute to the detection of the lesions by supporting specialists in clinical practices.","Deep Learning is more and more used in NLP tasks, such as in relation classification of texts. This paper assesses the impact of syntactic dependencies in this task at two levels. The first level concerns the generic Word Embedding (WE) as input of the classification model, the second level concerns the corpus whose relations have to be classified. In this paper, two classification models are studied, the first one is based on a CNN using a generic WE and does not take into account the dependencies of the corpus to be treated, and the second one is based on a compositional WE combining a generic WE with syntactical annotations of this corpus to classify. The impact of dependencies in relation classification is estimated using two different WE. The first one is essentially lexical and trained on the Wikipedia corpus in English, while the second one is also syntactical, trained on the same previously annotated corpus with syntactical dependencies. The two classification models are evaluated on the SemEval 2010 reference corpus using these two generic WE. The experiments show the importance of taking dependencies into account at different levels in the relation classification.","Mangoes are a common agricultural product in Asia that are sold to other nations. Exported mangoes must meet the standards of different countries, mangoes are classified into different groups for export. A method segmentation for an automatic mango classification system is proposed in this study. The KNN model is applied to segment the mangoes, however, there are many different varieties of mangoes so segmentation is also difficult. Therefore, a self-training model is introduced to increase the accuracy of the KNN model and one can adapt to many mango species. The mangoes are rated by deducting penalty points for failing to meet the requirements that have been established. The system achieved more than 98.7% accuracy for segmentation and 96.67% for the whole classification system.","Highlights\n\u2022\nWe propose a simple and effective framework to solve the multi-label weak-label classification problem via semantic reconstruction and label correlations.\n\u2022\nHigh-order label correlations are introduced into classifiers as encoding matrices combined with feature-label dependency learning to enhance semantic information.\n\u2022\nThe linear classification model is extended to solve the situation that the data is linearly inseparable.\nAbstract\nIn the multi-label classification task, an instance is simultaneously associated with multiple semantic labels. Due to the high complexity of the semantic space in practical applications, obtaining instances with full labels is difficult, leading to the weak-label problem. Existing methods focus on the low-rank and instance manifold regularization assumptions of the label matrix to recover the ground-truth label matrix but ignore the influence that the above assumptions may not hold due to the semantic noise caused by missing labels. To address the problem, this paper proposes a simple and effective method to recover the label space by reconstructing the label semantic space through joint label correlation to solve the multi-label weak-label classification task. Specifically, we leverage the label information consistency and feature-label dependency assumptions to reconstruct the semantic space and consider label correlations to enhance the information of semantic views. Moreover, l 2 , 1-norm is utilized to mitigate the effect of missing label space noise. Additionally, the linear model of the proposed method is expanded to a nonlinear version of the kernel method to address the problem of the inseparability of linear data. Extensive experiments on several real-world tasks show that the proposed method outperforms some state-of-the-art methods.","The importance of facial expressions in nonverbal communication is significant because they help better represent the inner emotions of individuals. Emotions can depict the state of health and internal wellbeing of individuals. Facial expression detection has been a hot research topic in the last couple of years. The motivation for applying the convolutional neural network-10 (CNN-10) model for facial expression recognition stems from its ability to detect spatial features, manage translation invariance, understand expressive feature representations, gather global context, and achieve scalability, adaptability, and interoperability with transfer learning methods. This model offers a powerful instrument for reliably detecting and comprehending facial expressions, supporting usage in recognition of emotions, interaction between humans and computers, cognitive computing, and other areas. Earlier studies have developed different deep learning architectures to offer solutions to the challenge of facial expression recognition. Many of these studies have good performance on datasets of images taken under controlled conditions, but they fall short on more difficult datasets with more image diversity and incomplete faces. This paper applied CNN-10 and ViT models for facial emotion classification. The performance of the proposed models was compared with that of VGG19 and INCEPTIONV3. The CNN-10 outperformed the other models on the CK\u2009+\u2009dataset with a 99.9% accuracy score, FER-2013 with an accuracy of 84.3%, and JAFFE with an accuracy of 95.4%.","Weeds are a significant threat to agricultural production. Weed classification systems based on image analysis have offered innovative solutions to agricultural problems, with convolutional neural networks (CNNs) playing a pivotal role in this task. However, CNNs are limited in their ability to capture global relationships in images due to their localized convolutional operation. Vision Transformers (ViT) and Pyramid Vision Transformers (PVT) have emerged as viable solutions to overcome this limitation. Our study aims to determine the effectiveness of CNN, PVT, and ViT in classifying weeds in image datasets. We also examine if combining these methods in an ensemble can enhance classification performance. Our tests were conducted on significant agricultural datasets, including DeepWeeds and CottonWeedID15. The results indicate that a maximum of 3 methods in an ensemble, with only 15 epochs in training, can achieve high accuracy rates of up to 99.17%. This study demonstrates that high accuracies can be achieved with ease of implementation and only a few epochs.","Aspect-based sentiment classification is an important task in natural language processing research, and in response to the fact that most studies at this stage ignore the influence of contextual semantic information on the sentiment polarity of aspect words, our model proposed in this paper combines local aspect word feature extraction and global contextual semantic information extraction based on Bi-directional Long Short-Term Memory (BiLSTM), and after a multi-headed attention mechanism to enhance the local aspect word sentiment representation. Comparative experiments were conducted on the restaurant and laptop datasets of the SEMEVAL2014 evaluation task. The experimental results show that the model proposed in this paper achieves good classification results in the aspect-level sentiment analysis task of text reviews. The method provides a new idea for ABSA task development.","Text sentiment analysis is an important task in natural language processing (NLP), which aims to determine people's emotional tendency towards a certain topic or event by analyzing the language and emotion in the text. Aiming at the traditional emotion classification model can't fully capture the semantic information implied in short text comments, a two-channel emotion classification model based on CNN and BiLSTMl is proposed.Dynamic allocation weights introduced since the attention mechanism, build fusion BiLSTM and CNN's dual channel neural network architecture, and extract the bureau of emotional characteristics and emotional characteristics as global pay attention to the input feature fusion layer, through your emotions full text feature fusion module integration characteristic information and emotional polarity to break..Compared to the experimental results show that the model of emotion classification performance of the optimum Transformer model, this model (CNN-BiLSTM-AFF) on a public data set senti_weibo_100k accuracy, F1 value, the recall rate of 1.034%, 1.265% and 1.045% respectively.","Highlights\n\u2022\nA novel lossis proposed for few-shot RE to enlarge the inter-class margin.\n\u2022\nTwo-step training improves the model performance in few-shot RE with NOTA detection.\n\u2022\nA SOTA sentence-pair model for few-shot RE with NOTA detection.\nAbstract\nFew-shot relation extraction aims to solve the problem of insufficient annotated data in relation extraction tasks. Through the comparison between samples, few-shot relation extraction achieves lower-cost relation classification. However, most existing methods only do classification within the scope of enumerated relations. For one of the main challenges faced by the application of few-shot relation extraction-the recognition of the none-of-the-above instances, there has been few works on it. In this paper, we propose an angular shrinkage BERT model for the few-shot relation extraction task with none-of-the-above detection, which uses an additive angular loss to enlarge the margins of different classes in the feature space, and obtain highly discriminative features to improve the recognition ability for none-of-the-above instances. Meanwhile, we present a two-stage training strategy to enhance the stability of the performance. We evaluate our model on the most used few-shot relation extraction dataset FewRel. Experimental results show that our approach outperforms previous sentence-pair methods in scenarios containing none-of-the-above instances, and also achieves improvements on the traditional few-shot relation extraction task compared with our baseline model.","The emergence of pre-trained language models (PLMs) has shown great success in many Natural Language Processing (NLP) tasks including text classification. Due to the minimal to no feature engineering required when using these models, PLMs are becoming the de facto choice for any NLP task. However, for domain-specific corpora (e.g., financial, legal, and industrial), fine-tuning a pre-trained model for a specific task has shown to provide a performance improvement. In this paper, we compare the performance of four different PLMs on three public domain-free datasets and a real-world dataset containing domain-specific words, against a simple SVM linear classifier with TFIDF vectorized text. The experimental results on the four datasets show that using PLMs, even fine-tuned, do not provide significant gain over the linear SVM classifier. Hence, we recommend that for text classification tasks, traditional SVM along with careful feature engineering can provide a cheaper and superior performance than PLMs.","In this paper, a new method for the problem of shape representation and classification is proposed. In this method, we define a radius function on the contour of the shape which captures for each point of the boundary, attributes of its related internal part of the shape. We call these attributes as \u201cdepth\u201d of the point. Depths of boundary points generate a descriptor sequence which represents the shape. Matching of sequences is performed using dynamic programming method and a distance measure is acquired. At last, different classes of shapes are classified using a hierarchical clustering method and the distance measure.\nThe proposed method can analyze features of each part of the shape locally which this leads to the ability of part analysis and insensitivity to local deformations such as articulation, occlusion and missing parts. We show high efficiency of the proposed method by evaluating it for shape matching and classification of standard shape datasets.","In multi-label classification, it is critical to capitalize on complicated data structures and semantic relationships. Metric learning serves as an effective strategy to provide a better measurement of distances between examples. Existing works on metric learning for multi-label classification mainly learn one single global metric that characterizes latent semantic similarity between multi-label instances. However, such single-semantics metric exploitation approaches can not capture the intrinsic properties of multi-label data possessed of rich semantics. In this paper, the first attempt towards multi-semantics metric learning for multilabel classification is investigated. Specifically, the proposed LIMIC approach simultaneously learns one global and multiple label-specific local metrics by exploiting label-specific side information. The global metric is learned to capture the commonality across all the labels and label-specific local metrics characterize the individuality of each semantic space. The combination of global metric and label-specific local metrics is utilized to construct latent semantic space for each label, in which similar intra-class instances are pushed closer and interclass instances are pulled apart. Furthermore, a metric-based label correlation regularization is constructed to maintain similarity between correlated label spaces. Extensive experiments on benchmark multi-label data sets validate the superiority of our proposed approach in learning effective distance metrics for multi-label classification.","Feature selection is one of the most significant procedures in machine learning algorithms. It is particularly to improve the performance and prediction accuracy for complex data classification. This paper discusses a hybrid feature selection technique with the decision tree-based classification algorithm. The feature selected using information gain (IG) is combined with the features selected from ReliefF which generates the resultant feature subset. Then the resultant feature subset is in turn combined with a correlation-based feature selection (CFS) method to generate the aggregated feature subset. To perform classification accuracy on the aggregated feature subset, different decision trees-based classification algorithm such as C4.5, decision stumps, naive Bayes tree, and random forest with ten-fold cross-validation have been deployed. To check the prediction accuracy of the proposed work eight different multilevel University of California, Irvine (UCI) machine learning datasets have been used with minimum to maximum numbers of features. The main objective of the hybrid feature selection is to improve the classification accuracy, prediction and to reduce the execution time using standard datasets.","A fit person who is health conscious always considers weighing what they eat and takes the calories on the food they eat. There is a certain number of calories per day that helps bodybuilders or people who want to stay fit. Taking in considerations of eating Fruits to have the calories, macros and nutrients they need. This study presents fruit calorie estimation using CNN or Convolutional Neural Network, the program was able to detect all the fruits with recognition accuracy of 70% and the percentage difference calorie estimation for each fruit are as follows: apple has 30.58%, banana garnered 21.15%, grapes garnered 44.07% and orange has 32.20%.","Diffuse large B-cell lymphoma (DLBCL) is an aggressive and most common type of non-Hodgkin lymphoma. The two major molecular subtypes of DLBCL, i.e. germinal center B-cell-like (GCB) and activated B-cell-like (ABC) types of DLBCL, have different clinical outcomes when treated with combined therapy R-CHOP. Cell-of-origin (COO) is a published prognostic method. Up to now, this classification requires either complex gene expression analysis or multiple immunohistochemistry (IHC) stains requiring expert scoring and assessment. In this paper, we aim to develop an effective and tissue-saving COO classification method based on H&amp;E stained whole slide images (WSIs). Specifically, we develop a new approach named Cellular Features Based Interpretable Network (CellFiNet), by leveraging both interpretable cellular features derived from image tiles and attention based multi-instance learning (AMIL) framework to train a WSI classification model. In comparison with the conventional AMIL approach based on image embeddings derived from convolutional neural networks (CNNs), the proposed approach achieved comparable classification accuracy, while being favorable in terms of explainability, as the model behavior can be interpreted through both attention scores and biologically relevant feature importances at whole slide as well as image tile levels.","In recent years, the prevalence of obesity and its related co-morbidities have been increasing significantly. Therefore, it is an important challenge to pursue an early prediction of obesity risk that could help in reducing the pace of obesity rise when appropriate interventions are placed, accordingly. The prediction and classification of obesity depend on different factors such as body mass index (BMI) and lifestyle aspects, including eating habits. By focusing on these lifestyles and eating habit factors, we can develop a more holistic approach to weight management and prevention of obesity. The aim of this paper is to propose a machine-learning model that can classify weight levels using lifestyle variables without relying on BMI which enables us to investigate how lifestyle factors affect different levels of weight categorization. Although BMI is the most widely used estimation of obesity, there are other factors that can contribute to gaining weight such as lifestyle factors. The accuracy of our lifestyle-based model reached 75% excluding weight, height, and family history. Our model could serve as a starting point for using an interpretable machine learning model to better understand the effect of lifestyle factors on obesity levels.","Emotion classification from text is the process of identifying and classifying emotions expressed in textual data. Emotions can be feelings such as anger, joy, suspense, sadness and neutral. Developing a machine learning model to identify emotions in a low-resourced language with a limited set of linguistic resources and annotated corpora is a challenge. This research proposes a Deep Learning Emotion Classification Framework to identify and classify emotions in low-resourced languages such as Hindi. The proposed framework combines a classification model and a low resource optimization technique in a novel way. An annotated corpus of Hindi short stories consisting of 20,304 sentences is used to train the models for predicting five categories of emotions: anger, joy, suspense, sadness, and neutral talk. To resolve the class imbalance in the dataset SMOTE technique is applied. The optimal classification model is selected through experimentation that compares machine learning models and pre-trained models. Machine learning and deep learning models are SVM, Logistic Regression, Random Forest, CNN, BiLSTM, and CNN+BiLSTM. The pre-trained models, mBERT, IndicBERT, and a hybrid model, mBERT+BiLSTM. The models are evaluated based on macro average recall, macro average precision, and macro average F1 score. Results demonstrate that the hybrid model mBERT+BiLSTM out perform other models with a test accuracy of 57%.","Nowadays, fine-grained Human Activity Recognition (HAR) has become extremely interesting among researchers due to its applications in fields such as healthcare, security, sports, and smart environments. In this paper, we provide a brief overview of the State of the Art approaches in fine-grained human activity recognition. We also discuss the characteristics, complexities, and scarcity of inertial datasets related to fine-grained and coarse-grained activities. To mitigate this scarcity, we collect our inertial dataset, consisting of 17 participants performing 4 fine-grained tasks while interacting with an Inertial Measurement Unit (IMU) sensor embedded in a solid object. Next, we test the most commonly used machine learning classifiers (e.g., kNN, XGboost) on the collected dataset and present the results. Finally, we demonstrate the necessity of a new approach to deal with the recognition of fine-grained activities, and we state our future research directions in this context.","Semi-supervised learning methods constitute a category of machine learning methods which use labelled points together with unlabelled data to tune the classifier. The main idea of the semi-supervised methods is based on an assumption that the classification function should change smoothly over a similarity graph, which represents relations among data points. This idea can be expressed using kernels on graphs such as graph Laplacian. Different semi-supervised learning methods have different kernels which reflect how the underlying similarity graph influences the classification results. In the present work, we analyse a general family of semi-supervised methods, provide insights about the differences among the methods and give recommendations for the choice of the kernel parameters and labelled points. In particular, it appears that it is preferable to choose a kernel based on the properties of the labelled points. We illustrate our general theoretical conclusions with an analytically tractable characteristic example, clustered preferential attachment model and classification of content in P2P networks.","Verifying the robustness of machine learning models against evasion attacks at test time is an important research problem. Unfortunately, prior work established that this problem is NP-hard for decision tree ensembles, hence bound to be intractable for specific inputs. In this paper, we identify a restricted class of decision tree ensembles, called large-spread ensembles, which admit a security verification algorithm running in polynomial time. We then propose a new approach called verifiable learning, which advocates the training of such restricted model classes which are amenable for efficient verification. We show the benefits of this idea by designing a new training algorithm that automatically learns a large-spread decision tree ensemble from labelled data, thus enabling its security verification in polynomial time. Experimental results on public datasets confirm that large-spread ensembles trained using our algorithm can be verified in a matter of seconds, using standard commercial hardware. Moreover, large-spread ensembles are more robust than traditional ensembles against evasion attacks, at the cost of an acceptable loss of accuracy in the non-adversarial setting.","Action recognition has become a prerequisite approach to fluent Human-Robot Interaction (HRI) due to a high degree of movement flexibility. With the improvements in machine learning algorithms, robots are gradually transitioning into more human-populated areas. However, HRI systems demand the need for robots to possess enough cognition. The action recognition algorithms require massive training datasets, structural information of objects in the environment, and less expensive models in terms of computational complexity. In addition, many such algorithms are trained on datasets derived from daily activities. The algorithms trained on non-industrial datasets may have an unfavorable impact on implementing models and validating actions in an industrial context. This study proposed a lightweight deep learning model for classifying low-level actions in an assembly setting. The model is based on optical flow feature elicitation and mobilenetV2-SSD action classification and is trained and assessed on an actual industrial activities\u2019 dataset. The experimental outcomes show that the presented method is futuristic and does not require extensive preprocessing; therefore, it can be promising in terms of the feasibility of action recognition for mutual performance monitoring in real-world HRI applications. The test result shows 80% accuracy for low-level RGB action classes. The study's primary objective is to generate experimental results that may be used as a reference for future HRI algorithms based on the InHard dataset.","Highlights\n\u2022\nA novel framework named graph auxiliary augmentation learning (GAU) is proposed, which co-trains the primary task together with a fine-grained auxiliary classification through a multi-task GNN.\n\u2022\nIt alleviates the sensitivity of the model to the pseudo-label quality and reduces the model degradation due to the accumulative error of the pseudo-labels.\n\u2022\nThe fine-grained auxiliary classification task helps to learn better node representations from a different view, thereby boosting the performance of the primary task.\n\u2022\nIt is architecture-agnostic so that it can be applied to any variant of GNN. Experiments show that it can achieve superior performance on different architectures when compared with other state-of-the-art methods.\nAbstract\nNode classification has become an important research topic in recent years. Since there are always a few training samples, researchers improve the performance by properly leveraging the predictions of unlabeled nodes during training. However, suffering from the model degradation resulted from the accumulative error of pseudo-labels, there is limited improvement. In this paper we present fine-grained Graph Auxiliary aUgmentation (GAU). It trains the primary task together with an automatically created auxiliary task which is a fine-grained node classification task. And an auxiliary augmentation strategy is designed to enlarge the labeled set for the auxiliary task by utilizing the pseudo-labels of the primary task. Comprehensive experiments show that GAU alleviates the sensitivity of the model to the pseudo-label quality, so more unlabeled nodes can participate in the training. From the perspective of co-training, the fine-grained auxiliary task which is trained by much more unlabeled nodes helps to learn better node representations from a different view, thereby boosting the final performance. Extensive experiments verify the superior performance of the GAU on different GNN architectures when compared with other state-of-the-art approaches.","The use of deep learning makes it possible to achieve extraordinary results in all kinds of tasks related to computer vision. However, this performance is strongly related to the availability of training data and its relationship with the distribution in the eventual application scenario. This question is of vital importance in areas such as robotics, where the targeted environment data are barely available in advance. In this context, domain adaptation (DA) techniques are especially important to building models that deal with new data for which the corresponding label is not available. To promote further research in DA techniques applied to robotics, this work presents Kurcuma (Kitchen Utensil Recognition Collection for Unsupervised doMain Adaptation), an assortment of seven datasets for the classification of kitchen utensils\u2014a task of relevance in home-assistance robotics and a suitable showcase for DA. Along with the data, we provide a broad description of the main characteristics of the dataset, as well as a baseline using the well-known domain-adversarial training of neural networks approach. The results show the challenge posed by DA on these types of tasks, pointing to the need for new approaches in future work.","This paper presents a data pre-processing algorithm to tackle class imbalance in classification problems by undersampling the majority class. It relies on a formalism termed Presumably Correct Decision Sets aimed at isolating easy (presumably correct) and difficult (presumably incorrect) instances in a classification problem. The former are instances with neighbors that largely share their class label, while the latter have neighbors that mostly belong to a different decision class. The proposed algorithm replaces the presumably correct instances belonging to the majority decision class with prototypes, and it operates under the assumption that removing these instances does not change the boundaries of the decision space. Note that this strategy opposes other methods that remove pairs of instances from different classes that are each other\u2019s closest neighbors. We argue that the training and test data should have similar distribution and complexity and that making the decision classes more separable in the training data would only increase the risks of overfitting. The experiments show that our method improves the generalization capabilities of a baseline classifier, while outperforming other undersampling algorithms reported in the literature.","Pneumonia is an important threat to human health, and different types of pneumonia have different treatment options, so the prediction and classification of pneumonia are important health issues. In this paper, chest X-ray images are used as data, and the final ensemble model can achieve excellent performance on these two tasks. In addition, this paper introduces the InceptionNeXt model to pneumonia prediction and classification problems for the first time, and finds that different model convolution kernels and perception fields may be more suitable for different medical image research tasks.","Agriculture is an important sector in India, and about 58% of the Indian population depends on it. This is why it is paramount it remains profitable and provides a high yield. One of the problems that lead to reduced productivity is the selection of the wrong crop. For maximum productivity, every crop needs specific environmental conditions like soil quality, water, etc. In our work, we have used various Machine learning techniques and based on their comparative analysis adopted the best model to predict the most suitable crop for a particular soil sample based on parameters like Nitrogen, Potassium, Phosphorus, ph. level, rainfall, temperature, and humidity. The dataset is pre-processed and optimized using pre-processing techniques. We have reviewed existing algorithms such as Decision Trees, Naive Bayes, Support Vector Machine (SVM), K Nearest Neighbor (KNN), and Random Forest to predict the most suitable crop and found Naive Bayes Classifier to be the best model, based on performance metrics of precision, recall, accuracy and F1 score.","In this modern scientific digital world, credit card usage has enormously increased everyday. Simultaneously a huge amount of credit card misuse also has been expressively popular. It prompts monetary misfortunes for both charge cardholders and monetary associations. To keep away from that, monetary associations created and conveyed Visa extortion discovery techniques. In the upcoming years, everybody will utilise the greatest exchange through online mode just to save their time. So we partition this review into two primary parts. In the first part, we centre around old-style AI models, and in this part we focus on what the client knows (knowledge-based strategy). For the second part, we focus more on the turn of events procedure of client verification, and their conduct biometrics to distinguish an individual remarkable conduct while utilising their electronic gadgets. An outline of the current methodology in this writing review means to grow a more precise, dependable, versatile, super-fast, effective, and modest model of charge card extortion identification.","Image classification is a critical topic due to its wide application and several challenges associated. Despite the huge progress made last decades, there is still a demand for context-aware image representation approaches capable of taking into the dataset manifold for improving classification accuracy. In this work, a representation learning approach is proposed, based on a novel feature augmentation strategy. The proposed method aims to exploit available contextual similarity information through rank-based manifold learning used to define and assign weights to samples used in augmentation. The approach is validated using CNN-based features and LSTM models to achieve even higher accuracy results on image classification tasks. Experimental results show that the feature augmentation strategy can indeed improve the accuracy of results on widely used image datasets (CIFAR10, Stanford Dogs, Linnaeus5, Flowers102 and Flowers17) in different CNNs (ResNet152, VGG16, DPN92). The results indicate gains up to 20% and show the potential of the developed approach in achieving higher accuracy results for image classification.\nHighlights\n\u2022\nA novel feature augmentation approach guided by ranking is proposed.\n\u2022\nEffective rankings were defined through a manifold learning method.\n\u2022\nAn association with Long Short-Term Memory (LSTM) algorithm is employed.\n\u2022\nThe combined strategy is evaluated in several image datasets.\n\u2022\nExperimental results demonstrated accuracy gains over different CNN-based features.","Recent research has shown that artificial intelligence (AI) models can exhibit bias in performance when trained using data that are imbalanced by protected attribute(s). Most work to date has focused on deep learning models, but classical AI techniques that make use of hand-crafted features may also be susceptible to such bias. In this paper we investigate the potential for race bias in random forest (RF) models trained using radiomics features. Our application is prediction of tumour molecular subtype from dynamic contrast enhanced magnetic resonance imaging (DCE-MRI) of breast cancer patients. Our results show that radiomics features derived from DCE-MRI data do contain race-identifiable information, and that RF models can be trained to predict White and Black race from these data with 60\u201370% accuracy, depending on the subset of features used. Furthermore, RF models trained to predict tumour molecular subtype using race-imbalanced data seem to produce biased behaviour, exhibiting better performance on test data from the race on which they were trained.","Some subtle features of planting structures in irrigation areas could only be visible on high-resolution panchromatic spectral images. However, low spatial resolution multispectral image makes it hard to recognize them. It is challenging to accurately obtain crop planting structure when using traditional methods. This paper proposes an extraction method of crop planting structure based on image fusion and U-Net depth semantic segmentation network, which can automatically and accurately extract multi-category crop planting structure information. This method takes Landsat8 commercial multispectral satellite data set as an example, chooses RGB pseudo-color synthetic image which highlights vegetation characteristics, and uses HLS(Hue, Luminance, Saturation), NND(Nearest-Neighbor Diffusion) and G-S(Gram-Schmidt) methods to fuse panchromatic band to obtain 15m high-resolution fusion image to obtain training set and test set, six types of land features including cities and rivers were labeled by manual to obtain the verification set. The training and validation sets are cut and enhanced to train the U-Net semantic segmentation network. Taking the Xiaokaihe irrigation area in Binzhou City, Shandong Province, China, as an example, the planting structure was classified, and the overall accuracy was 87.7%, 91.2%, and 91.3%, respectively. The accuracy of crop planting structures (wheat, cotton, woodland) was 74.2%, 82.5%, 82.3%, and the Kappa coefficient was 0.832, 0.880, and 0.881, respectively. The results showed that the NND-UNet method was suitable for large-scale continuous crop types (wheat, cotton), and the GS-UNet method had a better classification effect in discrete areas of cash crops (Jujube and many kinds of fruit trees).","In the Class Incremental Learning (CIL) setup, a learning model must have the ability to incrementally update its knowledge to recognize newly appeared classes (plasticity) while maintaining the knowledge to recognize the classes it has already learned (stability). Such conflicting requirements are known as the stability-plasticity dilemma, and most existing studies attempt to achieve a good balance between them by stability improvements. Unlike those attempts, we focus on the generality of representations. The basic idea is that a model does not need to change if it has already learned such general representations that they contain enough information to recognize new classes. However, the general representations are not optimal for recognizing the classes a model has already learned because the representations must contain unrelated and noisy information for recognizing them. To acquire representations suitable for recognizing known classes while leveraging general representations, in this paper, we propose a new CIL framework that learns general representations and transforms them into suitable ones for the target classification tasks. In our framework, we achieve the acquisition of general representations and their transformation by self-supervised learning and attention techniques, respectively. In addition, we introduce a novel knowledge distillation loss to make the transformation mechanism stable. Using benchmark datasets, we empirically confirm that our framework can improve the average incremental accuracy of four types of CIL methods that employ knowledge distillation in the CIL setting.","Highlights\n\u2022\nA novel classification method named projection twin extreme learning machine (PTELM) is proposed.\n\u2022\nA new method termed as capped L1-norm projection twin extreme learning machine (CL1-PTELM) is proposed by introducing capped L1-norm distance metric into PTELM.\n\u2022\nTwo efficient algorithms are designed for our PTELM and CL1-PTELM, and two theorems are proved for CL1-PTELM\u2019s convergence and local optimality.\n\u2022\nSufficient experiments on multiple datasets demonstrate the effectiveness of the proposed methods.\nAbstract\nIn this paper, we incorporate the idea of projection twin support vector machines (PTSVM) into the basic framework of twin extreme learning machines (TELM) and first propose a novel binary classifier named projection twin extreme learning machines (PTELM). PTELM is to seek two projection directions in the TELM feature space, such that the projected samples of one class are well separated from those of the other class. Compared with the PTSVM, PTELM tackles nonlinear cases without using several fixed kernel functions, thus PTELM is less sensitive to use specified parameters and can get better classification accuracy. Then, a new capped L 1-norm PTELM (CL 1-PTELM) is proposed by introducing capped L 1-norm distance metric in PTELM to reduce the effect of outliers. CL 1-PTELM overcomes the disadvantages of L 2-norm distance metric and hinge loss. Thus, CL 1-PTELM enhances the robust performance of our PTELM. Finally, two effective algorithms are designed to solve the problem of PTELM and to deal with the challenge of CL 1-PTELM brought by non-convex optimization problem, respectively. Simultaneously, we theoretically prove the convergence and local optimality of CL 1-PTELM algorithm. Numerical experiments on three synthetic datasets and several UCI datasets show the feasibility and effectiveness of our proposed methods.","Extreme Multi-label Text Classification (XMTC) is a key task of finding the most relevant labels from a large label set for a document. Although some deep learning-based methods have shown great success in XMTC, they still suffer from the following drawbacks. First, although several methods have improved the precision by clustering labels and combining several sub-models to train and predict for one dataset, they were not ideal in terms of computational efficiency. Second, most of those methods need a low dimensional bottleneck layer before the output layer to compress the feature representations to fit the GPU memory, which results in information loss of original features. In this paper, we proposed a novel two-stage XMTC framework with candidate Retrieving and deep Ranking (XRR) to address those drawbacks. In the retrieving stage, we designed two retrieval strategies, including an aligning Point Mutual Information (aPMI) method, and a Unified Label-Semantic Embedding (ULSE) method, to extract hundreds of candidates from massive labels. In the ranking stage, we presented a deep ranking model using a pre-trained transformer to distinguish the true labels from candidates. Extensive experiments show that XRR outperforms the state-of-the-art methods on five widely used multi-label datasets.","Improving the fairness of machine learning models is a nuanced task that requires decision makers to reason about multiple, conflicting criteria. The majority of fair machine learning methods transform the error-fairness trade-off into a single objective problem with a parameter controlling the relative importance of error versus fairness. We propose instead to directly optimize the error-fairness tradeoff by using multi-objective optimization. We present a flexible framework for defining the fair machine learning task as a weighted classification problem with multiple cost functions. This framework is agnostic to the underlying prediction model as well as the metrics. We use multiobjective optimization to define the sample weights used in model training for a given machine learner, and adapt the weights to optimize multiple metrics of fairness and accuracy across a set of tasks. To reduce the number of optimized parameters, and to constrain their complexity with respect to population subgroups, we propose a novel meta-model approach that learns to map protected attributes to sample weights, rather than optimizing those weights directly. On a set of real-world problems, this approach outperforms current state-of-the-art methods by finding solution sets with preferable error/fairness trade-offs.","Event detection (ED) aims to identify events of interest described in the text. With the current explosive growth of text data on the internet, ED is increasingly practical and has gained many researchers\u2019 attention. The existing works usually design ED as a token-level multi-class classification task. In this setting, given a sentence, ED models\u2019 prediction for each token is relatively independent and thus cannot fully utilize sentence-level information and the association relations between multiple events in this sentence. To handle these situations, this paper proposes a multi-task learning based event detection model, which introduces an event type oriented text classification as an auxiliary task to improve the model\u2019s understanding of sentence-level information. In addition, this model utilizes a Conditional Random Field (CRF) to explore the correlations between various event types and constrain the model\u2019s output space. Experimental comparisons with state-of-the-art baselines on DuEE dataset demonstrate the model\u2019s effectiveness.","Canonical Correlation Analysis (CCA) has been widely used in Steady-State Visually Evoked Potential (SSVEP) analysis, but there are still challenges in this research area, specifically regarding data quality and insufficiency. In contrast to most previous studies that primarily concentrate on the development of spatial or spectral templates for SSVEP data, this paper proposes a novel temporal filtering method based on a reinforcement learning (RL) algorithm for CCA on SSVEP data. The proposed method leverages RL to automatically and precisely detect and filter low-quality segments in the SSVEP data, thereby improving the accuracy of CCA. Additionally, the proposed RL-based Temporal Filtering is algorithm-independent and compatible with various CCA algorithms. The RL-based Temporal Filtering is evaluated using a wearable dataset consisting of 102 subjects. The experimental results demonstrate significant advancements in CCA accuracy, particularly when combined with the extended CCA (ECCA) algorithm. In addition to performance enhancement, the RL-based Temporal Filtering method provides visualizable filters, which can ensure the transparency of the filtering process and the reliability of the obtained results. By addressing data quality and insufficiency concerns, this novel RL-based Temporal Filtering approach demonstrates promise in advancing SSVEP analysis for various applications.","Multi-label Aspect Category Detection (MACD) is essential for aspect-based sentiment analysis, which aims to identify multiple aspect categories in a given sentence. Few-shot MACD is critical due to the scarcity of labeled data. However, MACD is a high-noise task, and existing methods fail to address it with only two or three training samples per class, which limits the application in practice. To solve above issues, we propose a group of Few-shot Sample-set Operations (FSO) to solve noisy MACD in fewer sample scenarios by identifying the semantic contents of samples. Learning interactions among intersection, subtraction, and union networks, the FSO imitates arithmetic operations on samples to distinguish relevant and irrelevant aspect contents. Eliminating the negative effect caused by noises, the FSO extracts discriminative prototypes and customizes a dedicated query vector for each class. Besides, we develop a multilabel architecture, which integrates with score-wise loss and multi-label loss to optimize the FSO for multilabel prediction, avoiding complex threshold training or selection. Experiments show that our method achieves considerable performance. Significantly, it improves by 11.01% at most and an average of 8.59% Macro-F in fewer sample scenarios.","Language Identification (LID) refers to the task of identifying an unknown language from the test utterances. In this paper, a new feature set, viz.,T-MFCC by amalgamating Teager Energy Operator (TEO) and well-known Mel frequency cepstral coefficients (MFCC) is developed. The effectiveness of the newly derived feature set is demonstrated for identifying perceptually similar Indian languages such as Hindi and Urdu. The modified structure of polynomial classifier of 2nd and 3rd order approximation has been used for the LID problem. The results have been compared with state-of-the art feature set, viz.,MFCC and found to be effective (an average jump 21.66%) in majority of the cases. This may be due to the fact that the T-MFCC represents the combined effect of airflow properties in the vocal tract (which are known to be language and speaker dependent) and human perception process for hearing.","Highlights\n\u2022\nLocal spectrum modules are constructed to observe the local changes of spectral values.\n\u2022\nBinary feature learning model with spectral context-aware attention is built to obtain the binary form.\n\u2022\nA proximal alternating optimization strategy is proposed.\nAbstract\nThe classification of hyperspectral images (HSIs) has achieved success in applications. For many approaches, features are directly extracted from whole spectral pixels, which can not well describe local characteristics. These methods are also susceptible to noise since each feature code is learned individually. Accordingly, a binary feature learning method with local spectral context-aware attention (BFLSC) is proposed for the classification. Specifically, for training samples, we first build the local spectrum models (LSMs) to describe local spectral properties, where each training sample is segmented into some parts and the difference between the central value and its neighborhoods is calculated in each part. Then, we construct the BFLSC model to learn a projection and binary features of training samples. In such model, the spectral context-awareness attention is established to collaboratively learn binary feature codes by enforcing one shift between 0/1 of each LSM, which enhances the robustness and stability of binary leaning. We also introduce the loss constraint, even distribution constraint, and variance constraint to reduce information loss and improve the quality of learned feature distribution. Additionally, an optimization scheme is designed to obtain the solution of the BFLSC model. Further, the learned binary features are added to train the support vector machine (SVM). For each testing sample, the LSMs are first extracted, and then mapped into binary features by the learned projection. The trained SVM is finally used for the mapped binary features to predict the label of the testing sample. Experimental results validate that our BFLSC realizes the better performance compared with some advanced approaches.","Deploying a person re-identification (Re-ID) system in a real scenario requires adapting a model trained on one labeled dataset to a different environment, with no person identity information. This poses an evident challenge that can be faced by unsupervised domain adaptation approaches. Recent state-of-the-art methods adopt architectures composed of multiple models (a.k.a. experts), and transfer the learned knowledge from the source domain by clustering and assigning hard pseudo-labels to unlabeled target data. While this approach achieves outstanding accuracy, the clustering procedure is typically sub-optimal, and the experts are simply combined to learn in a collaborative way, thus limiting the final performance. In order to mitigate the effects of noisy pseudo-labels and better exploit experts\u2019 knowledge, we propose to combine soft supervision techniques in a novel multi-expert domain adaptation framework. We introduce a novel weighting mechanism for soft supervisory learning, named Online Batch Confidence, which takes into account expert reliability in an online per-batch basis. We conduct experiments across popular cross-domain Re-ID benchmarks proving that our model outperforms the current state-of-the-art results.","The K-nearest neighbor interpolation method was used to fill in missing data of five indicators of coronary heart disease, diabetes, total cholesterol, triglycerides, and albumin;, and the SMOTE algorithm was used to balance the number of variable indicators. The Relief-F algorithm was used to remove 18 variable indicators and retain 42 variable indicators. LASSO and ridge regression algorithms were used to remove eight variable indicators and retain 52 variable indicators; The prediction accuracy, recall, and AUC values of the linear kernel support vector machine model filtered using Relief-F and LASSO features are high, and the prediction results are optimal; The test result of random forest screened by Relief-F and LASSO features is better than that of the support vector machine model. It is concluded that the random forest model screened by Relief-F features is better as a prediction of lung cancer typing. The research results provide theoretical data support for predicting lung cancer classification using machine learning methods.","State-of-the-art weakly supervised text classification methods, while significantly reduced the required human supervision, still requires the supervision to cover all the classes of interest. This is never easy to meet in practice when human explore new, large corpora without complete pictures. In this paper, we work on a novel yet important problem of weakly supervised open-world text classification, where supervision is only needed for a few examples from a few known classes and the machine should handle both known and unknown classes in test time. General open-world classification has been studied mostly using image classification; however, existing methods typically assume the availability of sufficient known-class supervision and strong unknown-class prior knowledge (e.g., the number and/or data distribution). We propose a novel framework \u00f8ur that lifts those strong assumptions. Specifically, it follows an iterative process of (a) clustering text to new classes, (b) mining and ranking indicative words for each class, and (c) merging redundant classes by using the overlapped indicative words as a bridge. Extensive experiments on 7 popular text classification datasets demonstrate that \u00f8ur outperforms strong baselines consistently with a large margin, attaining 23.33% greater average absolute macro-F1 over existing approaches across all datasets. Such competent accuracy illuminates the practical potential of further reducing human effort for text classification.","Imbalanced sample distribution is usually the main reason for the performance degradation of machine learning algorithms. Based on this, this study proposes a hybrid framework (RGAN-EL) combining generative adversarial networks and ensemble learning method to improve the classification performance of imbalanced data. Firstly, we propose a training sample selection strategy based on roulette wheel selection method to make GAN pay more attention to the class overlapping area when fitting the sample distribution. Secondly, we design two kinds of generator training loss, and propose a noise sample filtering method to improve the quality of generated samples. Then, minority class samples are oversampled using the improved RGAN to obtain a balanced training sample set. Finally, combined with the ensemble learning strategy, the final training and prediction are carried out. We conducted experiments on 41 real imbalanced data sets using two evaluation indexes: F1-score and AUC. Specifically, we compare RGAN-EL with six typical ensemble learning; RGAN is compared with three typical GAN models. The experimental results show that RGAN-EL is significantly better than the other six ensemble learning methods, and RGAN is greatly improved compared with three classical GAN models.","This paper presents a new corpus of radiology medical reports written in Spanish and labeled with ICD-10. CARES (Corpus of Anonymised Radiological Evidences in Spanish) is a high-quality corpus manually labeled and reviewed by radiologists that is freely available for the research community on HuggingFace. These types of resources are essential for developing automatic text classification tools as they are necessary for training and tuning computational systems. However, in the medical domain these are very difficult to obtain for different reasons including privacy and data protection issues or the involvement of medical specialists in the generation of these resources. We present a corpus labeled and reviewed by radiologists in their daily practice that is available for research purposes. In addition, after describing the corpus and explaining how it has been generated, a first experimental approach is carried out using several machine learning algorithms based on transformer language models such as BioBERT and RoBERTa to test the validity of this linguistic resource. The best performing classifier achieved 0.8676 micro and 0.8328 macro f1-score and these results encourage us to continue working in this research line.\nHighlights\n\u2022\nA freely-available corpus of radiological reports with ICD-10 codes is presented\n\u2022\nReports labeled with Z03.9 sub-code enable systems to classify normality cases\n\u2022\nReport classification into ICD-10 chapters is tackled with 0.83 macro F1-score","Data augmentation is a ubiquitous technique for improving image classification when labeled data is scarce. Constraining the model predictions to be invariant to diverse data augmentations effectively injects the desired representational invariances to the model (e.g., invariance to photometric variations) and helps improve accuracy. Compared to image data, the appearance variations in videos are far more complex due to the additional temporal dimension. Yet, data augmentation methods for videos remain under-explored. This paper investigates various data augmentation strategies that capture different video invariances, including photometric, geometric, temporal, and actor/scene augmentations. When integrated with existing semi-supervised learning frameworks, we show that our data augmentation strategy leads to promising performance on the Kinetics-100/400, Mini-Something-v2, UCF-101, and HMDB-51 datasets in the low-label regime. We also validate our data augmentation strategy in the fully supervised setting and demonstrate improved performance.\nHighlights\n\u2022\nWe study strong video data augmentations for data-efficient action recognition.\n\u2022\nWe introduce ActorCutMix to encourage scene invariance for action recognition.\n\u2022\nOur strong data augmentation strategies significantly improve the data efficiency.\n\u2022\nOur source code and pre-trained models are publicly available at https://github.com/vt-vl-lab/video-data-aug.","In this paper, we propose a method exploiting syntactic structure, semantic relations and word embeddings for recognizing textual entailment. The sentence pairs are analyzed using their syntactic structure and categorization of sentences in active voice, sentences in passive voice and sentences holding copular relations. The main syntactic relations such as subject, verb and object are extracted and lemmatized using a lemmatization algorithm based on parts-of-speech. The subject-to-subject, verb-to-verb and object-to-object similarity is identified using enhanced Wordnet semantic relations. Further similarity is analyzed using modifier relation, number relation, nominal modifier relation, compound relation, conjunction relation and negative relation. The experimental evaluation of the method on Stanford Natural Language Inference dataset shows that the accuracy of the method is 1.4% more when compared to the state-of-the-art zero shot domain adaptation methods.","Adequate selection of features may improve accuracy and efficiency of classifier methods. There are two main approaches for feature selection: wrapper methods, in which the features are selected using the classifier, and filter methods, in which the selection of features is independent of the classifier used. Although the wrapper approach may obtain better performances, it requires greater computational resources. For this reason, lately a new paradigm, hybrid approach, that combines both filter and wrapper methods has emerged. One of its problems is to select the filter method that gives the best relevance index for each case, and this is not an easy to solve question. Different approaches to relevance evaluation lead to a large number of indices for ranking and selection. In this paper, several filter methods are applied over artificial data sets with different number of relevant features, level of noise in the output, interaction between features and increasing number of samples. The results obtained for the four filters studied (ReliefF, Correlation-based Feature Selection, Fast Correlated Based Filter and INTERACT) are compared and discussed. The final aim of this study is to select a filter to construct a hybrid method for feature selection.","In the recommendation systems, there are multiple business domains to meet the diverse interests and needs of users, and the click-through rate(CTR) of each domain can be quite different, which leads to the demand for CTR prediction modeling for different business domains. The industry solution is to use domain-specific models or transfer learning techniques for each domain. The disadvantage of the former is that the data from other domains is not utilized by a single domain model, while the latter leverage all the data from different domains, but the fine-tuned model of transfer learning may trap the model in a local optimum of the source domain, making it difficult to fit the target domain. Meanwhile, significant differences in data quantity and feature schemas between different domains, known as domain shift, may lead to negative transfer in the process of transferring. To overcome these challenges, we propose the Collaborative Cross-Domain Transfer Learning Framework (CCTL). CCTL evaluates the information gain of the source domain on the target domain using a symmetric companion network and adjusts the information transfer weight of each source domain sample using the information flow network. This approach enables full utilization of other domain data while avoiding negative migration. Additionally, a representation enhancement network is used as an auxiliary task to preserve domain-specific features. Comprehensive experiments on both public and real-world industrial datasets, CCTL achieved SOTA score on offline metrics. At the same time, the CCTL algorithm has been deployed in Meituan, bringing 4.37% CTR and 5.43% GMV lift, which is significant to the business.","Imbalanced datasets are prominent in real-world problems. In such problems, the data samples in one class are significantly higher than in the other classes, even though the other classes might be more important. The standard classification algorithms may classify all the data into the majority class, and this is a significant drawback of most standard learning algorithms, so imbalanced datasets need to be handled carefully. One of the traditional algorithms, twin support vector machines (TSVM), performed well on balanced data classification but poorly on imbalanced datasets classification. In order to improve the TSVM algorithm\u2019s classification ability for imbalanced datasets, recently, driven by the universum twin support vector machine (UTSVM), a reduced universum twin support vector machine for class imbalance learning (RUTSVM) was proposed. The dual problem and finding classifiers involve matrix inverse computation, which is one of RUTSVM\u2019s key drawbacks. In this paper, we improve the RUTSVM and propose an improved reduced universum twin support vector machine for class imbalance learning (IRUTSVM). We offer alternative Lagrangian functions to tackle the primal problems of RUTSVM in the suggested IRUTSVM approach by inserting one of the terms in the objective function into the constraints. As a result, we obtain new dual formulation for each optimization problem so that we need not compute inverse matrices neither in the training process nor in finding the classifiers. Moreover, the smaller size of the rectangular kernel matrices is used to reduce the computational time. Extensive testing is carried out on a variety of synthetic and real-world imbalanced datasets, and the findings show that the IRUTSVM algorithm outperforms the TSVM, UTSVM, and RUTSVM algorithms in terms of generalization performance.","The main function of the internal control department of a bank is to inspect the banking operations to see if they are performed in accordance with the regulations and bank policies. To accomplish this, they pick up a number of operations that are selected randomly or by some rule and, inspect those operations according to some predetermined check lists. If they find any discrepancies where the number of such discrepancies are in the magnitude of several hundreds, they inform the corresponding department (usually bank branches) and ask them for a correction (if it can be done) or an explanation. In this study, we take up a real-life project carried out under our supervisory where the aim was to develop a set of predictive models that would highlight which operations of the credit department are more likely to bear some problems. This multi-classification problem was very challenging since the number of classes were enormous and some class values were observed only a few times. After providing a detailed description of the problem we attacked, we describe the detailed discussions which in the end made us to develop six different models. For the modeling, we used the logistic regression algorithm as it was preferred by our partner bank. We show that these models have Gini values of 51 per cent on the average which is quite satisfactory as compared to sector practices. We also show that the average lift of the models is 3.32 if the inspectors were to inspect as many credits as the number of actual problematic credits.","For imbalanced binary classification, most methods classify data in the original space by balancing data or modifying classification algorithms. However, for datasets with severe overlaps and complex class boundaries, the classification performance is limited by the ability of the classifier to fit the decision boundary. A practical alternative is to map the data to a latent space with lower classification difficulty. In this paper, we propose an imbalanced binary classification method via space mapping using normalizing flows with class discrepancy constraints. The flow-based model is employed to map the original data to a latent space, ensuring the accuracy of the mapping and the authenticity of the mapped distribution. To solve the problem of overlaps and within-class imbalance, class discrepancy constraints, including global and local constraints, are proposed to modify the flow-based model. The former maps different subclusters to the same cluster of the latent space, and the latter increases the separability of different classes. Classification thus can be carried out in the latent space with simpler distribution and high separability to achieve better classification performance. Experimental results on 35 KEEL and UCI public datasets indicate that the proposed method outperforms other state-of-the-art methods on F1-measure and G-mean.","In this paper, we present a new approach for hyperspectral image classification. The pixels\u2019 spectra are grouped into clusters in an unsupervised manner using an improved version of plane based clustering. Since the pixels containing the same substances are linearly correlated, the proposed plane-based clustering can effectively group the data points. Plane-based clustering is a more appropriate choice than point based clustering schemes for grouping the datasets which are distributed around hyperplanes instead of hyperspheres. Then, Kernel Principal Component Analysis (KPCA) is applied to each cluster individually to obtain multiple kernel vectors for each data point. Applying non-linear kernels, can greatly increase the discrimination power of the acquired features. The feature vectors are extracted by a weighted linear combination of the kernel components obtained from each cluster. We compute optimal weights using the cluster hyperplane parameters. Since the whole procedure is performed in an unsupervised manner, the proposed approach can enhance the generalization power of the extracted features. Then, morphological attribute filters are applied to the feature maps to effectively utilize spatial relations. Hence, the acquired compact feature vectors include both spectral and spatial information. SVM is used for classification. The experiments performed on three well-known hyperspectral datasets reveal the effectiveness of the proposed feature extraction approach.","Time series classification is a growing problem in different disciplines due to the progressive digitalization of the world. The best state-of-the-art algorithms focus on performance, seeking the best possible results, leaving interpretability at a second level, if any. Furthermore, interpretable proposals are far from providing competitive results. In this work, focused on time series classification, we propose a new representation of time series based on a robust and complete set of features. This new representation allows extracting more meaningful information on the underlying time series structure to develop effective classifiers whose results are much easier to interpret than current state-of-the-art models. The proposed feature set allows using the traditional vector-based classification algorithms in time series problems, significantly increasing the number of techniques available for this type of problem. To evaluate the performance of our proposal, we have used the state-of-the-art repository of time series classification, UCR, composed of 112 datasets. The experimental results show that through this representation, more interpretable classifiers can be obtained which are competitive. More specifically, they obtain no statistically significant differences from the second and third-best models of the state-of-the-art. Apart from competitive results in accuracy, our proposal is able to improve the model interpretability based on the set of features proposed.\nHighlights\n\u2022\nAllows the use of traditional vector-based algorithms on time series problems.\n\u2022\nA feature-based approach to improve the interpretability of models.\n\u2022\nNon-specific time series models achieve results of best state-of-the-art algorithms.","Highlights\n\u2022\nWe propose ReNAP for few-shot classification.\n\u2022\nThe APL module of ReNAP can learn class prototypes adaptively.\n\u2022\nExperimental results demonstrate the superior classification performance of ReNAP.\nAbstract\nTraditional deep learning-based image classification methods often fail to recognize a new class that does not exist in the training dataset, particularly when the new class only has a small number of samples. Such a challenging and new learning problem is referred to as few-shot learning. In few-shot learning, the relation network (RelationNet) is a powerful method. However, in RelationNet and its state-of-the-art variants, the prototype of each class is obtained by a simple summation or average over the labeled samples. These simple sample statistics cannot accurately capture the distinct characteristics of the diverse classes of real-world images. To address this problem, in this paper, we propose the Relation Network with Adaptive Prototypical Learning method (ReNAP), which can learn the class prototypes adaptively and provide more accurate representations of the classes. More specifically, ReNAP embeds an adaptive prototypical learning module constructed by a convolutional network into RelationNet. Our ReNAP achieves superior classification performances to RelationNet and other state-of-the-art methods on four widely used benchmark datasets, FC100, CUB-200-2011, Stanford-Cars, and Stanford-Dogs.","Depression is a common and debilitating mental illness. Given the shortage of mental health professionals, there are delays in depression detection. Interviews conducted by virtual agents could expedite depression screenings. While the interview audio and transcript have received more attention, facial features offer an attractive privacy-preserving screening modality. Thus, we conduct a comprehensive comparative evaluation of the effectiveness of temporal facial features to screen for depression. We extract time series of eye gaze, landmark, and action unit features from video responses to 15 clinical interview questions. We input them into CNN, LSTM, and recurrent convolutional neural network (RCNN) models. An extra attention layer proved critical for CNN and LSTM performance. For a general wellbeing question, eye gaze features screened for depression with an F1 of 0.81. Our study informs the use of temporal facial features in future digital mental illness screening technologies.","We are interested in analyzing specific model behaviors such as the model output(s) values within a given cluster using the probability theory. By showing the relevance of the multivariate weighted distribution theory to characterize some behaviors of interest thanks to the weight functions, we derive the distribution function of the model inputs that complies with a specific model behavior (called target inputs distribution). The target inputs lead to the target output(s) using the model, including clustering outcomes of rule-based ensembles, random forest, kernel-based learning and fuzzy clustering. Two expressions of that distribution are provided, including the copula. Conditional moments of the target output(s) and dependency models of the target inputs are also derived for the purpose of uncertainty quantification and sensitivity analysis. For computing such quantities of interest, consistent estimators and detailed illustrations of our approach are provided, including a real case study.","Detection of out-of-distribution (OoD) samples in the context of image classification has recently become an area of interest and active study, along with the topic of uncertainty estimation, to which it is closely related. In this paper we explore the task of OoD segmentation, which has been studied less than its classification counterpart and presents additional challenges. Segmentation is a dense prediction task for which the model\u2019s outcome for each pixel depends on its surroundings. The receptive field and the reliance on context play a role for distinguishing different classes and, correspondingly, for spotting OoD entities. We introduce MOoSe, an efficient strategy to leverage the various levels of context represented within semantic segmentation models and show that even a simple aggregation of multi-scale representations has consistently positive effects on OoD detection and uncertainty estimation.","Online stores in the US offer a unique scenario for Cross-Lingual Information Retrieval (CLIR) due to the mix of Spanish and English in user queries. Machine Translation (MT) provides an opportunity to lift relevance by translating the Spanish queries to English before delivering them to the search engine. However, polysemy-derived problems, high latency and context scarcity in product search, make generic MT an impractical solution. The wide diversity of products in marketplaces injects non-translatable entities, loanwords, ambiguous morphemes, cross-language ambiguity and a variety of Spanish dialects in the communication between buyers and sellers, posing a thread to the accuracy of MT. In this work, we leverage domain adaptation on a simplified architecture of Neural Machine Translation (NMT) to make both latency and accuracy suitable for e-commerce search. Our NMT model is fine-tuned on a mixed-domain corpus based on engagement data expanded with catalog back-translation techniques. Beyond accuracy, and given that translation is not the goal but the means to relevant results, the problem of Query Translatability is addressed by a classifier on whether the translation should be automatic or explicitly requested. We assembled these models into a query translation system that we tested and launched at Walmart.com , with a statistically significant lift in Spanish GMV and an nDCG gain for Spanish queries of +70%.","In the \u201cclassifier chains\u201d (CC) approach for multi-label classification, the predictions of binary classifiers are cascaded along a chain as additional features. This method has attained high predictive performance, and is receiving increasing analysis and attention in the recent multi-label literature, although a deep understanding of its performance is still taking shape. In this paper, we show that CC gets predictive power from leveraging labels as additional stochastic features, contrasting with many other methods, such as stacking and error correcting output codes, which use label dependence only as kind of regularization. CC methods can learn a concept which these cannot, even supposing the same base classifier and hypothesis space. This leads us to connections with deep learning (indeed, we show that CC is competitive precisely because it is a deep learner), and we employ deep learning methods \u2013 showing that they can supplement or even replace a classifier chain. Results are convincing, and throw new insight into promising future directions.","The deep learning techniques have been shown to make a traffic objects classification system for V2V communications to ensure traffic safety and traffic flow prediction. The robust classifier on the base of MLP and PointNet are explored to recognize the traffic objects from lidar point clouds. The features of a lidar sensor, the lidar point cloud coordinate system and its complex properties for creation a smart traffic object detection and recognition model are described. The best configuration of PointNet architecture with hyperparameters are shown, which is more efficient and robust with respect to input perturbation and corruption of lidar point clouds.","Using High Resolution (HR) and Very High Resolution (VHR) Remote Sensing (RS) images for post-disaster building damage assessment provides more information than low-resolution images. Consequently, a damage map is expected to be at building level, which requires both rooftop and facade information. Oblique imagery is therefore becoming increasingly popular for post-disaster analysis. However, oblique images are rarely available in the pre-disaster acquisition, and thus without any prior information it is a challenging task to assess the facade damage just from a post-disaster scene. As a solution, we aggregated information at the cluster level using pre-disaster neighborhood buildings\u2019 feature analysis, and thus the post-disaster building-level damage assessment is supported by the cluster-level information. Therefore, the proposed method, Intra-Cluster-Classification (ICC), uses hierarchical steps of unsupervised and supervised methods to detect damaged and undamaged areas within each cluster of buildings. The procedure is implemented on Google Earth Engine platform, and the results are evaluated using Hurricane Michael (2018) images. At the building-level, damage information is shown as a fractional number between 0 and 1, with the higher number indicating more destruction. R-squared (R2) value is 0.9688 between actual and predicted damage scores. In addition, the Overall Accuracy (OA) and the Kappa coefficient (K) in the 4-class RS-scale are 83.2% and 0.7438, respectively. Furthermore, in 3-class RS-scale, the OA and K of our results are 91.08%, and 0.8582, respectively.","Past research has demonstrated that the explicit use of protected attributes in machine learning can improve both performance and fairness. Many machine learning algorithms, however, cannot directly process categorical attributes, such as country of birth or ethnicity. Because protected attributes frequently are categorical, they must be encoded as features that can be input to a chosen machine learning algorithm, e.g. support vector machines, gradient boosting decision trees or linear models. Thereby, encoding methods influence how and what the machine learning algorithm will learn, affecting model performance and fairness. This work compares the accuracy and fairness implications of the two most well-known encoding methods: one-hot encoding and target encoding. We distinguish between two types of induced bias that may arise from these encoding methods and may lead to unfair models. The first type, irreducible bias, is due to direct group category discrimination and the second type, reducible bias, is due to the large variance in statistically underrepresented groups. We investigate the interaction between categorical encodings and target encoding regularization methods that reduce unfairness. Furthermore, we consider the problem of intersectional unfairness that may arise when machine learning best practices improve performance measures by encoding several categorical attributes into a high-cardinality feature.","This paper proposes two novel machine learning algorithms, namely Random Vector Functional Link Forests and Extreme Learning Forests, to develop an improved unmanned aerial vehicles automatic target recognition system. Such models take advantage of the stochastic procedure followed by Random Forests, where random subsets of instances and features are selected to build diverse Decision Trees. However, different from the usual uni-variate split criterion from Decision Tree algorithms, we propose and employ the novel Random Vector Functional Link Tree or Extreme Learning Tree, where each decision split is performed using the fast non-linear mapping of multiple features provided by either Random Vector Functional Link or Extreme Learning Machines. To prove the efficacy of the novel algorithm, experiments are performed using 90 binary classification problems to compare the performance of the proposed algorithm against other state-of-the-art ensemble learning techniques. Statistical analysis indicates the success of the proposed algorithms in terms of both predictive performance and computational complexity. While the model with deeper trees outperforms classical ensembles in terms of predictive performance (1.41% error reduction) and has similar results to state-of-the-art ensemble models, the model with shallow trees outperforms all ensembles in terms of computational burden (at least 36% faster). Finally, the novel methods are applied to develop an automatic target recognition system for unmanned aerial vehicles, achieving a valuable trade-off in terms of area under receiver operating characteristic curve (0.9309), F 1 score (0.8190), accuracy (0.8646), and computational time (4.14 s).","Credit-risk prediction is one of the challenging tasks in the banking industry. In this study, a hybrid convolutional neural network\u2014support vector machine/random forest/decision tree (CNN\u2014SVM/RF/DT) model has been proposed for efficient credit-risk prediction. We proposed four classifiers to develop the model. A fully connected layer with soft-max trained using an end-to-end process makes up the first classifier and by deleting the final fully connected with soft-max layer, the other three classifiers\u2014a SVM, RF, and DT classifier stacked after the flattening layer. Different parameter values were considered and fine-tuned throughout testing to select appropriate parameters. In accordance with the experimental findings, a fully connected CNN and a hybrid CNN with SVM, DT, and RF, respectively, achieved a prediction performance of 86.70%, 98.60%, 96.90%, and 95.50%. According to the results, our suggested hybrid method exceeds the fully connected CNN in its ability to predict credit risk.","For training and testing enhancer-promoter interaction (EPI) classifiers, the question on which non-positive EPIs are selected as negative instances must be answered. Most previous methods use the dataset of the EPI classifier TargetFinder where negative EP pairs are sampled from non-positive EP pairs. Consequently, over 92% of EPIs in the TargetFinder-positive and negative sets of cell line GM12878 have a 2-fold or greater positive/negative class imbalance of promoter occurrences between the positive and negative EP pairs. This situation negatively impacts the predictability of EPI classifiers trained using the datasets.\nThus, we first proposed the condition that the negative EPIs should satisfy. Second, we devised a method called CBOEP (class balanced occurrences of enhancers and promoters), to generate negative EPI sets that approximately fulfil this condition for a given positive EPI set. CBOEP solves the finding problem by reducing it to the maximum-flow problem. Third, we applied the generated negative EPI sets to existing EPI classifiers, TransEPI and TargetFinder. The negative datasets lead to higher prediction performance than the existing negative EPI datasets. The source code is available at https://github.com/maruyama-lab-design/CBOEP.","Highlights\n\u2022\nWe showed the versatility of our framework for rapid model deployment.\n\u2022\nWe created classification models for the HRMS data and aimed to find the best one.\n\u2022\nWe implemented a framework for automatically generating classification models.\n\u2022\nWe showcased the applicable feature engineering techniques within this framework.\n\u2022\nWe demonstrated the ways to improve model quality using this automation framework.\nAbstract\nRecently pipelines of machine learning-based classification models have become important to codify, orchestrate, and automate the workflow to produce an effective machine learning model. In this article, we propose a framework that combines feature engineering techniques such as data imputation, transformation, and class balancing to compare the performance of different prediction models and select the best final model based on predefined parameters. The proposed framework is extendable and configurable by adding algorithms supported by the CARET package implemented in the R programming language. This framework can generate different machine learning models, which provide comparable results compared to other studies. The framework allows practitioners and researchers to automatically generate different classification models. This research used High-Resolution Orbitrap-based Mass Spectrometers (HRMS) data to create automated prediction models for the first time in literature. We demonstrated the applicability of feature engineering techniques such as data imputation, transformation (e.g., scaling, centering, etc.), and data balancing using several case studies and the proposed semi-automated framework. We showed how the initial prediction models can be improved using the proposed framework.","The variations in muscular contraction between endurance and power athletes have usually been evaluated from lower limb muscles. The aim of this study is to integrate the application of machine learning in automatically classifying the muscle performance recorded from upper limb muscle. Muscle contraction of bicep brachii was recorded based on the surface electromyography (sEMG) analysis. The evaluation of muscle performance consists of three main processing parts, i.e., pre-processing, feature extraction, and classification. EMG features were extracted from three types of domains: time domain (TD), frequency domain (FD), and time-frequency domain (TFD). For classification purposes, a Support Vector Machine (SVM) classifier was used, and the classification performance was analysed based on the classification accuracy. The best classification performance was observed from the feature set selected from sequential backward selection (SBS). This finding shows that it is possible to differentiate muscle performance from non-specifically trained muscle, which might be further related to the intrinsic properties of different groups of athletes.","In this paper, we describe a weighted principal geodesic analysis (WPGA) method to extract features for gender classification based on 2.5D facial surface normals (needle-maps) which can be extracted from 2D intensity images using shape-from-shading (SFS). By incorporating the weight matrix into principal geodesic analysis (PGA), we control the obtained principal axis to be in the direction of the variance on gender information. Experiments show that using WPGA, the leading eigenvectors encode more gender discriminating power than using PGA, and that gender classification based on leading WPGA parameters is more accurate and stable than based on leading PGA parameters.","Insect pests have always been a global agricultural problem because the severity and extent of their occurrence threaten crop yield. Recognizing them early can help farmers have efficient measures to handle them, which can help mitigate negative impacts from insect pests. However, insect pest recognition still relies heavily on experts, which is expensive and time-consuming. With the power of Deep Learning, we propose two methods to solve this task in this paper. First, we proposed a method that uses models pre-trained on the ImageNet dataset, including ResNet-50, EfficientNet-B4, and VisionTransformer-B16, respectively. We also change the structure of these models by adding a Dropout layer before the output layer of these pre-trained models to avoid overfitting. Second, we apply hierarchical learning for this task. In the latter approach, we first use the baseline model to create a confusion matrix. Through this matrix, we cluster classes that the baseline model misses to each other because of the similar appearance across classes into bigger classes, and we consider them as sub-datasets. Then, we build each model for each sub-dataset using the identical backbones as the baseline methods with the hope that it helps the method classify better in these classes. We do experiments to evaluate the performance of methods on the IP102 dataset. From experiments, our proposed method, which uses VisionTransformer-B16 backbone combined with hierarchical learning, gets the best accuracy of 74.50% on the IP102 dataset. When ensemble 3 above models and combine with hierarchical learning, we get the best accuracy of 76.24% on this dataset.","Recent research has revealed that deep neural networks often take dataset biases as a shortcut to make decisions rather than understand tasks, leading to failures in real-world applications. In this study, we focus on the spurious correlation between word features and labels that models learn from the biased data distribution of training data. In particular, we define the word highly co-occurring with a specific label as biased word, and the example containing biased word as biased example. Our analysis shows that biased examples are easier for models to learn, while at the time of prediction, biased words make a significantly higher contribution to the models' predictions, and models tend to assign predicted labels over-relying on the spurious correlation between words and labels. To mitigate models' over-reliance on the shortcut (i.e. spurious correlation), we propose a training strategy Less-Learn-Shortcut (LLS): our strategy quantifies the biased degree of the biased examples and downweights them accordingly. Experimental results on Question Matching, Natural Language Inference and Sentiment Analysis tasks show that LLS is a task-agnostic strategy and can improve the model performance on adversarial data while maintaining good performance on in-domain data.","Multi-metric learning is important for improving performance of learners. For complex data, multi metric learning algorithms need intensive research. Moreover, the existing multi-metric learning methods may lead to the distance not being comparable. To solve these shortcomings and characterize better complexity data, we propose a novel multi-metric learning framework, where each class is divided into several clusters, and then a local metric and two concentric hypers-pheres are trained jointly in a cluster, such that the samples of the same cluster distribute within one hypersphere, and the classification margin are as large as possible simultaneously. This will leads to intra-class compactness and inter-class dispersion. During the test phase, the relative distance in learned metric space is designed to make classification decisions. A new example is classified to the class of its closest hyper-sphere center. This ensures that the comparison of distances is meaningful and avoids effectively the limitation of k-nearest neighbors (kNN) classifiers. Moreover,some important properties the proposed algorithm are analyzed theoretically. Further, an alternating iterative algorithm is developed to solve the problem. Numerical experiments are carried out on different scales and types datasets. Experiment results confirm the feasibility and effectiveness of the proposed method.","The alternative accumulated improvement curve stochastic order is a criterion for the comparison of the performance of classifiers that predict binary responses. An explicit optimal classifier for this criterion is obtained. That optimal classifier has the largest ROC and CAP curves and indexes, that is, it is also optimal for the criteria based on the comparison of such curves and indexes. An application of the results to the search of the best classifier to predict clients of a bank which will make a transaction in the future is developed.\nHighlights\n\u2022\nOptimal classifier for the alternative accumulated improvement curve stochastic order.\n\u2022\nThat classifier has the largest ROC and CAP curves and indexes.\n\u2022\nApplication to a problem of a bank to predict clients which will make a transaction.\n\u2022\nThe method based on the optimal classifier shows the best performance.","Welding processes are the most common manufacturing solution that frequently complete the final steps of an industrial production. Their large use makes these techniques worth of attention of the scientific community, so that several efforts are spent for optimising and improving the way the processes are executed.\nSubmerged Arc Welding is widely used in the industrial scenario being semi-automatic but it cannot be ignored that this joining technique is far from the full automatization and is still dependent by the operator expertise.\nIn this study, an experimental investigation has been performed to build a robust data set for the subsequent application of seven machine learning classifier. The aim of the work is the definition of a suitable classifier able to detect and predict invalid process conditions which could lead to failed joint.","Dataset scaling, also known as normalization, is an essential preprocessing step in a machine learning pipeline. It is aimed at adjusting attributes scales in a way that they all vary within the same range. This transformation is known to improve the performance of classification models, but there are several scaling techniques to choose from, and this choice is not generally done carefully. In this paper, we execute a broad experiment comparing the impact of 5 scaling techniques on the performances of 20 classification algorithms among monolithic and ensemble models, applying them to 82 publicly available datasets with varying imbalance ratios. Results show that the choice of scaling technique matters for classification performance, and the performance difference between the best and the worst scaling technique is relevant and statistically significant in most cases. They also indicate that choosing an inadequate technique can be more detrimental to classification performance than not scaling the data at all. We also show how the performance variation of an ensemble model, considering different scaling techniques, tends to be dictated by that of its base model. Finally, we discuss the relationship between a model\u2019s sensitivity to the choice of scaling technique and its performance and provide insights into its applicability on different model deployment scenarios. Full results and source code for the experiments in this paper are available in a GitHub repository. https://github.com/amorimlb/scaling_matters.\nHighlights\n\u2022\nCompares classification performances after applying five scaling techniques.\n\u2022\nPerformance difference between best and worst scaling technique is largely relevant.\n\u2022\nThis difference increases when highly imbalanced datasets are considered.\n\u2022\nThe performance variation of an ensemble tends to be dictated by that of its base model.\n\u2022\nProvides an analysis of sensitivity to the choice of scaling tech. vs model performance.","Crop disease in the plant is a significant issue in the agriculture sector, and it is currently very difficult to detect these illnesses in crop leaves. The foundation of the global economy is agriculture. India ranks second in the production of tomatoes worldwide. The tomato crop is affected by various diseases which lead to a reduction in product quality and quantity. The advancement in computer vision and deep learning opens up the door for predicting diseases that appear in the crops. The aim of this paper is classification among 10 different categories of tomato plant leaves using the proposed novel TomConv model which deploys an improved Convolutional Neural Network (CNN). For this purpose, the publicly available dataset called PlantVillage comprising of more than 16000 images of tomato leaves, both diseased and healthy was used for the experimentation purpose. The proposed model is the simplest model among all the available state-of-the-art models. The tomato leaf images were preprocessed for reducing the size in 150\u202f\u00d7\u202f150 dimension. The model constitutes four layered CNN followed by a max pooling layer. The model splits the corpus into training and validation datasets in 80:20 ratio, is trained under 105 epochs for tomato leaf images, and achieved an accuracy of 98.19%. The proposed model is compared with existing models under different parameters such as no. of classes, no. of layers, and accuracy. The results are promising as they outperform all the available state-of-the-art models.","The goal of few-shot learning is to use limited labeled samples to complete independent classification tasks. The feature extractor of few-shot learning needs to have a stronger feature expression ability to generalize in unseen novel classes. To further enhance the expressive ability, in this paper, we propose an inherited feature extraction method, named Base and Meta Feature Extraction (BMFE). Base feature represents the task-irrelevant classification information of each sample. Meta feature obtained by the proposed Triplet Meta-train Mechanism (TMM) inherits the classification information and also contains the task-related meta information of each sample. We concatenate both the base and meta features to complementarily express the rich information of each sample. Besides, instead of relying on limited support samples to obtain the prototype, we propose a novel unsupervised prototype correction module, named Prototype Self-updating (PSU). All unlabeled query samples in a few-shot test task participate in the iterative updating of each prototype in the task without training. Extensive experiments prove that our overall method can obtain richer features by BMFE and more accurate prototypes by PSU. Our overall method outperforms state-of-the-art methods on miniImageNet and tiredImageNet datasets, and especially under the 1-shot case we obtains 78.45% and 81.21% classification accuracy respectively.\nHighlights\n\u2022\nBMFE can obtain the complementary features representation of FSL.\n\u2022\nTMM mechanism can better perform meta-training of BMFE.\n\u2022\nPSU can correct prototypes without training process.\n\u2022\nThe proposed CFPSU gets outstanding results on two standard datasets.","Insider threats refer to cyber-attacks originating from within an organization that can cause significant damage, such as intellectual property theft, sabotage, and sensitive data exposure. Traditional cybersecurity strategies tend to focus on external threats, leaving organizations vulnerable to insider attacks. In this paper, we propose an approach for insider threat classification with various classification models. Aggregated numerical features are generated using the access patterns of the employees of the organization. We used the CERT dataset for training and testing. The proposed method is evaluated with classification models like Logistic Regression, Decision Tree, Random Forest, and Xgboost. The experimental results of the model's performance, measured using evaluation metrics such as accuracy, recall, precision, and F1-Score, demonstrated improved accuracy and performance compared to existing works in terms of high recall, precision, and F1-Score values, and effectively outperformed pre-trained CNN models.","Highlights\n\u2022\nAn experimental analysis on effect of inconsistency rate on prediction accuracy (PA) is conducted.\n\u2022\nBoth unsupervised and supervised techniques are used to granulate 36 datasets.\n\u2022\nAn algorithm is developed to extract consistent data from original granulated datasets.\n\u2022\nFive classifiers are used to train original, granulated, and consistent datasets.\n\u2022\nComparison and improvement of PA by five levels of managing inconsistency rate are presented.\nAbstract\nAn experiment was conducted to investigate the effect of the inconsistency rate (IR) of granulated datasets on classification performance. Unsupervised (equal-width interval, EWI) and supervised (minimum description length, MDL) techniques were used to granulate 36 datasets. An algorithm was developed to divide the original granulated datasets into consistent and inconsistent subsets. Five classifiers including one simple tree-based and four ensemble-based on datasets before granulation (BG), after granulation but before removal of inconsistent granulated datasets (AGBR), and after removal of inconsistent granulated datasets (AR) were used, followed by testing and comparisons of predication accuracy (PA). The experimental results showed the following: (1) 24 out of 36 via EWI and 28 out of 36 via MDL datasets contain inconsistent datasets. (2) PA of AR is more likely higher than of BG and AGBR datasets with both EWI and MDL by all classifiers. (3) Mean PA improvement ranges from 5.74% to 10.01% with EWI and from 8.74% to 13.73% with MDL. (4) The correlation coefficient between IR and PA improvement ranges from 0.7413 to 0.7901 with EWI and 0.7870 to 0.9683 with MDL. These results demonstrate the value of uncovering the effect of IR on classification performance in the domain of machine learning.","NICO challenge of out-of-distribution (OOD) generalization for image recognition features two tracks: common context generalization and hybrid context generalization, based on a newly proposed OOD dataset called NICO\n+\n+\n. Strong distribution shifts between the training and test data are constructed for both tracks. In contrast to the current OOD generalization benchmarks where models are tested on a single domain, NICO challenge tests models on multiple domains for a thorough and comprehensive evaluation. To prevent the leakage of target context knowledge and encourage novel and creative solutions instead of leveraging additional training data, we prohibit the model initialization with pretrained parameters, which is not noticed in the previous benchmarks for OOD generalization. To ensure the random initialization of models, we verify and retrain models from all top-10 teams and test them on the private test data. We empirically show that pretraining on ImageNet introduces considerable bias on test performance. We summarize the insights in top-4 solutions, which outperform the official baselines significantly, and the approach of jury award for each track.","The growing availability of time series data has increased the usage of classifiers for this data type. Unfortunately, state-of-the-art time series classifiers are black-box models and, therefore, not usable in critical domains such as healthcare or finance, where explainability can be a crucial requirement. This paper presents a framework to explain the predictions of any black-box classifier for univariate and multivariate time series. The provided explanation is composed of three parts. First, a saliency map highlighting the most important parts of the time series for the classification. Second, an instance-based explanation exemplifies the black-box\u2019s decision by providing a set of prototypical and counterfactual time series. Third, a factual and counterfactual rule-based explanation, revealing the reasons for the classification through logical conditions based on subsequences that must, or must not, be contained in the time series. Experiments and benchmarks show that the proposed method provides faithful, meaningful, stable, and interpretable explanations.","Due to the limited funds for the acquisition of university library resources, in order to improve the collection resource profitability and reduce the labor cost of libraries, a method of text classification using natural language processing is proposed to build an accurate procurement system for electronic book resources. Feature extraction is performed for the text information of the book collection, a feature vector matrix is established, and classification prediction is performed by machine learning algorithms. By optimizing the prediction model and evaluating the results with ten cross-validations, the accuracy of the LightGBM model testing reached 77.49%, so the acquisition system constructed based on this model can make effective decisions and thus optimize the structure of the collection resources.","Cannabinoid receptors, as part of the family of the G protein-coupled receptors (GPCRs), are involved in various physiological functions. Its subtype cannabinoid receptor subtype 2 (CB2), mainly distributed in the periphery, is a crucial therapeutic target for anti-epileptic, anti-inflammation, anti-fibrosis, and bone metabolism regulation, and it regulates these physiological functions without psychiatric side effects. Recently machine learning methods for predicting biophysics properties have attracted much attention. Successful application of machine learning usually highly depends on the appropriate representation of the compounds. In this study, we comprehensively evaluate the performance of the descriptor-based models (including XGBoost, Random Forest, and KNN) and two graph-based models (D-MPNN, MolMap) for the prediction of the CB2 regulators, and found that XGBoost offers outstanding performance for both regression tasks and classification tasks. 13 different molecular fingerprints and 12 descriptors, as well as their combination were further screened; AvalonFP + AtomPairFP + RDkitFP + MorganFP and AtomPairFP + MorganFP + AvalonFP were the optimum combinations for regression task (R2 increase to 0.667) and classification task (AUC-ROC increase to 0.933), respectively. Specifically, the best XGBoost regression model with optimum features achieves better performance than Mizera's QSAR model on the same dataset developed by Mizera (R2 0.664 versus 0.62). It also achieves optimal performance with an AUC-ROC of 0.917 on the external validation set. By comparison, MolMap and D-MPNN only provide 0.912 and 0.898. The Shapley additive explanation method was used to interpret the models, and features importance were shown for both regression and classification task. The XGBoost model equipped with essential molecular fingerprints combination in this paper may provide valuable clues to designing novel CB2 ligands and developing models for other properties prediction.","Highlights\n\u2022\nA novel classification model via Siamese network for fentanyl analogs was proposed;\n\u2022\nSiamese network and spectral library searching were combined;\n\u2022\nThe proposed model can achieve relatively high classification accuracy;\n\u2022\nProblem of small sample classification of fentanyl analogs was resolved effectively.\nAbstract\nFentanyl and its analogs, as emerging psychotropic drugs, have led to a sharp increasing fatality due to their abuse in recent years. It is difficult to identify their differences due to the diversified molecular structures and small sample characteristics. This paper proposed a novel deep classification model based on Siamese network and mass spectral library searching to classify fentanyl analogs accurately. After embedding the query mass spectrum and reference spectrum into a low-dimensional space, the best matched spectrum is obtained by calculating their similarity, so as to determine the category of the query analogue. Three experiments were performed to verify the classification performance of the proposed model on two open datasets of fentanyl analogs. Compared with two spectral library searching methods of simple match factor (sMF) and hybrid match factor (hMF), four machine learning methods of linear discriminant analysis (LDA), support vector machine (SVM), random forest (RF) and Adaboost, and two deep learning methods of deep clustering and contrastive learning, the proposed model can achieve the highest classification accuracy of 96.13%, 95.83% and 94.17%, respectively.","Long-tailed learning aims to tackle the crucial challenge that head classes dominate the training procedure under severe class imbalance in real-world scenarios. Supervised contrastive learning has turned out to be worth exploring research direction, which seeks to learn class-specific feature prototypes to enhance long-tailed learning performance. However, little attention has been paid to how to calibrate the empirical prototypes which are severely biased due to the scarce data in tail classes. Without the aid of correct prototypes, these explorations have not shown the significant promise expected. Motivated by this, we propose the meta-prototype contrastive learning to automatically learn the reliable representativeness of prototypes and more discriminative feature space via a meta-learning manner. In addition, on top of the calibrated prototypes, we leverage it to replace the mean of class statistics and predict the targeted distribution of balanced training data. By this procedure, we formulate the feature augmentation algorithm which samples additional features from the predicted distribution and further balances the over-whelming dominance severity of head classes. We summarize the above two stages as the meta-prototype decouple training scheme and conduct a series of experiments to validate the effectiveness of the framework. Our method outperforms previous work with a large margin and achieves state-of-the-art performance on long-tailed image classification and semantic segmentation tasks (e.g., we achieve 55.1% overall accuracy with ResNetXt-50 in ImageNet-LT).","This paper presents our solution for the Requests Sub-challenge of the ACM Multimedia 2023 Computational Paralinguistics Challenge. Drawing upon the framework of self-supervised learning, we put forth an automated data augmentation technique for audio classification, accompanied by a multi-channel fusion strategy aimed at enhancing overall performance. Specifically, to tackle the issue of imbalanced classes in complaint classification, we propose an audio data augmentation method that generates appropriate augmentation strategies for the challenge dataset. Furthermore, recognizing the distinctive characteristics of the dual-channel HC-C dataset, we individually evaluate the classification performance of the left channel, right channel, channel difference, and channel sum, subsequently selecting the optimal integration approach. Our approach yields a significant improvement in performance when compared to the competitive baselines, particularly in the context of the complaint task. Moreover, our method demonstrates noteworthy cross-task transferability.","Resource-constrained classification tasks are common in real-world applications such as allocating tests for disease diagnosis, hiring decisions when filling a limited number of positions, and defect detection in manufacturing settings under a limited inspection budget. Typical classification algorithms treat the learning process and the resource constraints as two separate and sequential tasks. We develop an adaptive learning approach that considers resource constraints and learning jointly by iteratively fine-tuning misclassification costs. Via a structured experimental study using a publicly available data set, we evaluate a decision tree classifier that utilizes the proposed approach. The adaptive learning approach performs significantly better than alternative approaches, especially for difficult classification problems in which the performance of common approaches may be unsatisfactory. The suggested approach reaches similar classification decisions for different costs, thus it may be useful when misclassification costs are not known precisely or are costly to achieve. We envision the suggested learning approach as an important addition to the repertoire of techniques for handling resource-constrained classification problems.","Skip Abstract Section\nAbstract\nIn a complex classification task, samples are represented by various types of multimodal features, including structured data, text, images, video, audio, etc. These data are usually high dimensionally, large-sized, structurally complex, and semantically inconsistent. The representation, translation, alignment, fusion and co-learning of multimodal data are core technical challenges to traditional classification tasks. Kernel functions are applied in dealing with multimodal data for extracting some nonlinear information. However, they cannot consider the aspects of complex structures and uncertain semantics in a multimodal classification task. Fuzzy granular computing emerges as a powerful vehicle to handle the structured and uncertain multimodal data. In this paper, we propose a framework of multimodal classification based on kernel functions and fuzzy granular computing. First, a fuzzy granulation based on kernel functions is introduced to extract nonlinear features for the multimodal classification. Then, a model of multimodal fuzzy classification including fuzzy granular representation, fusion and learning for multimodal data is constructed. Finally, we design an efficient fuzzy granular classification algorithm for big multimodal data based on the proposed model. Experimental results demonstrate the effectiveness of our proposed model and its corresponding algorithm.\nSkip Graphical abstract Section\nGraphical abstract","In this paper, we present a new method for gender classification based on fusion of multi-view gait sequences. For each silhouette of gait sequences, we first use a simple method to divide the silhouette into 7 (for 90 degree, i.e. fronto-parallel view) or 5 (for 0 and 180 degree, i.e. front view and back view) parts, and then fit ellipses to each of the regions. Next, the features are extracted from each sequence by computing the ellipse parameters. For each view angle, every subject\u2019s features are normalized and combined as a feature vector. The combination of feature vector contains enough information to perform well on gender recognition. Sum rule and SVM are applied to fuse the similarity measures from 0o, 90o, and 180o. We carried our experiments on CASIA Gait Database, one of the largest gait databases as we know, and achieved the classification accuracy of 89.5%.","To maintain development consciousness, simplify project coordination, and prevent misinterpretation, communication is essential for software development teams. Instant private messaging, group chats, and sharing code are just a few of the capabilities that chat rooms provide to assist and meet the communication demands of software development teams. All of this is capacitated to happen in real-time. Consequently, chat rooms have gained popularity among developers. Gitter is one of these platforms that has gained popularity, and the conversations it contains may be a treasure trove of data for academics researching open-source software systems. This research made use of the GitterCom dataset, The largest collection of Gitter developer messages that have been carefully labelled and curated and perform multi-label classification for the \u2019Purpose\u2019 category in the dataset. An extensive empirical analysis is performed on 6 feature selection techniques, 14 machine learning classifiers, and BERT transformer layer architecture with layer-by-layer comparison. Consequently, we achieve proficient results through our research pipeline involving Extra Trees Classifier and Random Forest classifiers with AUC (OvR) median performance of 0.94 and 0.92 respectively. Furthermore, The research proposed research pipeline could be utilized for generic multi-label text classification on software developer forum text data.","Face manipulation detection has become a recent research hotpot, and many detection methods have been proposed. Most existing detection methods treat face manipulation detection as a vanilla binary classification problem. However, according to the visual effects of facial manipulation, face manipulation approaches are divided into face replacement and face reenactment. In this study, we propose a three-classification face manipulation detection method(TFMD). To implement three-classification face manipulation detection, we introduce a new face forgery feature representation, where the face forgery features are jointly represented by the identity-changing features and the face real-fake features. To decompose the face forgery features into the identity-changing features and the face real-fake features, we design an attention-based feature decomposition module (AFDM). Moreover, we extract high-frequency noise features from the shallow features of the RGB stream to enrich high-frequency information. Through extensive experiments on two datasets, FaceForensics++ and Celeb-DF, we achieve state-of-the-art performance and demonstrate the effectiveness and robustness of our proposed method.","Recent studies on deep metric learning have taken efforts to learn a common feature representation for each category, which is conflict to the intrinsic intra-class variance caused by the fact that latent attributes of the subcategories enlarge the variability of samples from the same class, leading to difficulties of inferring more fine-gained similarity retrieval. This paper proposes a hierarchical multi-proxy representation to establish both the common feature representation of the main-proxy and the complementary representations of their sub-proxies. Specifically, the main-proxy represents the intrinsic characteristics of a certain category for inter-class distinction. Meanwhile, sub-proxies belonging to each main-proxy provide additional capacity for describing intra-class discrepancy. Therefore, a hierarchical multi-proxy representation naturally captures both inter-class and intra-class diversity, yielding a more complete and distinguishable data representation. Significantly, the instances from distinct categories with similar main-proxy attributes can be further separated by the multiple proxy representation due to distinguishable difference among sub-proxy attributes. Correspondingly, we propose Hierarchical Multi-proxy loss as a reliable guidance for deep metric learning. Performance improvement of around 0.5% in precision on the benchmarks as well as the theoretical analysis demonstrate the effectiveness of the proposed method, especially on datasets with high intra-class variance.","With the emergence of new Internet services and the drastic increase in Internet traffic, traffic classification has become increasingly important to effectively satisfy the quality of service to users. The traffic classification system should be resilient and operate smoothly regardless of network conditions or performance and should be capable of handling various classes of Internet services. This paper proposes a traffic classification method in a software-defined network environment that employs a variational autoencoder (VAE) to accomplish this. The proposed method trains the VAE using six statistical features and extracts the distributions of latent features for the flows in each service class. Furthermore, it classifies the query traffic by comparing the distributions of latent features for the query traffic with the learned distributions of the service classes. For the experiment, the statistical features of network flows were collected from real-world domestic and overseas Internet services for training and testing. According to the experimental results, the proposed method has an average accuracy of 89%. This accuracy was 52%, 47%, 39%, 59%, and 26% higher than conventional statistics-based classification methods, MLP, AE+MLP, VAE+MLP, and SVM, respectively. This result clearly suggests that probability distributions of latent features, rather than specific values for latent features, can be used as more stable features.","Classification plays a crucial role in mining the evolving data streams. The concept drift and concept evolution are the major issues of data streams classification, which greatly affect the classification performance. Most existing works of concept drift and evolution are supervised in nature, where labeling the data is time and resource consuming. In this paper, for the evolving data streams, a semi-supervised classification approach using partially labeled data is proposed. Firstly, an ensemble model dynamically maintains a series of micro-clusters to capture the concept drift. The ensemble model processes the instances in an online fashion rather than chunk-based. Secondly, the concept evolution detection module is constructed to detect the outliers by the local density. The module examines the current buffer to capture the class emergence in data streams with complex class distribution. For improving the execution efficiency of emerging class detection without compromising performance, several constructive strategies are adopted, including removing part of the buffer and selectively executing sample generation. The extensive experiments are constructed about the popular data streams sets and the processed industry data streams, whose results indicate the practicality and effectiveness of the proposed approach for the classification of evolving data streams.\nHighlights\n\u2022\nFor the evolving data streams, a semi-supervised classification approach using partial labeled data is proposed.\n\u2022\nThe ensemble model processes the instances in an online fashion rather than chunk-based.\n\u2022\nAn ensemble model dynamically maintains a series of micro-clusters to capture the concept drift.\n\u2022\nThe concept evolution detection module is constructed to detect the outliers by local density.","Parkinson\u2019s disease is one of the most common neurodegenerative chronic diseases which can affect the patient\u2019s quality of life by creating several motor and non-motor impairments. The freezing of gait is one such motor impairment which can cause the inability to move forward despite the intention to walk. The identification of the freezing-of-gait events using sensor technology and machine-learning algorithms can result in an improvement in the quality of life and can decrease the risk of fall in Parkinson\u2019s patients. Our study focuses on a systematic performance evaluation of machine learning algorithms for developing a good fit and generalized model. In this work, we train time-domain and frequency-domain-transform-based features on fully connected artificial and deep neural network algorithm for classifying the events of freezing of gait in patients by using accelerometer data. We evaluate these algorithms for hyperparameters such as batch size, optimizer type, and window sizes in a step-wise process. We identify an optimal combination of parameters according to the accuracy and model fit optimality metrics, for artificial and deep neural network to classify freezing of gait events in Parkinson\u2019s patients. We were able to achieve classification accuracy of - with Adam optimizer, batch sizes (BS) of 256 and 8 and epochs of 60 and 40 for ANN and DNN respectively.","We explore solutions for text classification applied to online cooking recipes, in a multitask, multilingual approach. The main objective is designing a solution that ensures high accuracy on the prediction tasks from, but not constrained to, 6 European Languages, considering also the cross-lingual transferability. The challenges of the problem are structured on two main dimensions: (1) data driven - such as imbalance and noise in the training data, and (2) solution driven - such as multilingualism, or the need to easily extend the model to new languages. We propose a solution focused on the XLM-R architecture, fine-tuned jointly on all tasks. We apply self-supervised domain adaptation via additional pre-training and analyze the enhancements produced by performing a 0-shot evaluation for underrepresented languages. Compared to basic language modeling solutions, we obtained an increase of 1.32% and 2.42%, respectively for the two most difficult classification tasks. In the 0-shot context, the absolute improvements are of 16.71% and 7.83% respectively, on underrepresented languages.","The selection of the optimum machine learning technique is a crucial step to detect faults efficiently in the predictive maintenance (PdM) area. Because the performance of the machine learning algorithm changes with respect to a data set, which has different characteristics, including feature number, data size and nonlinearity. The paper considers the problem of detecting faults observed in an autonomous electric drive without using any sensor information. More importantly, we aim to show the opportunities and explore the limitations of machine learning techniques in fault detection. Accordingly, the advantages and disadvantages of different types of machine learning methods including logistic regression, support vector machine, decision tree, navie Bayes, gradient boosting etc. for condition monitoring are discussed with an emphasis given to an autonomous electric drive train. Experimental comparison of machine learning algorithms suggests that the boosting methods yield promising performance in fault classification. The results are supported by statistical analysis.","Transformer-based pre-trained Language Models (PLMs) have emerged as the foundations for the current state-of-the-art algorithms in most natural language processing tasks, in particular when applied to context rich data such as sentences or paragraphs. However, their impact on the tasks defined in terms of abstract individual word properties, not necessary tied to their specific use in a particular sentence, has been inadequately explored, which is a notable research gap. Addressing this gap is crucial for advancing our understanding of natural language processing. To fill this void, we concentrate on classification of semantic relations: given a pair of concepts (words or word sequences) the aim is to identify the semantic label to describe their relationship. E.g. in the case of the pair green/colour, \u201cis a\u201d is a suitable relation while \u201cpart of\u201d, \u201cproperty of\u201d, and \u201copposite of\u201d are not suitable. This classification is independent of a particular sentence in which these concepts might have been used. We are first to incorporate a language model into both existing approaches to this task, namely path-based and distribution-based methods. Our transformer-based approaches exhibit significant improvements over the state-of-the-art and come remarkably close to achieving human-level performance on rigorous benchmarks. We are also first to provide evidence that the standard datasets over-state the performance due to the effect of \u201clexical memorisation.\u201d We reduce this effect by applying lexical separation. On the new benchmark datasets, the algorithmic performance remains significantly below human-level, highlighting that the task of semantic relation classification is still unresolved, particularly for language models of the sizes commonly used at the time of our study. We also identify additional challenges that PLM-based approaches face and conduct extensive ablation studies and other experiments to investigate the sensitivity of our findings to specific modelling and implementation choices. Furthermore, we examine the specific relations that pose greater challenges and discuss the trade-offs between accuracy and processing time.","Mixup is an efficient data augmentation method which generates additional samples through respective convex combinations of original data points and labels. Although being theoretically dependent on data properties, Mixup is reported to perform well as a regularizer and calibrator contributing reliable robustness and generalization to neural network training. In this paper, inspired by Universum Learning which uses out-of-class samples to assist the target tasks, we investigate Mixup from a largely under-explored perspective - the potential to generate in-domain samples that belong to none of the target classes, that is, universum. We find that in the framework of supervised contrastive learning, universum-style Mixup produces surprisingly high-quality hard negatives, greatly relieving the need for a large batch size in contrastive learning. With these findings, we propose Universum-inspired Contrastive learning (UniCon), which incorporates Mixup strategy to generate universum data as g-negatives and pushes them apart from anchor samples of the target classes. Our approach not only improves Mixup with hard labels, but also innovates a novel measure to generate universum data. With a linear classifier on the learned representations, on Resnet-50, our method achieves 81.68% top-1 accuracy on CIFAR-100, surpassing the state of art by a significant margin of 5% with a much smaller batch size.","Semi-supervised learning, a system dedicated to making networks less dependent on labeled data, has become a popular paradigm due to its strong performance. A common approach is to use pseudo-labels with unlabeled data for training, however, pseudo-labels cannot correct their own errors. In this paper, we propose a semi-supervised method that uses nearest neighbor samples to obtain pseudo-labels and combines consistency regularization for image classification. Our method obtains pseudo-labels by computing the similarity of the data distribution between the weakly-augmented version of the unlabeled data and the labeled data stored in the support set and combines the consistency of the strongly-augmented version and the weakly-augmented version of the unlabeled data. We compared with several standard semi-supervised learning benchmarks and achieved a competitive performance. For example, we achieved an accuracy of\n94.02\n%\non CIFAR-10 with 250 labels and\n97.50\n%\non SVNH with 250 labels. It even achieved\n91.59\n%\naccuracy with only 40 labels data in the CIFAR-10.","From the perspective of a dialog system, the identification of the intention behind the segments in a dialog is important, as it provides cues regarding the information present in the segments and how they should be interpreted. The ISO 24617-2 standard for dialog act annotation defines a hierarchically organized set of general-purpose communicative functions that correspond to different intentions that are relevant in the context of a dialog. In this paper, we explore the automatic recognition of these functions. To do so, we propose to adapt existing approaches to dialog act recognition, so that they can deal with the hierarchical classification problem. More specifically, we propose the use of an end-to-end hierarchical network with cascading outputs and maximum a posteriori path estimation to predict the communicative function at each level of the hierarchy, preserve the dependencies between the functions in the path, and decide at which level to stop. Additionally, we rely on transfer learning processes to address the data scarcity problem. Our experiments on the Dialog-Bank show that this approach outperforms both flat and hierarchical approaches based on multiple classifiers and that each of its components plays an important role in the recognition of general-purpose communicative functions.","Aspect-based summarization of a legal case file related to regulating bodies allows different stakeholders to consume information of interest therein efficiently. In this paper, we propose a multi-step process to achieve the same. First, we explore the semantic sentence segmentation of SEBI case files via classification. We also propose a dataset of Indian legal adjudicating orders which contain tags from carefully crafted domain-specific sentence categories with the help of legal experts. We experiment with various machine learning and deep learning methods for this multi-class classification. Then, we examine the performance of numerous summarization methods on the segmented document to generate persona-specific summaries. Finally, we develop a pipeline making use of the best methods in both sub-tasks to achieve high recall.","The combination of class imbalance and overlap is currently one of the most challenging issues in machine learning. While seminal work focused on establishing class overlap as a complicating factor for classification tasks in imbalanced domains, ongoing research mostly concerns the study of their synergy over real-word applications. However, given the lack of a well-formulated definition and measurement of class overlap in real-world domains, especially in the presence of class imbalance, the research community has not yet reached a consensus on the characterisation of both problems. This naturally complicates the evaluation of existing approaches to address these issues simultaneously and prevents future research from moving towards the devise of specialised solutions. In this work, we advocate for a unified view of the problem of class overlap in imbalanced domains. Acknowledging class overlap as the overarching problem \u2013 since it has proven to be more harmful for classification tasks than class imbalance \u2013 we start by discussing the key concepts associated to its definition, identification, and measurement in real-world domains, while advocating for a characterisation of the problem that attends to multiple sources of complexity. We then provide an overview of existing data complexity measures and establish the link to what specific types of class overlap problems these measures cover, proposing a novel taxonomy of class overlap complexity measures. Additionally, we characterise the relationship between measures, the insights they provide, and discuss to what extent they account for class imbalance. Finally, we systematise the current body of knowledge on the topic across several branches of Machine Learning (Data Analysis, Data Preprocessing, Algorithm Design, and Meta-learning), identifying existing limitations and discussing possible lines for future research.\nHighlights\n\u2022\nA unique and global view of the problem of overlap in imbalanced domains is proposed.\n\u2022\nA novel taxonomy of class overlap complexity measures is established.\n\u2022\nFeature, instance, structural, and multiresolution overlap are formally acknowledged.\n\u2022\nThe current state of knowledge across several areas of Machine Learning is presented.\n\u2022\nOpen avenues and future directions, including software contributions, are discussed.","Semantic matching of short texts is a hot research problem in the field of natural language processing, which has a wide range of application needs in information retrieval, dialogue system, text paraphrase questions, and question answering system etc. Short texts have the characteristics of less information and lack of contextual background. The existing semantic matching methods for short texts also generally have the problem of low matching accuracy. With the speedy development of deep learning technology, various models of deep learning have been extensively used in natural language processing and achieved good results. In this paper, we design a FERNIE model based on a lightweight pre-training model ERNIE3.0-medium of BERT architecture, which integrates low-level features and high-level features. Experiments on several datasets show that the FERNIE model has good results in short text semantic matching, and the accuracy is further improved compared to ERNIE3.0-medium.","Graph representation learning (GRL) is a powerful tool for graph analysis, which has gained massive attention from both academia and industry due to its superior performance in various real-world applications. However, the majority of existing works for GRL are dedicated to node-based tasks and thus focus on producing node representations. Despite such methods can be used to derive edge representations by regarding edges as nodes, they suffer from sub-par result utility in practical edge-wise applications, such as financial fraud detection and review spam combating, due to neglecting the unique properties of edges and their inherent drawbacks. Moreover, to our knowledge, there is a paucity of research devoted to edge representation learning. These methods either require high computational costs in sampling random walks or yield severely compromised representation quality because of falling short of capturing high-order information between edges. To address these challenges, we present TER and AER, which generate high-quality edge representation vectors based on the graph structure surrounding edges and edge attributes, respectively. In particular, TER can accurately encode high-order proximities of edges into low-dimensional vectors in a practically efficient and theoretically sound way, while AER augments edge attributes through a carefully-designed feature aggregation scheme. Our extensive experimental study demonstrates that the combined edge representations of TER and AER can achieve significantly superior performance in terms of edge classification on 8 real-life datasets, while being up to one order of magnitude faster than 16 baselines on large graphs.","Federated learning (FL) has recently been applied to skin lesion analysis, but the challenges of huge communication requirements and non-independent and identical distributions have not been fully addressed. The former problem arises from model parameter transfer between the server and clients, and the latter problem is due to differences in imaging protocols and operational customs. To reduce communication costs, dataset distillation methods have been adopted to distill thousands of real images into a few synthetic images (1 image per class) in each local client, which are then used to train a global model in the server. However, these methods often overlook the possible inter-client distribution drifts, limiting the performance of the global model. In this paper, we propose a generalizable dataset distillation-based federated learning (GDD-FL) framework to achieve communication-efficient federated skin lesion classification. Our framework includes the generalization dataset distillation (GDD) method, which explicitly models image features of the dataset into an uncertain Gaussian distribution and learns to produce synthetic images with features close to this distribution. The uncertainty in the mean and variance of the distribution enables the synthetic images to obtain diverse semantics and mitigate distribution drifts. Based on the GDD method, we further develop a communication-efficient FL framework that only needs to transmit a few synthesized images once for training a global model. We evaluate our approach on a large skin lesion classification dataset and compare it with existing dataset distillation methods and several powerful baselines. Our results show that our model consistently outperforms them, particularly in comparison to the classical FL method. All resources can be found at https://github.com/jcwang123/GDD-FL.","Languages with comprehensive alphabets in written form, such as the ideographic system of Chinese adopted to Japanese, have specific combinatorial potential for text summarization and categorization. Modern Japanese text is composed of strings over the Roman alphabet, components of two phonetic systems, Japanese syllabaries hiragana and katakana, and Chinese characters. This richness of information expression facilitates, unlike from most other languages, creation of synonyms and paraphrases, which may but do not need to be context-wise substantiable, depending not only on circumstance but also on the user of the text. Therefore readability of Japanese text is largely individual; it depends on education and incorporates life-long experience. This work presents a quantitative study into common readability factors of Japanese text, for which thirteen text markers were developed. Our statistical analysis expressed as a numerical readability index is accompanied by categorization of text contents, which is visualized as a specific location on self-organizing map over a reference text corpus.","The long-tailed characteristic leads to a significant performance drop for various models on long-tailed distribution datasets. Existing works mainly focus on mitigating the data shortage in tail classes at dataset level by data re-sampling, loss re-weighting or knowledge transfer from head to tail. While in this paper, we focus on another perspective which is also related to the performance drop: the gap between total dataset class number and training batch size. To address this issue, we propose a Weight-Guided (WG) loss which utilizes the classifier weights as auxiliary tail samples. It can be easily deployed to different models. By simply adding WG loss to Mask R-CNN with ResNet-50 backbone, we improve the performance by (i) 0.5 box AP and 0.4 mask AP on COCO dataset, (ii) 0.4 box and mask AP (1.8 mask AP for rare classes) on LVIS v1.0 dataset. Codes will be released.\nHighlights\n\u2022\nPoint out the small class ratio covered in one batch can cause gradient shifting.\n\u2022\nPropose a weight-guided loss to overcome the limitation of small batch size.\n\u2022\nOur WG loss improves various frameworks consistently, especially for rare classes.","Highlights\n\u2022\nA novel non-parallel bounded support matrix machine (NPBSMM) is proposed.\n\u2022\nA constraint norm group (CNG) is constructed, which can suppress negative influence of outliers and enhance robustness.\n\u2022\nThe dual problem of NPBSMM avoids the calculation of matrix inversion.\n\u2022\nMulti-rank left and right projection matrices are employed to realize a better ability of data fitting.\nAbstract\nAt present, the excellent performance of support vector machine (SVM) has made it successfully applied in many fields. However, when SVM is used for two-dimensional matrix data classification, vectorization of these data easily leads to dimension curse and the loss of structural information. Moreover, SVM is sensitive to outliers, which causes the hyperplane to move towards outliers. Therefore, this paper proposes a novel classification method for data in matrix-form, named non-parallel bounded support matrix machine (NPBSMM). In NPBSMM, a constraint norm group (CNG) is constructed and applied to objective function, which can not only suppress the negative impact of outliers on the model, but also make NPBSMM has better sparsity. By constructing CNG, the operation of matrix inversion in dual problem of traditional classification methods is avoided, so NPBSMM is more suitable for solving large-scale data problems. Further, to extract structure information of matrix for modeling, multi-rank left and right projection matrices are employed to establish objective function, which makes NPBSMM has a better ability of data fitting. Experiments performed on three roller bearing fault datasets show that the proposed NPBSMM method has powerful performance and robustness as compared with other typical classification methods.","The point cloud is a 3D geometric data of irregular format. As a result, they are needed to be transformed into 3D voxels or a collection of images before being fed into models. This unnecessarily increases the volume of the data and the complexities of dealing with it. PointNet is a pioneering approach in this direction that feeds the 3D point cloud data directly to a model. This research work is developed on top of the existing PointNet architecture. The ModelNet10 dataset, a collection of 3D images with 10 class labels, has been used for this study. The goal of the study is to improve the accuracy of PointNet. To achieve this, a few variations of encoder models have been proposed along with improved training protocol, and transfer learning from larger datasets in this research work. Also, an extensive hyperparameter study has been done. The experiments in this research work achieve a 6.10% improvement over the baseline model. The code for this work is publicly available at https://github.com/snehaputul/ImprovedPointCloud.","Modern power systems are experiencing the challenge of high uncertainty with the increasing penetration of renewable energy resources and the electrification of heating systems. In this paradigm shift, understanding electricity users\u2019 demand is of utmost value to the retailers, aggregators, and policymakers. However, behind-the-meter (BTM) equipment and appliances at the household level are unknown to the other stakeholders mainly due to privacy concerns and tight regulations. In this paper, we seek to identify residential consumers based on their BTM equipment, mainly rooftop photovoltaic (PV) systems and electric heating, using imported/purchased energy data from utility meters. To solve this problem with an interpretable, fast, secure, and maintainable solution, we propose an integrated method called Interpretable Refined Motifs And binary Classification (IRMAC). The proposed method comprises a novel shape-based pattern extraction technique, called Refined Motif (RM) discovery, and a single-neuron classifier. The first part extracts a sub-pattern from the long time series considering the frequency of occurrences, average dissimilarity, and time dynamics while emphasising specific times with annotated distances. The second part identifies users\u2019 types with linear complexity while preserving the transparency of the algorithms. With the real data from Australia and Denmark, the proposed method is tested and verified in identifying PV owners and electrical heating system users. The performance of the IRMAC is studied and compared with various state-of-the-art methods. The proposed method reached an accuracy of 96% in identifying rooftop PV users and 94.4% in identifying electric heating users, which is comparable to the best solution based on deep learning, while the speed of the inference model is a thousand times faster. Last but not least, the proposed method is a transparent algorithm, which can tackle the concerns regarding the agnostic decision-making process when policies prohibit some machine learning methods.","Sleep is one of the elements most vital to human life. However, the modern lifestyle continues to push people to neglect this critical requirement. With a vast majority of people falling victim to various sleep disorders, it has become increasingly essential to have a robust system for diagnosing and treating such ailments. Sleep stage classification is one of the primary steps for identifying sleep-related anomalies. Sleep stages are classified according to the frequency and nature of signals received during a polysomnography test. Since the early days, this has been performed manually with the help of trained technicians. However, manual scoring is often prone to error and subjectivity and requires tremendous time and effort. It is, therefore, essential to automatize this process. Several challenges from the correct selection of features remain to be faced in the machine learning-based sleep stage classification system. As an alternative, Deep Learning, capable of automatic feature extraction, proves far more reliable for this task. This experimental study analyses both techniques to compare and decide on a better approach. Three popular Machine Learning classifiers, namely Random Forest (RF), K-Nearest Neighbours (KNN), and Support Vector Machines (SVM), and a neural network comprising CNN and LSTM, have been trained on a vast base of diverse data. The proposed model reported an accuracy of 87.4% with CNN\u202f+\u202fLSTM.","Most recent machine learning research focuses on developing new classifiers for the sake of improving classification accuracy. With many well-performing state-of-the-art classifiers available, there is a growing need for understanding interpretability of a classifier necessitated by practical purposes such as to find the best diet recommendation for a diabetes patient. Inverse classification is a post modeling process to find changes in input features of samples to alter the initially predicted class. It is useful in many business applications to determine how to adjust a sample input data such that the classifier predicts it to be in a desired class. In real world applications, a budget on perturbations of samples corresponding to customers or patients is usually considered, and in this setting, the number of successfully perturbed samples is key to increase benefits. In this study, we propose a new framework to solve inverse classification that maximizes the number of perturbed samples subject to a per-feature-budget limits and favorable classification classes of the perturbed samples. We design algorithms to solve this optimization problem based on gradient methods, stochastic processes, Lagrangian relaxations, and the Gumbel trick. In experiments, we find that our algorithms based on stochastic processes exhibit an excellent performance in different budget settings and they scale well. The relative improvement of the proposed stochastic algorithms over an existing method with a traditional formulation is 15% in the real-world dataset and 21% in two public datasets on average.\nHighlights\n\u2022\nA new framework to solve inverse classification is developed.\n\u2022\nIt obtains successfully perturbed samples within a per-feature budget.\n\u2022\nAlgorithms are designed based on a gradient method and the Gumbel trick.\n\u2022\nReal data from an industrial partner and two healthcare public datasets are used.\n\u2022\nThe stochastic approaches perform great on various budget scenarios, and they scale.","Motivation. Immunoglobulin proteins (IGP) (also called antibodies) are glycoproteins that act as B-cell receptors against external or internal antigens like viruses and bacteria. IGPs play a significant role in diverse cellular processes ranging from adhesion to cell recognition. IGP identifications via the in-silico approach are faster and more cost-effective than wet-lab technological methods. Methods. In this study, we developed an intelligent theoretical deep learning framework, \u201cIGPred-HDnet\u201d for the discrimination of IGPs and non-IGPs. Three types of promising descriptors are feature extraction based on graphical and statistical features (FEGS), amphiphilic pseudo-amino acid composition (Amp-PseAAC), and dipeptide composition (DPC) to extract the graphical, physicochemical, and sequential features. Next, the extracted attributes are evaluated through machine learning, i.e., decision tree (DT), support vector machine (SVM), k-nearest neighbour (KNN), and hierarchical deep network (HDnet) classifiers. The proposed predictor IGPred-HDnet was trained and tested using a 10-fold cross-validation and independent test. Results and Conclusion. The success rates in terms of accuracy (ACC) and Matthew\u2019s correlation coefficient (MCC) of IGPred-HDnet on training and independent dataset (Dtrain Dtest) are ACC\u2009=\u200998.00%, 99.10%, and MCC\u2009=\u20090.958, and 0.980 points, respectively. The empirical outcomes demonstrate that the IGPred-HDnet model efficacy on both datasets using the novel FEGS feature and HDnet algorithm achieved superior predictions to other existing computational models. We hope this research will provide great insights into the large-scale identification of IGPs and pharmaceutical companies in new drug design.","Small sample is an acute problem in many application domains, which may be partially addressed by feature selection or dimensionality reduction. For the purpose of distance learning, we describe a method for feature selection using equivalence constraints between pairs of datapoints. The method is based on L1 regularization and optimization. Feature selection is then incorporated into an existing non-parametric method for distance learning, which is based on the boosting of constrained generative models. Thus the final algorithm employs dynamical feature selection, where features are selected anew in each boosting iteration based on the weighted training data. We tested our algorithm on the classification of facial images, using two public domain databases. We show the results of extensive experiments where our method performed much better than a number of competing methods, including the original boosting-based distance learning method and two commonly used Mahalanobis metrics.","Deep learning architectures used for text classification are becoming increasingly prevalent. However, the existing deep architectures have flaws such as slow speed, long training times, and the local minimum problem. Multi-layer Extreme Learning Machine has overcome these problems by avoiding backpropagation and thus saves a significant amount of training time, ensures global optimal, and can handle a vast quantity of data. The most important characteristic of Multi-layer ELM is its feature space (FS), which allows the input features to be linearly separated without using any kernel techniques. The architecture of Multi-layer ELM and its technique of feature mapping are examined in this research with the help of a novel feature selection technique termed as Correlation-based Feature Selection (CORFS). Empirical results of the proposed feature selection technique are compared with state-of-the-art techniques. Different classification algorithms are extensively tested on Multi-layer ELM feature space and on TFIDF vector space to demonstrate the efficiency of the feature mapping technique. Results of the experiment revealed that the proposed feature selection technique is better than the conventional feature selection techniques, and the feature space of Multi-layer ELM outperforms TFIDF.","Highlights\n\u2022\nWe propose a new two-stage noisy-label learning algorithm, called LongReMix.\n\u2022\nThe first stage finds a highly precise, but potentially small, set of clean samples.\n\u2022\nThe second stage is designed to be robust to small sets of clean samples.\n\u2022\nLongReMix reaches SOTA performance on the main noisy-label learning benchmarks.\nAbstract\nState-of-the-art noisy-label learning algorithms rely on an unsupervised learning to classify training samples as clean or noisy, followed by a semi-supervised learning (SSL) that minimises the empirical vicinal risk using a labelled set formed by samples classified as clean, and an unlabelled set with samples classified as noisy. The classification accuracy of such noisy-label learning methods depends on the precision of the unsupervised classification of clean and noisy samples, and the robustness of SSL to small clean sets. We address these points with a new noisy-label training algorithm, called LongReMix, which improves the precision of the unsupervised classification of clean and noisy samples and the robustness of SSL to small clean sets with a two-stage learning process. The stage one of LongReMix finds a small but precise high-confidence clean set, and stage two augments this high-confidence clean set with new clean samples and oversamples the clean data to increase the robustness of SSL to small clean sets. We test LongReMix on CIFAR-10 and CIFAR-100 with introduced synthetic noisy labels, and the real-world noisy-label benchmarks CNWL (Red Mini-ImageNet), WebVision, Clothing1M, and Food101-N. The results show that our LongReMix produces significantly better classification accuracy than competing approaches, particularly in high noise rate problems. Furthermore, our approach achieves state-of-the-art performance in most datasets. The code is available at https://github.com/filipe-research/LongReMix.","One of the most pressing challenges prevalent in the steel manufacturing industry is the identification of surface defects. Early identification of casting defects can help boost performance, including streamlining production processes. Though, deep learning models have helped bridge this gap and automate most of these processes, there is a dire need to come up with lightweight models that can be deployed easily with faster inference times. This research proposes a lightweight architecture that is efficient in terms of accuracy and inference time compared with sophisticated pre-trained CNN architectures like MobileNet, Inception, and ResNet, including vision transformers. Methodologies to minimize computational requirements such as depth-wise separable convolution and global average pooling (GAP) layer, including techniques that improve architectural efficiencies and augmentations, have been experimented. Our results indicate that a custom model of 590K parameters with depth-wise separable convolutions outperformed pretrained architectures such as Resnet and Vision transformers in terms of accuracy (81.87%) and comfortably outdid architectures such as Resnet, Inception, and Vision transformers in terms of faster inference times (12 ms). Blurpool fared outperformed other techniques, with an accuracy of 83.98%. Augmentations had a paradoxical effect on the model performance. No direct correlation between depth-wise and 3\u202f\u00d7\u202f3 convolutions on inference time, they, however, they played a direct role in improving model efficiency by enabling the networks to go deeper and by decreasing the number of trainable parameters. Our work sheds light on the fact that custom networks with efficient architectures and faster inference times can be built without the need of relying on pre-trained architectures.","Autism spectrum disorder (ASD) is a neurodevelopmental condition characterized in part by difficulties in verbal and nonverbal social communication. Evidence indicates that autistic people, compared to neurotypical peers, exhibit differences in head movements, a key form of nonverbal communication. Despite the crucial role of head movements in social communication, research on this nonverbal cue is relatively scarce compared to other forms of nonverbal communication, such as facial expressions and gestures. There is a need for scalable, reliable, and accurate instruments for measuring head movements directly within the context of social interactions. In this study, we used computer vision and machine learning to examine the head movement patterns of neurotypical and autistic individuals during naturalistic, face\u2013to\u2013face conversations, at both the individual (monadic) and interpersonal (dyadic) levels. Our model predicts diagnostic status using dyadic head movement data with an accuracy of , highlighting the value of head movement as a marker of social communication. The monadic data pipeline had lower accuracy () compared to the dyadic approach, emphasizing the importance of studying back-and-forth social communication cues within a true social context. The proposed classifier is not intended for diagnostic purposes, and future research should replicate our findings in larger, more representative samples.","Hierarchical text classification (HTC) is a challenging task that classifies textual descriptions with a taxonomic hierarchy. Existing methods have difficulties in modeling the hierarchical label structure. They focus on using the graph embedding methods to encode the hierarchical structure, ignoring that the HTC labels are based on a tree structure. There is a difference between tree and graph structure: in the graph structure, message passing is undirected, which will lead to the imbalance of message transmission between nodes when applied to HTC. As the nodes in different layers have inheritance relationships, the information transmission between nodes should be directional and hierarchical in the HTC task. In this paper, we propose a Top-Down Tree Structure Awareness Model to extract the hierarchical structure features, called ToSA. We regard HTC as a sequence generation task and introduce a priori hierarchical information in the decoding process. We block the information flow in one direction to ensure the graph embedding method is more suitable for the HTC task, then get the enhanced tree structure representation. Experiment results show that our model can achieve the best results on both the public WOS dataset and a collected E-commerce user intent classification dataset\n3\n.","Traffic safety is directly affected by poor road conditions. Automating the detection of road defects allows improvements in the maintenance process. The identification of defects such as cracks and potholes can be done using computer vision techniques and supervised learning. In this paper, we propose the detection of cracks and potholes in images of paved roads using machine learning techniques. The images are subdivided into blocks, where Gray-Level Co-Occurrence Matrix (GLCM), Local Binary Pattern (LBP), and Gabor Filter\u2019s texture descriptors are used to extract features of the images. For the classification task, the Support Vector Machines (SVM), k-Nearest Neighbors (kNN), and Multi-Layer Perceptron (MLP) models are compared. We performed two experiments on a dataset built with images of Brazilian highways. In the first experiment, we obtained a F-measure of 75.16% when classifying blocks of images that have cracks and potholes, and 79.56% when comparing roads with defects and without defects. In the second experiment, a F-measure of 87.06% was obtained for the equivalent task. Thus, it is possible to state that the use of the techniques presented is feasible for locating faults in highways.","Geomorphic Flood Area is a plugin used to classify flood-prone areas using a linear binary classifier method based on Geomorphic Flood Index. However, key determinant data in this classification are geographic data like Digital Elevation Model. But other determinant factors are neglected in the Geomorphic Flood Area plugin. For example, land cover data. The purpose of this research is to develop the Geomorphic Flood Area by adding land cover as a determinant factor. This research was conducted by finding the runoff coefficient value based on land cover, then adding the runoff coefficient value to the Geomorphic Flood Index calculation. The result shows there is an increase between F1-score of the Geomorphic Flood Area plugin and F1-score of the Geomorphic and Land Cover Flood Area plugin. The conclusion of this research is F1-score of the Geomorphic and Land Cover Flood Area plugin is more accurate in flood-prone area classification because if flow accumulation data that is used in Geomorphic Flood Index calculation, multiplied by runoff coefficient value, flow accumulation data becomes more accurate.","Highlights\n\u2022\nWe approach object point cloud classification from a more practical perspective, and propose the single-view, partial setting under which point clouds covering the partial surface of object instances are observed.\n\u2022\nWe discuss the limitations of existing methods, and show that their performance drops drastically under the practical setting.\n\u2022\nWe propose a baseline method of Pose-Accompanied Point cloud classification Network (PAPNet), which accompanies the classification task with an auxiliary one of supervised object pose learning.\n\u2022\nTo advance the research field, we adapt existing ModelNet40 and ScanNet benchmarks to the single-view, partial setting.\nAbstract\nObject point cloud classification has drawn great research attention since the release of benchmarking datasets, such as the ModelNet and the ShapeNet. These benchmarks assume point clouds covering complete surfaces of object instances, for which plenty of high-performing methods have been developed. However, their settings deviate from those often met in practice, where, due to (self-)occlusion, a point cloud covering partial surface of an object is captured from an arbitrary view. We show in this paper that performance of existing point cloud classifiers drops drastically under the considered single-view, partial setting; the phenomenon is consistent with the observation that semantic category of a partial object surface is less ambiguous only when its distribution on the whole surface is clearly specified. To this end, we argue for a single-view, partial setting where supervised learning of object pose estimation should be accompanied with classification. Technically, we propose a baseline method of Pose-Accompanied Point cloud classification Network (PAPNet); built upon S E ( 3 )-equivariant convolutions, the PAPNet learns intermediate pose transformations for equivariant features defined on vector fields, which makes the subsequent classification easier (ideally) in the category-level, canonical pose. By adapting existing ModelNet40 and ScanNet datasets to the single-view, partial setting, experiment results can verify the necessity of object pose estimation and superiority of our PAPNet to existing classifiers.","Joint intent detection and slot filling, which is also termed as joint NLU (Natural Language Understanding) is invaluable for smart voice assistants. Recent advancements in this area have been heavily focusing on improving accuracy using various techniques. Explainability is undoubtedly an important aspect for deep learning-based models including joint NLU models. Without explainability, their decisions are opaque to the outside world and hence, have tendency to lack user trust. Therefore to bridge this gap, we transform the full joint NLU model to be 'inherently' explainable at granular levels without compromising on accuracy. Further, as we enable the full joint NLU model explainable, we show that our extension can be successfully used in other general classification tasks. We demonstrate this using sentiment analysis and named entity recognition.","Constrained clustering is a semi-supervised task that employs a limited amount of labelled data, formulated as constraints, to incorporate domain-specific knowledge and to significantly improve clustering accuracy. Previous work has considered exact optimization formulations that can guarantee optimal clustering while satisfying all constraints, however these approaches lack interpretability. Recently, decision trees have been used to produce inherently interpretable clustering solutions, however existing approaches do not support clustering constraints and do not provide strong theoretical guarantees on solution quality. In thiswork, we present a novel SAT-based framework for interpretable clustering that supports clustering constraints and that also provides strong theoretical guarantees on solution quality. We also present new insight into the trade-off between interpretability and satisfaction of such user-provided constraints. Our framework is the first approach for interpretable and constrained clustering. Experiments with a range of real-world and synthetic datasets demonstrate that our approach can produce high-quality and interpretable constrained clustering solutions.","Finding the right business partner to drive innovation or acquire technology transfer is a labor and time-intensive process. To simplify this process, there is a need for improved methods of automated matchmaking that can quickly identify the best potential collaboration partners. This paper presents a novel approach for semi-automated business matchmaking between companies and research institutes, that is applied to a first case study. For this purpose, we compare two transformer-based text classification models and evaluate how dataset quality affects few-shot learning performance. Flair's TARS classifier performed very well in our use case, requiring only 40 examples per class to achieve an F1 score of about 90%. This is already very close to the Hugging Face standard text classifier, which achieved an F1 score of 92% with much more annotation effort. The results show that few-shot learning models like TARS can achieve accurate results even with few training samples compared to regular transformer-based language models. Our novel approach allows the time-consuming and labor-intensive task of manual partner matchmaking to be significantly reduced.","Node classification is an important task in many fields, e.g., predicting entity types in knowledge graphs, classifying papers in citation graphs, or classifying nodes in social networks. In many cases, it is crucial to explain why certain predictions are made. Towards this end, concept learning has been proposed as a means of interpretable node classification: given positive and negative examples in a knowledge base, concepts in description logics are learned that serve as classification models. However, state-of-the-art concept learners, including EvoLearner and CELOE exhibit long runtimes. In this paper, we propose to accelerate concept learning with graph sampling techniques. We experiment with seven techniques and tailor them to the setting of concept learning. In our experiments, we achieve a reduction in training size by over 90% while maintaining a high predictive performance.","We propose a joint model that performs instance-level feature selection and classification. For a given case, the joint model first skims the full feature vector, decides which features are relevant for that case, and makes a classification decision using only the selected features, resulting in compact, interpretable, and case-specific classification decisions. Because the selected features depend on the case at hand, we refer to this approach as context-aware feature selection and classification. The model can be trained on instances that are annotated by experts with both class labels and instance-level feature selections, so it can select instance-level features that humans would use. Experiments on several datasets demonstrate that the proposed model outperforms eight baselines on a combined classification and feature selection measure, and is able to better emulate the ground-truth instance-level feature selections. The supplementary materials are available at https://github.com/IIT-ML/IJCAI23-CFSC.","Lambani is an under-resourced Indo-Aryan language spoken by a nomadic tribe known as the \u2018Banjara people\u2019 across central and southern India. Due to its contact with several major languages of India, Lambani has been influenced both linguistically as well as culturally. One of the major influences has been observed in its phonemic inventory. This paper is a preliminary investigation into the acoustic characteristics of vowels of the language. The paper analyses spectral and temporal features of six Lambani vowels, viz. [inline-graphic not available: see fulltext] spoken in the Bagalkot district of Karnataka. The results obtained throw light on the distinctiveness of this variety. The paper then uses spectral and temporal features to explore both machine learning and deep learning approaches to classify Lambani vowel perceptual space. Results show that Fully Connected Dense Layer achieves better accuracy in classifying Lambani vowels.","In multi-label learning, each instance is associated with a set of labels. To improve the accuracy and efficiency of multi-label learning tasks, label correlations have been explored. However, existing conventional algorithms obtain label correlation directly from the raw label space, ignoring the significance of the label to the instance. In this study, a novel feature selection method was proposed to select a more relevant and compact feature subset by considering the label distribution and inter-label correlations. First, the concept of label distribution was defined to reflect the significance of each label. Second, a new algorithm for mining association rules was designed to obtain the correlation between labels by improving the existing association rules algorithm. Thus, a new information system was designed by combining the label distribution and correlation between labels. Subsequently, a novel feature selection algorithm was designed in this information system which can effectively remove irrelevant and redundant features in the feature space. Finally, the experimental results demonstrated that the proposed algorithm effectively improves the classification performance and perform better than some state-of-the-art methods.","In this article we propose a two-step classification method. At the first step it constructs a tolerance relation from the data, and at second step it uses correlation clustering to construct the base sets, which are used at the classification of the objects. Besides the exposition of the theoretical background we also show this method in action: we present the details of the classification of the well-known iris data set. Moreover we frame some open question due this kind of classification.","For imbalanced classification, data-level methods can achieve inter-class balance, but the samples generated do not contain new information and cannot avoid the problem of introducing noise. Algorithm-level methods may lead to overfitting of the model, and its classification effect is more dependent on the specific dataset and classification task, which means they lack universality. In addition, how to deeply mine the differences in the distribution of data overlap areas, and how to effectively mine the differences between categories when the absolute number of minority samples is small, are also important challenges in imbalanced classification. This paper proposes an imbalanced binary classification method using multi-label confidence comparisons based on contrastive learning. Different from the previous idea of directly learning its distribution characteristics from minority samples, combined with the idea of contrastive learning, the classification task is redefined as the multi-label matching task by mining the deep features that can represent the commonality and difference between the neighboring samples. Multiple differentiated contrastive sample groups are obtained through random sampling in its neighbor sample pool for each sample. This sample is combined with its contrastive sample groups to form multiple sample-neighbor pairs as training samples in the multi-label matching task. The original dataset is multiplied without introducing noise, laying a foundation for the effective mining of class differences when the absolute number of minority class samples is small. Based on the corresponding reconstruction error generated by Variational AutoEncoder (VAE), for sample-neighbor pairs, a multi-label matching loss between target samples and contrastive sample groups that integrates the idea of contrastive learning is designed. a robust classifier is obtained through simultaneous iterative learning of reconstruction error and multi-label matching loss, which can better mine the distribution differences of overlapping regions. In the testing phase, multiple different contrastive sample groups and the corresponding prediction results of the samples to be classified are obtained, which categories can be judged by integrating the predictions of each group for reverse reasoning. Experimental results on 38 public datasets show that the method outperforms typical imbalanced classification methods in both F1-measure and G-mean.","In this paper, the problem of automatic document classification by a set of given topics is considered. The method proposed is based on the use of the latent semantic analysis to retrieve semantic dependencies between words. The classification of document is based on these dependencies. The results of experiments performed on the basis of the standard test data set TREC (Text REtrieval Conference) confirm the attractiveness of this approach. The relatively low computational complexity of this method at the classification stage makes it possible to be applied to the classification of document streams.","In the classification of benign and malignant breast tumors, in addition to the extraction of features, the selection of classifier is also an important factor affecting the accuracy of classification. At present, the ensemble classifier has limitations of local optimization and weak scalability in the integration process, which affect the accuracy and stability of classification. In this study, an ensemble classifier based on adaptive weighted ensemble is proposed to classify benign and malignant breast tumors. First, the original ultrasonic RF signal is preprocessed by down-sampling, dilation, and adaptive decomposition. Then, based on the intrinsic modal function obtained by decomposition, the ring region of interest (ROI) containing the tissue surrounding the tumor is determined. Next, texture features are extracted based on ring ROIs. Weighted k-nearest neighbors (KNN), bagged trees, and Gaussian Naive Bayes (NB) are integrated by bagging method, and each classifier is adaptively weighted based on genetic algorithms during the integration process. Finally, benign and malignant breast tumors are classified by an ensemble classifier based on adaptive weights. Experiments based on the Open Access Series of Breast Ultrasound Data database show that the classification accuracy of the proposed adaptive weight-based ensemble classifier is 90%, which is improved by 10.77%, 8.43%, 1.12% and 5.88% compared with the weighted KNN, bagged trees and Gaussian NB alone, and voting-based ensemble classifier. The conclusion is that the proposed ensemble classifier based on adaptive weighting can effectively improve the classification accuracy of breast tumors, which is helpful for the diagnosis of clinical breast tumors.","In this work, we propose DocLangID, a transfer learning approach to identify the language of unlabeled historical documents. We achieve this by first leveraging labeled data from a different but related domain of historical documents. Secondly, we implement a distance-based few-shot learning approach to adapt a convolutional neural network to new languages of the unlabeled dataset. By introducing small amounts of manually labeled examples from the set of unlabeled images, our feature extractor develops a better adaptability towards new and different data distributions of historical documents. We show that such a model can be effectively fine-tuned for the unlabeled set of images by only reusing the same few-shot examples. We showcase our work across 10 languages that mostly use the Latin script. Our experiments on historical documents demonstrate that our combined approach improves the language identification performance, achieving 74% recognition accuracy on the four unseen languages of the unlabeled dataset.","Human behavior recognition is one of the most important research directions in the field of computer vision, and it plays an important role in the fields of rehabilitative medicine, auxiliary security, and scene entertainment. To address the shortcomings of traditional HAR recognition methods with tedious feature extraction and severe overfitting, we propose a human behavior recognition model based on XGBoost and feature simplification methods with a limited data set. The model uses the XGBoost algorithm to classify the collected sensor data to recognize human behaviors. In addition, to improve the efficiency and accuracy of the model, we also propose a feature simplification method to reduce the computational complexity and the risk of model overfitting by reducing the number of features. Experimental results show that the model has high accuracy and computational efficiency and can be applied to different human behavior recognition scenarios.\nCCS Concepts: Computing methodologies\u223cMachine learning\u223cMachine learning approaches","The heart, as the main organ of our human body, plays an important role in pumping blood through our body. Early prevention and prediction of cardiovascular disease (CVD) can save more lives, especially for ordinary people. Hence, this study proposes a voting ensemble-based prediction model for the risk of CVD in ordinary people. We first integrate 2 years of data from the Korea National Health and Nutrition Examination Survey (KNHANES) and then extract the experimental data. Thereafter, the extracted data is preprocessed with missing value imputation and data normalization. A filter-based feature selection approach is also applied to select the efficient attributes for the experiment, then split the data into training (80%) data and test (20%) data. Thenceforth, we use two kinds of hybrid data sampling techniques such as synthetic minority oversampling techniques (SMOTE) plus Tomek Links (SMOTETomek) and SMOTE plus Edited Nearest Neighbors (SMOTEENN) to solve the imbalance issue in the training data. Next, the voting ensemble-based prediction model is designed based on different machine learning algorithms such as logistic regression, support vector machine, and AdaBoost on the balanced training data with selected features and complete features. Lastly, the proposed model is evaluated on the test data and compared with other popularly used machine learning-based models. In the experimental results, the proposed voting ensemble-based prediction model with the SMOTEENN technique on selected features by using the filter-based feature selection approach achieved the best performance with the accuracy of 0.8102, recall 0.8102, g-mean 0.8102, and AUC 0.8102, respectively for the risk of CVD in ordinary people and outperformed other machine learning-based prediction models.","The neural network has a good nonlinear fitting ability, which is also a cornerstone of deep learning. However, some disadvantages of neural networks are higher complexity, more iterations and longer training time. In deep learning, the gradient is easy to disappear when the network has many hidden layers. Considering the inherent defects of neural networks, we propose a neural network model: the Granular Neural network Classifier (GNC). Firstly, a reference frame is constructed by a random sampling method. In the reference frame, a training sample is granulated on features to form conditional granules by the feature similarity, and these granules are combined into a conditional granular vector. Meanwhile, the training sample on the decision feature is expanded to a decision granule. Furthermore, some measures and operations of granules are defined, and several activation functions of granules are constructed, then the GNC is proposed. Further, a loss function of the GNC is presented, and its derivative form is proved. Therefore, a gradient descent algorithm of the GNC is designed. Since the granules have good structural characteristics, the GNC can be parallel computed. Finally, several UCI datasets and image datasets are used to test the GNC from some aspects on the number of network layers, the influence of reference frames and classification accuracy. The experimental results show that the GNC is correct and effective. In addition, the GNC with less hidden layers achieves a good result as the traditional neural network with multi layers, which relieves the problem of gradient disappearance.","Planktons are the building blocks of marine food webs and key indicators of ocean health. Monitoring of plankton populations help study the biological diversity of microbial eukaryotes. Recent years have witnessed the wide usage of digital holographic microscopes (DHM) for in situ detection of underwater microplanktons. Holography has an edge over other imaging techniques due to its unique ability to provide a 3D hologram of the microplankton without disturbing its orientations. In this paper, a novel network architecture with 5.29 GFLOPs is developed for the classification of microplanktons in digital holographic images. The proposed method achieved a class-wise F1-scores above\n80\n%\nat a lower computational cost. The proposal provided competitive performance with respect to six baseline network architectures. This technique has the potential to be appealing for future applications of in situ classification of microplanktons.","Vision Transformers (VTs) are becoming a valuable alternative to Convolutional Neural Networks (CNNs) when it comes to problems involving high-dimensional and spatially organized inputs such as images. However, their Transfer Learning (TL) properties are not yet well studied, and it is not fully known whether these neural architectures can transfer across different domains as well as CNNs. In this paper we study whether VTs that are pre-trained on the popular ImageNet dataset learn representations that are transferable to the non-natural image domain. To do so we consider three well-studied art classification problems and use them as a surrogate for studying the TL potential of four popular VTs. Their performance is extensively compared against that of four common CNNs across several TL experiments. Our results show that VTs exhibit strong generalization properties and that these networks are more powerful feature extractors than CNNs.","Recently, solving ordinal classification problems using machine learning and deep learning techniques has acquired important attention. There are many real-world problems in different areas of knowledge where a categorical variable needs to be predicted, and the existing categories follow an order associated with the nature of the problem: e.g. medical diagnosis with different states of a disease, or industrial quality assessment with different levels of quality. In these problems, it is quite common that the final label for each sample is determined by a group of experts with different opinions, and all opinions are usually summarised in a single crisp label by means of a given statistic (e.g. the median or the mode). Applying standard ordinal classifiers to these crisp labels could result in overfitting, as the labelling information is considered as totally certain. In this work, we propose a unimodal regularisation approach based on soft labelling, i.e. the ordinal information is used to introduce the inherent uncertainty of the label fusion. Specifically, said regularisation is based on using triangular distributions to simulate the aforementioned fusion of the expert opinions, where a parameter is used to decide the amount of probability that is assigned to the target category and the adjacent ones (according to the ordinal scale). The strategy could be applied to the loss function used by any ordinal classification learning algorithm, but we focus on deep learning in this paper. The proposal is compared to a baseline approach for nominal classification tasks and other state-of-the-art unimodal regularisation methods, and the experimental validation includes six benchmark datasets and five performance metrics. The results along with the statistical analysis show that the proposed methodology significantly outperforms the rest of the methods.\nHighlights\n\u2022\nUnimodal regularised loss function based on triangular distributions.\n\u2022\nApplication to different CNN models and ordinal benchmark datasets.\n\u2022\nComparison with other regularisation methods based on different distributions.\n\u2022\nImprovement of state-of-the-art classification performance regarding different metrics.","With the introduction of spatial-spectral fusion and deep learning, the classification performance of hyperspectral imagery (HSI) has been promoted greatly. For some widely used datasets, the classification accuracy almost reaches 100%. However, for hyperspectral image classification, random sampling is still the most common strategy to collect the training and test samples. Because the training and test samples are randomly selected from the same image, so they have a high correlation and the classification results are overoptimistic. Besides, random sampling is not a good choice for practical applications because we cannot always collect training and test samples from the same region. Disjoint sampling selects training and testing samples from different local regions, which will provide a more objective performance evaluation for HSI classification models. In this paper, we first show the huge classification performance difference caused by different sampling strategies with a simple experiment, then we analyze the underlying reasons from the spectral information, spatial-spectral combination, sample overlapping and spatial distance, finally, a semi-supervised feature learning method is proposed for disjoint HSI classification, in which the spatial and spectral information are exploited effectively and reasonably. The experimental results based on three HSI datasets demonstrate the effectiveness of the proposed method.","The natural distribution of textual data used in text classification is often imbalanced. Categories with fewer examples are under-represented and their classifiers trained on the datasets transformed to bag-of-words representations or basic topic modeling transformations often perform far below a satisfactory level. We tackle this problem using a two-pass non-negative matrix factorization algorithm. This approach finds topics for each category independently allowing to better define topics for underrepresented categories. The results are analyzed from multiple goal perspectives - H-loss, accuracy, F-measure, precision, and recall, from the micro, macro and example-based aspect since each is appropriate in different situations. Through experimental validation, it is shown that the two-pass matrix factorization improves classification results achieved using bag-of-words representations.","Meta-learning excels in few-shot learning by extracting shared knowledge from the observed tasks. However, it needs the tasks to adhere to the i.i.d. constraint, which is challenging to achieve due to complex task relationships between data content. Current methods that create tasks in a one-dimensional structure and use meta-learning to learn all tasks flatly struggle with extracting shared knowledge from tasks with overlapping concepts. To address this issue, we propose further constructing tasks from the same environment into hyper-tasks. Since the distributions of hyper-tasks and tasks in a hyper-task can both be approximated as i.i.d. due to further summarization, the meta-learning algorithm can capture shared knowledge more efficiently. Based on the hyper-task, we propose a hierarchical meta-learning paradigm to meta-learn the meta-learning algorithm. The paradigm builds a customized meta-learner for each hyper-task, which makes meta-learners more flexible and expressive. We apply the paradigm to three classic meta-learning algorithms and conduct extensive experiments on public datasets, which confirm the superiority of hierarchical meta-learning in the few-shot learning setting. The code is released at https://github.com/tuantuange/H-meta-learning.","Electroencephalogram (EEG)-based major depressive disorder (MDD) machine learning detection models can objectively differentiate MDD from healthy controls but are limited by high complexities or low accuracies. This work presents a self-organized computationally lightweight handcrafted classification model for accurate MDD detection using a reference subject-based validation strategy. We used the public Multimodal Open Dataset for Mental Disorder Analysis (MODMA) comprising 128-channel EEG signals from 24 MDD and 29 healthy control (HC) subjects. The input EEG was decomposed using multilevel discrete wavelet transform with Daubechies 4 mother wavelet function into eight low- and high-level wavelet bands. We used a novel Twin Pascal\u2019s Triangles Lattice Pattern(TPTLP) comprising an array of 25 values to extract local textural features from the raw EEG signal and subbands. For each overlapping signal block of length 25, two walking paths that traced the maximum and minimum L1-norm distances from v1 to v25 of the TPTLP were dynamically generated to extract features. Forty statistical features were also extracted in parallel per run. We employed neighborhood component analysis for feature selection, a k-nearest neighbor classifier to obtain 128 channel-wise prediction vectors, iterative hard majority voting to generate 126 voted vectors, and a greedy algorithm to determine the best overall model result. Our generated model attained the best channel-wise and overall model accuracies. The generated system attained an accuracy of 76.08% (for Channel 1) and 83.96% (voted from the top 13 channels) using leave-one-subject-out(LOSO) cross-validation (CV) and 100% using 10-fold CV strategies, which outperformed other published models developed using same (MODMA) dataset.","One of the major challenges in drug development is having acceptable levels of efficacy and safety throughout all the phases of clinical trials followed by the successful launch in the market. While there are many factors such as molecular properties, toxicity parameters, mechanism of action at the target site, etc. that regulates the therapeutic action of a compound, a holistic approach directed towards data-driven studies will invariably strengthen the predictive toxicological sciences. Our quest for the current study is to find out various reasons as to why an investigational candidate would fail in the clinical trials after multiple iterations of refinement and optimization. We have compiled a dataset that comprises of approved and withdrawn drugs as well as toxic compounds and essentially have used time-split based approach to generate the training and validation set. Five highly robust and scalable machine learning binary classifiers were used to develop the predictive models that were trained with features like molecular descriptors and fingerprints and then validated rigorously to achieve acceptable performance in terms of a set of performance metrics. The mean AUC scores for all the five classifiers with the hold-out test set were obtained in the range of 0.66\u20130.71. The models were further used to predict the probability score for the clinical candidate dataset. The top compounds predicted to be toxic were analyzed to estimate different dimensions of toxicity. Apparently, through this study, we propose that with the appropriate use of feature extraction and machine learning methods, one can estimate the likelihood of success or failure of investigational drugs candidates thereby opening an avenue for future trends in computational toxicological studies. The models developed in the study can be accessed at https://github.com/gnsastry/predicting_clinical_trials.git.\nGraphical abstract\nDisplay Omitted\nHighlights\n\u2022\nEstimate the likelihood of success or failure of investigational candidates.\n\u2022\nFive highly robust and scalable machine learning algorithms have been used.\n\u2022\nDataset of 2,212 compounds consisting of approved and withdrawn drugs.\n\u2022\nTime-split approach generates training and hold-out test set.","In this paper, we present our new and automatically tuned gradient boosting software, Classifium GB, which beats its closest competitor, H2O, for all datasets that we ran. The primary reason that we found it easy to develop Classifium GB is that we employed meta machine learning, based on evolution, to automatically program its most important parts.\nGradient boosting is often the most accurate classification algorithm for tabular data and quite popular in machine learning competitions. However, its practical use has been hampered by the need to skilfully tune many hyperparameters in order to achieve the best accuracy.\nClassifium GB contains novel regularization methods and has automatic tuning of all regularization parameters. We show that Classifium GB gives better accuracy than another automatically tuned algorithm, H2O, and often also outperforms manually tuned algorithms such as XGBoost, LightGBM and CatBoost even if the tuning of these is done with exceptional care and uses huge computational resources.\nThus, our new Classifium GB algorithm should rapidly become the preferred choice for practically any tabular dataset. It is quite easy to use and even say Random Forest or C5.0 require more skilled users. The primary disadvantage is longer run time.","Evaluating speaker emotion in conversations is crucial for various applications requiring human-computer interaction. However, co-occurrences of multiple emotional states (e.g. 'anger' and 'frustration' may occur together or one may influence the occurrence of the other) and their dynamic evolution may vary dramatically due to the speaker's internal (e.g., influence of their personalized socio-cultural-educational and demographic backgrounds) and external contexts. Thus far, the previous focus has been on evaluating only the dominant emotion observed in a speaker at a given time, which is susceptible to producing misleading classification decisions for difficult multi-labels during testing. In this work, we present Self-supervised Multi- Label Peer Collaborative Distillation (SeMuL-PCD) Learning via an efficient Multimodal Transformer Network, in which complementary feedback from multiple mode-specific peer networks (e.g.transcript, audio, visual) are distilled into a single mode-ensembled fusion network for estimating multiple emotions simultaneously. The proposed Multimodal Distillation Loss calibrates the fusion network by minimizing the Kullback-Leibler divergence with the peer networks. Additionally, each peer network is conditioned using a self-supervised contrastive objective to improve the generalization across diverse socio-demographic speaker backgrounds. By enabling peer collaborative learning that allows each network to independently learn their mode-specific discriminative patterns,SeMUL-PCD is effective across different conversation environments. In particular, the model not only outperforms the current state-of-the-art models on several large-scale public datasets (e.g., MOSEI, EmoReact and ElderReact), but with around 17% improved weighted F1-score in the cross-dataset experimental settings. The model also demonstrates an impressive generalization ability across age and demography-diverse populations.","Plant pathogens in maize create a severe impact that directly affects agricultural productivity. The foliar disease affects maize growth, where diagnosing and controlling them becomes challenging for farmers. Automatic and early detection of such conditions will aid in the prevention of yield loss by providing appropriate treatment. Leaf textures play a significant role in plant disease recognition, and analyzing them makes the task faster and more efficient. With the computer vision approach, we fused an image processing technique called Gabor filtering as a core pipeline for extracting textural features effectively. This paper proposes an enhanced Convolutional Neural Network (CNN) with Gabor filters called GF-CNN for maize disease classification. Several experiments have been conducted with both machine learning classifiers and CNN and a comparison study was made with the existing approaches. Furthermore, the analysis of the proposed method on maize Plant Village datasets shows that GF-CNN outperforms other existing models with improved accuracy of 99.25%. We also experimented by limiting training samples and attained a significant improvement. Thus, exploring Gabor\u2019s textural patterns for recognizing crop disease can increase the robustness of the classification model.","Smart speaker voice assistants (VAs) such as Amazon Echo and Google Home have been widely adopted due to their seamless integration with smart home devices and the Internet of Things (IoT) technologies. These VA services raise privacy concerns, especially due to their access to our speech. This work considers one such use case: the unaccountable and unauthorized surveillance of a user's emotion via speech emotion recognition (SER). This paper presents DARE-GP, a solution that creates additive noise to mask users' emotional information while preserving the transcription-relevant portions of their speech. DARE-GP does this by using a constrained genetic programming approach to learn the spectral frequency traits that depict target users' emotional content, and then generating a universal adversarial audio perturbation that provides this privacy protection. Unlike existing works, DARE-GP provides: a) real-time protection of previously unheard utterances, b) against previously unseen black-box SER classifiers, c) while protecting speech transcription, and d) does so in a realistic, acoustic environment. Further, this evasion is robust against defenses employed by a knowledgeable adversary. The evaluations in this work culminate with acoustic evaluations against two off-the-shelf commercial smart speakers using a small-form-factor (raspberry pi) integrated with a wake-word system to evaluate the efficacy of its real-world, real-time deployment.","Data-efficient learning on graphs (GEL) is essential in real-world applications. Existing GEL methods focus on learning useful representations for nodes, edges, or entire graphs with \"small\" labeled data. But the problem of data-efficient learning for subgraph prediction has not been explored. The challenges of this problem lie in the following aspects: 1) It is crucial for subgraphs to learn positional features to acquire structural information in the base graph in which they exist. Although the existing subgraph neural network method is capable of learning disentangled position encodings, the overall computational complexity is very high. 2) Prevailing graph augmentation methods for GEL, including rule-based, sample-based, adaptive, and automated methods, are not suitable for augmenting subgraphs because a subgraph contains fewer nodes but richer information such as position, neighbor, and structure. Subgraph augmentation is more susceptible to undesirable perturbations. 3) Only a small number of nodes in the base graph are contained in subgraphs, which leads to a potential \"bias\" problem that the subgraph representation learning is dominated by these \"hot\" nodes. By contrast, the remaining nodes fail to be fully learned, which reduces the generalization ability of subgraph representation learning. In this paper, we aim to address the challenges above and propose a Position-Aware Data-Efficient Learning framework for subgraph neural networks called PADEL. Specifically, we propose a novel node position encoding method that is anchor-free, and design a new generative subgraph augmentation method based on a diffused variational subgraph autoencoder, and we propose exploratory and exploitable views for subgraph contrastive learning. Extensive experiment results on three real-world datasets show the superiority of our proposed method over state-of-the-art baselines.","Highlights\n\u2022\nWe design a new grouping scheme. It can provide not only a theoretical basis for selecting the minority class samples in an oversampling method but also a new explanation for the poor performance of SVM on imbalanced data sets.\n\u2022\nWe design a new oversampling algorithm for generating the minority class samples, which can effectively reduce the bias of the decision hyperplane obtained on the imbalanced data sets toward the minority class. At the same time, it makes full use of the repeated sample pairs and reduces the risk of overfitting of the classifier trained on the balanced data set.\n\u2022\nExtensive experimental results show that the proposed oversampling method outperforms the compared benchmark algorithms.\nAbstract\nThe class-imbalanced classification is a difficult problem because not only traditional classifiers are more biased towards the majority classes and inclined to generate incorrect predictions, but also the existing algorithms often have difficulty tackling this kind of problem with the class overlapping. Oversampling is a widely used and effective method to obtain balanced samples for imbalanced data, but the existing oversampling methods usually result in more serious class overlapping due to improper choice of the reference samples. To circumvent this shortcoming, according to the different possibilities of minority class samples appearing in the overlapping regions in the feature space, a grouping scheme for the minority class samples is first designed to identify the overlapping region samples. Then, a new oversampling method based on this grouping scheme is proposed to make the new samples far away from the overlapping region and rectify the decision boundary properly. Subsequently, a new effective classification algorithm is developed for imbalanced data. Extensive experiments show that the proposed algorithm is superior to the seventeen benchmark algorithms in terms of three performance metrics, especially on high imbalance ratio data sets.","With the continuous development of information technology, massive information processing has become an important problem in business systems. However, the metadata information from different business systems lacks a unified and standardized description method. Mapping data by the manual way greatly reduces the efficiency. Therefore, an automated data mapping method is very necessary. In this paper, we regard data mapping as a text classification problem based on the following reasons: 1) the text classification technology has become more and more mature in the field of the natural language processing (NLP), which is very suitable for processing massive data; 2) a large number of heterogeneous mapping data can be treated as text. In order to implement automated data mapping, in this paper, we propose a classification model based on FastText and long-short term memory (LSTM) for data mapping in business systems. By observing the characteristics of mapping data in business systems, we firstly use FastText to learn word representation containing semantic information, and then adopt the LSTM model to extract features for text classification automatically. Experimental results show that the proposed method can automatically classify mapping data in business systems with common quality.","Highlights\n\u2022\nMulti-label classification models to predict pipe failures in water supply networks.\n\u2022\nAvoided pipe failures\u2019 percentage raises as the time period to predict for grows.\n\u2022\nA uninorm is used to integrate the output probabilities of the different years.\nAbstract\nThe unexpected failure of pipes is a problem that is hitting the water networks of many cities around the world. Nowadays, many proposals based on the use of machine learning techniques are emerging to combat this problem. However, most studies focus their efforts on predicting failures in short time periods, usually a year, while longer time period predictions would be more valuable to address strategic decisions.\nIn this study, the use of multi-label classification techniques is proposed to simultaneously predict pipe failures in water supply systems for multiple years. For this purpose, three models (discriminant analysis, logistic regression and random forest) and different prediction time periods (one, two and three years) have been analysed. As multi-label data require specific quality metrics and sampling techniques, part of this work is dedicated to their exploration and discussion.\nThe models are evaluated on a real-world seven-year database, achieving successful results. An insightful analysis of the use of the methodology shows how the percentage of avoided pipe failures increases over time. In fact, it is demonstrated that 30.2%, 51.4% and 54.0% of the pipe failures of three consecutive years are avoided according to data from a real network.","In scenarios with long-tailed distributions, the model's ability to identify tail classes is limited due to the under-representation of tail samples. Class rebalancing, information augmentation, and other techniques have been proposed to facilitate models to learn the potential distribution of tail classes. The disadvantage is that these methods generally pursue models with balanced class accuracy on the data manifold, while ignoring the ability of the model to resist interference. By constructing noisy data manifold, we found that the robustness of models trained on unbalanced data has a long-tail phenomenon. That is, even if the class accuracy is balanced on the data domain, it still has bias on the noisy data manifold. However, existing methods cannot effectively mitigate the above phenomenon, which makes the model vulnerable in long-tailed scenarios. In this work, we propose an Orthogonal Uncertainty Representation (hOUR) of feature embedding and an end-to-end training strategy to improve the long-tail phenomenon of model robustness. As a general enhancement tool, OUR has excellent compatibility with other methods and does not require additional data generation, ensuring fast and efficient training. Comprehensive evaluations on long-tailed datasets show that our method significantly improves the long-tail phenomenon of robustness, bringing consistent performance gains to other long-tailed learning methods.","This paper presents an interpretation as classification problem for standard desirability and other instances of nonlinear desirability (convex coherence and positive additive coherence). In particular, we analyze different sets of rationality axioms and, for each one of them, we show that proving that a subject respects these axioms on the basis of a finite set of acceptable and a finite set of rejectable gambles can be reformulated as a binary classification problem where the family of classifiers used changes with the axioms considered. Moreover, by borrowing ideas from machine learning, we show the possibility of defining a feature mapping, which allows us to reformulate the above nonlinear classification problems as linear ones in higher-dimensional spaces. This allows us to interpret gambles directly as payoffs vectors of monetary lotteries, as well as to provide a practical tool to check the rationality of an agent.","Human activity recognition (HAR) models suffer significant performance degradation when faced with data heterogeneity (device, users, environments) at test time. Current approaches to this problem using domain adaptation or transfer learning attempt to improve performance in one specific target domain, often using data from said domain. Requiring access to data from the target domain is limiting and cannot be generally assumed. In addition, there is often no single target domain, but rather multiple ones arising from different sources of data heterogeneity. One way to achieve good performance in this setting would be to gather data from all potential domains the model may encounter at deployment - this is generally infeasible.\nThis work presents the case for training models which are domain-agnostic, i.e., that generalise to unseen test domains. This requires a new way to evaluate models; we discuss a regime called leave-datasets-out, and present a starting benchmark for HAR using binary classification. Two state-of-the-art deep models in the literature are tested; they significantly under-perform in unseen domains when compared to their performance on seen domains. It is shown that under this new evaluation regime, a simple model with an appropriate inductive bias performs at least as well as two current deep models on the benchmark, with a p-value of 5.75x10\u2212 4 and 0.13 when testing for a difference in mean accuracy, whilst being at least 10 times faster to train. Additionally, we provide evidence that domain diversity under certain conditions improves performance on both seen and unseen domains. We hope this work provides useful insights to further develop HAR models suitable for real world deployment.","Highlights\n\u2022\nA 10 m crop type map with an overall accuracy of 93% was created.\n\u2022\nPhenological and textural features in spring are most informative.\n\u2022\nRF classifier with 20 RF-selected features generated the optimal results.\nAbstract\nThe North China Plain (NCP), a major agricultural area in China, plays an important role in China's grain production. Timely and accurate crop information for NCP is very important to China's food security and sustainable development. Due to high variability of the temporal profiles of vegetation indices, classification models using temporally aggregated remote sensing data often exhibit suboptimal performance for multi-crop classification in the NCP with complex cropping patterns. Therefore, optimal feature sets and classification models should be developed for efficient and accurate crop mapping in this region. In this study, we used all available Sentinel-2 imagery in 2020 to map major crops including winter wheat/corn, cotton, peanut, and millet in a typical winter wheat production city in the central part of the NCP. NDVI time series, textural, and phenological features from Sentinel-2 time series and topographic features of the study area were derived as input features (394 features in total). Two feature selection methods, random forest and unsupervised feature selection based on multi-subspace randomization and collaboration (SRCFS), were used to select 20 informative features from the 394 features. Then, four groups of features were evaluated with three machine learning classifiers, i.e., random forest (RF), support vector machine (SVM), and artificial neural network (ANN). The results showed that the most useful features for crop type classification for the region were phenological and textural features during February to March and April to May. When using the full feature set, RF provided the best results compared with SVM and ANN. However, both RF and SVM classifier with 20 RF-selected features generated the optimal results. The crops were identified with an overall accuracy of 93% and a kappa coefficient of 0.9 in the final 10-meter resolution crop map. The feature selection and machine learning classification methods can be applied to high-resolution crop mapping using time series of Sentinel-2 data in agricultural regions with mixed cropping patterns in an efficient manner.","According to the characteristics of complex feature information garbage classification application, a garbage classification method based on multi-source information fusion based on Bayesian network is proposed.In this method, the training sample data is preprocessed by La Pullaras smoothing method to solve the problem of zero prior probability in traditional Bayesian method and eliminate the influence of zero value prior probability on fusion results.Then, according to the decision information of the image sensor, combined with the multi-source heterogeneous characteristic information of other sensors of different types, the multi-source information fusion model is established by using the improved Bayesian parameter estimation algorithm.Bayesian networks are established and Bayesian classifiers are constructed to simplify the fusion results. Finally, the discriminant results are obtained by calculating the maximum posterior probability estimate.Through comparative experiments, the average discrimination accuracy of the improved data fusion method for complex feature information garbage samples is increased from 89.5% to 98.5%, which proves that the method can fully integrate multi-source heterogeneous feature information, reduce the high fuzziness of the discrimination process of hazardous waste and recyclable waste, so as to obtain more accurate classification results.This has important theoretical significance and practical value for the classification of complex garbage in daily life.","Cardiac Magnetic Resonance (CMR) is key in the evaluation of heart anatomy and function, and the diagnosis of multiple diseases. However, it requires extensive manual analysis by medical specialists, which slows the diagnostic process, and creates an additional burden for professionals. Different computational techniques have been proposed to automate and accelerate the segmentation of different heart structures within the images, and with the rise of Deep Learning (DL) techniques in the last decade, the performance has improved significantly. Nevertheless, there are still some limitations to be addressed in the automatic processing of CMR, being the respiratory motion artifacts the focus of this paper. This paper presents a DL-based approach for the task of image quality classification and segmentation of heart structures in the context of the CMRxMotion public challenge.","Sign language is the comprehensive medium of mass communication for hearing and speaking impaired individuals. As they cannot speak or hear, they are not able to use sound or vocal signals as an information medium for their communication. Rather, they are bound to exchange visual signals to express their feeling in their day-to-day life. For this, they use various body language mainly hand gestures as sign language. Sign language fundamentals can be largely divided into two parts namely digits (numerals) and characters (alphabetical). In this paper, we proposed a hybrid model consisting of a deep transfer learning-based convolutional neural network with a random forest classifier for the automatic recognition of Bangla Sign Language (numerals and alphabets). The overall performance of the presented system is verified on \u2018Ishara-Bochon\u2019 and \u2018Ishara-Lipi\u2019 datasets. \u2018Ishara-Bochon\u2019 and \u2018Ishara-Lipi\u2019 are datasets of isolated numerals and alphabets respectively which are the first complete multipurpose open-access dataset for Bangla Sign Language (BSL). Besides, we also proposed a background elimination algorithm that removes unnecessary features from the sign images. Along with the proposed background elimination technique, the system is able to achieve accuracy, precision, recall, f1-score values of 91.67%, 93.64%, 91.67%, 91.47% for character recognition and 97.33%, 97.89%, 97.33%, 97.37% for digit recognition respectively. The detailed experimental analysis assures the feasibility and effectiveness of the proposed system for BSL recognition.\nHighlights\n\u2022\nDemonstration of recent advancements in various Sign Language recognition research.\n\u2022\nEmployment of proposed background elimination algorithm to remove unwanted features.\n\u2022\nHybridization of transfer learning model with the Random Forest classifier.\n\u2022\nEmployment of backbone networks pre-trained on ImageNet to handle smaller datasets.\n\u2022\nImprovement of evaluation parameters compared to other existing recognition systems.","Competitive-collaborative representation based classification (CCRC) has been widely used in pattern recognition and machine learning due to its simplicity, effectiveness, and low complexity. However, its performance is highly dependent on the data distribution. When addressing imbalanced classification issue, its classification results usually tend towards the majority classes. To solve this deficiency, a class weight learning algorithm is introduced into the framework of CCRC for imbalanced classification. The weight of each class is adaptively generated according to the representation ability of each class of training samples, in which the minority classes can be given larger weights. Our proposed model is solved with a closed-form solution and inherits the efficiency property of CCRC. Extensive experimental results show that our model outperforms the commonly used imbalanced classification methods.","The family orientation process in Cuban Schools for children with Affective \u2013 Behavioral Maladies (SABM) involves clustering and classification of mixed type data with non-symmetric similarity functions. To improve this process, this paper includes some novel characteristics in clustering and prototype selection. The proposed approach uses a hierarchical clustering based on compact sets, making it suitable for dealing with non-symmetric similarity functions, as well as with mixed and incomplete data. The proposal obtains very good results on the SABM data, and over repository databases. In addition, the proposed clustering method is able to detect the true partitions of data and it was significantly better with respect to others according to external validity indexes. In prototype selection, the proposal obtains a highly reduced prototype set, while maintains the original classifier accuracy.","The task of annotating a data point with labels most relevant to it from a large universe of labels is referred to as Extreme Classification (XC). State-of-the-art XC methods have applications in ranking, recommendation, and tagging and mostly employ a combination architecture comprised of a deep encoder and a high-capacity classifier. These two components are often trained in a modular fashion to conserve compute. This paper shows that in XC settings where data paucity and semantic gap issues abound, this can lead to suboptimal encoder training which negatively affects the performance of the overall architecture. The paper then proposes a lightweight alternative DEXA that augments encoder training with auxiliary parameters. Incorporating DEXA into existing XC architectures requires minimal modifications and the method can scale to datasets with 40 million labels and offer predictions that are up to 6% and 15% more accurate than embeddings offered by existing deep XC methods on benchmark and proprietary datasets, respectively. The paper also analyzes DEXA theoretically and shows that it offers provably superior encoder training than existing Siamese training strategies in certain realizable settings. Code for DEXA is available at https://github.com/Extreme-classification/dexa.","Support Vector Machines (SVMs) are one of the most popular supervised learning models to classify using a hyperplane in an Euclidean space. Similar to SVMs, tropical SVMs classify data points using a tropical hyperplane under the tropical metric with the max-plus algebra. In this paper, first we show generalization error bounds of tropical SVMs over the tropical projective torus. While the generalization error bounds attained via Vapnik\u2013Chervonenkis (VC) dimensions in a distribution-free manner still depend on the dimension, we also show numerically and theoretically by extreme value statistics that the tropical SVMs for classifying data points from two Gaussian distributions as well as empirical data sets of different neuron types are fairly robust against the curse of dimensionality. Extreme value statistics also underlie the anomalous scaling behaviors of the tropical distance between random vectors with additional noise dimensions. Finally, we define tropical SVMs over a function space with the tropical metric.\nHighlights\n\u2022\nWe obtain generalization error bounds of tropical SVMs via the VC dimensions.\n\u2022\nWe demonstrate that the tropical SVMs are robust against the curse of dimensionality.\n\u2022\nWe define tropical SVMs over a function space to enable the classification of curves.","Highlights\n\u2022\nFour cattle call patterns produced by fattening cattle and estrus dams were classified.\n\u2022\nNormal calls with and without noise and estrus and hunger calls have high accuracy.\n\u2022\nSuccessful classification provides information for feeding monitoring and estrus prediction.\n\u2022\nMultiple acoustic features were extracted and combined for cattle call patterns classification.\nAbstract\nCattle calls have been used as indicators of the health and welfare of cattle, but the analysis of calls is laborious and time-consuming. In this study, we developed six machine learning classifiers to classify five cattle call patterns using the acoustic features of the calls. The classifiers were trained using calls collected from 31 Japanese Black cattle using a microphone and a webcam. The call patterns of interest included normal, estrus, feeding, normal calls with and without noise. We extracted 193 dimensions of acoustic features from each acoustic sample as the input for the call pattern classifiers. Two datasets were utilized to assess how background noise affected classification. Dataset2 combined two types of normal calls, whereas Dataset1 distinguished between normal calls with and without noise. The outcomes demonstrated that dataset-2 completed classification more accurately than dataset-1. K-nearest neighbor (95.62%), random forest (92.04%), decision tree (92.83%), and logistic regression (92.83%) all had high classification accuracy with dataset-2. When trained on dataset 2, Na\u00efve Bayes and support vector machine models' classification accuracy increased to 86.31% and 88.93%, respectively, from their initial values of 79.64% and 79.81% when using dataset-1. The similarity in call amplitudes and lengths resulted in incorrect classifications. In the future, a microphone with identity information could be attached near the cow's mouth to collect individual calls. In addition to calls associated with different physiological states, other types of information (e.g., sex, age, species, and individual acoustic characteristics), will be considered. This study offers a potential effective tool for precision livestock feeding and reproduction management based on cattle call patterns automatic recognition.","Skip Abstract Section\nAbstract\nNetwork slicing (Ns) is a key enabling technology to support the concurrent provisioning of better quality of service (QoS) in 5G networks. These services have become essential for a telecom service provider (SP) to offer better QoS and QoE (quality of experience). The QoS parameters are used to estimate the performance of the network, and QoE determines user satisfaction with the network services. The main challenges faced by the service provider are to select the appropriate slice for each service and accurately classify these services on a timely basis to satisfy the Service level agreement (SLA) while improving the QoS and QoE. To overcome this issue, machine learning (ML) is a good solution. In this paper, we have proposed a 5G-KPQI (5G-key performance and quality indicator) model that considers the 5G service-based dataset for the 5G services classification. Next, we used feature selection (FS) methods to rank and select the best feature subset, which increases the performance of ML models and also reduces the training time required by the models. We subsequently considered various ML models to classify the services. Results demonstrate that the 5G-KPQI model ranks the features using Relief-F and mrMR methods and also reduces the training time of the model, hence improving classification performance measured by precision, accuracy, F1-score, recall, MCC, and time. The evaluation of the key approach outperforms in high classification accuracy and less training time using decision tree (DT) and random forest (RF).","Deep convolutional neural networks have been widely used in scene classification of remotely sensed images. In this work, we propose a robust learning method for the task that is secure against partially incorrect categorization of images. Specifically, we remove and correct errors in the labels progressively by iterative multi-view voting and entropy ranking. At each time step, we first divide the training data into disjoint parts for separate training and voting. The unanimity in the voting reveals the correctness of the labels, so that we can train a strong model with only the images with unanimous votes. In addition, we adopt entropy as an effective measure for prediction uncertainty, in order to partially recover labeling errors by ranking and selection. We empirically demonstrate the superiority of the proposed method on the WHU-RS19 dataset and the AID dataset.","In this article we present a new polynomial function that can be used as a kernel for Support Vector Machines (SVMs) in binary classification and regression problems. We prove that this function fulfills the mathematical properties of a kernel. We consider here a set of SVMs based on this kernel with which we perform a set of experiments. Their efficiency is measured against some of the most popular kernel functions reported in the past.","Marine oil pollution causes major economic crises in major industrial sectors like fishing, shipping and tourism. It affects marine life and human even decades after spillage necessitating very quick detection and remediation. Generally, oils are exceedingly difficult to identify from high-resolution images as oil slicks and sea water possess identical spectral characteristics. Therefore a cohesive and synergistic classification method called hybrid spectral similarity measures (HSSM) that discerns the data-rich constituents of hyperspectral images (HSI) is recommended in this paper to classify oil spills. Oil spill HSI procured from spaceborne [earth observation (EO-1) Hyperion] and airborne [airborne visible/infrared imaging spectrometer (AVIRIS)] platforms are employed to discriminate different marine spectral classes. The statistical parameters like overall accuracy (OA), Kappa, ROC/PR curve, AUC/PRAUC, weighted Youden index (Jw), F1score and Noise analysis have identified spectral information divergence-chi square distance (SID-CHI) as the best HSSM promulgating its multi-class, multi-sensor, and multi-platform oil spill classification capability.","Learning management systems provide a wide breadth of data waiting to be analyzed and utilized to enhance student and faculty experience in higher education. As universities struggle to support students\u2019 engagement, success and retention, learning analytics is being used to build predictive models and develop dashboards to support learners and help them stay engaged, to help teachers identify students needing support, and to predict and prevent dropout. Learning with Big Data has its challenges, however: managing great quantities of data requires time and expertise. To predict students at risk, many institutions use machine learning algorithms with LMS data for a given course or type of course, but only a few are trying to make predictions for a large subset of courses. This begs the question: \u201cHow can student dropout be predicted on a very large set of courses in an institution Moodle LMS?\u201d In this paper, we use automation to improve student dropout prediction for a very large subset of courses, by clustering them based on course design and similarity, then by automatically training, testing, and selecting machine learning algorithms for each cluster. We developed a promising methodology that outlines a basic framework that can be adjusted and optimized in many ways and that further studies can easily build on and improve.","Breast cancer is the second major cause of cancer deaths in women. Machine learning classification techniques can be used to increase the precision of diagnosis and bring it closer to 100%, thus saving the lives of many people. This paper proposed four different models, built using different combinations of selected features and applying five ML classification techniques to all the models to identify the best model with the highest accuracy. It analyzes five machine learning techniques, namely logistic regression (LR), support vector machines (SVM), naive bayes (NB), decision trees (DT), and k-nearest neighbor (KNN), for prediction of breast cancer using the Wisconsin Diagnostic Breast Cancer Dataset on these four models. The objective of the paper is to find the best ML algorithm that can most accurately predict breast cancer for a particular model. The outcome of this paper helps the doctors to improvise the diagnosis by knowing the effect of combinations of symptoms with the growth of breast cancer.","As one of the major threats to the healthy development of various online platforms, fraud has become increasingly committed in the form of gangs since collusive fraudulent activities are much easier to obtain illicit benefits with lower exposure risk. To detect fraudsters in a gang, spatio-temporal graph neural network models have been widely applied to detect both temporal and spatial collusive patterns. However, a closer peek into real-world records of fraudsters can reveal that fraud gangs usually conduct community-level camouflage, specified by two types, i.e., temporal and spatial camouflage. Such camouflage can disguise gangs as benign communities by concealing collusive patterns and thus deceiving many existing graph neural network models. In the meantime, many existing graph neural network models suffer from the challenge of extreme sample imbalance caused by rare fraudsters hidden among massive users. To handle all these challenges, in this paper, we propose a generative adversarial network framework, named Adversarial Camouflage Detector, to detect fraudsters. Concretely, this ACD framework consists of four modules, in charge of community division, camouflage identification, fraudster detection, and camouflage generation, respectively. The first three modules form up a discriminator that uses spatio-temporal graph neural networks as the foundation model and enhance fraudster detection by amplifying the gangs' collusive patterns through automatically identifying and removing camouflage. Meanwhile, the camouflage generation module plays as the generator role that generates fraudsters samples by competing against the discriminator to alleviate the challenge of sample imbalance and increase the model robustness. The experimental result shows that our proposed method outperforms other methods on real-world datasets.","Label distribution learning (LDL) is a new machine learning paradigm for solving label ambiguity. Since it is difficult to directly obtain label distributions, many studies are focusing on how to recover label distributions from logical labels, dubbed label enhancement (LE). Existing LE methods estimate label distributions by simply building a mapping relationship between features and label distributions under the supervision of logical labels. They typically overlook the fact that both features and logical labels are descriptions of the instance from different views. Therefore, we propose a novel method called Contrastive Label Enhancement (ConLE) which integrates features and logical labels into the unified projection space to generate high-level features by contrastive learning strategy. In this approach, features and logical labels belonging to the same sample are pulled closer, while those of different samples are projected farther away from each other in the projection space. Subsequently, we leverage the obtained high-level features to gain label distributions through a well-designed training strategy that considers the consistency of label attributes. Extensive experiments on LDL benchmark datasets demonstrate the effectiveness and superiority of our method.","Significant work has been done on learning regular expressions from a set of data values. Depending on the domain, this approach can be very successful. However, significant time is required to learn these expressions and the resulting expressions can become either very complex or inaccurate in the presence of dirty data. The alternative of manually writing regular expressions becomes unattractive when faced with a large number of values that must be matched.\nAs an alternative, we propose learning from a large corpus of manually authored, but uncurated regular expressions mined from a public repository. The advantage of this approach is that we are able to extract salient features from a set of strings with limited overhead to feature engineering. Since the set of regular expressions covers a wide range of application domains, we expect them to be widely applicable.\nTo demonstrate the potential effectiveness of our approach, we train a model using the extracted corpus of regular expressions for the class of semantic type classification. While our approach yields results that are overall inferior to the state-of-the-art, our feature extraction code is an order of magnitude smaller, and our model outperforms a popular existing approach on some classes. We also demonstrate the possibility of using uncurated regular expressions for unsupervised learning.","This paper explores three novel approaches to improve the performance of speaker verification (SV) systems based on deep neural networks (DNN) using Multi-head Self-Attention (MSA) mechanisms and memory layers. Firstly, we propose the use of a learnable vector called Class token to replace the average global pooling mechanism to extract the embeddings. Unlike global average pooling, our proposal takes into account the temporal structure of the input what is relevant for the text-dependent SV task. The class token is concatenated to the input before the first MSA layer, and its state at the output is used to predict the classes. To gain additional robustness, we introduce two approaches. First, we have developed a new sampling estimation of the class token. In this approach, the class token is obtained by sampling from a list of several trainable vectors. This strategy introduces uncertainty that helps to generalize better compared to a single initialization as it is shown in the experiments. Second, we have added a distilled representation token for training a teacher-student pair of networks using the Knowledge Distillation (KD) philosophy, which is combined with the class token. This distillation token is trained to mimic the predictions from the teacher network, while the class token replicates the true label. All the strategies have been tested on the RSR2015-Part II and DeepMine-Part 1 databases for text-dependent SV, providing competitive results compared to the same architecture using the average pooling mechanism to extract average embeddings.","This paper presents a LIghtweight Domain Adaptive Cell Segmentation (LIDACS) framework that achieves state-of-the-art results (0.9505 mIoU) in instance segmentation on the SegPC-21 challenge dataset featured in ISBI 2021, while being significantly parameter efficient than the existing methods. LIDACS is a hierarchical multi-stage approach that utilizes prior domain-specific information to perform statistical and empirical analysis. It also employs task-specific augmentations and improved transfer learning via shared representation to enable better data representation. LIDACS also applies a novel cell structure-based contrastive augmentation paired with cell cloning, increasing annotation density and promoting better stain color in-variance. Effectively, LIDACS is a lightweight architecture, efficient for practical deployment, that provides optimal generalization.","In order to improve the hit ratio of lost customers in telecom industry, a combination prediction model of customer churn based on one-dimensional convolutional neural network(1DCNN) and gradient boosting decision tree(GBDT) is proposed. Firstly, customer data is fed into 1DCNN model, which uses one-dimensional convolution to automatically extract customer features and then predicts customer churn through full connection layer. If the prediction result of 1DCNN model is churn, the result is directly output. If the prediction result is non-churn, the customer data will be re-introduced into GBDT model for second forecast, and the new prediction result will be output. Experiments on two publicly available telecom customer data set show that the proposed combined model significantly improves the recall rate and F1 score of customer churn prediction.","The number of individuals who use credit cards has increased dramatically in recent decades, as has the volume of credit card fraud transactions. Consequently, banks and credit card companies must be able to classify fraudulent credit card transactions so that clients do not have to pay for products they did not purchase. Data Science can easily tackle such challenges, and the value of Machine Learning methodologies cannot be emphasized. The study demonstrates how to model utilizing multiple classifiers and data balance using machine learning approaches to learning about Credit Card Fraud Detection. The data has been observed as an imbalanced dataset that could have inferred not much optimal performance of models. The experimentation on the imbalanced data has been done and observed that XGBoost has yielded good performance with 0.91 precision score and 0.99 accuracy score. The different sampling techniques have been carried out in procedure so as to enhance the scores in terms of precision, recall, f1-score, and accuracy. The Random Oversampling technique has come out to be the best suited technique over the imbalance data and yields 0.99 precision and 0.99 accuracy score, when applied on the best model i.e., XGBoost.\nThe models are then used to compare the results of all of the classifiers employed, resulting in varied conclusions and further research. While working on the study, many data balancing procedures such as oversampling, under sampling, and SMOTE are used, with XGBoost beating residual algorithms with a 99% accuracy score and precision score when Random Over-Sampling is considered. The research suggested the use of data sampling techniques to balance data over the algorithms that show best results under the imbalanced data scenarios, to conclude the best possible performance of the model for fraudulent activities classification.","This paper proposes an empirical framework that aims to classify difficulty according to the player\u2019s physiological response. As part of the experimental protocol, a simple puzzle-based Virtual Reality (VR) videogame with three levels of difficulty was developed, each targeting a distinct region of the valence-arousal space. A study involving 32 participants was conducted, during which physiological responses (EDA, ECG, Respiration), were measured alongside emotional ratings, which were self-assessed using the Self-Assessment Manikin (SAM) during gameplay. Statistical analysis of the self-reports verified the effectiveness of the three levels in eliciting different emotions. Furthermore, classification using a Support Vector Machine (SVM) was performed to predict difficulty considering the physiological responses associated with each level. Results report an overall F1-score of 74.05% in detecting the three levels of difficulty, which validates the adopted methodology and encourages further research with a larger dataset.","The steady degeneration of neurons is the hallmark of neurodegenerative illnesses, which are, by definition, incurable. Corticobasal Syndrome (CS), Huntington's Disease (HD), Dementia, Amyotrophic Lateral Sclerosis (ALS), Progressive supranuclear palsy (PSP) and Parkinson's Disease (PD) are some of the common neurodegenerative diseases which has impacted millions of people, predominantly among the older population. Various computational techniques, including but not limited to machine learning, are emerging as discrimination and detection of neuro-related diseases. This research proposed a machine learning-based framework to correctly detect PD, HD, and ALS from the gait signals of subjects both in binary and multi-class detection environment. The detection approach proposed here combines the classification power of Na\u00efve Bayes and Logistic Regression jointly in a modern UltraBoost ensemble framework. The proposed method is unique in its ability to detect neuro diseases with a small number of gait features. The proposed approach ascertains most essential gait features through three state-of-the-art feature selection schemes, infinite feature selection, infinite latent feature selection and Sigmis feature selection. It has been observed that the gait signal features of the subjects are identified through Infinite Feature Selection manifests better detection results than the features obtained through Infinite Latent Feature and Sigmis feature selection while detecting Parkinson's and Huntington's Disease in a multi-class environment. So far as the binary detection environment is concern, the Amyotrophic lateral sclerosis is detected with 99.1% detection accuracy using 18 Sigmis gait features, with 99.1% sensitivity and 98.9% specificity, respectively. Similarly, Huntington's disease was detected with 94.2% detection accuracy, 94.2% sensitivity, and 94.5% specificity using 5 Sigmis gait features. Finally, Parkinson's disease was detected with 98.4% sensitivity, specificity, and detection accuracy.","The rapid development of the Internet has led to a geometric expansion of text information resources online. Among them, corpus, as the basic data source of natural language processing based on statistical language model, its construction and application have become a hot issue in current language processing research. After consulting a large number of relevant literature and materials, it was found that many researchers have provided new ideas for multi label corpus text classification methods. However, this article was adding its own understanding and taking this as the direction and basis. In the introduction, the research significance of text classification was introduced, and then academic research and analysis were carried out on the two key sentences of corpus text classification and natural language processing in multi tag corpus text classification. This article then utilized an algorithm model to provide a theoretical basis for the study of multi label corpus text classification methods; At the end of the article, a simulation comparative experiment would be conducted, and the experiment would be summarized and discussed; In the Enterprise L corpus, the difference in recall rates before and after the use of Entrance 1 was 5.5%, the difference in recall rates before and after the use of Entrance 2 was 7.8%, the difference in recall rates before and after the use of Entrance 3 was 3.3%, and the difference in recall rates before and after the use of Entrance 4 was 4.5%. At the same time, with the continuous research of natural language processing and machine learning, the research on text classification methods of multi tag corpus is also facing new opportunities and challenges.","Ensemble classifiers have been investigated by many in the artificial intelligence and machine learning community. Majority voting and weighted majority voting are two commonly used combination schemes in ensemble learning. However, understanding of them is incomplete at best, with some properties even misunderstood. In this paper, we present a group of properties of these two schemes formally under a geometric framework. Two key factors, every component base classifier\u2019s performance and dissimilarity between each pair of component classifiers are evaluated by the same metric\u2014the Euclidean distance. Consequently, ensembling becomes a deterministic problem and the performance of an ensemble can be calculated directly by a formula. We prove several theorems of interest and explain their implications for ensembles. In particular, we compare and contrast the effect of the number of component classifiers on these two types of ensemble schemes. Some important properties of both combination schemes are discussed. And a method to calculate the optimal weights for the weighted majority voting is presented. Empirical investigation is conducted to verify the theoretical results. We believe that the results from this paper are very useful for us to understand the fundamental properties of these two combination schemes and the principles of ensemble classifiers in general. The results are also helpful for us to investigate some issues in ensemble classifiers, such as ensemble performance prediction, diversity, ensemble pruning, and others.","In this paper, we propose a generalization of classical Rough Sets, the Nearest Neighborhood Rough Sets, by modifying the indiscernible relation without using any similarity threshold. We also combine these Rough Sets with Compact Sets, to obtain a prototype selection algorithm for Nearest Prototype Classification of mixed and incomplete data as well as arbitrarily dissimilarity functions. We introduce a set of rules to a priori predict the performance of the proposed prototype selection algorithm. Numerical experiments over repository databases show the high quality performance of the method proposed in this paper according to classifier accuracy and object reduction.","Automated machine learning (AutoML) has allowed for many innovations in biomedical data science; however, most AutoML approaches do not support image or text data. To rectify this, we implemented four feature extractors in the Tree-based Pipeline Optimization Tool (TPOT) to make TPOT with Feature Extraction (TPOT-FE), an automated machine learning system that uses genetic programming (GP) to create ideal pipelines for a classification or regression task. These feature extractors enable TPOT-FE to build pipelines that can analyze non-tabular data, including text and images, which are increasingly common biomedical big data modalities that can contain rich quantities of information. We evaluate this approach on six image datasets and four text datasets, including three biomedical datasets, and show that TPOT-FE is able to consistently construct and optimize classification pipelines on all of the datasets.","A classification model for predicting the main activity of bitcoin addresses based on their balances is proposed. Since the balances are functions of time, methods from functional data analysis are applied; more specifically, the features of the proposed classification model are the functional principal components of the data. Classifying bitcoin addresses is a relevant problem for two main reasons: to understand the composition of the bitcoin market, and to identify addresses used for illicit activities. Although other bitcoin classifiers have been proposed, they focus primarily on network analysis rather than curve behavior. The proposed approach, on the other hand, does not require any network information for prediction. Furthermore, functional features have the advantage of being straightforward to build, unlike expert-built features. Results show improvement when combining functional features with scalar features, and similar accuracy for the models using those features separately, which points to the functional model being a good alternative when domain-specific knowledge is not available.","This paper proposes a novel binary classification approach named Laplacian quadratic surface optimal margin distribution machine (LapQSODM), for semisupervised learning. This new model exploits the geometric information embedded in unlabeled samples through a manifold regularizer to overcome the problem of insufficient labeled samples. Different from the traditional support vector machines (SVMs) based on the largest minimum margin idea and using the kernel technique to address the nonlinearity in the datasets, our proposed model optimizes the margin distribution and directly generates a quadratic surface to perform the classification. This new model not only improves the generalization performance but also avoids the difficulty of searching for an appropriate kernel function and tuning its parameters. For the regular-scale datasets, we extend the classical conjugate gradient algorithm for the proposed method and design an easy iterative method to calculate its exact step size; for the large-scale datasets, we develop an unbiased estimation for the LapQSODM gradient by utilizing one or several samples and design a stochastic gradient descent with variance reduction (SVRG) algorithm that is efficient and effective. Then, we conduct a comprehensive numerical experiment on both artificial and public benchmark datasets. The experimental results show LapQSODM has a better generalization performance than most of the well-known benchmark methods and is more robust due to the optimization of the margin distribution. Finally, we apply LapQSODM to three realistic credit risk assessment problems. The promising numerical results demonstrate the great potential and ability of our model in the real practice.\nHighlights\n\u2022\nA novel semi-supervised Laplacian optimal margin distribution machine is proposed.\n\u2022\nThe optimal margin distribution is incorporated into a non-kernel nonlinear SVM.\n\u2022\nA stochastic gradient descent algorithm is developed for large-sized data.\n\u2022\nNumerical results indicate the superior performance of proposed method.\n\u2022\nThe effectiveness of proposed method in credit risk assessment is validated.","Domain generalization aims to train a robust model on multiple source domains that generalizes well to unseen target domains. While considerable attention has focused on training domain generalizable models, a few recent studies have shifted the attention to test time, i.e., leveraging test samples for better target generalization. To this end, this paper proposes a novel test-time domain generalization method, Domain Conditioned Normalization (DCN), to infer the normalization statistics of the target domain from only a single test sample. In order to learn to predict the normalization statistics, DCN adopts a meta-learning framework and simulates the inference process of the normalization statistics at training. Extensive experimental results have shown that DCN brings consistent improvements to many state-of-the-art domain generalization methods on three widely adopted benchmarks.","This paper presents tree-based machine learning techniques like decision trees, and random forest algorithms to categorize various permanent faults such as a line to ground, line to line, line-line to ground, and line-line-line to ground fault simulated in an underground cable. The simulation has been performed with the variation in fault types, fault location, fault resistance, and fault inception angle and the sending end current signals are presented as input to the classifier for classifying the fault. A relative comparison between the decision tree technique and random forest technique has been performed and the random forest algorithm turns out to be the best for classifying the permanent faults in an underground cable. Both the algorithms are deliberated by precision, F1 score, and recall. The best result is presented by Random Forest Algorithm with an accuracy of 98.8%.","Applications involving Extreme Multi-Label Classification (XMLC) face several practical challenges with respect to scale, model size and prediction latency, while maintaining satisfactory predictive accuracy. In this paper, we propose a Multi-Label Factorization Machine (MLFM) model, which addresses some of the challenges in XMLC problems. We use behavioral ad targeting as a case study to illustrate the benefits of the MLFM model. Predicting user qualifications for targeting segments plays a major role in both personalization and real-time bidding. Considering the large number of segments and the prediction time requirements of real-world production systems, building scalable models is often difficult and computationally burdensome. To cope with these challenges, we (1) reformulate the problem of assigning users to segments as a multi-label classification (XMLC) problem, and (2) leverage the benefits of the conventional FM model and generalize its capacity to joint prediction across a large number of targeting segments. We have shown that the MLFM model is both effective and computationally efficient compared to several baseline models on publicly available datasets in addition to the targeting use case.","This paper investigates the application of novelty detection techniques to the problem of drug profiling in forensic science. Numerous one-class classifiers are tried out, from the simple k-means to the more elaborate Support Vector Data Description algorithm. The target application is the classification of illicit drugs samples as part of an existing trafficking network or as a new cluster. A unique chemical database of heroin and cocaine seizures is available and allows assessing the methods. Evaluation is done using the area under the ROC curve of the classifiers. Gaussian mixture models and the SVDD method are trained both with and without outlier examples, and it is found that providing outliers during training improves in some cases the classification performance. Finally, combination schemes of classifiers are also tried out. Results highlight methods that may guide the profiling methodology used in forensic analysis.","Highlights\n\u2022\nThe method is designed for the semi-supervised domain generalization problem.\n\u2022\nThe method predicts accurate pseudo-labels under domain shift via domain-aware pseudo-labeling and dual-classifier structure.\n\u2022\nThe method leverages both confident and ambiguous unlabeled samples to improve generalization.\nAbstract\nWith the goal of directly generalizing trained model to unseen target domains, domain generalization (DG), a newly proposed learning paradigm, has attracted considerable attention. Previous DG models usually require a sufficient quantity of annotated samples from observed source domains during training. In this paper, we relax this requirement about full annotation and investigate semi-supervised domain generalization (SSDG) where only one source domain is fully annotated along with the other domains totally unlabeled in the training process. With the challenges of tackling the domain gap between observed source domains and predicting unseen target domains, we propose a novel deep framework via joint domain-aware labels and dual-classifier to produce high-quality pseudo-labels. Concretely, to predict accurate pseudo-labels under domain shift, a domain-aware pseudo-labeling module is developed. Also, considering inconsistent goals between generalization and pseudo-labeling: former prevents overfitting on all source domains while latter might overfit the unlabeled source domains for high accuracy, we employ a dual-classifier to independently perform pseudo-labeling and domain generalization in the training process. When accurate pseudo-labels are generated for unlabeled source domains, the domain mixup operation is applied to augment new domains between labeled and unlabeled domains, which is beneficial for boosting the generalization capability of the model. Extensive results on publicly available DG benchmark datasets show the efficacy of our proposed SSDG method.","The deep forest model, a random forest (RF) ensemble approach and an alternative to Deep Neural Network (DNN), has performance highly competitive to DNN in many classification tasks. However, deep forest model may encounter overfitting and characteristic dispersion issues as processing small-scale, class-imbalance or high-dimension data. Therefore, this paper proposes a Weighted Cascade Deep Forest framework, called WCDForest. In WCDForest, an equal multi-grained scanning module is used to scan each feature equally. Meanwhile, this framework adopts a class vector weighting module to emphasis the performance of each forest and each sliding window by weight. Furthermore, this study proposes a feature enhancement module to reduce the information loss in the first few cascade layers to improve the classification accuracy. Subsequently, systematic comparison experiments on 18 widely used public datasets demonstrate that the proposed model outperforms the state-of-the-art model. In particular, WCDForest improves the accuracy, precision, recall and F1-score by an average of 5.47%,7.04%,8.23% and 8.94%,respectively.","Bees play an important role as pollinating agents, contributing to the reproduction of many plant species around the world. Brazil is the home for different species of stingless bees, with around 200 registered species out of the more than 500 species classified worldwide. Each species constructs the entrance to its colony in an unique but similar way among colonies of the same species. In this work, we proposed a new dataset created in collaboration with stingless beekeepers from Brazil for the exploration of stingless bee species classification. The dataset consists of 158 samples distributed unequally among the 13 species: Boca de Sapo, Bor\u00e1, Bugia, Ira\u00ed, Japur\u00e1, Jata\u00ed, Lambe Olhos, Mandaguari, Mirim Droryana, Mirim Pregui\u00e7a, Mo\u00e7a Branca, Manda\u00e7aia, and Tubuna. The results presented in this work were obtained using deep learning models (i.e. CNN architectures) such as VGG and DenseNet, which are commonly used for image classification task in different application domains. Pre-trained models from ImageNet were used, along with transfer learning techniques, and due to the small size of the dataset, data augmentation techniques were applied, resulting in an expanded dataset of 1,106 samples. The experimental results demonstrated that the DenseNet model achieved the best results, reaching an accuracy of\n95\n%\n. The dataset created will be also made available as a contribution of these work. As far as we know, the stingless bee species identification task based on the colony entrance is addressed for the first time in this work.","Graphical abstract\nDisplay Omitted\nAbstract\nLiver cancer is a common malignant tumor, and its clinical stage is closely related to the clinical treatment and prognosis of patients. Currently, the BCLC staging system revised by the BCLC group of University of Barcelona is the globally recognized staging system for liver cancer. However, with the deepening of related research, the current staging system can no longer fully meet the clinical needs. In this work, we propose a novel machine learning method for constructing an automatic hepatocellular carcinoma staging model that incorporates far more clinical variables than any existing staging system. Our model is based on random survival forests, which generates a unique hazard function for each patient. B-splines are used to embed hazard functions into vectors in low-dimensional space and hierarchical clustering method groups similar patients to form staging cohorts. The resulting staging system significantly outperforms the BCLC system in terms of distinctiveness between patients in different stages.","In this article, we tend to examine the text classification task by using various neural networks. A small number of previously classified texts can change the accuracy of the studied text classifiers. This is often vital in many text classification applications because an oversized range of uncategorized data is effortlessly reachable. However, getting an annotated text is a quite challenging task. The article additionally demonstrates that the Convolution Neural Network (CNN) does not demand semantic or syntactic knowledge and can perform in a better way on a words level. Secondly, a Recurrent Neural Network (RNN) model can effectively classify the text data (sequence type). RNN outperforms the other Neural Networks for the sequence test classification task. We used corpora of two different types from separate sources (IMDB and self-created bloggers corpus). The results of our experiments provide evidence that vector representation of the text can improve the score of the task.","In recent years, Local differential privacy (LDP), as a strong privacy preserving methodology, has been widely deployed in real world applications. It allows the users to perturb their data locally on their own devices before being sent out for analysis. In particular, LDP serves as an effective solution for the construction of privacy-preserving classifiers. While several approaches in the literature have been proposed to build classifiers over distributed locally differential private data, an understanding of the difference in the performance of these LDP-based classifiers is currently missing. In this study, we investigate the impact of using LDP on four well-known classifiers, i.e., Na\u00efve Bayes, Decision Tree, Random Forest, and Logistic Regression classifiers. We evaluate the impact of dataset\u2019s properties, LDP mechanisms, privacy budget, and classifiers\u2019 structure on LDP-based classifiers\u2019 performance.","Image classification is one of the most important research tasks in computer vision. Current image classification methods with supervised learning have achieved good classification accuracy. However, supervised image classification methods mainly focus on the semantic differences at the class level, while lacking attention to the instance level. The core idea of contrastive learning is to compare positive and negative samples in the feature space to learn the feature representation, and the focus on instance-level information can make up for the lack of supervised learning. To this end, in this paper, we combine supervised learning and contrastive learning to propose labeled contrastive learning (LCL). Here, the supervised learning component ensures the distinguishability of different classes, the contrastive learning component enhances the compactness within classes and the separability between classes. In the contrastive learning component, instances with the same label are set as positive samples and instances with different labels are set as negative samples, which avoids the problem of false negative samples (positive samples are mislabeled as negative samples). Also, we applied a dynamic label memory bank and a momentum updated encoder. The experimental results show that LCL can further improve the accuracy of image classification compared with some supervised learning method.","One of the most common mental illnesses that affects 5% of adults globally is depression. The advancement of social media has meant that more and more people have gained a platform to voice their thoughts and beliefs. People\u2019s social media interactions and posted content can be used to infer critical characteristics such as depressive tendencies which will allow for timely intervention and help. This paper describes a novel supervised approach to detect depressive tendencies in Twitter users using multimodal frameworks which account for user interaction and online behaviour in addition to the tweet content processed using transformers like BERT. The performance of three multimodal frameworks is described with different methods for combining modalities. The best result is obtained a cross-modality based model which improves the baseline by 12% points.","Siamese Neural Networks (SNN) are known to perform well in resource-constrained scenarios where computation and data availability are limited. They utilise the similarity space of a given dataset to extract distinguishing features between dissimilar data samples. Such features have also been utilised for classification tasks. Though several works on enhancing the accuracies and inference times using such similarity spaces have been reported, there is still scope for investigations that can yield more efficient strategies. The Biological Immune System (BIS) is known for employing such a transformation to recognise and contain antigenic attacks. Concepts from a BIS can thus, aid in boosting the classification performance of SNNs. This paper summarizes such an attempt made in our work \"Immuno-Inspired Augmentation of Siamese Neural Network for Multi-class Classification\" [8] presented at IVCNZ 2022, first published in Lecture Notes in Computer Science, 2023, vol 13836, pages 486--500 by Springer. A novel SNN-based multi-class classification method augmented with an immuno-inspired approach that allows an SNN to plug class-specific characteristics into its architecture is presented herein. The empirical analyses and results conducted on three benchmark datasets, clearly indicate that this method delivers higher accuracies and lower inference times when compared to recent SNN-based multi-class classification techniques.","Training deep learning models in technical domains is often accompanied by the challenge that although the task is clear, insufficient data for training is available. Additional to that, often also no similar source-datasets are available which can be used for transfer-learning to reduce the need for data in the target-domain. In this work, a novel approach based on the combination of siamese networks and radial basis function networks is proposed where the siamese networks serve as effective feature extractors. The architecture performs data-efficient classification without pretraining by measuring the distance between images in the semantic space in a data- efficient manner. The so called SBF-Net structure is developed and tested on three technical as well as two non-technical datasets. The architecture shows superior performance for all data sets, especially when only small data is available for training. The approach significantly outperforms existing ResNet50 and ResNet100 architectures when only 3, 5, 10 and 20 data points per class are available. Also, in data-setups where 75% of the data is used for training, the model yields the same performance as state-of-the-art-models. The main contribution of this work is a model that works particularly data efficient with small amounts of data without making prior constraints.","Hyperspectral image(HSI) classification is a crucial topic within remote sensing. Recently, deep self-supervised learning methods have gained widespread use in HSI classification, effectively addressing the scarcity of labeled samples issue. In particular, masked image modeling and contrastive learning have achieved commendable performance in the field of self-supervised learning. Therefore, to better investigate the association and synergy between the two self-supervised learning methods, we propose a novel hybrid self-supervised learning framework (HSL) for HSI classification that conforms to the properties of hyperspectral data. The HSL exploits the efficacy of masked image modeling and contrastive learning, and combines masked image reconstruction and instance contrastive learning to improve performance. The HSL specifically employs an asymmetric encoder-decoder two-branch structure. The structure adopts the Vision Transformer as the backbone network to efficiently extract spatial spectral information. Experiments on two commonly used HSI datasets demonstrate that this pre-training task results in better modeling of the feature relationships between shallow and deep layers and achieves superior performance.","As a fundamental part of natural language processing, text classification is the backbone of tasks and applications such as machine translation and classification. Among the text classification tasks of all languages, the one for Chinese appears to be one of the most challenging due to the complex structures and expressions within the nature of Chinese. Researchers generally require a significant amount of data for model training and tuning, while most of the time, that desired amount of data cannot be fulfilled and satisfied. Given the circumstances, we propose an effective data enhancement technique to lower the demand for data. The central principle is as follows: Randomize the acquired word vectors and tokens from tokenizing the text based on a certain density level (i.e., every group contains five words), then use the randomized results as data input. During the above process, a considerable number of data variations would be generated, easing the demand for data. From the experiments, we tested our theory on multiple Chinese natural language processing datasets and received signs of improvements in model performance across all the datasets used, thus proving the validity of the previously mentioned method.","Learning from Label Proportions (LLP) is a machine learning problem where the training data are composed of bags of instances, and only the class label proportions for each bag are given. In some domains, we can directly obtain label distributions; for example, one can use census statistics and social media user information grouped by location to build a classifier for user demographics. However, label proportions are unavailable in many domains, such as product review sites. The solution is to modify the model fit on data from where label proportion are available domains (the source domain) to apply to a domain where the label distributions are not available (target domain). Such problems can be regarded as the unsupervised domain adaptation problems in an LLP setting. The goal of this paper is to introduce domain adaptation methods to the original LLP solutions such that the proposed model can classify instances from a new domain. We propose a model combining domain-adversarial neural network (DANN) and label regularization, which can be fit on the source-domain bags and predict labels for target-domain instances. This approach requires only label proportions in the source domain. Our experiments on both synthetic tasks and sentiment classification tasks indicate a noticeable improvement in accuracy as compared to using LLP without domain adaptation.","Highlights\n\u2022\nIn order to improve the performance of CRC, we extract more discriminant features and use more representative samples simultaneously.\n\u2022\nBy analyzing the collaborative representation mechanism, we propose a simple but effective classification method based on robust marginal training samples, which is called Robust Margin Collaborative Representation based Classification method (RMCRC).\n\u2022\nA deep feature extraction method termed Margin Embedding Net (MEN) is proposed to enhance the performance of robust marginal training samples and has a close connection with RMCRC.\n\u2022\nA collaborative representation-based triplet mining mechanism for MEN is proposed.\nAbstract\nCollaborative Representation-based Classification method (CRC) shows great potential in classification task. However, redundancies in both features and samples limit the application of CRC seriously. The existing works only solve one of them and ignore the other, which leads to performance degradation. To address this problem, we explore collaborative representation mechanism and propose a classification method termed Robust Margin Collaborative Representation-based Classification (RMCRC) which uses a few but more representative robust marginal samples to eliminate redundancy between samples. As the performance of RMCRC is related to robust marginal samples and class separability assumption closely, we further propose a feature extraction method termed Margin Embedding Net (MEN) for RMCRC. In MEN, virtual samples are generated by a generative model to enhance effectiveness of robust marginal samples and generalizability of RMCRC. Then, an embedding network with triplet loss is used to eliminate the redundancy in features and ensure the assumption is satisfied. Specifically, we construct triplet according to the collaborative representation. Hence, MEN fits RMCRC very well. Extensive experimental results validate effectiveness of proposed method.","Yelp is one of the most popular international web resources about products and services that provide users with useful information on local businesses and helps the business owners to make their business more attractive for the users. The Yelp dataset consists of attributes for describing the business, reviews in free text form and numeric star ratings out of 5. The utility of such a dataset has provoked dozens of publications related to classifiers of ratings, which used various smart tools of opinion mining. Unlike them, in this paper we propose to use simpler approaches, namely: (a) selection of descriptors based on term specificity, and (b) formation of classifiers with these descriptors based on inductive modeling. The latter is implemented by the well-known tool GMDH Shell, where GMDH stands for Group Method of Data Handling. This method allows us to build models with high noise immunity. We compare 96 prediction models with identified descriptors by combining various variants: (i) preprocessing with data transformation and balancing classes, (ii) algorithms of classification; and (iii) post processing with ensembling. Instead of the typical 5- star classification we consider combined classes reflecting a more practical view on purchase of goods or development of business. The experiments refer to the most popular categories of business: restaurants and shopping. To evaluate the quality of classifiers we consider the results of predecessors, and we also introduce the so-called defensible accuracy. With this comparison the results presented in the paper prove to be promising.","Highlights\n\u2022\nA new model agnostic feature selection method for supervised learning.\n\u2022\nSensitivity analysis with the predictive distribution of a scalable Gaussian process.\n\u2022\nNew L 2 divergence derivation to better measure classification results and changes.\n\u2022\nBetter performance on several learning models than existing methods.\nAbstract\nFeature selection is one of the most important issues in supervised learning and there are a lot of different feature selection approaches in the literature. Among them one recent approach is to use Gaussian process (GP) because it can capture well the hidden relevance between the features of the input and the output. However, the existing feature selection approaches with GP suffer from the scalability problem due to high computational cost of inference with GP. Moreover, they use the Kullback\u2013Leibler (KL) divergence in the sensitivity analysis for feature selection, but we show in this paper that the KL divergence underestimates the relevance of important features in some cases of classification.\nTo remedy such drawbacks of the existing GP based approaches, we propose a new feature selection method with scalable variational Gaussian process (SVGP) and L 2 divergence. With the help of SVGP the proposed method exploits given large data sets well for feature selection through so-called inducing points while avoiding the scalability problem. Moreover, we provide theoretical analysis to motivate the choice of L 2 divergence for feature selection in both classification and regression. To validate the performance of the proposed method, we compare it with other existing methods through experiments with synthetic and real data sets.","Most existing OCR methods focus on alphanumeric characters due to the popularity of English and numbers, as well as their corresponding datasets. On extending the characters to more languages, recent methods have shown that training different scripts with different recognition heads can greatly improve the end-to-end recognition accuracy compared to combining characters from all languages in the same recognition head. However, we postulate that similarities between some languages could allow sharing of model parameters and benefit from joint training. Determining language groupings, however, is not immediately obvious. To this end, we propose an automatic method for multilingual text recognition with a task grouping and assignment module using Gumbel-Softmax, introducing a task grouping loss and weighted recognition loss to allow for simultaneous training of the models and grouping modules. Experiments on MLT19 lend evidence to our hypothesis that there is a middle ground between combining every task together and separating every task that achieves a better configuration of task grouping/separation.","The advancement in technology with multiple sensors embedded in smartphones results in the widespread of smartphones in the applications of human activity analysis and recognition. This promotes a variety of ambient assistive living applications, such as fitness tracking, fall detection, home automation system, healthcare monitoring etc. In this paper, a human activity recognition based on the amalgamation of statistical global features and local deep features is presented. The proposed model adopts temporal convolutional architecture to extract the long-range temporal patterns from the inertial activity signals captured by smartphones. To further enrich the information, statistical features are computed so that the global features of the time series data are encoded. Next, both global and local deep features are combined for classification. The proposed model is evaluated by using WISDM and UCI HAR datasets for user-dependent and independent protocols, respectively, to ensure its feasibility as user-dependent and independent HAR solutions. The obtained empirical results exhibit that the proposed model is outperforming the other existing deep learning models on both user-dependent and independent testing protocols.","Text classification by large deep learning networks achieves high accuracy, but uses a lot of computing power and requires high resource investment. In the age of climate change, sustainable solutions are sought that can attain acceptable accuracy with less resource investment. In this paper, we investigate lightweight text classifiers and combine them with a human-in-the-loop approach. Our solution identifies instances that are uncertain to classify and assigns them preferentially to a human domain expert. Experiments performed with nine classifiers on six datasets show that with manually labelling less than 30%, an F1-score between 95% and 99% is achievable for five of six datasets. The inference on energy-efficient GPU-less infrastructure with only 4 GB can be done in less than 1 s.","Blood Pattern Analysis is a technique in forensic science that focuses on leftover bloodstains from the crime to recreate the event. However, the fluctuation in air resistance and drop deformity causes the calculations to deviate from the exact values. Therefore, machine learning models were constructed to overcome this limitation of calculations. A series of experiments was conducted by dropping porcine blood on paper across nine distinct heights: 20, 40, 60, 80, 100, 120, 140, 160, and 180 cm with four different drop volumes: 13, 16, 25, and 30 \u03bcL resulting in 36 classes. A simple simulation of a free-fall spherical object was also created to convert any drop height into impact velocity. Regarding both the empirical data and simulation, the correlation between the spreading factor and modified Reynold number, along with the number of spines and modified Weber number, were expressed as equations that can be used to determine drop height and drop volume. Concurrently, the same dataset used in physics calculations was used to train machine learning models that implement VGG-19 and XGBoost. For VGG-19, the inputs are images of bloodstains, while for XGBoost, the inputs are stain area, stain perimeter, and the number of spines. As a result, the accuracy for physics equations VGG-19 and XGBoost were 0.26, 0.56, and 0.49, respectively.","Background: Breast cancer is one of the greatest health threats to women worldwide. Mammography is an effective and inexpensive tool for breast cancer early detection. Mammography-based breast cancer screening requires a lot of manpower from professional experts. Thus, computer-aided diagnosis tools, especially accurate classifiers which can distinguish the breast masses from the background tissues, are needed. However, since the sample size of publicly available mammography data sets is relatively small, the performance of the published breast mass identification models was not great, and the models were not well-embraced by clinical practice due to their low interpretability. Methods: In this work, using two independent and well-known mammography data sets, the CBIS-DDSM and the INbreast, we proposed a novel patch generation method for data augmentation and negative case generation. We implemented two successful deep learning models, the ResNet and the ViT, to classify the generated mass and non-mass patches. We also proposed to apply the patch-level model to the full-view mammogram screening in a sliding window manner and visualize/interpret the prediction results using a heatmap so that the clinic practice could potentially benefit from the well-trained model. Result: For the CBIS-DDSM dataset, we compared the performance of the ResNet and the ViT with and without data augmentation. The F1 score is 0.91, 0.86, 0.85, and 0.70, respectively. We also evaluated our models using other metrices such as accuracy, precision, recall, and ROC curve. The results show that the ResNet model outperforms the ViT model. And the data augmentation improves the overall performance of the models. The similar conclusions are further supported using the independent INbreast data. Furthermore, we also explored to use probability-based heatmaps to visualize the potential mass regions in mammogram images. Conclusion: The study shows that our patch-level data augmentation is effective in improving the classification performance of the deep learning models. The comparable performance on the CBIS-DDSM data and the independent INbreast data demonstrates the generalizability of our methods. The proposed heatmap visualization tool increases the interpretability of our results and could be a potential approach for clinic utilization.","Text classification is a fundamental task for natural language processing, and adapting text classification models across domains has broad applications. Self-training generates pseudoexamples from the model's predictions and iteratively train on the pseudo-examples, i.e., mininizes the loss on the source domain and the Gibbs entropy on the target domain. However, Gibbs entropy is sensitive to prediction errors, and thus, self-training tends to fail when the domain shift is large. In this paper, we propose Meta-Tsallis Entropy minimization (MTEM), which applies meta-learning algorithm to optimize the instance adaptive Tsallis entropy on the target domain. To reduce the computation cost of MTEM, we propose an approximation technique to approximate the Second-order derivation involved in the meta-learning. To efficiently generate pseudo labels, we propose an annealing sampling mechanism for exploring the model's prediction probability. Theoretically, we prove the convergence of the meta-learning algorithm in MTEM and analyze the effectiveness of MTEM in achieving domain adaptation. Experimentally, MTEM improves the adaptation performance of BERT with an average of 4 percent on the benchmark dataset.","In Multi-Label Classification, utilizing label relationship is a key to improve classification accuracy. Label Space Dimension Reduction or Classifier Chains utilizes the relationship explicitly however those utilization are still limited. In this paper, we propose Retargeted Regression methods for Multi-Label classification by extending Retargeted Linear Least Squares originally proposed for Multi-Class Classification. Retargeted methods not only learn classifiers but also modify targets with margin constraints. Since in Multi-Label Classification, an instance may have more than one label, large margin constraints between all pairs of positive labels and negative labels are introduced. This enables to utilize the label relationship with taking ranking of labels for each instance into consideration. We also propose a simple heuristic to determine a threshold parameter for each instance to earn zero-one classification. On nine benchmark datasets, the proposed method outperformed conventional methods in the sense of instance-wise ranking. In best cases, classification accuracy was improved at\n7\n%\non AUC metric.","The tone recognition is a necessary function in human-computer interaction (HCI) systems of Mandarin training, especially for foreign learner whose mother tongue is non-tonal. This paper presents a novel tone recognition algorithm which is based on random forest (RF) model and features fusion. First, a variety of voiced source features related to Mandarin syllable tone were selected and three fusion feature sets (FFSs) were obtained by different fusion processing. Then the three FFSs were used to construct CART decision trees respectively and the corresponding RF tone classifiers were modeled and optimized. The RF algorithms were experimented with multi-speakers Syllable Corpus of Standard Chinese (SCSC) and compared with other classifiers which are back propagation neural network (BPNN), support vector machine (SVM), Naive Bayes model (NBM) and AdaBoost. And for the small sample training sets, the effects were also evaluated. The results showed that the three optimized RF classifiers all achieved high tone recognition rate (&gt;97.50%), model generalization ability (&gt;98.35%) and ability to classify unbalanced data (&gt;97.50%), and obtained the highest recognition rates on each FFS respectively, even for small training sets, the recognition rates remained above 93.57%. This indicates that the proposed algorithm is high efficient, robustness and suitable for portable HCI of Mandarin training.","Speech is one of the most promising features that reflects the underlying emotion of a human being. There are some measurable parameters in speech signals that reveal a persons affective state. Speech Emotion Recognition (SER) is a process of identifying the emotional elements in communication regardless of contextual relevance. Leveraging studies have taken place in this area. This paper proposes an ensemble model to automatically classify emotion from speech signals to one among the seven emotional classes neutral, calm, angry, sad, happy, fear, disgust, and surprised. In this work, speech spectral features have been extracted using Mel Frequency Cepstral Coefficient (MFCC). An emotion classification model based on 2-Dimensional Convolutional Neural Networks (2D-CNN) and eXtreme Gradient Boosting (XG-Boost) is proposed in this paper. This work also compares the performance of the proposed ensemble model with baseline models and other ensemble models. The accuracy of each model on the Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS) dataset is computed and the proposed model shows maximum accuracy in classifying emotions.","Multi-instance learning (MIL) allows predictive algorithms to use complex data representation. The data in MIL is organized in the form of labeled bags of instances, and the labels of instances are not available in the training phase. The processing and classification of complex bag representation result in a complicated hypothesis space. Moreover, identifying essential instances inside is also important, as these instances trigger positive labels for the bag and play a vital role in the model interpretation and bag classification. The recent MIL algorithms are not robust to tackle hypothesis space complication. Additionally, the existing instance selection algorithms are based on explicit assumptions regarding the relationship of instances to the bag label. However, these assumptions may hold for a specific bag in the dataset but may not apply to the whole dataset. To deal with the hypothesis space complication and robust instance selection without any prior assumption, this paper proposes a fuzzy subspace clustering approach for robust instance selection and ensemble-based variant for Fisher vector (FV) encoding to solve MIL problems, named (FCBE-miFV). Specifically, the proposed algorithm uses a subspace fuzzy clustering approach to compute instance selection probabilities, selects essential instances from the bag, transforms the input bag into FV, and classifies the generated bag encodings using a stacking-based ensemble approach to obtain improved bag level classification performance. The proposed FCBE-miFV improves model performance by incorporating essential instances in the bag encoding process. The experimental results show that the FCBE-miFV obtained comparable performance to the state-of-the-art MIL problems.","This paper proposes a texture-based domain-specific data augmentation technique applicable when training on small datasets for deep learning classification tasks. Our method focuses on label-preservation to improve generalization and optimization robustness over data-dependent augmentation methods using textures. We generate a small perturbation in an image based on a randomly sampled texture image. The textures we use are naturally occurring and domain-independent of the training dataset: regular, near regular, irregular, near stochastic and stochastic classes. Our method uses the textures to apply sparse, patterned occlusion to images and a penalty regularization term during training to help ensure label preservation. We evaluate our method against the competitive soft-label Mixup and RICAP data augmentation methods with the ResNet-50 architecture using the unambiguous \u201cBird or Bicyle\u201d and Oxford-IIT-Pet datasets, as well as a random sampling of the Open Images dataset. We experimentally validate the importance of label-preservation and improved generalization by using out-of-distribution examples and show that our method improves over competitive methods.","Cancer is the second cause of mortality worldwide and it has been identified as a perilous disease. Breast cancer accounts for \u223c20% of all new cancer cases worldwide, making it a major cause of morbidity and mortality. Mammography is an effective screening tool for the early detection and management of breast cancer. However, the identification and interpretation of breast lesions is challenging even for expert radiologists. For that reason, several Computer-Aided Diagnosis (CAD) systems are being developed to assist radiologists to accurately detect and/or classify breast cancer. This review examines the recent literature on the automatic detection and/or classification of breast cancer in mammograms, using both conventional feature-based machine learning and deep learning algorithms. The review begins with a comparison of algorithms developed specifically for the detection and/or classification of two types of breast abnormalities, micro-calcifications and masses, followed by the use of sequential mammograms for improving the performance of the algorithms. The available Food and Drug Administration (FDA) approved CAD systems related to triage and diagnosis of breast cancer in mammograms are subsequently presented. Finally, a description of the open access mammography datasets is provided and the potential opportunities for future work in this field are highlighted. The comprehensive review provided here can serve both as a thorough introduction to the field but also provide indicative directions to guide future applications.\nHighlights\n\u2022\nReview of the automated detection/classification of breast cancer in mammograms.\n\u2022\nComprehensive comparison of the effectiveness of various dissimilar approaches.\n\u2022\nOverview of studies utilizing sequential mammograms for increased performance.\n\u2022\nPresentation of the FDA-approved CAD systems for breast cancer diagnosis.\n\u2022\nDescription of extend and limitations of open access mammography datasets.","Zero-shot object classification aims to recognize the object of unseen classes whose supervised data are unavailable in the training stage. Recent zero-shot learning (ZSL) methods usually propose to generate new supervised data for unseen classes by designing various deep generative networks. In this paper, we propose an end-to-end deep generative ZSL approach that trains the data generation module and object classification module jointly, rather than separately as in the majority of existing generation-based ZSL methods. Due to the ZSL assumption that unseen data are unavailable in the training stage, the distribution of generated unseen data will shift to the distribution of seen data, and subsequently causes the projection domain shift problem. Therefore, we further design a novel meta-learning optimization model to improve the proposed generation-based ZSL approach, where the parameters initialization and the parameters update algorithm are meta-learned to assist model convergence. We evaluate the proposed approach on five standard ZSL datasets. The average accuracy increased by the proposed jointly training strategy is 2.7% and 23.0% for the standard ZSL task and generalized ZSL task respectively, and the meta-learning optimization further improves the accuracy by 5.0% and 2.1% on two ZSL tasks respectively. Experimental results demonstrate that the proposed approach has significant superiority in various ZSL tasks.\nHighlights\n\u2022\nAn end-to-end deep approach is proposed for zero-shot object classification.\n\u2022\nProposed approach trains the data generator and object classifier jointly.\n\u2022\nA meta-learning optimization is designed for the projection domain shift problem.\n\u2022\nExperimental results demonstrate the significant superiority of proposed approach.","Graph neural networks (GNNs) are increasingly used in critical human applications for predicting node labels in attributed graphs. Their ability to aggregate features from nodes' neighbors for accurate classification also has the capacity to exacerbate existing biases in data or to introduce new ones towards members from protected demographic groups. Thus, it is imperative to quantify how GNNs may be biased and to what extent their harmful effects may be mitigated. To this end, we propose two new GNN-agnostic interventions namely, (i) PFR-AX which decreases the separability between nodes in protected and non-protected groups, and (ii) PostProcess which updates model predictions based on a blackbox policy to minimize differences between error rates across demographic groups. Through a large set of experiments on four datasets, we frame the efficacies of our approaches (and three variants) in terms of their algorithmic fairness-accuracy tradeoff and bench- mark our results against three strong baseline interventions on three state-of-the-art GNN models. Our results show that no single intervention offers a universally optimal tradeoff, but PFR-AX and PostProcess provide granular control and improve model confidence when correctly predicting positive outcomes for nodes in protected groups.","The field of survival analysis is devoted to predicting the probability and time of the occurrence of an event. The global problem is to predict the event probability over time. It has applications in healthcare, credit scoring, etc. The most widely used method for assessing the covariate impacts on survival is the Cox proportional hazards approach. However, the assumption of non-overlapping survival functions usually does not hold on real data, and the linear dependence on features limits the quality of the method. There are tree-based machine learning methods to solve these problems. Usually, to evaluate the difference between the samples, it used the log-rank test. Obtained survival decision tree models also have strong interpretability, they can evaluate the importance of predictors, but they demonstrate inferior performance in comparison to Cox proportional hazards models.\nTo overcome these issues, this paper proposes a new boosting of the survival decision tree model that uses adaptive sampling and weighted log-rank split criteria. The model iteratively corrects an error in the ensemble. Each decision tree is trained on a sample, taking into account the weights of observations and subsequently adjusting the probabilities of getting into the next sample. We introduce an experimental comparison of the proposed adaptive boosting method against Cox proportional hazard and widely used survival trees and their ensembles: random forest and gradient boosting. Experiments on healthcare datasets show that our model outperforms the state-of-the-art survival models in terms of the following metrics: the concordance index, the integrated Brier score, and the integrated AUC.","Despite the effective application of deep learning to image classification tasks, these techniques are yet to be fully implemented in the industrial environment. In particular in brazing processes, where the high temperatures destroy the majority of the identification systems, product identification is required to allow automatic product traceability. This work addresses this issue in an international company facility producing water heaters, overcoming the lack of information regarding which model of the combustion chamber is starting the brazing process. Relying on deep learning algorithms, a solution is presented to develop and implement a vision system capable of classifying, through convolutional neural networks (CNN), the 16 models of combustion chambers. The system deployed at the company's production line, although trained with a relatively small collection of images, achieved an F1-score weighted average of 96% and 100% accuracy in classifying some of the models of combustion chambers, already implemented on the shop floor. From the building of the vision system for image acquisition until the training and deployment of the algorithms to classifying the combustion chambers are discussed.","Semantic labelling of remote sensing images, technically termed as remote sensing scene classification, plays significant role in understanding huge volume of complex remote sensing images. Eventually, this aids in a number of real-world decision making tasks, including land-cover management, urban planning, environmental monitoring, and so on. Over the past few years, different strategies have been adopted for proper labelling of remote sensing image scenes, and these are already studied by the existing surveys or review papers. Unlike these exploratory works, in this article, we exclusively focus on the recently proposed models that deal with remote sensing image scene classification under limited labelled sample scenario. We classify the existing works into three broad categories: Data-level, Model-level, and Algorithm-level. For each category, we formally present the working principles to tackle scarcity of labelled data, and also, provide insights into advantages/disadvantages of the adopted schemes. We further systematically analyse several interesting and open challenges, such as the incremental class problem (in streaming environment), presence of noisy training samples, multi-modality of the data, privacy issue, and so on, which give rise to additional complications during scene-level classification under limited labelled sample scenario. Finally, we provide a future vision of opportunities and discuss new perspectives pertaining to the development of this field.\nHighlights\n\u2022\nA survey of remote sensing (RS) scene classification based on deep network models.\n\u2022\nSystematically reviews RS scene classification under scarcity of labelled samples.\n\u2022\nSummarizes popular datasets for scene-level classification of remote sensing images.\n\u2022\nCategorizes scene classification methodologies into three novel theoretical classes.\n\u2022\nAnalyses methodological aptness and provides new insights into future research.","Self Normalizing Neural Networks (SNN) proposed on Feed Forward Neural Networks (FNN) outperform regular FNN architectures in various machine learning tasks. Particularly in the domain of Computer Vision, the activation function Scaled Exponential Linear Units (SELU) proposed for SNNs, perform better than other non linear activations such as ReLU. The goal of SNN is to produce a normalized output for a normalized input. Established neural network architectures like feed forward networks and Convolutional Neural Networks (CNN) lack the intrinsic nature of normalizing outputs. Hence, requiring additional layers such as Batch Normalization. Despite the success of SNNs, their characteristic features on other network architectures like CNN haven\u2019t been explored, especially in the domain of Natural Language Processing. In this paper we aim to show the effectiveness of proposed, Self Normalizing Convolutional Neural Networks (SCNN) on text classification. We analyze their performance with the standard CNN architecture used on several text classification datasets. Our experiments demonstrate that SCNN achieves comparable results to standard CNN model with significantly fewer parameters. Furthermore it also outperforms CNN with equal number of parameters.","This paper applies Natural Language Processing (NLP) methods to analyze the exposure to trauma experienced by witnesses in international criminal tribunals when testifying in court. One major contribution of this study is the creation of a substantially extended version of the Genocide Transcript Corpus (GTC) that includes 52,845 text segments of transcripts from three different genocide tribunals. Based on this data, we first examine the prevalence of trauma-related content in witness statements. Second, we are implementing a binary classification algorithm to automatically detect potentially traumatic content. Therefore, in a preparatory step, an Active Learning (AL) approach is applied to establish the ideal size for the training data set. Subsequently, this data is used to train a transformer model. In this case, the two models BERTbase and HateBERT are used for both steps, allowing for a comparison of a base-level model with a model that has already been pre-trained on data more relevant in the context of harmful vocabulary. In a third step, the study employs an Explainable Artificial Intelligence (XAI) model to gain a deeper understanding of the reasoning behind the model's classifications. Our results suggest that both BERTbase and HateBERT perform comparatively well on this classification task, with no model clearly outperforming the other. The classification outcomes further suggest that a reduced data set size can achieve equally high performance metrics and might be a preferable choice in certain use cases. The results can be used to establish more trauma-informed legal procedures in genocide-related tribunals, including the identification of potentially re-traumatizing examination approaches at an early stage.\nWarning: Due to the overall purpose of the study, this paper contains descriptions of violent events in Section 4.1 (Examples 1 and 2) and in Figure 3 that may be distressing for some readers.","In this paper, a three-step model based on the integration of Deep Neural Networks (DNN) and Decision Models is introduced for image classification which is inspired by the human visual system. To make a decision about an object, many actions should be done in a hierarchical process in the brain. First, the retina receives visual stimuli and transfers them to the visual cortex in the brain. The information extracted in the visual cortex, is accumulated over time to select an appropriate response. Many of the current decision-making models do not show how each image is converted into useful information for the decision model. Some models have used neural networks to convert each image into the information needed in the decision-making model; however, the role of the retina is ignored among these models. In this paper, a combination of retina inspired filters, CNN-based description and accumulator-based decision model is used to classify images. This model\u2019s structure resembles the human brain due to the usage of the DoG filter bank as retina inspired filter in the first stage of it. This model shows a significant improvement in accuracy in comparison to other models; furthermore, its performance is acceptable even with the small sample training set.","The training method for deep neural networks mainly adopts the gradient descent (GD) method. These methods, however, are very sensitive to initialization and hyperparameters. In this paper, an enhanced gradient descent method guided by the trajectory-based method for training deep neural networks, termed the Trajectory Unified Framework (TJU) method, is presented. From a theoretical viewpoint, the robustness of the TJU-based method is supported by an analytical basis presented in the paper. From a computational viewpoint, a TJU methodology consisting of a Block-Diagonal-Pseudo-Transient-Continuation method and a gradient descent method, termed the TJU-GD method, for training deep neural networks is added to obtain high-quality results. Furthermore, to resolve the issue of imbalanced classification, a TJU-Focal-GD method is developed and evaluated. Experimental numerical evaluation of the proposed TJU-GD on various public datasets reveals that the proposed method can achieve great improvements over baseline methods. Specifically, the proposed TJU-Focal-GD also possesses several advantages over other methods for a class of imbalanced datasets from the homemade power line inspection dataset (PLID).","Coronary artery disease (CAD) is a condition in which the heart is not fed sufficiently as a result of the accumulation of fatty matter. As reported by the World Health Organization, around 32% of the total deaths in the world are caused by CAD, and it is estimated that approximately 23.6 million people will die from this disease in 2030. CAD develops over time, and the diagnosis of this disease is difficult until a blockage or a heart attack occurs. In order to bypass the side effects and high costs of the current methods, researchers have proposed to diagnose CADs with computer-aided systems, which analyze some physical and biochemical values at a lower cost. In this study, for the CAD diagnosis, (i) seven different computational feature selection (FS) methods, one domain knowledge-based FS method, and different classification algorithms have been evaluated; (ii) an exhaustive ensemble FS method and a probabilistic ensemble FS method have been proposed. The proposed approach is tested on three publicly available CAD data sets using six different classification algorithms and four different variants of voting algorithms. The performance metrics have been comparatively evaluated with numerous combinations of classifiers and FS methods. The multi-layer perceptron classifier obtained satisfactory results on three data sets. Performance evaluations show that the proposed approach resulted in 91.78%, 85.55%, and 85.47% accuracy for the Z-Alizadeh Sani, Statlog, and Cleveland data sets, respectively.\nHighlights\n\u2022\nEnsemble feature selection (FS) methods have been proposed.\n\u2022\nThe proposed approach is tested on three coronary artery disease (CAD) data sets.\n\u2022\nSeven computational FS and one domain knowledge-based FS methods have been evaluated.\n\u2022\nSix classification and four ensemble voting algorithms have been incorporated.\n\u2022\nThe proposed coronary artery disease diagnosis system is accurate and reliable.","Fisher\u2019s Linear Discriminant Analysis (LDA) is a traditional dimensionality reduction method that has been proven to be successful for decades. To enhance the LDA\u2019s power for high-dimensional pattern classification, such as face recognition, numerous LDA-extension approaches have been proposed in the literature. This paper proposes a new method that improves the performance of LDA-based classification by simply increasing the number of (sub)-classes through clustering a few of classes of the training set prior to the execution of LDA. This is based on the fact that the eigen space of the training set consists of the range space and the null space, and that the dimensionality of the range space increases as the number of classes increases. Therefore, when constructing the transformation matrix, through minimizing the null space, the loss of discriminative information resulted from this space can be minimized. To select the classes to be clustered, in the present paper, the intraset distance is employed as a criterion and the k-means clustering is performed to divide them. Our experimental results for an artificial data set of XOR-type samples and a well-known benchmark face database of Yale demonstrate that the classification efficiency of the proposed method could be improved.","In this study, we propose an oversampling method called the minority-predictive-probability-based synthetic minority oversampling technique (MPP-SMOTE) for imbalanced learning. First, MPP-SMOTE removes noisy samples from minority classes. Subsequently, it divides minority samples into two types (hard-to-learn and easy-to-learn) by predicting the probability of samples belonging to the minority class. For both sample types, we adopt a divide-and-conquer strategy. We separately calculate the probability of each sample being selected to generate a new synthetic sample. The relative density of a sample in both the majority and minority classes is considered in the method for calculating the selection probability of hard-to-learn samples, and the relative density of a sample in only the minority class is considered in that of easy-to-learn samples. Finally, according to the types and selection probabilities, MPP-SMOTE separately selects samples and generates synthetic samples based on them by using different sample-generation schemes. Experimental results reveal that the proposed method outperforms other oversampling methods in terms of three imbalanced-learning metrics for three common classifiers. According to the results, when a support vector machine classifier is applied, the area under the curve performance of the MPP-SMOTE improves by a factor of 1.44%.","Text classification is a central task in Natural Language Processing (NLP) that aims at categorizing text documents into predefined classes or categories. It requires appropriate features to describe the contents and meaning of text documents, and map them with their target categories. Existing text feature representations rely on a weighted representation of the document terms. Hence, choosing a suitable method for term weighting is of major importance and can help increase the effectiveness of the classification task. In this study, we provide a novel text classification framework for Category-based Feature Engineering titled CFE. It consists of a supervised weighting scheme defined based on a variant of the TF-ICF (Term Frequency-Inverse Category Frequency) model, embedded into three new lean classification approaches: (i) IterativeAdditive (flat), (ii) GradientDescentANN (1-layered), and (iii) FeedForwardANN (2-layered). The IterativeAdditive approach augments each document representation with a set of synthetic features inferred from TF-ICF category representations. It builds a term-category TF-ICF matrix using an iterative and additive algorithm that produces category vector representations and updates until reaching convergence. GradientDescentANN replaces the iterative additive process mentioned previously by computing the term-category matrix using a gradient descent ANN model. Training the ANN using the gradient descent algorithm allows updating the term-category matrix until reaching convergence. FeedForwardANN uses a feed-forward ANN model to transform document representations into the category vector space. The transformed document vectors are then compared with the target category vectors, and are associated with the most similar categories. We have implemented CFE including its three classification approaches, and we have conducted a large battery of tests to evaluate their performance. Experimental results on five benchmark datasets show that our lean approaches mostly improve text classification accuracy while requiring significantly less computation time compared with their deep model alternatives.\nHighlights\n\u2022\nProvides a supervised weighting scheme based on Term Frequency-Inverse Category Frequency model.\n\u2022\nIntroduces three lean-architecture classifiers compared with more complex deep learning models.\n\u2022\nIterative-additive classifier builds term-category matrix producing category vector features.\n\u2022\nGradient descent classifier computes term-category matrix using gradient descent ANN model.\n\u2022\nANN classifier trains feed-forward ANN to transform documents to the category vector space.","Parkinson\u2019s Disease is the second most prevalent neurodegenerative disorder, currently affecting as high as 3% of the global population. Research suggests that up to 80% of patients manifest phonatory symptoms as early signs of the disease. In this respect, various systems have been developed that identify high risk patients by analyzing their speech using recordings obtained from natural dialogues and reading tasks conducted in clinical settings. However, most of them are centralized models, where training and inference take place on a single machine, raising concerns about data privacy and scalability. To address these issues, the current study migrates an existing, state-of-the-art centralized approach to the concept of federated learning, where the model is trained in multiple independent sessions on different machines, each with its own dataset. Therefore, the main objective is to establish a proof of concept for federated learning in this domain, demonstrating its effectiveness and viability. Moreover, the study aims to overcome challenges associated with centralized machine learning models while promoting collaborative and privacy-preserving model training.","Image generation has seen huge leaps in the last few years. Less than 10 years ago we could not generate accurate images using deep learning at all, and now it is almost impossible for the average person to distinguish a real image from a generated one. In spite of the fact that image generation has some amazing use cases, it can also be used with ill intent. As an example, deepfakes have become more and more indistinguishable from real pictures and that poses a real threat to society. It is important for us to be vigilant and active against deepfakes, to ensure that the false information spread is kept under control. In this context, the need for good deepfake detectors feels more and more urgent. There is a constant battle between deepfake generators and deepfake detection algorithms, each one evolving at a rapid pace. But, there is a big problem with deepfake detectors: they can only be trained on so many data points and images generated by specific architectures. Therefore, while we can detect deepfakes on certain datasets with near 100% accuracy, it is sometimes very hard to generalize and catch all real-world instances. Our proposed solution is a way to augment deepfake detection datasets using deep learning architectures, such as Autoencoders or U-Net. We show that augmenting deepfake detection datasets using deep learning improves generalization to other datasets. We test our algorithm using multiple architectures, with experimental validation being carried out on state-of-the-art datasets like CelebDF and DFDC Preview. The framework we propose can give flexibility to any model, helping to generalize to unseen datasets and manipulations.","The classification of low-magnitude tectonic earthquakes, explosions and mining-induced earthquakes is an important task in regional earthquake monitoring. Seismic events occurring at local and regional distances are classified primarily based on the characteristics of their waveform. We established 36-dimensional and 201-dimensional datasets by using feature extraction and amplitude spectral analysis. The Extreme Gradient Boosting (XGBoost) supervised algorithm is introduced for the discrimination of couples-class and three-class. The accuracies in the earthquakes/explosions discrimination with feature extraction dataset and amplitude spectrum dataset are 97.48% and 95.12%, respectively, which shows that feature extraction can effectively quantify the differences between earthquakes and explosions. For the classification of earthquakes/mining-induced earthquakes and explosions/mining-induced earthquakes, the performance of XGBoost with the amplitude spectrum dataset is greater, with accuracies of 99.24% and 95.33%, respectively. In the classification of the three types of events, the accuracies of XGBoost are 96.41% for earthquakes, 90.38% for explosions, and 94.04% for mining-induced earthquakes. The performance indices of XGBoost for different input parameters are invariably greater than those of the support vector machine (SVM), with stable classification ability, suggesting that the XGBoost model has good prospects for application in seismic event classification.\nHighlights\n\u2022\nApply XGBoost Algorithm to study the classification of seismic events occurring at local and regional distances.\n\u2022\nThe input dataset is based on feature extraction and amplitude spectrum.\n\u2022\nCompare the performance of XGBoost and SVM using confusion matrix.\n\u2022\nXGBoost outperforms SVM in different performance indices (AUC, Accuracy, TPR, TNR).","Deep neural networks tend to be vulnerable to catastrophic forgetting when learning new tasks. To address it, continual learning has become a promising and popular research field in recent years. It is noticed that plentiful research predominantly focuses on class-incremental (CI) settings. However, another practical setting, domain-incremental (DI) learning, where the domain distribution shifts in new tasks, also suffers from deteriorating rigidity and should be emphasized. Concentrating on the DI setting, in which the learned model is overwritten by new domains and is no longer valid for former tasks, a novel method named Consolidating Learned and Undergoing Experience (CLUE) is proposed in this paper. In particular, CLUE consolidates former and current experiences by setting penalties on feature extractor distortion and sample outputs alteration. CLUE is highly applicable to classification models as neither extra parameters nor processing steps are introduced. It is observed through extensive experiments that CLUE achieves significant performance improvement compared with other baselines in the three benchmarks. In addition, CLUE is robust even with fewer replay samples. Moreover, its feasibility is supported by both theoretical derivation and model interpretability visualization. The code is available at: https://github.com/Multiplied-by-1/CLUE.","Malignant brain tumors including parenchymal metastatic (MET) lesions, glioblastomas (GBM), and lymphomas (LYM) account for 29.7% of brain cancers. However, the characterization of these tumors from MRI imaging is difficult due to the similarity of their radiologically observed image features. Radiomics is the extraction of quantitative imaging features to characterize tumor intensity, shape, and texture. Applying machine learning over radiomic features could aid diagnostics by improving the classification of these common brain tumors. However, since the number of radiomic features is typically larger than the number of patients in the study, dimensionality reduction is needed to balance feature dimensionality and model complexity.\nAutoencoders are a form of unsupervised representation learning that can be used for dimensionality reduction. It is similar to PCA but uses a more complex and non-linear model to learn a compact latent space. In this work, we examine the effectiveness of autoencoders for dimensionality reduction on the radiomic feature space of multiparametric MRI images and the classification of malignant brain tumors: GBM, LYM, and MET. We further aim to address the class imbalances imposed by the rarity of lymphomas by examining different approaches to increase overall predictive performance through multiclass decomposition strategies.","Meta-learning has made tremendous progress in recent years and was demonstrated to be particularly suitable in low-resource settings where training data is very limited. However, meta-learning models still require large amounts of training tasks to achieve good generalisation. Since labelled training data may be sparse, self-supervision-based approaches are able to further improve performance on downstream tasks. Although no labelled data is necessary for this training, a large corpus of unlabelled text needs to be available. In this paper, we improve on recent advances in meta-learning for natural language models that allow training on a diverse set of training tasks for few-shot, low-resource target tasks. We introduce a way to generate new training data with the need for neither more supervised nor unsupervised datasets. We evaluate the method on a diverse set of NLP tasks and show that the model decreases in performance when trained on this data without further adjustments. Therefore, we introduce and evaluate two methods for regularising the training process and show that they not only improve performance when used in conjunction with the new training data but also improve average performance when training only on the original data, compared to the baseline.","Naive-Bayes classifier is a popular technique of classification in machine learning. Improving the accuracy of naive-Bayes classifier will be significant as it has great importance in classification using numerical attributes. For numeric attributes, the conditional probabilities are either modeled by some continuous probability distribution over the range of that attribute\u2019s values or by conversion of numeric attribute to discrete one using discretization. The limitation of the classifier using discretization is that it does not classify those instances for which conditional probabilities of any of the attribute value for every class is zero. The proposed method resolves this limitation of estimating probabilities in the naive-Bayes classifier and improve the classification accuracy for noisy data. The proposed method is efficient and robust in estimating probabilities in the naive-Bayes classifier. The proposed method has been tested over a number of databases of UCI machine learning repository and the comparative results of existing naive-Bayes classifier and proposed method has also been illustrated.","Arrhythmia categorization is an exciting research in the early prevention and detection of cardiovascular illnesses, using Electrocardiogram (ECG). In the case of ECG signals, time series data are obtained by changing the time. This type of signal has the drawback of requiring repeated acquisition of comparison data with the same size as the registration data. Resolving the issue of inconsistent data size is accomplished by the use of an additional classifier-based adversarial neural networks. Adversarial data synthesis using Generative Adversarial Networks (GANs) and the generation of additional training examples solves the basic problem of insufficient data labelling. Recent studies have used the GAN architecture to create synthetic adversarial ECG signals in order to boost the amount of training data already available. The arrhythmia detection system, on the other hand, has a fragmented Convolution Neural Network (CNN) classification architecture. No flexible structural design has yet been suggested that can simultaneously discover and order abnormalities. An exceptional Prioritized Feature Subset Vector-Associated Generative Adversarial Network (PFSV-AGAN) is proposed in this research in order at a time produce ECG indications for multiple classes and sense heart-related problems. Furthermore, the model is based on class-specific ECG signals in order to generate realistic adversarial cases. This research presents a framework for ECG signal abnormality identification that has an unbalanced distribution among classes and achieves high accuracy in abnormalities categorization. After training on datasets, the classification model reliably identifies abnormalities in the proposed model. The proposed model when compared to the traditional model exhibits better performance levels.","Histology images with multi-gigapixel of resolution yield rich information for cancer diagnosis and prognosis. Most of the time, only slide-level label is available because pixel-wise annotation is labour intensive task. In this paper, we propose a deep learning pipeline for classification in histology images. Using multiple instance learning, we attempt to predict the latent membrane protein 1 (LMP1) status of nasopharyngeal carcinoma (NPC) based on haematoxylin and eosin-stain (H &amp;E) histology images. We utilised attention mechanism with residual connection for our aggregation layers. In our 3-fold cross-validation experiment, we achieved average accuracy, AUC and F1-score 0.936, 0.995 and 0.862, respectively. This method also allows us to examine the model interpretability by visualising attention scores. To the best of our knowledge, this is the first attempt to predict LMP1 status on NPC using deep learning.","Cardiac arrhythmia (CA) is an irregular rhythm that can cause an increase or decrease in heart rate. Electrocardiogram (ECG) is a noninvasive diagnostic technique widely used to identify CAs. The cardiologist analyzes these ECG signals for the accurate diagnosis and treatment of CA. However, diagnosing minute changes with the naked eye for longer ECG recordings is difficult and takes much time for the cardiologist to conclude the disease type. Therefore, developing an automated diagnostic tool for classifying CA from ECG recordings is essential. In this work, we propose a novel technique for effectively diagnosing CAs from the ECG signal. The proposed method optimizes the hyper-parameters of the tunable Q wavelet transform (TQWT) and random subspace of the stacked ensemble classifier using an enhanced Jaya optimization algorithm (EJOA) for CA classification. This optimized parameter of TQWT extracts features from ECG signals by decomposing them into high- and low-pass sub-band signals. Then, the low-dimensional features are extracted from the sub-band signal coefficients by applying principal component analysis (PCA). Finally, the optimized random subspace stacked ensemble classifier is trained with the optimal random subspace features of the principal components for subsequent classification of CAs. The trained random subspace stacked ensemble classifier was independently tested with inter-patient testing beats of the MIT-BIH arrhythmia and St Petersburg INCART 12-lead arrhythmia databases to diagnose different CA classes. The proposed method achieved an average accuracy of 98.30% and a sensitivity of 93.42% for the MIT-BIH database, an average accuracy of 96.60%, and a sensitivity of 80.2% for the INCART database. These performance measures demonstrate the proposed classifier\u2019s efficacy and are better than those reported in the existing literature.","Decision trees are widely-used classification and regression models because of their interpretability and good accuracy. Classical methods such as CART are based on greedy approaches but a growing attention has recently been devoted to optimal decision trees. We investigate the nonlinear continuous optimization formulation proposed in Blanquero et al. (2020) for training sparse optimal randomized classification trees. Sparsity is important not only for feature selection but also to improve interpretability. We first consider alternative methods to sparsify such trees based on concave approximations of the l 0 \u201cnorm\u201d. Promising results are obtained on 24 datasets in comparison with the original l 1 and l \u221e regularizations. Then, we derive bounds on the VC dimension of multivariate randomized classification trees. Finally, since training is computationally challenging for large datasets, we propose a general node-based decomposition scheme and a practical version of it. Experiments on larger datasets show that the proposed decomposition method is able to significantly reduce the training times without compromising the testing accuracy.\nHighlights\n\u2022\nl 0-based regularization terms induce sparsity in multivariate randomized classification trees.\n\u2022\nnew lower and upper bounds on the VC dimension of multivariate randomized classification trees.\n\u2022\nnode-based decomposition methods for multivariate randomized classification trees.","We present a baseline for gesture recognition using state-of-the-art sequence classifiers on a new freely available multi-modal dataset of free-hand gestures. The dataset consists of roughly 100,000 samples, grouped into six classes of typical and easy-to-learn hand gestures. The dataset was recorded using two independent sensors, allowing for experiments on multi-modal data fusion at several depth levels and allowing research on multi-modal fusion for early, intermediate, and late fusion techniques. Since the whole dataset was recorded by a single person we ensure a very high quality of data with little to no risk for incorrectly performed gestures. We show the results of our experiments on unimodal sequence classification using a LSTM as well as a CNN classifier. We also show that multi-modal fusion of all four modalities results in higher precision using late-fusion of the output layer of an LSTM classifier trained on a single modality. Finally, we demonstrate that it is possible to perform live gesture classification using an LSTM-based gesture classifier, showing that generalization to other persons performing the gestures is high.","Proteins play a vital role by booming out a number of activities within an organism to withstand its life. The field of Natural Language Processing has successfully adapted deep learning to get a better insight into the semantic nature of languages. In this paper, we propose semantic approaches based on deep learning to work with protein amino acid sequences and compare the performances of these approaches with traditional classifiers to predict their respective families. The Bidirectional Encoder Representations from Transformers (BERT) approach was tested over 103 protein families from UniProt consortium database. The results show the average prediction accuracy to 99.02%, testing accuracy to 97.70%, validation accuracy to 97.69%, Normalized Mutual Information (NMI) score on overall data to 98.45, on test data to 96.99, on validation data 96.93 with high weighted average F1 scores of 99.02 on overall data, 97.72 on test data and 97.70 on validation data, and high macro average F1 scores of 99.00 on overall data, 98.00 on test data and 98.00 on validation data. From the results, it is justified that our proposed approach is outperforming well when compared to the existing approaches.","Sparse representation-based classification (SRC) has attracted much attention by casting the recognition problem as simple linear regression problem. SRC methods, however, still is limited to enough labeled samples per category, insufficient use of unlabeled samples, and instability of representation. For tackling these problems, an unlabeled data driven inverse projection pseudo-full-space representation-based classification model is proposed with low-rank sparse constraints. The proposed model aims to mine the hidden semantic information and intrinsic structure information of all available data, which is suitable for few labeled samples and proportion imbalance between labeled samples and unlabeled samples problems in frontal face recognition. The mixed Gauss\u2013Seidel and Jacobian ADMM algorithm is introduced to solve the model. The convergence, representation capability and stability of the model are analyzed. Experiments on three public datasets show that the proposed LR-S-PFSRC model achieves stable results, especially for proportion imbalance of samples.\nHighlights\n\u2022\nAn inverse projection-based SRC model is proposed.\n\u2022\nAims to mine semantic and structure information.\n\u2022\nSolved by MADMM.","Recently, an emerging trend of representing objects by graphs can be observed. As a matter of fact, graphs offer a versatile alternative to feature vectors in pattern recognition, machine learning and data mining. However, the space of graphs contains almost no mathematical structure, and consequently, there is a lack of suitable methods for graph classification. Graph kernels, a novel class of algorithms for pattern analysis, offer an elegant solution to this problem. Graph kernels aim at bridging the gap between statistical and symbolic object representations. In the present paper we propose a general approach to transforming graphs into n-dimensional real vector spaces by means of graph edit distance. As a matter of fact, this approach results in a novel family of graph kernels making a wide range of kernel machines applicable for graphs. With several experimental results we prove the robustness and flexibility of our new method and show that our approach outperforms a standard graph classification method on several graph data sets of diverse nature.","With the rapid development of the Internet, the number of online news are increasing significantly. The efficient identification and classification of news through news headlines is of great significance for news regulation and information gathering. This paper proposes a Chinese news title classification model based on the combination of ERNIE and TextRCNN. It uses the ERNIE model to generate news text title vectors. Through the convolutional layer of TextRCNN, the generated vector is extracted with a bidirectional long short-term memory network. Then, the important parts of the text are obtained through the pooling layer, and finally classified by softmax. The experimental results show that the improved ERNIE-TextRCNN model has an accuracy value of 95.23% and an F1 value of 95.21% on the public THUCNews data set, which is significantly better than other models in evaluation indicators such as accuracy, precision, recall, and F1 value.","Many approaches to Information Extraction (IE) have been proposed in literature capable of finding and extract specific facts in relatively unstructured documents. Their application in a large information space makes data ready for post-processing which is crucial to many context such as Web mining and searching tools. This paper proposes a new IE strategy, based on symbolic and neural techniques, and tests it experimentally within the price comparison service domain. In particular the strategy seeks to locate a set of atomic elements in free text which is preliminarily extracted from web documents and subsequently classify them assigning a class label representing a specific product.","This work presents a new approach to analysis of shapes represented by finite set of landmarks, that generalizes the notion of Procrustes distance - an invariant metric under translation, scaling, and rotation. In many shape classification tasks there is a large variability in certain landmarks due to intra-class and/or inter-class variations. Such variations cause poor shape alignment needed for Procrustes distance computation, and lead to poor classification performance. We apply a general framework to the task of supervised classification of shapes that naturally deals with landmark distributions exhibiting large intra class or inter-class variabilty. The incorporation of Procrustes metric and of a learnt general quadratic distance inspired by Fisher linear discriminant objective function, produces a generalized Procrustes distance. The learnt distance retains the invariance properties and emphasizes the discriminative shape features. In addition, we show how the learnt metric can be useful for kernel machines design and demonstrate a performance enhancement accomplished by the learnt distances on a variety of classification tasks of organismal forms datasets.","This paper addresses the missing modality problem in multimodal person classification, where an incomplete multimodal input with one modality missing is classified into predefined person classes. A multimodal cascaded framework with three deep learning models is proposed, where model parameters, outputs, and latent space learnt at a given step are transferred to the model in a subsequent step. The cascaded framework addresses the missing modality problem by, firstly, generating the complete multimodal data from the incomplete multimodal data in the feature space via a latent space. Subsequently, the generated and original multimodal features are effectively merged and embedded into a final latent space to estimate the person label. During the learning phase, the cascaded framework uses two novel latent loss functions, the missing modality joint loss, and latent prior loss to learn the different latent spaces. The missing modality joint loss ensures that the similar class latent data are close to each other, even if a modality is missing. In the cascaded framework, the latent prior loss learns the final latent space using a previously learnt latent space as a prior. The proposed framework is validated on the audio-visible RAVDESS and the visible-thermal Speaking Faces datasets. A detailed comparative analysis and an ablation analysis are performed, which demonstrate that the proposed framework enhances the robustness of person classification even under conditions of missing modalities, reporting an average of 21.75% increase and 25.73% increase over the baseline algorithms on the RAVDESS and Speaking Faces datasets.","Infrared detector is an important device with a wide range of applications. Based on the fault sensitive parameter data of infrared detectors, this paper studies the fault classification and fault prediction model of infrared detectors by using machine learning methods such as neural network BPNN and long and short term memory network LSTM. Through the establishment and verification analysis of the fault classification model, it provides a model reference and basis for the multi-type fault diagnosis of infrared detectors. Through the establishment and analysis of the fault prediction model, it provides a modeling method for the lifetime prediction of infrared detectors. The application of infrared detector fault classification and prediction technology can improve the reliability of infrared detector products.","Highlights\n\u2022\nTo address the domain difference in domain generalization, we propose to align multiple domains P 1 ( x , y ) , \u2026 , P n ( x , y ) via the alignment of two distributions: the joint distribution P ( x , y , l ) and the product distribution P ( x , y ) P ( l ), where the domain label l \u2208 { 1 , \u22ef , n }.\n\u2022\nWe analytically derive an explicit estimate of the Relative Chi-Square (RCS) divergence between P ( x , y , l ) and P ( x , y ) P ( l ), and minimize this estimate to align distributions in the neural transformation space.\n\u2022\nWe demonstrate the effectiveness of our solution through conducting comprehensive experiments on several multi-domain image classification datasets.\nAbstract\nIn this work, we address the problem of domain generalization for classification, where the goal is to learn a classification model on a set of source domains and generalize it to a target domain. The source and target domains are different, which weakens the generalization ability of the learned model. To tackle the domain difference, we propose to align a joint distribution and a product distribution using a neural transformation, and minimize the Relative Chi-Square (RCS) divergence between the two distributions to learn that transformation. In this manner, we conveniently achieve the alignment of multiple domains in the neural transformation space. Specifically, we show that the RCS divergence can be explicitly estimated as the maximal value of a quadratic function, which allows us to perform joint-product distribution alignment by minimizing the divergence estimate. We demonstrate the effectiveness of our solution through comparison with the state-of-the-art methods on several image classification datasets.","Customer churn prediction is an essential strategy for companies, especially in telecommunications. Such industries face the challenge that customers frequently switch operators. Due to the higher cost of acquiring new customers compared to retaining existing ones, companies put considerable effort into keeping their current customers. Improving service quality and identifying the point at which customers are likely to terminate their engagement with the company are crucial in retaining customers. Customer Churn Prediction aims to predict potential customer churn by building an effective predictive model. However, the model\u2019s performance is sensitive to unnecessary and irrelevant features. Feature selection is used to eliminate irrelevant features while emphasizing significant ones. This study suggests utilizing a feature selection method to identify significant features and enhance the accuracy of the customer churn prediction model. We propose employing a recently developed evolutionary computation method known as the gravitational search algorithm (GSA) for the feature selection approaches. We elaborate on GSA and the SVM as the classifier to find the optimum features and to improve the prediction accuracy. Our method produced higher precision and AUC scores than the baseline model (without feature selection).","Recently, ESN has been applied to time series classification own to its high-dimensional random projection ability and training efficiency characteristic. The major drawback of applying ESN to time series classification is that ESN cannot capture long-term dependency information well. Therefore, the Multiscale Echo Self-Attention Memory Network (MESAMN) is proposed to address this issue. Specifically, the MESAMN consists of a memory encoder and a memory learner. In the memory encoder, multiple differently initialized ESNs are utilized for high-dimensional projection which is then followed by a self-attention mechanism to capture the long-term dependent features. A multiscale convolutional neural network is developed as the memory learner to learn local features using features extracted by the memory encoder. Experimental results show that the proposed MESAMN yields better performance on 18 multivariate time series classification tasks as well as three 3D skeleton-based action recognition tasks compared to existing models. Furthermore, the capacity for capturing long-term dependencies of the MESAMN is verified empirically.","Accurate classification of Acute Myeloid Leukemia (AML) subtypes is crucial for clinical decision-making and patient care. In this study, we investigate the potential presence of age and sex bias in AML subtype classification using Multiple Instance Learning (MIL) architectures. To that end, we train multiple MIL models using different levels of sex imbalance in the training set and excluding certain age groups. To assess the sex bias, we evaluate the performance of the models on male and female test sets. For age bias, models are tested against underrepresented age groups in the training data. We find a significant effect of sex and age bias on the performance of the model for AML subtype classification. Specifically, we observe that females are more likely to be affected by sex imbalance dataset and certain age groups, such as patients with 72 to 86 years of age with the RUNX1::RUNX1T1 genetic subtype, are significantly affected by an age bias present in the training data. Ensuring inclusivity in the training data is thus essential for generating reliable and equitable outcomes in AML genetic subtype classification, ultimately benefiting diverse patient populations.","For the established fault classification system, new faults cannot be identified due to lack of training data in the process of equipment operation. Aiming at the problems of multi-classification, small samples, and cross-domain brought by the new faults, one meta learning intelligent fault diagnosis method is proposed based on multi-scale dilated convolution and relation module. Firstly, multi-scale convolution is utilized to improve the feature extraction effectiveness in the extraction module. Subsequently, the fusion module is designed by dilated convolution and stochastic pooling. Finally, the relation module is employed to evaluate the distance between samples for fault diagnosis. Crucially, the meta learning strategy is executed to transform the training set into multiple tasks to train the proposed method. The superiority and effectiveness of the proposed method is validated by bearing and gearbox experiments with a few labeled fault samples. For the bearing fault diagnosis test, the verification results show that the accuracy rate of this method is 95.11% in 8way 1-shot, which is increased by 6.15% on average.","We investigate the generalization properties of a self-training algorithm with halfspaces. The approach learns a list of halfspaces iteratively from labeled and unlabeled training data, in which each iteration consists of two steps: exploration and pruning. In the exploration phase, the halfspace is found sequentially by maximizing the unsigned-margin among unlabeled examples and then assigning pseudo-labels to those that have a distance higher than the current threshold. These pseudo-labels are allegedly corrupted by noise. The training set is then augmented with noisy pseudolabeled examples, and a new classifier is trained. This process is repeated until no more unlabeled examples remain for pseudo-labeling. In the pruning phase, pseudo-labeled samples that have a distance to the last halfspace greater than the associated unsigned-margin are then discarded. We prove that the misclassification error of the resulting sequence of classifiers is bounded and show that the resulting semi-supervised approach never degrades performance compared to the classifier learned using only the initial labeled training set. Experiments carried out on a variety of benchmarks demonstrate the efficiency of the proposed approach compared to state-of-the-art methods.","This paper proposes a multi-task model for the classification and grasp detection of surgical tools so that the tasks such as handing, collection (from the surgeon or other person), disinfection, sorting, and assembling of surgical tools can be automatized with the help of a robotic system, which will in-turn allow health-care workers to spend their time on other complex tasks. The multi-task model uses a feature extractor and the extracted features are processed further to produce the output corresponding to both tasks. To train the model, we have prepared a custom dataset consisting of 800 images with 8 different classes taken from two publicly available datasets namely the HOSPI-Tools Dataset and the Surgical Image dataset. The model was trained using transfer learning in two phases with three different pre-trained feature extractors namely: MobileNetV3-Large, Inception-v3, and EfficientNetV2-S. We have achieved the best results with EfficientNetV2-S as a feature extractor and the results are classification accuracy\u201499.75%, localization accuracy\u201490.375%, and detection accuracy\u201490.25%.","Unsupervised Domain Adaptation (UDA) is an ideal transfer learning method, which can use labeled source data to improve the classification performance of unlabeled target data. At present, the UDA methods for Time Series Classification (TSC) only use time-domain data or frequency-domain data as the input, and ignore fusing them, resulting in insufficient feature extraction and inaccurate source-target distribution alignment. Therefore, we propose an unsupervised Multimodal Domain Adversarial Network (MDAN) for TSC tasks. Specifically, we adopt two feature extractors for the time-domain and frequency-domain feature representations, and employ-three classifiers to perform TSC of source data for training the two feature extractors; Then, we fuse the time-domain and frequency-domain feature representations of source and target data, respectively, input them into the unified domain discriminator for unsupervised multimodal domain adversarial learning, and combine the proposed Time-Frequency-domain Joint Maximum Mean Discrepancy (TF-JMMD) to accurately align the source-target distributions; Finally, we select CNN or ResNet18 as the feature extractors to carry out comprehensive experiments, and the results demonstrate the SOTA performance of MDAN.","In this paper the first results of the process of extracting survival patterns in diagnosed women with invasive cervical cancer with classification techniques from data reported in population-based cancer registry of the municipality of Pasto (Colombia) for a time period of 10 years are presented. The generated knowledge will allow to understand the different socioeconomic and clinical factors affecting the survival of this population group. This knowledge will support effective decision making of government agencies and private health sector in relation to the approach of public policies and prevention programs designed to detect new cases of women with this disease early.","Haze classification plays a crucial role in air quality and visibility assessment. In contrast to traditional image classification, haze classification requires the classifier to capture the characteristics of different levels of haze. However, existing methods primarily focus on feature extraction while neglecting the interference of background information. To address this issue, this paper proposes a hard attention infused network (HAINet) for haze classification, consisting of an unsupervised segmentation module (USM) and a hybrid information fusion module (HIF). The USM is used to extract haze area information in an unsupervised manner, generating various forms of haze images. The HIA selects different various forms of haze images, as a hard attention mechanism, to reduce the impact of background and improve classification performance. We conduct experiments on two datasets, Hazel-level and Haze-Wild, in terms of performance comparison, ablation study, and case studies. The results show that our method effectively reduces the impact of background noise in haze images and consistently improves the classification performance.","Classification of multispectral images is impacted by challenges such as inadequate training samples, limited ground truth, and complex spatiotemporal dependencies. The accuracy of classifiers due to the lack of training samples is also compounded by the class imbalance problem in spatial analytics.\nThis paper proposes a novel idea of exploiting the deep residual networks trained on millions of images for high discriminative feature representations. These derived representations are classified using bias-corrected Adam optimizer-based Support Vector Machine classifiers. The intuition behind this approach is the significance of representations in the last layers of the pre-trained network that contributes to the classification accuracy. These encapsulated representations from the pre-trained network can outperform traditional feature representations. Residual networks solved the vanishing gradients problem in deep learning and improved classification accuracy. Hence, this work explores the combined potential of the representation power of deep residual pre-trained networks and the classification ability of Support Vector Machines. The model implementation experiments have been conducted using two publicly available benchmark datasets, Eurosat and UC Merced for Land Use and Land Cover Classification. The proposed model DRSVM (Deep Residual Support Vector Machines) demonstrated higher efficiency in terms of computational parameters and time complexity than conventional convolutional neural networks, pre-trained networks, and Support Vector Machines with comparable accuracy. Data augmentation techniques are used further for enhancing the performance of the model. Conventional supervised deep learning algorithms like CNN overfit in dense layers in case of inadequate training data. This hybrid approach alleviates overfitting issues by eliminating dense layers and replacing the Softmax function with Support Vector Machines for classification.","Cautious classifiers are designed to make indeterminate decisions when the uncertainty on the input data or the model output is too high, so as to reduce the risk of making wrong decisions. In this paper, we propose two cautious decision-making procedures, by aggregating trees providing probability intervals constructed via the imprecise Dirichlet model. The trees are aggregated in the belief functions framework, by maximizing the lower expected discounted utility, so as to achieve a good compromise between model accuracy and determinacy. They can be regarded as generalizations of the two classical aggregation strategies for tree ensembles, i.e., averaging and voting. The efficiency and performance of the proposed procedures are tested on random forests and illustrated on three UCI datasets.","Imbalanced datasets pose significant challenges in the field of machine learning, as they consist of samples where one class (majority) dominates over the other class (minority). Although AdaBoost is a popular ensemble method known for its good performance in addressing various problems, it fails when dealing with imbalanced data sets due to its bias towards the majority class samples. In this study, we propose a novel weighting factor to enhance the performance of AdaBoost (called IMBoost). Our approach involves computing weights for both minority and majority class samples based on the performance of classifier on each class individually. Subsequently, we resample the data sets according to these new weights. To evaluate the effectiveness of our method, we compare it with six well-known ensemble methods on 30 imbalanced data sets and 4 synthetic data sets using ROC, precision-eecall AUC, and G-mean metrics. The results demonstrate the superiority of IMBoost. To further analyze the performance, we employ statistical tests, which confirm the excellence of our method.","This work introduces a novel algorithm, called Condensation rule based on Fuzzy Rough Sets (FRSC), based on the FCNN rule together with fuzzy rough sets theory, to compute training-set-consistent subset for the nearest neighbor decision rule. In combination with fuzzy rough set theory, the FRSC rule improves the performance of FCNN rule. Two variants, named as FRSC1 and FRSC2, of the FRSC rule, are presented. The FRSC1 rule is suitable for small data set and the FRSC2 adapts to larger data sets. Compared with the FCNN rule, the FRSC1 rule requires much less time cost and gets smaller subset for small data set. For medium-size data set, less than 5000 samples, the FRSC2 rule has better time performance than FCNN rule.","The COVID-19 (Coronavirus disease 2019) pandemic has become a major global threat to human health and well-being. Thus, the development of computer-aided detection (CAD) systems that are capable of accurately distinguishing COVID-19 from other diseases using chest computed tomography (CT) and X-ray data is of immediate priority. Such automatic systems are usually based on traditional machine learning or deep learning methods. Differently from most of the existing studies, which used either CT scan or X-ray images in COVID-19-case classification, we present a new, simple but efficient deep learning feature fusion model, called U n c e r t a i n t y F u s e N e t, which is able to classify accurately large datasets of both of these types of images. We argue that the uncertainty of the model\u2019s predictions should be taken into account in the learning process, even though most of the existing studies have overlooked it. We quantify the prediction uncertainty in our feature fusion model using effective Ensemble Monte Carlo Dropout (EMCD) technique. A comprehensive simulation study has been conducted to compare the results of our new model to the existing approaches, evaluating the performance of competing models in terms of Precision, Recall, F-Measure, Accuracy and ROC curves. The obtained results prove the efficiency of our model which provided the prediction accuracy of 99.08% and 96.35% for the considered CT scan and X-ray datasets, respectively. Moreover, our U n c e r t a i n t y F u s e N e t model was generally robust to noise and performed well with previously unseen data. The source code of our implementation is freely available at: https://github.com/moloud1987/UncertaintyFuseNet-for-COVID-19-Classification.\nHighlights\n\u2022\nAn uncertainty quantification-based model for COVID-19 classification is proposed.\n\u2022\nProposed a novel multi-stage hierarchical feature fusion model.\n\u2022\nInvestigated uncertainties during COVID-19 classification using Ensemble Monte Carlo dropout.\n\u2022\nValidated the performance with two different COVID-19 image datasets.\n\u2022\nAchieved outstanding outcomes using the proposed fusion model.","With the advancement of technology, the behavior of consumers turned into E-commerce, especially when the pandemic started. Defining the success and credibility of e-commerce is important as it is becoming a trend, and one of the factors that can affect it is product reviews. Product reviews build trust and loyalty, which influence the purchase decision. These product reviews can be in textual form, star rating, and emoji. Star rating is the commonly used product review but causes rating inflation. That is why word-based rating eliminates this problem which gives more accurate reviews. In this modern day, we do not only convey our sentiments in the language we are used to but also in other formats, such as using both text and emoji. Since Convolutional Neural Networks have been drawing attention to their reduced effort in feature definition, it has been gaining popularity in text classification.\nThis leads the researchers to create a tool that aims to determine the performance measure in detecting the polarity of a text-based and text-based sentiment analysis with emoji using Convolutional Neural Networks. The researchers determined the system's performance using Confusion Matrix and derived its Precision, Recall, and F1-Score.","The original K-nearest neighbour (KNN) algorithm was meant to classify homogeneous complete data, that is, data with only numerical features whose values exist completely. Thus, it faces problems when used with heterogeneous incomplete (HI) data, which has also categorical features and is plagued with missing values. Many solutions have been proposed over the years but most have pitfalls. For example, some solve heterogeneity by converting categorical features into numerical ones, inflicting structural damage. Others solve incompleteness by imputation or elimination, causing semantic disturbance. Almost all use the same K for all query objects, leading to misclassification. In the present work, we introduce KNNHI, a KNN-based algorithm for HI data classification that avoids all these pitfalls. Leveraging rough set theory, KNNHI preserves both categorical and numerical features, leaves missing values untouched and uses a different K for each query. The end result is an accurate classifier, as demonstrated by extensive experimentation on nine datasets mostly from the University of California Irvine repository, using a 10-fold cross-validation technique. We show that KNNHI outperforms six recently published KNN-based algorithms, in terms of precision, recall, accuracy and F-Score. In addition to its function as a mighty classifier, KNNHI can also serve as a K calculator, helping KNN-based algorithms that use a single K value for all queries that find the best such value. Sure enough, we show how four such algorithms improve their performance using the K obtained by KNNHI. Finally, KNNHI exhibits impressive resilience to the degree of incompleteness, degree of heterogeneity and the metric used to measure distance.","The predict-then-optimize framework is fundamental in many practical settings: predict the unknown parameters of an optimization problem and then solve the problem using the predicted values of the parameters. A natural loss function in this environment is to consider the cost of the decisions induced by the predicted parameters in contrast to the prediction error of the parameters. This loss function is referred to as the smart predict-then-optimize (SPO) loss. In this work, we seek to provide bounds on how well the performance of a prediction model fit on training data generalizes out of sample in the context of the SPO loss. Because the SPO loss is nonconvex and non-Lipschitz, standard results for deriving generalization bounds do not apply. We first derive bounds based on the Natarajan dimension that, in the case of a polyhedral feasible region, scale at most logarithmically in the number of extreme points but, in the case of a general convex feasible region, have linear dependence on the decision dimension. By exploiting the structure of the SPO loss function and a key property of the feasible region, which we denote as the strength property, we can dramatically improve the dependence on the decision and feature dimensions. Our approach and analysis rely on placing a margin around problematic predictions that do not yield unique optimal solutions and then providing generalization bounds in the context of a modified margin SPO loss function that is Lipschitz continuous. Finally, we characterize the strength property and show that the modified SPO loss can be computed efficiently for both strongly convex bodies and polytopes with an explicit extreme point representation.\nFunding: O. El Balghiti thanks Rayens Capital for their support. A. N. Elmachtoub acknowledges the support of the National Science Foundation (NSF) [Grant CMMI-1763000]. P. Grigas acknowledges the support of NSF [Grants CCF-1755705 and CMMI-1762744]. A. Tewari acknowledges the support of the NSF [CAREER grant IIS-1452099] and a Sloan Research Fellowship.","Class imbalance has been reported as an important obstacle to apply traditional learning algorithms to real-world domains. Recent investigations have questioned whether the imbalance is the unique factor that hinders the performance of classifiers. In this paper, we study the behavior of six algorithms when classifying imbalanced, overlapped data sets under uncommon situations (e.g., when the overall imbalance ratio is different from the local imbalance ratio in the overlap region). This is accomplished by analyzing the accuracy on each individual class, thus devising how those situations affect the majority and minority classes. The experiments corroborate that overlap is more important than imbalance for the classification performance. Also, they show that the classifiers behave differently depending on the nature of each model.","Label Ranking (LR) is an emerging non-standard supervised classification problem with practical applications in different research fields. The Label Ranking task aims at building preference models that learn to order a finite set of labels based on a set of predictor features. One of the most successful approaches to tackling the LR problem consists of using decision tree ensemble models, such as bagging, random forest, and boosting. However, these approaches, coming from the classical unweighted rank correlation measures, are not sensitive to label importance. Nevertheless, in many settings, failing to predict the ranking position of a highly relevant label should be considered more serious than failing to predict a negligible one. Moreover, an efficient classifier should be able to take into account the similarity between the elements to be ranked. The main contribution of this paper is to formulate, for the first time, a more flexible label ranking ensemble model which encodes the similarity structure and a measure of the individual label importance. Precisely, the proposed method consists of three item-weighted versions of the AdaBoost boosting algorithm for label ranking. The predictive performance of our proposal is investigated both through simulations and applications to three real datasets.\nHighlights\n\u2022\nDefining an item-weighted Label Ranking algorithm.\n\u2022\nMapping from instances to rankings over a finite set of predefined labels.\n\u2022\nImproving the predictive performance by aggregating many decision trees.\n\u2022\nInterpretative method to measure the overall covariates\u2019 importance.","As the latest representative of GNSS positioning technology, the PPP-RTK method, which is able to achieve centimeter-level positioning using a single receiver, has been recognized as a preferred alternative for emerging applications such as self-drive cars and unmanned ariel vehicles. Nevertheless, the performance of PPP-RTK faces serious challenges in urban environments due to the severe impact of multipath and non-line-of-sight (NLOS) reception. Presently, machine learning-based signal classification methods are increasingly prevalent, which have great potential to serve for detecting NLOS signals by leveraging a wide range of features and parameters. In this contribution, a novel NLOS signal detection method based on the machine learning algorithm is developed, aiming to improve the kinematic positioning performance of PPP-RTK in urban areas. A multilayer perceptron (MLP)-based signal classifier is proposed where the signal strength, satellite elevation and pseudorange consistency are considered as input and then mapped to the signal type labeled by the fish-eye camera. Furthermore, a new stochastic model derived from both the classification results and the prediction confidence is also developed and employed in PPP-RTK processing. Several vehicular experiments are conducted in diverse urban areas to verify the effectiveness of the proposed method. Results indicate that the proposed method outperforms the traditional PPP-RTK with the 3D positioning accuracy improved by 36.7\u201342.3%. Besides, the horizontal positioning availability within 0.1 m and 1 m is improved from 34.9% to 76.3% and 69.5% to 92.1%, respectively. In partly blocked areas, the proposed method is capable of providing continuous centimeter-level positions in both horizontal and vertical directions. Particularly, in urban canyon, the vertical positioning accuracy is dramatically improved by 80.3% with NLOS signals effectively mitigated.","Decentralized applications (DApps) are growing rapidly with the prevalence of blockchain, but security and performance issues plague network managers and developers. Encrypted network traffic classification (ETC) plays a fundamental role in application management, security detection, and QoS improvement and requires different granularity for different scenarios. Existing work focuses on a single scenario, and objects of them are traditional centralized applications (Apps). Since DApps use similar encrypted traffic settings and the same communication interface, the traffic is more complex than Apps. Under the premise of manual-design features, sophisticated architecture, or lots of training data, existing methods have good results, otherwise suffering from low accuracy. In this paper, we propose Wrapper-based Stacking Network (WSNet). According to traffic characteristics of different scenarios, WSNet adaptively selects optimal features for different algorithms to filter out irrelevant and redundant features without human intervention, thereby improving classification efficiency. Combining with stacking technology to integrate advantages of primary learners, hence it has good performance in complex traffic scenarios. Our comprehensive experiments on two real-world datasets show that WSNet adapts to and outperforms the state-of-the-art methods.","Open-set recognition generalizes a classification task by classifying test samples as one of the known classes from training or \u201cunknown.\u201d As novel cancer drug cocktails with improved treatment are continually discovered, classifying patients by treatments can naturally be formulated in terms of an open-set recognition problem. Drawbacks, due to modeling unknown samples during training, arise from straightforward implementations of prior work in healthcare open-set learning. Accordingly, we reframe the problem methodology and apply a recent Gaussian mixture variational autoencoder model, which achieves state-of-the-art results for image datasets, to breast cancer patient data. Not only do we obtain more accurate and robust classification results (14% average F1 increase compared to recent methods), but we also reexamine open-set recognition in terms of deployability to a clinical setting.\nHighlights\n\u2022\nFrame predicting (breast) cancer treatments in terms of open-set recognition.\n\u2022\nDispense with the need to model \u201cunknown\u201d samples during training.\n\u2022\nGaussian Mixture Variational Autoencoder model yields better accuracy.\n\u2022\nAddress and increase deployability to clinical setting via threshold selection.","A brain tumor is a serious malignant condition caused by unregulated as well as aberrant cell partitioning. Recent advances in deep learning have aided the healthcare business, particularly, diagnostic imaging for the diagnosis of numerous disorders. The most frequent and widely utilized machine learning model for image recognition is probably task CNN. Similarly, in our study, we categorize brain MRI scanning images using CNN and data augmentation and image processing techniques. We compared the performance of the scratch CNN model with that of pretrained VGG-16 models using transfer learning. Even though the investigation is carried out on a small dataset, the results indicate that our model\u2019s accuracy is quite successful and has extremely low complexity rates, achieving 100 percent accuracy compared to 96 percent accuracy for VGG-16. Compared to existing pretrained methods, our model uses much less processing resources and produces substantially greater accuracy.","Servitization business trends have impacted spare parts management processes significantly. These trends result in the need for firms to invest in increased inventory levels to address demand driven by the growth in the long tail of spare parts assortments. This study proposes data-driven spare parts inventory ranking and classification approaches for continuous review, multi-item and multi-echelon (MIME) spare part replenishment systems that assign group-specific service levels and control measures to spare parts. We first show that any form of, even sub-optimal, prescriptive data as an input for classification significantly improves classification performance. We also propose that the stochastic nature of the MIME systems necessitates the utilization of nonlinear dimension-reduction methods for ranking items as opposed to commonly used linear methods. Further, we introduce a detailed classification performance measurement and group-specific service level assignment that enhance decision-making after classification. Finally, based on the MIME spare part management system of a large public transit agency in the United States and several carefully synthesized problem instances, our numerical study indicates that the new approach strongly outperforms the alternatives by a margin of 8.5%.\nHighlights\n\u2022\nIncluding both prescriptive and descriptive data enhances inventory classification.\n\u2022\nNonlinear ranking outperforms linear methods, particularly for less collinear data.\n\u2022\nThe length of training does not improve the inventory classification performance.\n\u2022\nThe proposed method is evaluated by real public-transit and simulated datasets.","Credit card frauds have constantly been a significant threat to the worldwide economy since the advent of non-cash payments, leading to high demand for fraud detection research. Almost existing studies on supervised credit card fraud classifiers cannot leverage domain characteristics such as fraud rings and sudden changes in payment traffic. They also often fail to detect unseen fraud patterns. In this paper, we introduce a set of features, targeting both traditional and modern fraud patterns, generated from Isolation Forest, Spectral Residual, and Knowledge Graph to boost the performance of tree-based models. We evaluate different tree-based models on the Kaggle public dataset and demonstrate the enhancement of our feature scores in the enterprise payment context. The proposed feature set increases the AUC-ROC and AUC-PR of XGBoost by up to 4.5% and 47.89%, respectively, on the enterprise dataset with only 0.0028% of fraud samples. Our results suggest a method for further improvement on a wide range of fraud detection problems based on domain analysis and feature extractors, especially Knowledge Graph.","Highlights\n\u2022\nTree ensembles generally increase performance at the expense of interpretability.\n\u2022\nA post-hoc interpretation method that extracts simplified rulesets is proposed.\n\u2022\nTwo interpretability measures for tree-based models are presented.\n\u2022\nRuleCOSI+ could extract accurate and interpretable rulesets from tree ensembles in the experiments.\nAbstract\nDespite the advent of novel neural network architectures, tree-based ensemble algorithms such as random forests and gradient boosting machines still prevail in many practical machine learning problems in manufacturing, financial, and medical domains. However, tree ensembles have the limitation that the internal decision mechanisms of complex models are difficult to understand. Therefore, we present a post-hoc interpretation approach for classification tree ensembles. The proposed method, RuleCOSI+, extracts simple rules from tree ensembles by greedily combining and simplifying their base trees. Compared with its previous version, RuleCOSI, this new version can be applied to both bagging (e.g., random forest, RF) and boosting ensembles (e.g., gradient boosting machines, GBM) and run much faster for ensembles with hundreds of trees. To assess the performance and applicability of the method, empirical experiments were conducted using two bagging algorithms and four gradient boosting algorithms over 33 datasets. RuleCOSI+ could generate the best classification rulesets in terms of F-measure together with RuleFit for RF and GBM models of the datasets among five ensemble simplification algorithms, but the rulesets of RuleCOSI+ had, on average, less than half the size of those of RuleFit. Moreover, RuleCOSI+ had the best antecedent uniqueness rate (\u201cuniq\u201d) among the five algorithms, and had also ranked high in the number of rules (\u201cNrules\u201d) and the rule reduction rate (\u201credu\u201d). In addition, the proposed method could reduce generalization errors in the simplified rulesets to obtain, on average, slightly better classification errors than original models of two bagging and three gradient boosting algorithms except CATBoost.\nGraphical abstract\nDisplay Omitted","Predicting human mobility holds significant practical value, with applications ranging from enhancing disaster risk planning to simulating epidemic spread. In this paper, we present the GeoFormer, a decoder-only transformer model adapted from the GPT architecture to forecast human mobility. Our proposed model is rigorously tested in the context of the HuMob Challenge 2023---a competition designed to evaluate the performance of prediction models on standardized datasets to predict human mobility. The challenge leverages two datasets encompassing urban-scale data of 25,000 and 100,000 individuals over a longitudinal period of 75 days. Geo-Former stands out as a top performer in the competition, securing a place in the top-3 ranking. Its success is underscored by performing well on both performance metrics chosen for the competition---the GEO-BLEU and the Dynamic Time Warping (DTW) measures. The performance of the GeoFormer on the HuMob Challenge 2023 underscores its potential to make substantial contributions to the field of human mobility prediction, with far-reaching implications for disaster preparedness, epidemic control, and beyond.","Mobile application (App) reviews which are provided by users through different App stores are considered as a rich information source for developers to inform about bugs, new feature requests, performance issues, etc. These feedbacks help developers improve the quality of their apps which in turn will significantly impact the user experience and the App\u2019s overall ratings. Popular Apps receive a high number of user reviews daily which makes their manual analysis a very tedious and time-consuming task. Automating the classification of user reviews will save developers time and help them better prioritize the issues that need to be handled. Since an App review is text data in which a user may report more than one issue, we propose a multi-label text classification model which uses neural language models. These models have shown high performance in various natural language processing problems. Experimental results confirm that neural language models outperform frequency-based methods in the context of App reviews classification. In fact, with RoBERTa, we could achieve a 0.87 average F1-score and a 0.16 hamming loss performances.","Decision tree models are widely used for classification tasks in data mining. However, privacy becomes a significant concern when training data contain sensitive information from different parties. This paper proposes a novel framework for secure two-party decision tree classification that enables collaborative training and evaluation without leaking sensitive data. The critical techniques employed include homomorphic encryption, function secret sharing (FSS), and a custom secure comparison protocol. Homomorphic encryption allows computations on ciphertexts, enabling parties to evaluate an encrypted decision tree model jointly. FSS splits functions into secret shares to hide sensitive intermediate values. The comparison protocol leverages FSS to securely compare attribute values to node thresholds for tree traversal, reducing overhead through efficient cryptographic techniques. Our framework divides computation between two servers holding private data. A privacy-preserving protocol lets them jointly construct a decision tree classifier without revealing their respective inputs. The servers encrypt their data and exchange function secret shares to traverse the tree and obtain the classification result. Rigorous security proofs demonstrate that the protocol protects data confidentiality in a semihonest model. Experiments on benchmark datasets confirm that the approach achieves high accuracy with reasonable computation and communication costs. The techniques minimize accuracy loss and latency compared to prior protocols. Overall, the paper delivers an efficient, modular framework for practical two-party secure decision tree evaluation that advances the capability of privacy-preserving machine learning.","The ability to detect out-of-distribution (OOD) inputs is essential for safely deploying machine learning models in an open world. Most existing research on OOD detection, and more generally uncertainty quantification, has focused on multi-class classification. However, for many information retrieval (IR) applications, the classification of documents or images is by nature not multi-class but multi-label. This paper presents a pure theoretical analysis of the under-explored problem of OOD detection in multi-label classification using deep neural networks. First, we examine main existing approaches such as MSP (proposed in ICLR-2017) and MaxLogit (proposed in ICML-2022), and summarize them as different combinations of label-wise scoring and aggregation functions. Some existing methods are shown to be equivalent. Then, we prove that JointEnergy (proposed in NeurIPS-2021) is indeed the optimal probabilistic solution when the class labels are conditionally independent with each other for any given data sample. This provides a more rigorous explanation for the effectiveness of JointEnergy than the original joint-likelihood interpretation, and also reveals its reliance upon the assumption of label independence rather than the exploitation of label relationships as previously thought. Finally, we discuss potential future research directions in this area.","Abstract: Marine ship target recognition can effectively identify the categories of sailing ships and realize effective management of ships. It is strategically important for both civil and military domains, but it is highly demanding in terms of accuracy. In this paper, a novel neural network ByCTE(Bayesian Classification Transformer-Encoder) is proposed to realize ship target recognition by using track information. First, the raw data is preprocessed to make the processed data more favorable for model learning. Secondly, four BayesianLinear Encoder(BLE) modules are used to learn the complex relationship between different spatial positions of the sequence, so as to capture the long-term dependence relationship between the input sequences, and further extract the deep features of the sequence. Finally, complete the recognition by attention layer and softmax function. We select the best performing model in the training and use open dataset Automatic Identification System (AIS) data from Europe for training and validating the validity of the proposed model. ByCTE can achieve better accuracy by comparison with other methods.","Highlights\n\u2022\nA novel method is presented to tackle classification problems with imbalanced data.\n\u2022\nIt is in the categories of under-sampling data-level and ensemble-based approaches.\n\u2022\nIt partitions majority samples into clusters by using the convex-hull concept.\n\u2022\nComputational results confirm the performance of the proposed two-phase method.\nAbstract\nClassification problems with imbalanced data are challenging because traditional classifiers tend to misclassify minority samples. This paper introduces a novel two-phase method in the categories of under-sampling data-level and ensemble-based approaches to tackle these problems. Compared to classic k-means clustering, the main property of our method is that the majority class is partitioned into clusters so that there are no minority samples in the convex-hull of the majority samples of each cluster, and at the same time, the size of each cluster is controlled. Thus, it is expected that by using our method, the general pattern of data in the feature space is kept, and the possibility of changing the data distribution is reduced. Computational results over a variety of imbalanced classification datasets confirm the superiority of our method over the existing methods from different metrics.","Imbalanced datasets pose frequent and challenging problems to many real-world applications. Classification models are often biased towards the majority class when learning from class-imbalanced data. Typical imbalanced learning (IL) approaches, e.g., SMOTE, AdaCost, and Cascade, often suffer from poor performance in complex tasks where class overlapping or a high imbalance ratio occurs. In this paper, we systematically investigate the IL problem and propose a novel framework named sparse projection infinite selection ensemble (SPISE). SPISE iteratively resamples balanced subsets and combines the classifiers trained on these subsets for imbalanced classification. The diversity of classifier ensembles and the similarity between the subsets and the whole dataset are considered in this process. Specifically, we present a graph-based approach named infinite subset selection to adaptively sample diverse and similar subsets. Additionally, a random sparse projection is combined with feature selection at the beginning of each iteration to augment the training features and enhance the diversity of the generated subsets. SPISE can be easily adapted to most existing classifiers (e.g., support vector machine and random forest) to boost their performance for IL. Quantitative experiments on 26 imbalanced benchmark datasets substantiate the effectiveness and superiority of the proposed model compared with other popular approaches.","Class incremental continual learning aims to improve the ability of modern classification models to continually recognize new classes without forgetting the previous ones. Prior art in the field has largely considered using a replay buffer. In this article, we start from an observation that the existing replay-based method would fail when the stored exemplars are not hard enough to get a good decision boundary between a previously learned class and a new class. To prevent this situation, we propose a method from the perspective of remedy after forgetting for the first time. In the proposed method, a set of exemplars is preserved as a working memory, which helps to recognize new classes. When the working memory is insufficient to distinguish between new classes, more discriminating samples would be swapped from a long-term memory, which is built up during the early training process, in an adaptive way. Our continual recognition model with adaptive memory update is capable of overcoming the problem of catastrophic forgetting with various new classes coming in sequence, especially for similar but different classes. Extensive experiments on different real-world datasets demonstrate that the proposed model is superior to existing state-of-the-art algorithms. Moreover, our model can be used as a general plugin for any replay-based continual learning algorithm to further improve their performance.","Classification of imbalanced data is a challenging task that has captured considerable interest in numerous scientific fields by virtue of the great practical value of minority accuracy. Some methods for improving generalization performance have been developed to address this classification situation. Here, we propose a cost-sensitive ensemble learning method using a support vector machine as a base learner of AdaBoost for classifying imbalanced data. Considering that the existing methods are not well studied in terms of how to precisely control the classification accuracy of the minority class, we developed a novel way to rebalance the weights of AdaBoost, and the weights influence the base learner training. This weighting strategy increases the sample weight of the misclassified minority while decreasing the sample weight of the misclassified majority until their distributions are even in each round. Furthermore, we included P-mean as one of the assessment markers and discussed why it is necessary. Experiments were conducted to compare the proposed and comparison 10 models on 18 datasets in terms of six different metrics. Through comprehensive experimental findings, the statistical study is performed to verify the efficacy and usability of the proposed model.","Rolling element bearings are essential components of a wide variety of industrial machinery and the leading cause of equipment failure. The prediction of Remaining Useful Life (RUL) and fault diagnosis is an essential component of equipment diagnosis and health management. Real-time Conditional Based Monitoring (CBM) faces a significant challenge in the Internet of Things and Industrial 4.0 eras due to automatically processing enormous quantities of data. A novel three-phase architecture based on deep learning has been proposed to perform real-time monitoring. The model is end-to-end adaptive, and the data is sequentially processed. The proposed hybrid approach incorporates change-point detection, RUL prediction, and fault classification. Adaptive data pre-processing is employed in conjunction with the unsupervised CPD module, followed by a model training on bearing data deterioration to increase efficiency. The autoencoder employed for CPD is also utilized to supply additional features from the bottleneck layer to acquire an enhanced Health Index (HI). The experiments were conducted on NASA\u2019s prognosis and diagnosis datasets, and the proposed method\u2019s effectiveness was compared to other benchmark approaches. The results show that the proposed methodology can execute real-time CBM efficiently and reliably, outperforming other methods.\nGraphical abstract\nDisplay Omitted\nHighlights\n\u2022\nReal-time condition-based monitoring of bearings has been proposed.\n\u2022\nThe methodology can determine RUL as well as bearing fault categorization.\n\u2022\nThe methodology is based on a three-phase architecture that is end-to-end adaptive.\n\u2022\nFor real-time monitoring, data is sequentially processed between each phase.\n\u2022\nImproved accuracy is achieved as the methodology integrates change-point detection.","Visual recognition methods assume models will be evaluated on the same class distribution as training data, but real-world data is often heavily class-imbalanced. To address this, the essential idea is to provide discriminative fitting abilities for classes with different sample sizes, i.e., the model achieves better generalization on less frequent classes, while maintaining high classification ability on the recurring classes. In this work, we propose to unify representation learning and classification learning with robust margin adjustment, which enforces a suitable margin in logit space and regularizes the distribution of embeddings. This procedure reduces representation bias in the feature space and reduces classification bias in the logit space at the same time. We further augment the under-represented tail classes on the feature level via re-balanced sampling from the robust prototype, calibrated with the knowledge from well-represented head classes and adaptive embedding uncertainty estimation. We conduct extensive experiments on a common long-tailed benchmark CIFAR100-LT. Experimental results demonstrate the advantage of the proposed AMDRG for the long-tailed recognition problem.","Accuracy is a key focus of current work in time series classification. However, speed and data reduction are equally important in many applications, especially when the data scale and storage requirements rapidly increase. Current multivariate time series classification (MTSC) algorithms need hundreds of compute hours to complete training and prediction. This is due to the nature of multivariate time series data which grows with the number of time series, their length and the number of channels. In many applications, not all the channels are useful for the classification task, hence we require methods that can efficiently select useful channels and thus save computational resources. We propose and evaluate two methods for channel selection. Our techniques work by representing each class by a prototype time series and performing channel selection based on the prototype distance between classes. The main hypothesis is that useful channels enable better separation between classes; hence, channels with a larger distance between class prototypes are more useful. On the UEA MTSC benchmark, we show that these techniques achieve significant data reduction and classifier speedup for similar levels of classification accuracy. Channel selection is applied as a pre-processing step before training state-of-the-art MTSC algorithms and saves about 70% of computation time and data storage with preserved accuracy. Furthermore, our methods enable efficient classifiers, such as ROCKET, to achieve better accuracy than using no selection or greedy forward channel selection. To further study the impact of our techniques, we present experiments on classifying synthetic multivariate time series datasets with more than 100 channels, as well as a real-world case study on a dataset with 50 channels. In both cases, our channel selection methods result in significant data reduction with preserved or improved accuracy.","Linear discriminant analysis (LDA) is a well-known method for multiclass classification and dimensionality reduction. However, in general, ordinary LDA does not achieve high prediction accuracy when observations in some classes are difficult to be classified. A novel cluster-based LDA method is proposed that significantly improves prediction accuracy. Hierarchical clustering is adopted, and the dissimilarity measure of two clusters is defined by the cross-validation (CV) value. Therefore, clusters are constructed such that the misclassification error rate is minimized. The proposed approach involves a heavy computational load because the CV value must be computed at each step of the hierarchical clustering algorithm. To address this issue, a regression formulation for LDA is developed and an efficient algorithm that computes an approximate CV value is constructed. The performance of the proposed method is investigated by applying it to both artificial and real datasets. The proposed method provides high prediction accuracy with fast computation from both numerical and theoretical viewpoints.","In this era of Coronavirus disease 2019 (COVID-19), an accurate method of diagnosis with less diagnosis time and cost can effectively help in controlling the disease spread with the new variants taking birth from time to time. In order to achieve this, a two-dimensional (2D) tunable Q-wavelet transform (TQWT) based on a memristive crossbar array (MCA) is introduced in this work for the decomposition of chest X-ray images of two different datasets. TQWT has resulted in promising values of peak signal-to-noise ratio (PSNR) and structural similarity index measure (SSIM) at the optimum values of its parameters namely quality factor (Q) of 4, and oversampling rate (r) of 3 and at a decomposition level (J) of 2. The MCA-based model is used to process decomposed images for further classification with efficient storage. These images have been further used for the classification of COVID-19 and non-COVID-19 images using ResNet50 and AlexNet convolutional neural network (CNN) models. The average accuracy values achieved for the processed chest X-ray images classification in the small and large datasets are 98.82% and 94.64%, respectively which are higher than the reported conventional methods based on different models of deep learning techniques. The average accuracy of detection of COVID-19 via the proposed method of image classification has also been achieved with less complexity, energy, power, and area consumption along with lower cost estimation as compared to CMOS-based technology.\nHighlights\n\u2022\nQualitative and quantitative investigations on selection of optimum TQWT parameters for image decomposition.\n\u2022\nExtended mathematical investigation for memristive crossbar array (MCA) model based two dimensional (2D) TQWT to decompose the CXIs for further image classification.\n\u2022\nComputational diagnosis of COVID-19 by using CXIs through pretrained CNN models.","With the exponential increase of interdisciplinary research, identifying accurate disciplines of scientific documents has become increasingly important in various research management tasks. Interdisciplinary classification, which classifies documents into multiple disciplines, is essential for multidisciplinary research development. Due to the scarcity of labeled multidiscipline data, existing scientific document classification methods can't solve the interdisciplinary issue. Most of them also have the problem of explainability with curtly providing classification results. This study proposes an explainable transfer-learning-based classification method for interdisciplinary documents. First, we trained a single-discipline classification model using existing labeled single-discipline documents. Then, we transfer the knowledge learned from single-discipline classification to interdisciplinary classification to address the scarcity of labeled interdisciplinary data. We also added discipline co-occurrence information into our proposed model. Finally, we obtained our final model by training the transferred model with interdisciplinary data. In addition, keyword-based explanations for classifying texts are provided by employing layer-wise relevance propagation. Experiments on real-life NSFC data show the effectiveness of the proposed method, which can promote interdisciplinary development by constructing an efficient and fair classification for interdisciplinary review systems.","We presented the Pyramid Swin Transformer, a versatile and efficient architecture tailored for object detection and image classification. This time we applied it to a wider range of tasks, such as object detection, image classification, semantic segmentation, and video recognition tasks. Our architecture adeptly captures local and global contextual information by employing more shift window operations and integrating diverse window sizes. The Pyramid Swin Transformer for Multi-task is structured in four stages, each consisting of layers with varying window sizes, facilitating a robust hierarchical representation. Different numbers of layers with distinct windows and window sizes are utilized at the same scale. Our architecture has been extensively evaluated on multiple benchmarks, including achieving 85.4% top-1 accuracy on ImageNet for image classification, 51.6\nA\nP\nbox\nwith Mask R-CNN and 54.3\nA\nP\nbox\nwith Cascade Mask R-CNN on COCO for object detection, 49.0 mIoU on ADE20K for semantic segmentation, and 83.4% top-1 accuracy on Kinetics-400 for video recognition. The Pyramid Swin Transformer for Multi-task outperforms state-of-the-art models in all tasks, demonstrating its effectiveness, adaptability, and scalability across various vision tasks. This breakthrough in multi-task learning architecture opens the door to new research and applications in the field.","Much of Earth's charismatic megafauna is endangered by human activities, particularly the rhino, which is at risk of extinction due to the poaching crisis in Africa. Monitoring rhinos' movement is crucial to their protection but has unfortunately proven difficult because rhinos are elusive. Therefore, instead of tracking rhinos, we propose the novel approach of mapping communal defecation sites, called middens, which give information about rhinos' spatial behavior valuable to antipoaching, management, and reintroduction efforts. This paper provides the first-ever mapping of rhino midden locations by building classifiers to detect them using remotely sensed thermal, RGB, and LiDAR imagery in passive and active learning settings. As existing active learning methods perform poorly due to the extreme class imbalance in our dataset, we design MultimodAL, an active learning system employing a ranking technique and multimodality to achieve competitive performance with passive learning models with 94% fewer labels. Our methods could therefore save over 76 hours in labeling time when used on a similarly-sized dataset. Unexpectedly, our midden map reveals that rhino middens are not randomly distributed throughout the landscape; rather, they are clustered. Consequently, rangers should be targeted at areas with high midden densities to strengthen anti-poaching efforts, in line with UN Target 15.7.","At the end of 2021, there were more than 200 million proteins in which their molecular functions were still unknown. As the empirical determination of these functions is slow and expensive, several research groups around the world have applied machine learning to perform the prediction of protein functions. In this work, we evaluate the use of Transformer architectures to classify protein molecular functions. Our classifier uses the embeddings resulting from two Transformer-based architectures as input to a Multi-Layer Perceptron classifier. This model got\nF\nmax\nof 0.562 in our database and, when we applied this model to the same database used by DeepGOPlus, we reached the value of 0.617, surpassing the best result available in the literature.","This paper introduces a binary classification network that utilizes the Informer Encoder to classify ping pong actions as either correct or incorrect. The dataset used in this study comprises 949 action videos capturing two fundamental ping pong stroke actions performed by athletes, including both correct and incorrect actions. The average frame count for each action is 38.62. Temporal skeletal data is extracted from the videos using a 2D pose estimation model, and a fully connected layer is employed to perform binary classification on the temporal skeletal data. During training and testing, the extracted skeletal data is segmented into temporal sequences of 39 frames for training and evaluation. On the test set, the Informer Encoder-based model achieves 100% accuracy, while the MLP-based model reaches 94%.","Ensemble methods are advanced learning algorithm proposed for generating base classifiers and accumulating them all together to derive a new classifier which is expected to perform better than the constituent classifier. This study proposes a novel ensemble technique where a base learning classifier is trained repeatedly by using different weightings over the training samples or examples, and the process is governed by the conceptualization of evolutionary processes and the aggregation operators. We utilize the evolutionary technique that can efficiently search a large weighing space for enriching suitable weights (chromosome) to the training samples. For finding an appropriate weighting, the crossover and mutation processes are applied on the weighting space to get the optimized set of weights which is accomplished through different generations. The considered base learning classifier is trained over the training examples along with their respective weightings by utilizing a learning algorithm, and for the finite number of generations, the weights are evolved and optimized through the evolutionary process. All the classifiers obtained in different generations of the evolutionary process are utilized for efficiently building the final ensemble. The set of classifiers obtained in different generations are combined together by utilizing the concept of priority-based averaging aggregation operator by availing priority to different generations. The classifier ensemble is done with two forms of operators: one without priority degree and the other with the priority degree. The proposed classifier ensemble algorithm is tested over the UCI benchmark dataset. The results obtained through the experimental process are more accurate, consistent, and reliable while comparing to other state-of-the-art methods, which ensures the efficacy of the proposed algorithm.","Dynamic Classifier Selection (DCS) techniques aim to select the most competent classifiers from an ensemble per test sample. For each test sample, only a subset of the most competent classifiers is used to estimate its target value. The performance of the DCS highly depends on how we define the local region of competence, which is a local region in the feature space around the test sample. In this paper, we propose a new definition of region of competence based on a new proximity measure. We exploit the observed similarities between traffic profiles at different links, days and hours to obtain similarities between different values. Furthermore, long-term traffic pattern prediction is a complex problem and most of the traffic prediction literature are based on time-series and regression approaches and their prediction time is limited to next few hours or days. We tackle the long-term traffic pattern prediction as a classification of discretized traffic indicators to improve the accuracy of urban traffic pattern forecasting of next weeks by using DCS. We also employ two different link clustering methods, for grouping traffic links. For each cluster, we train a dynamic classifier system for predicting the traffic variables (flow, speed and journey time). Our results on strategic road network data shows that the proposed method outperforms the existing ensemble and baseline models in long-term traffic prediction.","Data-driven bearing fault diagnosis methods have become increasingly crucial for the health management of rotating machinery equipment. However, in actual industrial scenarios, the scarcity of labeled data presents a challenge. To alleviate this problem, many transfer learning methods have been proposed. Some domain adaptation methods use models trained on source domain to generate pseudo labels for target domain data, which are further employed to refine models. Domain shift issues may cause noise in the pseudo labels, thereby compromising the stability of the model. To address this issue, we propose a Hierarchical Pseudo Label Domain Adversarial Network. In this method, we divide pseudo labels into three levels and use different training approach for diverse levels of samples. Compared with the traditional threshold filtering methods that focus on high-confidence samples, our method can effectively exploit the positive information of a great quantity of medium-confidence samples and mitigate the negative impact of mislabeling. Our proposed method achieves higher prediction accuracy compared with the-state-of-the-art domain adaptation methods in harsh environments.","Feature selection is an important data pre-processing procedure in the classification task, whose main purpose is to remove negligible/redundant features to reduce the computational cost, while improving the performance of the subsequent machine learning method. However, most feature selection methods can only work well with the complete data, but show poorly in the presence of the missing information, especially with a high missing rate, due probably to the severe irrelevance and redundancy of features. To address this issue, we proposed a novel feature selection method, namely non-negative latent factor incorporated duplicate maximal information coefficient (NLF-DMIC), which improves the effectiveness of feature selection for the classification of incomplete data. The NLF-DMIC method is fulfilled by the following three steps: (1) select category-friendly features by MIC based on \u201cpartial sample strategy\u201d roughly; (2) use the NLF model to make an imputation for the missing data in terms of the filtered features; and (3) select features again by an improved maximal information coefficient (i.e., low-redundancy MIC, LMIC) method on the complete dataset. Finally, experiments on an artificially synthetic dataset and 8 real datasets show that the proposed NLF-DMIC method outperforms some state-of-the-art feature selection methods.\nHighlights\n\u2022\nA three-step method is proposed for dimensionality reduction of incomplete data.\n\u2022\nThe initial feature selection improves the effectiveness of imputation.\n\u2022\nThe improved maximal information coefficient effectively reduces redundancy.\n\u2022\nThe combination of two filters achieves less computation and higher accuracy.","Bitcoin is a crypto asset with transactions recorded on a decentralised, publicly accessible ledger. The real-world identity of the bitcoin blockchain users is masked behind a pseudonym, known as an address that provides a high level of anonymity, which is one of the reasons for its widespread use in criminal operations such as ransomware attacks, gambling, etc. As a result, the classification of diverse cybercriminal users' activities and addresses in the bitcoin blockchain is demanded. This research work presents a classification of user activities and addresses associated with illicit transactions using supervised machine learning (ML). The labelled dataset samples are trained using decision trees, ensemble, Bayesian, and instance-based learning. Extra Trees emerged as the best classification model, whereas Gaussian na\u00efve Bayes as the worst. GridSearchCV is employed to optimise the CV accuracy of classification models with CV accuracy below 85% which led to an improvement in the CV accuracy.","Currently, most feature weights estimation methods are independent on the classification algorithms. The combination of discriminant analysis and classifiers for effective pattern classification remains heuristic. The present study address the topics of learning of feature weights by using a recently reported classification algorithm, K-Local Hyperplane Distance Nearest Neighbor (HKNN) [18], in which the data is modeled as embedded in a linear hyperplane. Motivated by the encouraging performance of the Learning Discriminative Projections and Prototypes, the feature weights are estimated by minimizing the classifier leave-one-out cross validation error of HKNN. Approximated explicit solution is obtained to give feature estimation. Therefore, the feature weighting and classification are perfectly matched. The performance of the combinational model is extensively assessed via experiments on both synthetic and benchmark datasets. The superior results demonstrate that the method is competitive compared with some state-of-art models.","Fake news is a major challenge in social media, particularly in the health domain where it can lead to severe consequences for both individuals and society as a whole. To contribute to combating this problem, we present a novel solution for improving the accuracy of detecting fake health news, utilizing a fine-tuned BERT model that integrates both user- and content-related socio-contextual information. Specifically, this information is combined with the textual content itself to form a socio-contextual input sequence for the BERT model. By fine-tuning such a model with respect to the health misinformation detection task, the resulting classifier can accurately predict the category to which each piece of content belongs, i.e., either \u201creal health news\u201d or \u201cfake health news\u201d. We validate our solution through a series of experiments conducted on distinct publicly available datasets constituted by health-related tweets. These results illustrate the superiority of the proposed solution compared to the standard BERT baseline model and other advanced models. Indeed, they show that the integration of socio-contextual information in the detection process positively contributes to increasing the overall accuracy of the fake health news detection task. The study also suggests, in a preliminary way, how such information could be used for the explainability of the model itself.","Existing aspect-based/category sentiment analysis methods have shown great success in detecting sentiment polarity toward a given aspect in a sentence with supervised learning, where the training and inference stages share the same pre-defined set of aspects. However, in practice, the aspect categories are changing rather than keeping fixed over time. Dealing with unseen aspect categories is under-explored in existing methods. In this article, we formulate a new few-shot aspect category sentiment analysis (FSACSA) task, which aims to effectively predict the sentiment polarity of previously unseen aspect categories. To this end, we propose a novel Aspect-Focused Meta-Learning (AFML) framework that constructs aspect-aware and aspect-contrastive representations from external knowledge to match the target aspect with aspects in the training set. Concretely, we first construct two auxiliary contrastive sentences for a given sentence with the incorporation of external knowledge, enabling the learning of sentence representations with a better generalization. Then, we devise an aspect-focused induction network to leverage the contextual sentiment toward a given aspect to refine the label vectors. Furthermore, we employ the episode-based meta-learning algorithm to train the whole network, so as to learn to generalize to novel aspects. Extensive experiments on multiple real-life datasets show that our proposed AFML framework achieves the state-of-the-art results for the FSACSA task.","Neural architecture search (NAS), which automates the design of neural network (NN) architectures for scientific datasets, requires significant computational resources and time \u2014 often on the order of days or weeks of GPU hours and training time. We design the Analytics for Neural Network (A4NN) workflow, a composable workflow that significantly reduces the time and resources required to design accurate and efficient NN architectures. We introduce a parametric fitness prediction strategy and distribute training across multiple accelerators to decrease the aggregated NN training time. A4NN rigorously record neural architecture histories, model states, and metadata to reproduce the search for near-optimal NNs. We demonstrate A4NN\u2019s ability to reduce training time and resource consumption on a dataset generated by an X-ray Free Electron Laser (XFEL) experiment simulation. When deploying A4NN, we decrease training time by up to 37% and epochs required by up to 38%.","Detection of diseases in plants at an early stage is crucial to achieving high yields, preserving crop quality, and effective disease management. Existing research focuses mostly on leaf disease detection, despite the fact that disease may develop everywhere on the plant. We developed a new dataset using the PlantVillage dataset and other online sources. We used Convolutional Neural Network (CNN) architectures, Alexnet and MobileNet to analyze and evaluate the performance of the models on the new dataset (i.e., consists of over 50,000 images). The models were trained on the new dataset for 100 epochs. MobileNet outperformed the other two models, attaining 99.69% training accuracy, 94.37% validation accuracy, 96% average precision, 96% recall, and an F1-score. The MobileNet model predicted diseases that affect portions of the plant other than the leaf better. This work demonstrates detecting plant disease and provides a feasible technique for enhancing crop management.","Traditional classification algorithms are not suitable for feature extraction on high resolution satellite images, given the heterogeneity of the pixels of this type of imagery due to a great amount of detail. Most of this type of imagery is taken by the satellite in several bands and at different resolutions, and the method presented in this paper takes advantage of this situation, merging information provided by the multispectral bands with the panchromatic band. An Ikonos image of 2 x 2 km of the university campus of Alcala has been used for obtaining a classification with seven land use classes. A comparison is carried out between the traditional maximum likelihood method and the method developed here. The latter using context information obtained by the texture from the band with the maximum resolution, the panchromatic band. The results show how texture information improves maximum likelihood classification of the multispectral bands for smooth-textured classes.","Smart support systems for the recognition of Activities of Daily Living (ADLs) can help elderly people live independently for longer improving their standard of living. Many machine learning approaches have been proposed lately for Human Activity Recognition (HAR), including elaborated networks that contain convolutional, recurrent, and attentive layers. The ubiquity of wearable devices has provided an increasing amount of time-series data that can be used for such applications in an unobtrusive manner. But there are not many studies on the performance of the attention-based Transformer model in HAR, especially not for complex activities such as ADLs. This work implements and evaluates the novel self-attention Transformer model for the classification of ADLs and compares it to the already well-established approach of recurrent Long-Short Term Memory (LSTM) networks. The proposed method is a two-level hierarchical model, in which atomic activities are initially recognized in the first step and their probability scores are extracted and utilized for the Transformer-based classification of seven more complex ADLs in the second step. The Transformer is used at the second step to classify seven ADLs. Our results show that the Transformer model reaches the same performance and even outperforms LSTM networks cleary in the subject-dependent configuration (73.36 % and 69.09 %), while relying only on attention-mechanism to depict global dependencies between input and output without the need to use any recurrence. The proposed model was tested using two different segment lengths, indicating its effectiveness in learning long-range dependencies of shorter actions in complex activities.","The curse of dimensionality is a common problem in classification tasks. However, feature selection is an exciting approach to deal with this type of problem by searching for a suboptimal feature set, either by eliminating irrelevant attributes, those with redundant information, or even both. Furthermore, although the use of machine learning techniques for the classification of drum cymbals is found in the literature, little attention has been paid to approaches that focus on classifying these instruments according to their bronze alloys. This paper aims to explore and evaluate the temporal information retrieved from audios, using TSFEL (Time Series Feature Extraction Library) as a tool to extract 18 temporal attributes, three feature selection approaches to assess these features, and logistic regression as a classifier. To this end, 276 audios referring to four drum cymbals of three different bronze alloys were captured in the studio through a sound acquisition procedure that took into account environment and microphone variations. Hence, one expects to find an optimal subset of features that contains enough information from the audios to achieve the best classification performance in the proposed problem. The experimental results show that a feature selection approach sequentially based on L 1 Regularization and Correlation Analysis was able to find a subset consisting of 5 attributes that achieved an average accuracy of 97.08%.\nHighlights\n\u2022\nThe computational model can identify the cymbals\u2019 bronze alloys through their sound.\n\u2022\nA standardized impact procedure is developed for the audio signals acquisition.\n\u2022\nFeature selection could reduce the dimensionality up to 70%, improving accuracy.\n\u2022\nLogistic Regression with feature selection achieved high material classification rate.\n\u2022\nHigh accuracy is attained even considering different sound capture perspectives.","Accuracy of a classifier is important for the success of any prediction model. The more accuracy a classifier possesses, the more robust the system is made on it. In this paper, a disease prediction model is developed in Python for the classification of diabetes in patients. In the research paper, study is performed to make a comparative analysis of the performance of machine learning classification algorithms. The classifier's performances are enhanced by of tuning the hyperparameters of classifiers and applied different dataset preprocessing methods. In this experimental analysis, four models have been created, and each model is based on a dataset, obtained by different preprocessing methods of PIMA dataset. For each model, K-Nearest Neighbors, Decision Tree, Random Forest, and Support vector machines classification algorithms, have been applied and classifier's hyperparameters are tuned to get better results from these models.\nA detail analysis has also performed to get the best prediction model, the best classifier and effective preprocessing methods for it. The prediction model use F1score as the main metric. The highest F1score and accuracy are 75.68 % and 88.61% respectively, which is achieved by Random Forest classifier for dataset model D3 obtained by removing the samples having missing or unknown values from PIMA dataset.","Extracellular action potentials (EAP) are one of the most important features in biological study. Many researchers have studied the classification of EAP by their differences in voltage and magnitude. However, most research ignored the fundamental origin of the EAP variation around the neurons in their classification and treated waveforms of different shapes as signals recorded from different neurons. In our research, we theoretically investigated the shapes of EAP by clustering the spatially-varied EAP around the neuron. We use an unsupervised machine-learning algorithm to classify all EAPs measured around the same neuron. To eliminate the influence of the non-characteristic part of the EAP curve, we also compared the classification results by eliminating the unchanged part at the front and end of the curve in the second group of our study. Our results illustrate the previously overlooked relationship between different shaped EAP and the biological structure of the neuron. The results show that EAP measured is closer to classical theory prediction in the axon while more eccentric, even with a shape similar to an intracellular action potential in the dendrite. Our research has important implications for further device design to record accurate electric signals and extracting biological related information from extracellular recordings.","Smart meters are key elements of a smart grid. These data from Smart Meters can help us analyze energy consumption behaviour. The machine learning and deep learning approaches can be used for mining the hidden theft detection information in the smart meter data. However, it needs effective data extraction. This research presents a theft detection dataset (TDD2022) and a machine learning-based solution for automated theft identification in a smart grid environment. An effective theft generator is modelled and used for obtaining a multi-class theft detection dataset from publicly available consumer energy consumption data, owned by the \u201cOpen Energy Data Initiative\u201d (OEDI) platform. This is an important and interesting phase to explore in the smart grid field. The proposed dataset can be used for benchmarking and comparative studies. We evaluated the proposed dataset using five different machine learning techniques: k-nearest neighbours (KNN), decision trees (DT), random forest (RF), bagging ensemble (BE), and artificial neural networks (ANN) with different evaluation alternatives (mechanisms). Overall, our best empirical results have been recorded to the theft detection-based RF model scoring an improvement in the performance metrics by 10% or more over the other developed models.","As the social impact of visual recognition has been under scrutiny, several protected-attribute balanced datasets emerged to address dataset bias in imbalanced datasets. However, in facial attribute classification, dataset bias stems from both protected attribute level and facial attribute level, which makes it challenging to construct a multi-attribute-level balanced real dataset. To bridge the gap, we propose an effective pipeline to generate high-quality and sufficient facial images with desired facial attributes and supplement the original dataset to be a balanced dataset at both levels, which theoretically satisfies several fairness criteria. The effectiveness of our method is verified on sex classification and facial attribute classification by yielding comparable task performance as the original dataset and further improving fairness in a comprehensive fairness evaluation with a wide range of metrics. Furthermore, our method outperforms both resampling and balanced dataset construction to address dataset bias, and debiasing models to address task bias.","In this article we present the Amharic Speech Emotion Dataset (ASED), which covers four dialects (Gojjam, Wollo, Shewa, and Gonder) and five different emotions (neutral, fearful, happy, sad, and angry). We believe it is the first Speech Emotion Recognition (SER) dataset for the Amharic language. Sixty-five volunteer participants, all native speakers of Amharic, recorded 2,474 sound samples, 2 to 4 seconds in length. Eight judges (two for each dialect) assigned emotions to the samples with high agreement level (Fleiss kappa = 0.8). The resulting dataset is freely available for download. Next, we developed a four-layer variant of the well-known VGG model, which we call VGGb. Three experiments were then carried out using VGGb for SER, using ASED. First, we investigated which features work best for Amharic, FilterBank, Mel Spectrogram, or Mel-frequency Cepstral Coefficient (MFCC). This was done by training three VGGb SER models on ASED, using FilterBank, Mel Spectrogram, and MFCC features, respectively. Four forms of training were tried, standard cross-validation and three variants based on sentences, dialects, and speaker groups. Thus, a sentence used for training would not be used for testing, and the same for a dialect and speaker group. MFCC features were superior under all four training schemes. MFCC was therefore adopted for Experiment 2, where VGGb and three well-known existing models were compared on ASED: RESNet50, AlexNet, and LSTM. VGGb was found to have very good accuracy (90.73%) as well as the fastest training time. In Experiment 3, the performance of VGGb was compared when trained on two existing SER datasets\u2014RAVDESS (English) and EMO-DB (German)\u2014as well as on ASED (Amharic). Results are comparable across these languages, with ASED being the highest. This suggests that VGGb can be successfully applied to other languages. We hope that ASED will encourage researchers to explore the Amharic language and to experiment with other models for Amharic SER.","Assessment of new research topics and emerging technologies in any branch of knowledge is important for researchers, universities and research institutes, research investors, industry sectors, and scientific policymakers for a variety of reasons. The basic premise of this research is that the topics of interest for academic research are those that are yet underdeveloped, but are relatively well sponsored by investors. This paper proposes a method to identify and evaluate topics for their research, industrial and commercial potential based on development, investment and investment-to-development ratio (investment appeal). Since the target audience of this paper is researchers in all fields of knowledge who are mostly unfamiliar with scientometric schemes, the proposed method is aimed to be simple, based on meta-databases with easy access, without any need to clustering on keywords. The development index is defined as the keyword link strength obtained from the keyword co-occurrence network, and investment is introduced as the number of sponsors associated with each keyword. From the qualitative analysis of the development-investment diagram, six sets of keywords, entitled as: for Research, for Industry, for Commerce, Matured, Academic and Chaotic, are identified. Due to uncertain membership of research topics to these sets and their relative overlapping, they are defined as fuzzy sets. A fuzzy model, called as Fuzzy Research Ranking System (FRRS), is designed to characterize the fuzzy behavior of research topics and their potential assessment, the output of which is the membership of keywords to any of the six predefined fuzzy sets. The proposed method has been implemented for a sample knowledge domain, Geo-Engineering, which is an interdisciplinary field with significant technological capacity. Expert review of the results shows that the method is relatively well qualified for its ability to identify research topics with technological and industrial perspectives from purely scientific keywords, and may efficiently introduce a ranked list of research topics to the researchers.","The coordination of the body with the central nervous system has been studied using various biomechanical, neurophysiological, and neuroimaging studies. Different postural strategies provide evidence of cortical involvement to maintain postural stability, which can be utilised to minimise the risk of falls in the elderly and various pathological individuals. In this paper, we investigated the effect of vibrotactile feedback in Electroencephalography (EEG) based classification of voluntary postural sway during weight-shifting exercises in healthy and transfemoral amputees. The EEG data recorded during forward, backward, right, and left shifting as well as normal standing, with and without vibrotactile feedback, is decomposed using discrete wavelet transform. The energy of the coefficients from levels 4 to 7 forms the feature space to be forwarded to the weighted kNN classifier and ensemble bagged trees. We have achieved significantly higher classification rates across all the conditions for healthy and amputee subjects. Predictor importance from ensemble bagged tree models provides the highest contributions from the low-frequency band of 0\u20133.9 Hz and channels located over the motor and somatosensory cortex. We have also observed the contributions associated with the spinocerebellum and cerebrocerebellum.","Highlights\n\u2022\nTransferring the training experience among constituent networks facilitates semi-supervised image classification.\n\u2022\nWe design a collaborative learning mechanism based on unreliability adaptation among constituent networks.\n\u2022\nWe improve the complementarity of constituent networks by resisting adversarial perturbation.\n\u2022\nThe unreliability adaptation and perturbation-based regularization lead to the superior performance on multiple datasets.\nAbstract\nConstructing training goals for unlabeled data is crucial for image classification in the semi-supervised setting. Consistency regularization typically encourages a model to produce consistent predictions with the given training goals, while unreliability adaptation aims to learn the transition probabilities from model predictions to training goals, instead of enforcing their consistency. In this paper, we present a model of Collaborative learning with Unreliability Adaptation (CoUA), in which multiple constituent networks collaboratively learn with each other by adapting their predictions. Toward this end, an additional adaptation module is incorporated into each network to learn a transition probability from its own prediction to that of the paired network. Therefore, the networks can exchange training experience, without being overly sensitive to the unreliability of predictions. To further enhance the collaborative learning, each network is encouraged to produce consistent predictions with the consensus results, while being resistant to the adversarial perturbations against others. Therefore, the networks are able to mutually reinforce each other. We perform extensive experiments on multiple image classification benchmarks to verify the superiority of the co-adaptation based collaborative learning mechanism.","This paper presents an approach to WBC classification by employing Gray Level Co-occurrence Matrix (GLCM) along with the Analysis of Variance (ANOVA) test and Zero Phase Component Analysis (ZCA) whitening. Moreover, the performance is evaluated through the K- Nearest Neighbor (K-NN) classifier. The proposed approach has achieved an accuracy of 94.25% on the Blood Cell Count and Detection (BCCD) dataset for the classification of four categories of WBC namely, lymphocytes, monocytes, neutrophils, and eosinophils. The experimental results reveal an improvement in accuracy (11.05, 8.15, and 14.25%) in comparison to the state-of-the-art approaches i.e., Watershed segmentation, Local Binary Pattern (LBP), and Extreme Learning Machines (ELMs) respectively.","Attention, one of the most important features of modern CNNs, has been shown to improve the performance of mammogram classification, but our understanding of why attention offers improvements is rather limited. In this paper, we present the first comprehensive comparison of different combinations of baseline models and attention methods at multiple resolutions for whole mammogram image classification of masses and calcifications. Our findings indicate that attention generally helps to improve the baseline model scores, but the benefits are variable depending on the resolution and abnormality type. Furthermore, we find that pooling and overall model architecture (i.e., combination of baseline and attention) significantly impact mammogram classification scores. Specifically, scores are generally improved by architectural features that allow the model to retain as much information as possible while still focusing on relevant features. We also find that attention improves the correlation between model performance and LayerCAM activation in the region of interest. Our work provides insightful information to help guide the future construction of attention-based models for mammogram classification.","We propose an approach dedicated to recognize characters from binary images by an hybrid strategy. A statistical method is used to identify the global shape of each alphanumeric symbol. The recognition is managed by a Hierarchical Neural Network (HNN), that is able to deal with topological errors in the contour extraction. This strategy is extremely efficient for the majority of the classes: the recognition rate reaches about 99.5%. However, the performances sensitively decrease for \u2019similar characters\u2019, i.e. \u20198\u2019/\u2019B\u2019. In that case, we adopt a strategy that revolves around decomposing the characters into structural elements. The Reeb graph generated from the binary images and a simple polygonal approximation permit to capture both topological and geometrical relevant features. The classification stage is carried out by a boosting algorithm.","The inspection of products and assessment of quality is connected with high costs and time effort in many industrial domains. This also applies to the forestry industry. Utilizing state-of-the-art deep learning models allows automizing the analysis of wooden piles in a vision-based manner. In this work, a parallel two-step approach is presented for the segmentation and multi-facet classification of individual logs, according to the wood type and quality. The present approach is based on a preliminary log localization step and like this allows determining the quality, volume and also the value of individual logs, respectively the whole wooden pile. Using a YOLOv4 model for wood species classification for douglas firs, pines and larches results in an accuracy of 74.53%, while a quality classification model for spruce logs reaches 86.58%. In addition to that, the trained U-NET segmentation model reaches an accuracy of 93%. In the future, the underlying data set and models will be further improved and integrated to a mobile application for the on site analysis of wooden piles by foresters.","Automated credit risk assessment plays an important role in agricultural lending. However, credit risk assessment in the agricultural domain has unique challenges due to the impact of weather, pest outbreaks, commodities market dynamics, and other volatile forces that drive risk. Training a model to account for these factors requires immense data assets that are challenging to obtain. Indeed, even the best credit risk assessment models in this domain are trained using data from single-institutions that often focus on dedicated geographical regions, or singular commodities. Hence, most agricultural credit risk models exhibit poor out-of-domain performance. In this paper, we use a novel dataset describing nearly 100 thousand historical loans, sourced from 9 large agricultural lenders to train a Bayesian network model for loan delinquency classification. The proposed model exhibited improved calibration (relative improvement in Expected Calibration Error) in out-of-domain performance tests when compared to three state-of-the-art credit risk scoring approaches: Logistic regression (81 \u00b1 15% improvement), XGBoost (80 \u00b1 14% improvement), and an Artificial Neural Networks (7 \u00b1 2% improvement). We conclude that Bayesian networks provide better modeling of agricultural credit risk by combining (limited) data assets with expert domain knowledge. Our approach is likely to generalize to any credit risk assessment task where small sample sizes is of concern.","Highlights\n\u2022\nWe proposed a new feature extraction method.\n\u2022\nFeatures are extracted based on optimal Poincare sections that fitted on phase space.\n\u2022\nOptimal phase space reconstructed based on filtered data using CSP.\n\u2022\nBCI Competition III and BCI Competition IV datasets is utilized to evaluation.\n\u2022\nPresented model reached the accuracy of 89.76% and 71.87% for datasets, respectively.\nAbstract\nSkip Background Section\nBackground\nBrain-Computer Interface (BCI) based on Motor Imagery (MI) is one of the emerging technology that has been used in smart healthcare applications that help disables connect with the real world by imagining a specific movement in the brain.\nSkip Method Section\nMethod\nIn this study, some novel features are presented for the classification of electroencephalography (EEG) signals which are named CSP-Ph-PS. This feature is extracted based on the phase space reconstructed by the filtered signal using CSP. In the proposed method, we fit Poincar\u00e9 sections in phase space to analyze data trajectory. The hyperparameters of Phase Space Reconstruction (PSR) and Poincar\u00e9 sections are learned with Evolutionary Algorithm (EA). Finally, statistical features extracted from the Poincar\u00e9 intersection points are given for classification.\nSkip Result Section\nResult\nThe proposed method is implemented on two public datasets that are BCI Competition III Dataset Iva and BCI Competition IV Dataset 1 and have been achieved an accuracy of 89.76% and 71.87% on these two datasets, respectively. These are 3.16% and 1.25% better than the best previous results, respectively.\nSkip Conclusion Section\nConclusion\nThe experimental results show that the proposed method can separate different classes of signals and has increased the classification accuracy compared to other feature extraction and deep learning methods.","During the several years of production of an animated movie, review meetings take place daily, where supervisors and directors generate text notes about fixes needed for the movie. These notes are manually assigned to artistic departments for them to fixed. Being manual, many notes are not properly assigned and are never fixed, lowering the quality of the final movie. This paper presents a proposal for automating the distribution of these notes using multi-label text classification techniques. The comparison of the results obtained by fine-tuning several transformer-based language models is presented. A highest mean accuracy of 0.776 is achieved assigning several departments to each of the review notes in the test set with a BERT Multilingual model. A mean accuracy of 0.762 was reached in just 10 epochs and 10 min of training on an RTX-3090 with a DistilBERT transformer model.","Glaciers play a critical role in the Earth\u2019s climate system, and accurate estimates of their behaviours are essential for understanding the impacts of climate change and informing policy decisions. One of the most important parameters for such a task is ice distribution, which is difficult to measure and predict using traditional physics-based models. In this study, we propose a deep learning approach to predict glacier thickness by learning directly from ice velocity and topography. Our approach overcomes the limitations of traditional physics-based models, such as computational cost and the need for expert knowledge to calibrate the models. In addition, deep learning models are flexible enough to explore the relevance of multimodality and multitasking to address the physical problem. Our results demonstrate the feasibility of quickly training a neural network model with sufficient training data and producing stable, high-quality ice thickness estimates. We highlight the importance of some specific input features suggested by geophysicists that have a positive impact on model stability.","Despite significant amount of research on automatic classification of facial expressions, recognizing a facial expression remains a complex task to be achieved by a computer vision system. Our approach is based on a close look at the mechanisms of the human visual system, the best automatic facial expression recognition system yet. The proposed model is made for the classification of the six basic facial expressions plus Neutral on static frames based on the permanent facial features deformations using the Transferable Belief Model. The aim of the proposed work is to understand how the model behaves in the same experimental conditions as the human observer, to compare their results and to identify the missing informations so as to enhance the model performances. To do this we have given our TBM based model the ability to deal with partially occluded stimuli and have compared the behavior of this model with that of humans in a recent experiment, in which human participants had to classify the studied expressions that were randomly sampled using Gaussian apertures. Simulations show first the suitability of the TBM to deal with partially occluded facial parts and its ability to optimize the available information to take the best possible decision. Second they show the similarities of the human and model observers performances. Finally, we reveal important differences between the use of facial information in the human and model observers, which open promising perspectives for future developments of automatic systems.","This paper presents an automatic dialect identification in Ao using modulation-based approach. Ao is a low-resource, Tibeto-Burman tonal language spoken in Nagaland, a North-East state of India. This work aims to investigate dialect-specific characteristics to build a more robust DID system for classifying the three Ao dialects. In this direction, modulation-based representation is explored. Considering Ao is a tone language, the experiments were evaluated for 3 sec segment duration in order to capture the temporal information of the modulation spectrogram. In addition, the log Mel spectrogram is used as the feature for the baseline DID system. The proposed modulation spectrogram shows a significant performance of\n\u2248\n8\n%\nimprovement in accuracy over the baseline Ao DID system. Hence, the result indicates the effectiveness of modulation-based representation in automatically identifying the three dialects of Ao.","Highlights\n\u2022\nRecent advances in automatic feature detection of fruits is reviewed.\n\u2022\nDetection of fruits is explored via image-based approach, and their analysis.\n\u2022\nThe classification of fruits is divided into supervised and unsupervised tasks.\n\u2022\nWatermelon studies focus on spectral features for detection or classification.\n\u2022\nThis review discusses issues and future research avenues for fruit detection systems.\nAbstract\nThis document provides an overview of advances in the task of automatic feature detection and classification of fruits, with and special interest in watermelon (Citrillus lanatus). The review was written with the objective to highlight the wealth of knowledge that exist in the application of analytical, smart and sensing image recognition techniques commonly used in agro-industry, and the computational approaches used to make the classification possible. Also, an specific interest is put in the contributions made in the development of automatic recognition systems geared towards for watermelon using images, acoustic and spectroscopy methodologies. The importance of this document is that it provides a conceptual summary of the methods for the automatic recognition of fruits, including machine learning and evolutionary computational algorithms to analyze their sensed data. In conclusion this is a first step into recognizing the challenges and opportunities that can be addressed in this field to augment the visibility of these methods and to further modernize the agro-industrial sector.","Online English teaching resources have recently surged, highlighting the exigency for efficient organization and categorization. This manuscript introduces an innovative strategy to classify university-level English teaching resources, employing a sophisticated density clustering algorithm. Initially, student discourse was mined within a teaching platform comment section, and in-depth textual analysis was conducted. Subsequently, the term frequency-inverse document frequency (TF\u2013IDF) feature extraction algorithm was enhanced, while emotive attributes were seamlessly integrated into the textual manifestation layer during the classification procedure. This enabled the distribution of topics and emotions to be acquired for each comment, facilitating subsequent analyses of emotion feature extraction and model training. An improved weight calculation was designed based on TF\u2013IDF to evaluate the importance of feature items for each corpus file. The simulation results demonstrate the proposed scheme's effectiveness. The algorithm facilitates faster scholarly access to educational resource information and effectively classifies data for high research adaptability.","Gaussian conditional random fields (GCRF) are a well-known structured model for continuous outputs that uses multiple unstructured predictors to form its features and at the same time exploits dependence structure among outputs, which is provided by a similarity measure. In this paper, a Gaussian conditional random field model for structured binary classification (GCRFBC) is proposed. The model is applicable to classification problems with undirected graphs, intractable for standard classification CRFs. The model representation of GCRFBC is extended by latent variables which yield some appealing properties. Thanks to the GCRF latent structure, the model becomes tractable, efficient and open to improvements previously applied to GCRF regression models. In addition, the model allows for reduction of noise, that might appear if structures were defined directly between discrete outputs. Two different forms of the algorithm are presented: GCRFBCb (GCRGBC \u2014 Bayesian) and GCRFBCnb (GCRFBC \u2014 non-Bayesian). The extended method of local variational approximation of sigmoid function is used for solving empirical Bayes in Bayesian GCRFBCb variant, whereas MAP value of latent variables is the basis for learning and inference in the GCRFBCnb variant. The inference in GCRFBCb is solved by Newton\u2013Cotes formulas for one-dimensional integration. Both models are evaluated on synthetic data and real-world data. We show that both models achieve better prediction performance than unstructured predictors. Furthermore, computational and memory complexity is evaluated. Advantages and disadvantages of the proposed GCRFBCb and GCRFBCnb are discussed in detail.\nHighlights\n\u2022\nGaussian conditional random field model for structured classification is proposed.\n\u2022\nTwo different forms of the algorithm are presented Bayesian and non-Bayesian.\n\u2022\nThe extension of local variational approximation of sigmoid function is presented.\n\u2022\nBoth variants are evaluated on synthetic data and real-world data.","Data-enabled cities are recently accelerated and enhanced with automated learning for improved Smart Cities applications. In the context of an Internet of Things (IoT) ecosystem, the data communication is frequently costly, inefficient, not scalable and lacks security. Federated Learning (FL) plays a pivotal role in providing privacy-preserving and communication efficient Machine Learning (ML) frameworks. In this paper we evaluate the feasibility of FL in the context of a Smart Cities Street Light Monitoring application. FL is evaluated against benchmarks of centralised and (fully) personalised machine learning techniques for the classification task of the lampposts operation. Incorporating FL in such a scenario shows minimal performance reduction in terms of the classification task, but huge improvements in the communication cost and the privacy preserving. These outcomes strengthen FL's viability and potential for IoT applications.","This research conducts an investigation on the effect of visually similar images within a publicly available diabetic foot ulcer dataset when training deep learning classification networks. The presence of binary-identical duplicate images in datasets used to train deep learning algorithms is a well known issue that can introduce unwanted bias which can degrade network performance. However, the effect of visually similar non-identical images is an under-researched topic, and has so far not been investigated in any diabetic foot ulcer studies. We use an open-source fuzzy algorithm to identify groups of increasingly similar images in the Diabetic Foot Ulcers Challenge 2021 (DFUC2021) training dataset. Based on each similarity threshold, we create new training sets that we use to train a range of deep learning multi-class classifiers. We then evaluate the performance of the best performing model on the DFUC2021 test set. Our findings show that the model trained on the training set with the 80% similarity threshold images removed achieved the best performance using the InceptionResNetV2 network. This model showed improvements in F1-score, precision, and recall of 0.023, 0.029, and 0.013, respectively. These results indicate that highly similar images can contribute towards the presence of performance degrading bias within the Diabetic Foot Ulcers Challenge 2021 dataset, and that the removal of images that are 80% similar from the training set can help to boost classification performance.","Unmanned Aerial Systems (UAS), commonly known as drones, have revolutionized various industries with their diverse applications. As the demand for seamless and intuitive drone control grows, researchers are exploring innovative approaches to improve human-swarm interaction. This paper presents a novel method for operating a swarm of drones in real time using wearable technology and machine learning. Through the integration of motion capture data and classification algorithms, we strive to achieve an intuitive level of control that is accessible to users with varying skill levels. While the full realization of this approach remains a work in progress, our research lays the groundwork for future endeavors in this domain. In this paper, we discuss the limitations of existing control methods and present our methodology for data preprocessing, model training and testing, and result analysis. Our findings indicate the potential of this approach and open avenues for refining the interaction between humans and drone swarms.","Automated pedagogical error repair (APER) is the task of suggesting fixes to buggy programs written by beginner or novice programmers. APER tools have been shown to greatly improve the learning experience for students for whom error messages offered by compilers or runtime environments are either unhelpful and often misleading. Consequently, several APER tools have been proposed in literature using a variety of powerful machine learning techniques including sequence-to-sequence modelling (TRACER), reinforcement learning (RLAssist), graph attention (DrRepair) and decision trees (MACER). Despite offering high repair rates, these tools are often bulky, requiring several days of training and extensive GPU resources. This paper describes CAPER, a novel APER tool for the C programming language that offers 4-5% higher repair accuracy than existing APER tools on multiple benchmark error repair datasets. CAPER has the added advantage of being significantly lighter and faster than most existing tools, being able to train on a CPU and yet offering training speedups of 2 \u00d7 over TRACER, 600 \u00d7 over RLAssist and 700 \u00d7 over DrRepair. The paper also describes PyPER, an extension of CAPER to the Python programming language. Code for CAPER is available at https://github.com/purushottamkar/caper/","This paper presents an intuitive, robust and efficient One-Class Classification algorithm. The method developed is called OCENCH (One-class Classification via Expanded Non-Convex Hulls) and bases its operation on the construction of subdivisible and expandable non-convex hulls to represent the target class. The method begins by reducing the dimensionality of the data to two-dimensional spaces using random projections. After that, an iterative process based on Delaunay triangulations is applied to these spaces to obtain simple polygons that characterizes the non-convex shape of the normal class data. In addition, the method subdivides the non-convex hulls to represent separate regions in space if necessary. The method has been evaluated and compared to several main algorithms of the field using real data sets. In contrast to other methods, OCENCH can deal with non-convex and disjointed shapes. Finally, its execution can be carried out in a parallel way, which is interesting to reduce the execution time.\nHighlights\n\u2022\nA method is proposed for anomaly detection through expanded non-convex hulls.\n\u2022\nThe training algorithm is based on One-Class Classification.\n\u2022\nThe method can be applied over high dimensional data sets using random projections.\n\u2022\nThe boundaries of the projected data are computed using a Delaunay triangulation.\n\u2022\nNon-convex hulls can be subdivided to represent disjointed regions if necessary.","In this paper, we propose a novel maximal margin hyper-sphere support vector machine (MMHS-SVM) for binary pattern classification. Our proposed MMHS-SVM aims to find two hyper-spheres simultaneously by solving a single quadratic programming problem and is consistent between its predicting and training processes. An essential difference that distinguishes it from other hyper-sphere SVMs is that the optimization model is constructed by maximizing the sum of the square distance between centers of two hyper-spheres, but not the sum of squares distances from the center of hyper-sphere to all examples of the opposite class. Such a principle of structural risk in our MMHS-SVM not only helps us grasp the critical samples and eliminate a large number of redundant samples, but also reduces the test cost due to the sparsity. In addition, an effective SMO-typed algorithm is designed to decrease the high time complexity and storage. Finally, a large number of experiments verify the above statements again. The experimental results on several artificial and publicly available benchmark datasets show the feasibility and effectiveness of the proposed method.","Highlights\n\u2022\nA unified theoretical framework for long-tailed recognition is established.\n\u2022\nCorresponding mitigation solutions for prior gap and representation gap are proposed.\n\u2022\nTheoretically analyzing the existing methods and the proposed methods in terms of the impact on two gaps.\n\u2022\nThe proposed methods yield superior performance on five long-tailed benchmarks.\nAbstract\nMost deep learning models are elaborately designed for balanced datasets, and thus they inevitably suffer performance degradation in practical long-tailed recognition tasks, especially to the minority classes. There are two crucial issues in learning from imbalanced datasets: skew decision boundary and unrepresentative feature space. In this work, we establish a theoretical framework to analyze the sources of these two issues from Bayesian perspective, and find that they are closely related to the prior gap and the representation gap, respectively. Under this framework, we show that existing long-tailed recognition methods manage to remove either the prior gap or the presentation gap. Different from these methods, we propose to simultaneously remove the two gaps to achieve more accurate long-tailed recognition. Specifically, we propose the prior calibration strategy to remove the prior gap and introduce three strategies (representative feature extraction, optimization strategy adjustment and effective sample modeling) to mitigate the representation gap. Extensive experiments on five benchmark datasets validate the superiority of our method against the state-of-the-art competitors.","Objective: The objective is to develop a predictive model utilizing Support Vector Machines (SVM) for the purpose of classifying the clinical stage of breast cancer.\nMaterials and Methods: Accurate determination of the clinical stage of breast cancer patients holds significant importance in selecting suitable treatment options and minimizing avoidable complications. In this study, we present the application of radiomics and SVM for breast cancer computed tomography (CT) to anticipate the preoperative clinical stage in breast cancer patients. The training dataset encompasses 166 cases obtained from the Affiliated Hospital of Xiangnan University, while the test dataset comprises 91 cases from Chenzhou Third People's Hospital. The integration of clinical parameters with radiomics exhibits the most superior diagnostic efficacy in forecasting the clinical stage of breast cancer. As part of the evaluation, various metrics were calculated, including the area under the curve (AUC), the accuracy (ACC), sensitivity (Sen), specificity (Spe), positive predictive value (PPV) and negative predictive value (NPV). To differentiate between the radiomics model, clinical data model, and fusion model, the Delong test was utilized. The precision of the prediction model was evaluated by generating a calibration curve using 1,000 bootstrap weight samples. Furthermore, the decision curve analysis (DCA) was conducted to assess the model's practicality.\nResults: The fusion model exhibits superior predictive performance compared to both the single radiomics model and clinical model. The fusion model's test sets of AUC, ACC, Sen, Spe, PPV, and NPV are 0.824, 0.780, 0.932, 0.652, 0.707, and 0.909, respectively.\nConclusion: The fusion model exhibits greater efficacy than both the single radiomics model and clinical model, and thus holds significant potential for facilitating the diagnosis of breast cancer stage and the development of individualized treatment plans.CCS","Surface electromyography (sEMG) signal is essential for accurately controlling prosthetic devices with numerous degrees of freedom in human-machine interfaces for robotics and assistive technologies. The controlling method of the upper-limb prosthesis device depends on electromyogram (EMG) pattern recognition, which requires the efficient blending of conventional signal processing and machine learning. This paper focuses on stacked ensemble models, one of the popular methods for reducing generalization error. The proposed work uses a dataset of sEMG signals from different upper-limb positions in subjects. The raw signals are transformed into correlated time-domain descriptors (cTDD) for feature extraction, which are then used to train the stacked ensemble framework. The framework includes four base classifiers (support vector machine (SVM), K-nearest neighbours (KNN), logistic regression (LR), and decision tree (DT)) and two meta-classifiers (random forest (RF) and multi-layer perceptrons (MLP). The performance of the meta-classifiers is evaluated on two test sets, showing superior classification accuracy compared to the basic classifiers. The proposed approach demonstrates the capability to accurately classify limb position invariant EMG signal classification for prosthetic device control.","Reducing traffic accidents is a crucial global public safety concern. Accident prediction is key to improving traffic safety, enabling proactive measures to be taken before a crash occurs, and informing safety policies, regulations, and targeted interventions. Despite numerous studies on accident prediction over the past decades, many have limitations in terms of generalizability, reproducibility, or feasibility for practical use due to input data or problem formulation. To address existing shortcomings, we propose Crash-Former, a multi-modal architecture that utilizes comprehensive (but relatively easy to obtain) inputs such as the history of accidents, weather information, map images, and demographic information. The model predicts the future risk of accidents on a reasonably acceptable cadence (i.e., every six hours) for a geographical location of 5.161 square kilometers. CrashFormer is composed of five components: a sequential encoder to utilize historical accidents and weather data, an image encoder to use map imagery data, a raw data encoder to utilize demographic information, a feature fusion module for aggregating the encoded features, and a classifier that accepts the aggregated data and makes predictions accordingly. Results from extensive real-world experiments in 10 major US cities show that CrashFormer outperforms state-of-the-art sequential and non-sequential models by 1.8% in F1-score on average when using \"sparse\" input data.","We introduce a method for computing classifier-based semantic spaces on top of text2ddc . To this end, we optimize text2ddc, a neural network-based classifier for the Dewey Decimal Classification (DDC). By using a wide range of linguistic features, including sense embeddings, we achieve an F-score of 87,4%. To show that our approach is language independent, we evaluate text2ddc by classifying texts in six different languages. Based thereon, we develop a topic model that generates probability distributions over topics for linguistic input at the word (sense), sentence and text level. In contrast to related approaches, these probabilities are estimated with text2ddc, so that each dimension of the resulting embeddings corresponds to a separate DDC class. We finally evaluate this Classifier-based Semantic space (CaSe) in the context of text classification and show that it improves the classification results.","This full-day tutorial introduces modern techniques for practical uncertainty quantification specifically in the context of multi-class and multi-label text classification. First, we explain the usefulness of estimating aleatoric uncertainty and epistemic uncertainty for text classification models. Then, we describe several state-of-the-art approaches to uncertainty quantification and analyze their scalability to big text data: Virtual Ensemble in GBDT, Bayesian Deep Learning (including Deep Ensemble, Monte-Carlo Dropout, Bayes by Backprop, and their generalization Epistemic Neural Networks), Evidential Deep Learning (including Prior Networks and Posterior Networks), as well as Distance Awareness (including Spectral-normalized Neural Gaussian Process and Deep Deterministic Uncertainty). Next, we talk about the latest advances in uncertainty quantification for pre-trained language models (including asking language models to express their uncertainty, interpreting uncertainties of text classifiers built on large-scale language models, uncertainty estimation in text generation, calibration of language models, and calibration for in-context learning). After that, we discuss typical application scenarios of uncertainty quantification in text classification (including in-domain calibration, cross-domain robustness, and novel class detection). Finally, we list popular performance metrics for the evaluation of uncertainty quantification effectiveness in text classification. Practical hands-on examples/exercises are provided to the attendees for them to experiment with different uncertainty quantification methods on a few real-world text classification datasets such as CLINC150.","The rapidly growing number of depressed people increases the burden of clinical diagnosis. Due to the abnormal speech signal of depressed patients, automatic audio-based depression recognition has the potential to become a complementary method for diagnosing. However, recognition performance varies largely with different speech acquisition tasks and classifiers, making results not comparable, and the performance requires further improvement before clinical application. This work extracted high-level statistical acoustic features (prosodic, voice-quality, and spectral features) of 23 depressed patients and 29 healthy subjects under spontaneous pronunciation tasks (interview and picture description) and mechanical pronunciation tasks (story reading and word reading), then applied principal component analysis (PCA) to reduce features dimensions, finally employed multilayer perceptron (MLP) to establish the classification model and compared with traditional classifiers (logistic regression, support vector machine, decision tree, and naive Bayes). The results showed that spontaneous pronunciation induced more significantly discriminative acoustic features and achieved better recognition performance accordingly. And the PCA retained 90% useful information with 50% features. Furthermore, MLP achieved the best performance with the accuracy 0.875 and average F1 score 0.855 under the picture description task. This study provides support for task design and classifier building for audio-based depression recognition, which could assist in mass screening for depression.","In the era of digital information, ensuring the accuracy and reliability of information is crucial, making fact-checking a vital process. Currently, English fact-checking has thrived due to various language processing tools and ample datasets. However, the same cannot be said for Vietnamese fact-checking, which faces significant challenges due to the lack of such resources. To address these challenges, we propose a model for checking Vietnamese facts by synthesizing three popular technologies: Knowledge Graph (KG), Datalog, and KG-BERT. The KG serves as the foundation for the fact-checking process, containing a dataset of Vietnamese information. Datalog, a logical programming language, is used with inference rules to complete the knowledge within the Vietnamese KG. KG-BERT, a Deep Learning (DL) model, is then trained on this KG to rapidly and accurately classify information that needs fact-checking. Furthermore, to put Vietnamese complex sentences into the fact-checking model, we present a solution for extracting triples from these sentences. This approach also contributes significantly to the ease of constructing foundational datasets for the Vietnamese KG. To evaluate the model's performance, we create a Vietnamese dataset comprising 130,190 samples to populate the KG. Using Datalog, we enrich this graph with additional knowledge. The KG is then utilized to train the KG-BERT model, achieving an impressive accuracy of 95%. Our proposed solution shows great promise for fact-checking Vietnamese information and has the potential to contribute to the development of fact-checking tools and techniques for other languages. Overall, this research makes a significant contribution to the field of data science by providing an accurate solution for fact-checking information in Vietnamese language contexts.","Highlights\n\u2022\nAs a multi-instance multi-label learning method, HAMIL has the permutation-invariant property, which can not only learn input bags with varying size but also give preference to informative instances.\n\u2022\nDifferent from pooling-based and attention-based methods, HAMIL has learnable aggregation units and leverages hierarchical clustering to determine the aggregation order.\n\u2022\nExperiments on two biomedical image datasets show that HAMIL achieves obvious improvement on all evaluation metrics over the SOTA methods.\nAbstract\nMulti-instance learning is common for computer vision tasks, especially in biomedical image processing. Traditional methods for multi-instance learning focus on designing feature aggregation methods and multi-instance classifiers, where the aggregation operation is performed either in the feature extraction or learning phase. As deep neural networks (DNNs) achieve great success in image processing via automatic feature learning, certain feature aggregation mechanisms need to be incorporated into common DNN architecture for multi-instance learning. Moreover, flexibility and reliability are crucial considerations to deal with varying quality and number of instances. In this study, we propose a hierarchical aggregation network for multi-instance learning, called HAMIL. The hierarchical aggregation protocol enables feature fusion in a defined order, and the simple convolutional aggregation units lead to an efficient and flexible architecture. We assess the model performance on two microscopy image classification tasks, namely protein subcellular localization using immunofluorescence images and gene annotation using spatial gene expression images. The experimental results show that HAMIL outperforms the state-of-the-art feature aggregation methods and the existing models for addressing these two tasks. The visualization analyses also demonstrate the ability of HAMIL to focus on high-quality instances.","In recent times, deep neural networks achieved outstanding predictive performance on various classification and pattern recognition tasks. However, many real-world prediction problems have ordinal response variables, and this ordering information is ignored by conventional classification losses such as the multi-category cross-entropy. Ordinal regression methods for deep neural networks address this. One such method is the CORAL method, which is based on an earlier binary label extension framework and achieves rank consistency among its output layer tasks by imposing a weight-sharing constraint. However, while earlier experiments showed that CORAL\u2019s rank consistency is beneficial for performance, it is limited by a weight-sharing constraint in a neural network\u2019s fully connected output layer, which may restrict the expressiveness and capacity of a network trained using CORAL. We propose a new method for rank-consistent ordinal regression without this limitation. Our rank-consistent ordinal regression framework (CORN) achieves rank consistency by a novel training scheme. This training scheme uses conditional training sets to obtain the unconditional rank probabilities through applying the chain rule for conditional probability distributions. Experiments on various datasets demonstrate the efficacy of the proposed method to utilize the ordinal target information, and the absence of the weight-sharing restriction improves the performance substantially compared to the CORAL reference approach. Additionally, the suggested CORN method is not tied to any specific architecture and can be utilized with any deep neural network classifier to train it for ordinal regression tasks.","Ensemble-based techniques are deployed to yield better performance than individual classifiers. Existing ensembling approaches fail to consider data complexity during their design. This work presents an ensemble-based approach for resolving complex patterns in real-world classification problems. A novel Minimum Spanning Tree (MST)-based approach is employed for decomposing the original problem into subproblems with reduced data complexity, and SVM is utilized for the development of component classifiers for these subproblems. A novel dynamic ensemble-based technique is utilized to generate the outcome. This work is evaluated by using 28 datasets retrieved from the KEEL dataset repository. Additionally, statistical tests are performed to illustrate a significant difference in the performance of the proposed model compared to the state-of-art classification models and two recently proposed dynamic ensembling approaches.\nHighlights\n\u2022\nNovel Ensemble-based technique.\n\u2022\nDivides the original problem into subproblems of lower data complexity.\n\u2022\nUtilizes Minimum Spanning Tree for creating subproblems.\n\u2022\nCreates models for subproblems using SVM.\n\u2022\nNovel approach for dynamic selection of the SVM model to yield the final outcome.","Highlights\n\u2022\nA two-stage framework to detect fraudulent transactions is proposed.\n\u2022\nThe framework incorporates a deep Autoencoder as a representation learning method.\n\u2022\nThe deep Autoencoder improves the performance of the Deep learning classifiers.\n\u2022\nThe results indicate the superiority of deep autoencoder over PCA.\nAbstract\nDue to the growth of e-commerce and online payment methods, the number of fraudulent transactions has increased. Financial institutions with online payment systems must utilize automatic fraud detection systems to reduce losses incurred due to fraudulent activities. The problem of fraud detection is often formulated as a binary classification model that can distinguish fraudulent transactions. Embedding the input data of the fraud dataset into a lower-dimensional representation is crucial to building robust and accurate fraud detection systems. This study proposes a two-stage framework to detect fraudulent transactions that incorporates a deep Autoencoder as a representation learning method, and supervised deep learning techniques. The experimental evaluations revealed that the proposed approach improves the performance of the employed deep learning-based classifiers. Specifically, the utilized deep learning classifiers trained on the transformed data set obtained by the deep Autoencoder significantly outperform their baseline classifiers trained on the original data in terms of all performance measures. Besides, models created using deep Autoencoder outperform those created using the principal component analysis (PCA)-obtained dataset as well as the existing models.","In practical applications, degradation level estimation is often facing the challenge of dealing with unlabeled time series characterized by long-term temporal dependencies, which are typically not properly represented using sliding time windows. Inspired by the idea of representing temporal patterns by a mechanism of neurodynamical pattern learning, called Conceptors, a two-stage method for the estimation of the equipment degradation level is developed. In the first stage, clusters of Conceptors representing similar patterns of degradation within complete run-to-failure trajectories are identified; in the second stage, the obtained clusters are used to supervise the training of a convolutional neural network classifier of the equipment degradation level. The proposed method is applied to a synthetic case study and to two literature case studies regarding bearings degradation level estimation. The obtained results show that the proposed method provides more accurate estimation of the equipment degradation level than other state-of-the-art methods.\nHighlights\n\u2022\nA novel Conceptors-aided clustering approach for variable-length time series.\n\u2022\nA novel Conceptors-based CNN for degradation level estimation.\n\u2022\nOur Conceptor-aided method avoids the use of sliding time windows.\n\u2022\nValidate our method on a synthetic case and two bearing cases of degradation level estimation.\n\u2022\nOur two-stage degradation level estimation method is superior to other alternative methods.","The three most important necessities for human life are food, shelter, and clothing. Young people who are technologically savvy have witnessed a significant scientific increase in the latter two areas. Despite this, farming is still regarded as a labor-intensive endeavour. Most farmers are uneducated and lack a scientific understanding of farming practices. Crop cultivation anywhere in the world is dependent on the climate, also known as seasons, and soil properties; however, increasing crop production is dependent on a variety of factors, most notably temperature. This work proposes a crop recommendation system to address the issue of increasing crop production. A vision of the perfect harvest before planting would be extremely beneficial to farmers and other stakeholders in making appropriate decisions about improving yields for local use, and it may inspire increased capacity and a wider range of product options for businesses. Precision agriculture is a modern farming strategy that advises farmers on the sorts of crops they should plant based on data collected through studies on soil features, soil types, and crop yields. This style of agriculture is also known as \"high-intensity agriculture\". Our system employed Machine Learning procedures to recommend the appropriate crops. This system then reduces the financial losses experienced by farmers because of establishing the ominous harvests. This problem is addressed in this paper by proposing a recommendation system using an ensemble model with majority voting and an accuracy of 99.4 percent.","The grain size is an important steel grading parameter. For metallographic steel images with various grain sizes and complex textures, it is not possible for a human expert to determine the grain size efficiently. Meanwhile, conventional computer vision models are designed based on general images and they are not capable of achieving high performance in metallographic steel grain size recognition. To solve these problems, a method based on multiple receptive field fusion is proposed. A multi-scale convolutional net is used to extract information of microstructures in various scales. In addition, to augment the extracted features, a self-attention module is used to improve the robustness of feature representation with complex metallographic textures. At last, via a multiple feature fusion module, the data capacity is extended by projecting features into multiple hidden spaces. A comprehensive experiment was conducted on the Huawei Cloud Dataset and the classification accuracy was improved by 27% compared with other SOTA models, while our computation cost was only 0.06 GFLOPs.","Highlights\n\u2022\nA class imbalance metric to assess classifier performance considering imbalance distribution impacts.\n\u2022\nSensitive in comparison between classifier performances in different imbalance distribution statuses.\n\u2022\nAn Algorithm for prediction on ideal classifier performance with a fair distribution (1:1) using limited size of samples.\nAbstract\nClass imbalance is a common problem in many classification domains. This paper provides an evaluation index and one algorithm for this problem based on binary classification. The Model Performance Index (MPI) is proposed for assessing classifier performance as a new evaluation metric, considering class imbalance impacts. Based on MPI, we investigate algorithms to estimate ideal classifier performance with a fair distribution (1:1), referred to as the Ideal Model Performance Algorithm. Experimentally, compared with traditional metrics, MPI is more sensitive. Specifically, it can detect all types of changes in classifier performances, while others might remain at the same levels. Moreover, for the estimation of classifier performances, the algorithm reaches small differences between predictions and the values observed. Generally, for ideal performances, it achieved error rates of 0.060% - 1.3% for rare class in four experiments, showing a practical value on estimation and representation on the classifier performances.","In industrial settings, it is often necessary to achieve language-level accuracy targets. For example, Amazon business teams need to build multilingual product classifiers that operate accurately in all European languages. It is unacceptable for the accuracy of product classification to meet the target in one language (e.g, English), while falling below the target in other languages (e.g, Portuguese). To fix such issues, we propose Language Aware Active Learning for Multilingual Models (LAMM), an active learning strategy that enables a classifier to learn from a small amount of labeled data in a targeted manner to improve the accuracy of Low-resource languages (LRLs) with limited amounts of data for model training. Our empirical results on two open-source datasets and two proprietary product classification datasets demonstrate that LAMM is able to improve the LRL performance by 4%--11% when compared to strong baselines.","The trade-off between privacy and accuracy presents a challenge for current federated learning (FL) frameworks, hindering their progress from theory to application. The main issues with existing FL frameworks stem from a lack of interpretability and targeted privacy protections. To cope with these, we proposed Disentangled Federated Learning for Privacy (DFLP) which employes disentanglement, one of interpretability techniques, in private FL frameworks. Since sensitive properties are client-specific in nature, our main idea is to turn this feature into a tool that strikes the balance between data privacy and FL model performance, enabling the sensitive attributes to be private. DFLP disentangles the client-specific and class-invariant attributes to mask the sensitive attributes precisely. To our knowledge, this is the first work that successfully integrates disentanglement and the nature of sensitive attributes to achieve privacy protection while ensuring high FL model performance. Extensive experiments validate that disentanglement is an effective method for accuracy-aware privacy protection in FL frameworks.","Explaining black-box classification models is a hot topic in AI, with the overall goal of improving trust in decisions made by such models. Several works have been done and diverse functions have been proposed. However, their formal properties and links have not been sufficiently studied. This paper presents four contributions: The first consists of investigating global explanations of black-box classifiers. We provide a formal and unifying framework in which such explanations are defined from the whole feature space. The framework is based on two concepts, which are seen as two types of global explanations: arguments in favour of (or pro) predictions and arguments against (or con) predictions. The second contribution consists of defining various types of local explanations (abductive explanations, counterfactuals, contrastive explanations) from the whole feature space, investigating their properties, links and differences, and showing how they relate to global explanations. The third contribution consists of analysing and defining explanation functions that generate (global, local) abductive explanations from incomplete information (i.e., from a subset of the feature space). We start by proposing two desirable properties that an explainer would satisfy, namely success and coherence. The former ensures the existence of explanations while the latter ensures their correctness. We show that in the incomplete case, the two properties cannot be satisfied together. The fourth contribution consists of proposing two functions that generate abductive explanations and which satisfy coherence at the expense of success.","Inverse gravity moment (IGM) is a recent term weighting scheme in the text classification literature. The idea is that a distinguishing term should concentrate around preferably one or limited number of classes. IGM considers document frequencies of a term over all classes. However, it cannot handle the class imbalance problem. The natural distribution of documents in the text classification is frequently imbalanced. The classifier generally tend to bias toward majority classes, classes with many samples. Therefore, documents from minority classes might be ignored. In this study, we tackle the class imbalance problem in IGM and propose to use a factor called relative imbalance ratio (RIR). The aim of RIR coefficient is to scale document frequencies of the terms from minority classes in order to amplify the IGM score for the terms from the minority classes. Otherwise, those terms might be dwarfed due to the fact that majority classes have many more documents. Experimental results with three data sets, two of which are imbalanced, show that our proposed method manage to outperform the original IGM method as well as the improved IGM (IIGM) and seven other the state-of-the-art term weighting schemes (TF-ICF, TF-ICSDF, TF-RF, TF-PROB, TF-MONO, RE, AFE-MERT) in terms of f 1 - m a c r o results while not comprising f 1 - m i c r o.\nHighlights\n\u2022\nThe class imbalance in inverse gravity moment, IGM, term weighting scheme is studied.\n\u2022\nIGM-RIR, which integrates relative imbalance ratio, RIR, into IGM, is proposed.\n\u2022\nIGM-RIR promotes minority classes in the term weighting task.\n\u2022\nIGM-RIR generally manages to outperform both IGM and improved IGM (IIGM).\n\u2022\nIGM-RIR also outperform other state-of-the-art term weighting schemes.","We report the design, development and deployment of PRIORITY, an intelligent portal aimed at reducing the workload of instructors, tutors and teaching assistants in large programming courses of creating lab, assignment and exam problems every week. PRIORITY offers a scalable, user friendly and indexed repository of problems that can be queried to retrieve problems related to a particular programming concept, say for loops. PRIORITY accomplishes this by casting problem retrieval as a multi-label learning problem and using solving it using novel feature selection and AI-techniques. We also report the results of an A/B test and user survey, both conducted while PRIORITY was being used to offer a CS1 course taught at IIT Kanpur with over 500 students. PRIORITY has been in deployment at IIT Kanpur for almost 2 years now and our experience thus far suggests that it not only presents a valuable tool for course administrators, but also opens up several intriguing problems at the intersection of programming instruction, pedagogy, machine learning, semi-supervised learning and information retrieval. Code for PRIORITY is available at https://github.com/purushottamkar/priority/","An unsupervised sentiment analysis method is presented to classify user comments on laptops into positive ones and negative ones. The method automatically extracts informative features in testing dataset and labels the sentiment polarity of each feature to make a domain-specific lexicon. The classification accuracy of this lexicon will be compared to that with an existing general sentiment lexicon. Besides, the concept of three-way decision will be applied in the classifier as well, which combines lexicon-based methods and supervised learning methods together. Results indicate that the overall performance can reach considerable improvements with three-way decision.","Highlights\n\u2022\nThe method of implicit feature interaction with multi-layer perceptron is introduced into fine-grained recognition, which enables the modeling of higher-order feature interactions.\n\u2022\nThe factorized bilinear pooling is compared with multi-layer perceptron. Then, a model that can fuse multi-order statistical information is proposed, and the information of each order complements each other, thereby improving recognition performance.\n\u2022\nBy using the CAM visualization method, how higher-order statistical features improve the model recognition effect is analyzed.\nAbstract\nThe bilinear model using second-order statistical features is an important weakly supervised method for fine-grained recognition. Based on this, fusing higher-order statistical features to obtain more discriminant features is an effective approach for improving the performance of the model. However, the existing framework is difficult to fuse higher-order features due to a sharp increase in the number of parameters caused by the increase in fusion order. To address the issue, this paper proposes a DeepBP model composed of a deep network module and a bilinear pooling module. The bilinear module explicitly captures low-order statistical features, and the deep network module implicitly learns high-order features. The two modules are integrated to achieve multi-level information integration. To verify the model's ability, experiments are conducted on the CUB, Cars, and Aircrafts datasets, and the accuracy of 85.6%, 91.6%, and 88.6% is achieved, respectively.","Highlights\n\u2022\nA novel network architecture is proposed for open set recognition (OSR).\n\u2022\nThe random network distillation is the first to be introduced for OSR.\n\u2022\nMultitask learning coordinates multiple subnetworks with different training goals.\n\u2022\nProposed decision fusion method improves unknown defect detection performance.\n\u2022\nThe proposed approach significantly outperforms other existing methods.\nAbstract\nIn semiconductor manufacturing, wafer bin maps (WBMs) present specific defect patterns that provide crucial information for tracking abnormal processes. Thus, it is necessary to correctly classify WBM defects to achieve yield improvements. However, unknown types of defects constantly emerge due to the development of new process technologies and devices, degrading classification performance. Thus, a novel open set recognition (OSR) method that aims to detect unknown defects while correctly classifying known defect types is proposed in this work. The proposed method first extracts the reconstruction errors using an autoencoder and the random network errors using a random network distillation technique. Next, the two types of errors are combined by applying the extreme value theory-based Weibull calibration technique to produce probability scores for unknown detection. Finally, a deep convolutional neural network classifier assigns known classes to the samples determined as known. In the proposed method, multiple networks are interconnected, and we apply a simple sequential multitask learning mechanism to coordinate all networks. Extensive experiments were conducted on a real-world WBM defect dataset, and the results showed that each component proposed in this paper helped improve the unknown detection performance of our approach while minimizing its closed set classification performance loss. As a result, the proposed method outperformed the state-of-the-art OSR methods, achieving area under the receiver operating characteristic curve (AUROC) increases of at least 0.124 for unknown detection cases.","The explosion of data which is happening now must be utilized to support decision making both in terms of business and other matters. Data which are becoming assets today needs to be analyzed and extracted in order to find valuable information. The results of data analysis can be used to make predictions, one of which is classification. For high dimensions data, we require preprocessing stage so that the model building process is not complex and the analysis is accurate. One of the preprocessing stages that need attention is feature selection. Feature selection is applied to reduce features without diminishing the accuracy and information in the data. Performing feature selection can also be done by using the association rule. Association rule refers to considering the association relationship between items and the frequency of items occurrence as features. However, the obstacle in implementing the association rule is when determining the minimum support value. Therefore, an adaptive support method is proposed to determine the minimum support value automatically based on the characteristics of the dataset. In this present study, a feature selection method using adaptive support is proposed. Based on the experimental results using 3 classifiers, the accuracy and F1-Score values for the feature selection method using adaptive support are higher compared to the Information gain method.","The remarkable production speed of documents and, consequently, the volume of unstructured data stored in the Brazilian Government facilities requires processes that enable the capacity of classifying documents. This requirement is compliant with the existing archival legislation. In this sense, Natural Language Processing (NLP) stands as an important asset related to document classification, considering the reality of current document production, where there is a considerable number of unlabeled documentary samples. The Self-Learning approach applied to the BERT fine-tuning step delivers a model capable of classifying a partially labeled set of data according to the Requirements Model for Computerized Document Management Systems (e-ARQ Brazil). The developed model was capable of reaching a human-level performance, outperforming Active Learning and BERT in a series of defined confidence levels.","Skip Purpose Section\nPurpose\nOne of the significant problems in data stream classification is the concept drift phenomenon, which consists of the change in probabilistic characteristics of the classification task. Such changes in posterior probability destabilize the classification model performance, seriously degrading its quality. It is necessary to design appropriate strategies to counteract this phenomenon, allowing the classifier to adapt to the changing probabilistic characteristics. It is tough to propose such an approach with limited access to data labels. A human bias of high quality is usually costly, so to minimize the expenses related to this process, it is also necessary to propose learning strategies based on semi-supervised learning. Such strategies employ active learning methods indicating which of the incoming objects are valuable to be labeled for improving the classifier's performance.\nSkip Methods Section\nMethods\nThis paper proposes Active Weighted Aging Ensemble algorithm, a novel chunk-based method for non-stationary data stream classification. It employs a classifier ensemble approach and utilizes the changing ensemble lineup to react to concept drift appropriately. It also proposed a new active learning method, considering a limited budget that may be applied to any data stream classifier.\nSkip Results Section\nResults\nAWAE has been evaluated through computer experiments using real and synthetic data streams, confirming the proposed algorithm's high quality over state-of-the-art methods.\nSkip Conclusion Section\nConclusion\nThe research conducted on benchmark data streams confirmed the effectiveness of the proposed solution and highlighted its strengths in comparison with state-of-the-art methods. The estimated computational complexity is acceptable and comparable to the benchmark algorithms.\nHighlights\n\u2022\nThe proposal of a new chunk-base classifier ensemble for non-stationary data streams.\n\u2022\nNew weighting and decoying of base classifiers in the ensemble.\n\u2022\nNovel active learning method with limited labeling budget.\n\u2022\nExtensive experimental studies and comparison with SOTA methods.","Despite the arsenal of existing cancer therapies, the ongoing recurrence and new cases of cancer pose a serious health concern that necessitates the development of new and effective treatments. Cancer immunotherapy, which uses the body's immune system to combat cancer, is a promising treatment option. As a result, in silico methods for identifying and characterizing tumor T cell antigens (TTCAs) would be useful for better understanding their functional mechanisms. Although few computational methods for TTCA identification have been developed, their lack of model interpretability is a major drawback. Thus, developing computational methods for the effective identification and characterization of TTCAs is a critical endeavor. PSRTTCA, a new machine learning (ML)-based approach for improving the identification and characterization of TTCAs based on their primary sequences, is proposed in this study. Specifically, we introduce a new propensity score representation learning algorithm that allows one to generate various sets of propensity scores of amino acids, dipeptides, and g-gap dipeptides to be TTCAs. To enhance the predictive performance, optimal sets of variant propensity scores were determined and fed into the final meta-predictor (PSRTTCA). Benchmarking results revealed that PSRTTCA was a more precise and promising tool for the identification and characterization of TTCAs than conventional ML classifiers and existing methods. Furthermore, PSR-derived propensities of amino acids in becoming TTCAs are used to reveal the relationship between TTCAs and their informative physicochemical properties in order to provide insights into TTCA characteristics. Finally, a user-friendly online computational platform of PSRTTCA is publicly available at http://pmlabstack.pythonanywhere.com/PSRTTCA. The PSRTTCA predictor is anticipated to facilitate community-wide efforts in accelerating the discovery of novel TTCAs for cancer immunotherapy and other clinical applications.\nHighlights\n\u2022\nA novel propensity score representation learning (PSR) algorithm is proposed.\n\u2022\nPSRTTCA is developed for improving the identification and characterization of TTCAs.\n\u2022\nThe proposed PSRTTCA achieves a better performance compared with the existing methods.\n\u2022\nPSR-derived propensity scores provide insights on the characteristics of TTCAs.","In the field of text classification, current research ignores the role of part-of-speech features, and the multi-channel model that can learn richer text information compared to a single model. Moreover, the method based on neural network models to achieve final classification, using fully connected layer and Softmax layer can be further improved and optimized. This paper proposes a hybrid model for text classification using part-of-speech features, namely PAGNN-Stacking1. In the text representation stage of the model, introducing part-of-speech features facilitates a more accurate representation of text information. In the feature extraction stage of the model, using the multi-channel attention gated neural network model can fully learn the text information. In the text final classification stage of the model, this paper innovatively adopts Stacking algorithm to improve the fully connected layer and Softmax layer, which fuses five machine learning algorithms as base classifier and uses fully connected layer Softmax layer as meta classifier. The experiments on the IMDB, SST-2, and AG_News datasets show that the accuracy of the PAGNN-Stacking model is significantly improved compared to the benchmark models.","Diabetic retinopathy (DR) is a type of diabetes mellitus that attacks the retina of the eye. DR will cause patients to experience blindness slowly. Generally, DR can be detected by using a special instrument called an ophthalmoscope to view the inside of the eyeball. However, in conditions where there is a very small difference between the normal image and the DR image, computer-based assistance is needed for maximizing image reading value. In this research, a method of image quality improvement will be carried out which will then be integrated with a classification algorithm based on deep learning. The results of image improvement using Contrast Limited Adaptive Histogram Equalization (CLAHE) shows that the average accuracy of the method on several models is very competitive, 91% for the VGG16 model, 95% for InceptionV3, and 97% for EfficientNet compared to the results original image which only has an accuracy of 87% for VGG16 model, 90% for InceptionV3 model, and 95% for EfficientNet. However, in ResNet34 better accuracy is obtained in the original image with an accuracy of 95% while in the CLAHE image the accuracy value is only 84%. The results of this comprehensive evaluation and recommendation of famous backbone networks can be useful in the computer-aided diagnosis of diabetic retinopathy.","Sea observation through remote sensing technologies plays an essential role in understanding the health status of marine fauna species and their future behaviour. Accurate knowledge of the marine habitat and the factors affecting faunal variations allows to perform predictions and adopt proper decisions. This is even more relevant nowadays, with policymakers needing increased environmental awareness, aiming to implement sustainable policies. There is a connection between the biogeochemical and physical processes taking place within a biological system and the variations observed in its faunal populations. Mesoscale phenomena, such as upwelling, countercurrents and filaments, are essential processes to analyse because their arousal entails, among other things, variations in the density of nutrient substances, in turn affecting the biological parameters of the habitat. This paper concerns the proposal of a classification system devoted to recognising marine mesoscale events. These phenomena are studied and monitored by analysing Sea Surface Temperature images captured by satellite missions, such as Metop and MODIS Terra/Aqua. Classification of such images is pursued through dedicated algorithms that extract temporal and spatial features from the data and apply a set of rules to the extracted features, in order to discriminate between different observed scenarios. The results presented in this work have been obtained by applying the proposed approach to images captured over the south-western region of the Iberian Peninsula.","Technology development has been getting more advanced and greatly facilitated human life. One of them is machine learning automation which has been proven to be consistent for doing various computations against extensive data such as transaction data in the e-commerce area. Seeing this opportunity, we implemented the machine learning approach to detect fraudulent cashback transactions in e-commerce that are currently rife in Indonesia. The training data used to build the machine learning model were the transaction data from one of the leading e-commerce in Indonesia that had been processed. The supervised classification algorithms used were K-Nearest Neighbor (k-NN), Convolutional Neural Networks (CNN), and Long Short-Term Memory (LSTM). In the end, the best steps and methods that could be taken against fraudulent cashback activities in the future are shown in this paper.","Abstract\u2014 For areas with potential occurrence of blasting events, it is essential to distinguish them from natural earthquakes. An efficient processing method is needed to save manpower, especially under the current large amount of data records by seismic stations. We apply a SCOUTER algorithm to distinguish between the two types of events. The recognition precision of the trained model for natural earthquakes and blasts can reach 95% and 92.8%, respectively, and the recall can reach 93.4% and 94.6%, respectively. The testing results of data with different epicentral distances and SNR show that our method is stable, independent on regional waveform characteristics and insensitive to data of different SNR. The explanations for each classification at the final confidence also give us a profound enlightenment.","Time series classification (TSC) is an important task in time series data mining and has attracted a lot of research attention. Most TSC algorithms aim to achieve high classification accuracy while reducing the computational complexity. Currently, Time Series Combination of Heterogeneous and Integrated Embedding Forest (TS-CHIEF) is considered to be one of the state-of-the-art TSC algorithms. However, compared with fast algorithms such as Time Series Forest (TSF), TS-CHIEF still has high computation cost. On the premise that the TSF algorithm is fast, we propose a new TSC algorithm, Forest based on Interval Transformation (called FIT), which takes into account both accuracy and efficiency. FIT uses cross-validation to select appropriate transformation series and corresponding interval features, and adaptively converts the interval features of each series in the process of formal training. Subsequently, the transformed feature set is combined with the random forest training FIT model. We evaluate the performance of FIT on 85 UCR time series classification datasets. The experimental results demonstrate that FIT can achieve better accuracy while maintaining high efficiency compared with the state-of-the-art methods.\nHighlights\n\u2022\nA variety of series transformation and interval features to expand feature space.\n\u2022\nAdaptive selection of transformation series and feature by cross-validation.\n\u2022\nForest based on Interval Transformation for time series classification.\n\u2022\nExperiments on real datasets verify effectiveness of our proposal.","We present a novel boosting algorithm where temporal consistency is addressed in a short-term way. Although temporal correlation of observed data may be an important cue for classification (e.g. of human activities) it is seldom used in boosting techniques. The recently proposed Temporal AdaBoost addresses the same problem but in a heuristic manner, first optimizing the weak learners without temporal integration. The classifier responses for past frames are then averaged together, as long as the total classification error decreases.\nWe extend the GentleBoost algorithm by modeling time in an explicit form, as a new parameter during the weak learner training and in each optimization round. The time consistency model induces a fuzzy decision function, dependent on the temporal support of a feature or data point, with added robustness to noise. Our temporal boost algorithm is further extended to cope with multi class problems, following the JointBoost approach introduced by Torralba et. al. We can thus (i) learn the parameters for all classes at once, and (ii) share features among classes and groups of classes, both in a temporal and fully consistent manner.\nFinally, the superiority of our proposed framework is demonstrated comparing it to state of the art, temporal and non-temporal boosting algorithms. Tests are performed both on synthetic and 2 real challenging datasets used to recognize a total of 12 different human activities.","The writing style of the same writer varies from instance to instance in Arabic and English handwritten digit recognition, making handwritten digit recognition challenging. Currently, deep learning approaches are applied in many applications, including convolutional neural networks (CNNs) modified to produce other models, such as local binary convolutional neural networks (LBCNNs). An LBCNN is created by fusing a local binary pattern (LBP) with a CNN by reformulating the LBP as a convolution layer called a local binary convolution (LBC). However, LBCNNs suffer from the random assignment of 1, 0, or -1 to LBC weights, making LBCNNs less robust. Nevertheless, using another LBP-based technique, such as center-symmetric local binary patterns (CS-LBPs), can address such issues. In this paper, a new model based on CS-LBPs is proposed called center-symmetric local binary convolutional neural networks (CS-LBCNN), which addresses the issues of LBCNNs. Furthermore, an enhanced version of CS-LBCNNs called threshold center-symmetric local binary convolutional neural networks (TCS-LBCNNs) is proposed, which addresses another issue related to the zero-thresholding function. Finally, the proposed models are compared to state-of-the-art models, proving their ability by producing a more accurate and significant classification rate than the existing LBCNN models. For the bilingual dataset, the TCS-LBCNN enhances the accuracy of the LBCNN and CS-LBCNN by 0.15% and 0.03%, respectively. In addition, the comparison shows that the accuracy acquired by the TCS-LBCNN is the second-highest using the MNIST and MADBase datasets.","The largest and most diversified group of organisms are insects. Since insects play a crucial role in many ecosystems, it must be precisely identified for efficient management. However, it is difficult and labor-intensive to identify insect species. Due to advancements in deep learning, computer vision, and sensor technologies, there is rising interest in image-based systems for quick, accurate identification. This study investigates a novel hybrid feature set of shallow features from the tiger beetle dataset, such as texture and wavelet features and high-level features from SqueezeNet. The tiger beetle insect is classified into the cicindelini and collyridini classes with a random forest classifier with 97.65 percent accuracy using this hybrid feature set, which considers the texture and structural characteristics of the insect. Thus the technique provides insight into various features and indicates promising future directions for image-based insect identification and species classification relevant to Computer Science, Agriculture, and Ecology research.","Cyborg intelligence is committed to combining artificial intelligence with biological intelligence and human body technology through the tight integration of machines and biological organisms, so as to improve the natural capabilities of human beings. Brain\u2013computer integration as a typical and effective way to explore cyborg intelligence has been widely applied in many scenarios. In cyborg systems regarding brain\u2013computer integration, task planning in the artificial intelligent unit aims to identify intention of biological unit, which highly relies on a classifier. An urgent challenge for task planning is to effectively deal with the uncertainties associated with the complexity and variability of brain dynamics, which is reflected in the non-stationary nature of brain signals. This poses a severe problem for existing models to the intention identification in task planning. Additionally, model interpretability is also very important in task planning for several reasons such as human\u2013machine communication, trust, etc. In this study, we focus on EEG-based motor imagery and develop a stacking-based ensemble fuzzy inference framework for task planning. In this framework, the zero-order Takagi\u2013Sugeno\u2013Kang (0-TSK) fuzzy system is taken as the basic component and the output of one component is augmented into the input feature space as new input for another component. This framework has two main merits: (i) its promising performance is guaranteed by the general stacking principle asserting that the manifold structure of the input feature space can be opened by the outputs generated from this space. (ii) its interpretability is guaranteed from three levels. Additionally, from a probabilistic statistical point of view, we theoretically demonstrate that as long as the basic learning component is a weak classifier, the general stacking principle can guarantee promising performance. Experimental results conducted on motor imagery data demonstrate the promising performance and high interpretability of the proposed framework.","A startup is a recently established business venture led by entrepreneurs, to create and offer new products or services. The discovery of promising startups is a challenging task for creditors, policymakers, and investors. Therefore, the startup survival rate prediction is required to be developed for the success/failure of startup companies. In this paper, the feature selection using the Convex Least Angle Regression Least Absolute Shrinkage and Selection Operator (CLAR-LASSO) is proposed to improve the classification of startup survival rate prediction. The Swish Activation Function based Long Short-Term Memory (SAFLSTM) is developed for classifying the survival rate of startups. Further, the Local Interpretable Model-agnostic Explanations (LIME) model interprets the predicted classification to the user. Existing research such as Hyper Parameter Tuning (HPT)-Logistic regression, HPT-Support Vector Machine (SVM), HPT-XGBoost, and SAFLSTM are used to compare the CLAR-LASSO. The accuracy of the CLAR-LASSO is 95.67% which is high when compared to the HPT-Logistic regression, HPT-SVM, HPT-XGBoost, and SAFLSTM.","A methodology is introduced for the automated assessment of structural changes of breast tissue in mammograms. It employs a generic machine learning framework and provides objective breast density measures quantifying the specific biological effects of interest. In several illustrative experiments on data from a clinical trial, it is shown that the proposed method can quantify effects caused by hormone replacement therapy (HRT) at least as good as standard methods. Most interestingly, the separation of subpopulations using our approach is considerably better than the best alternative, which is interactive. Moreover, the automated method is capable of detecting age effects where standard methodologies completely fail.","Quantum computing is a rapidly growing field of science with many potential applications. One such field is machine learning applied in many areas of science and industry. Machine learning approaches can be enhanced using quantum algorithms and work effectively, as demonstrated in this paper. We present our experimental attempts to explore Quantum Support Vector Machine (QSVM) capabilities and test their performance on the collected well-known images of handwritten digits for image classification called the MNIST benchmark. A variational quantum circuit was adopted to build the quantum kernel matrix and successfully applied to the classical SVM algorithm. The proposed model obtained relatively high accuracy, up to 99%, tested on noiseless quantum simulators. Finally, we performed computational experiments on real and recently setup IBM Quantum systems and achieved promising results of around 80% accuracy, demonstrating and discussing the QSVM applicability and possible future improvements.","Blast furnace (BF) burden surface modeling is the basis for automating precise charging operations of BFs, and it can also be used to predict gas flow distributions based on a burden profile. In this paper, first, a mechanism model is established according to the charging operation, and it is convenient for predicting the burden profile after the charging operation. Then, the Gaussian process regression (GPR) algorithm is used to fuse the charging mechanism model and the radar detection data to better reconstruct the burden profile. Finally, the traditional shape of a burden surface is researched based on the point cloud data of a phased array radar, and 4 classes of burden surfaces are defined and reconstructed. The reconstructed burden surface is classified by expert-defined features and deep features extracted by convolutional neural networks (CNNs).\nHighlights\n\u2022\nThe two-model drived algorithm based on GPR for 3D burden surface reconstruction.\n\u2022\nThe 3D burden surface feature is expert-defined and CNN fusion features.\n\u2022\nThe burden surface classification is through GWO-KELM.","Algal blooms pose a significant threat to aquatic ecosystems and human health. To address this issue, this paper proposes a machine learning-based approach for predicting harmful algal blooms (HABs) by analyzing environmental features. Algae, as primary organic matter and oxygen producers, play a vital role in the biosphere. However, the exponential increase in algal growth worldwide poses significant challenges to economic development and long-term sustainability. The paper employs three popular machine learning algorithms: Artificial Neural Network (ANN), Gradient Boosting Decision Tree (GBDT), and Support Vector Machine (SVM) to predict algal blooms. The research utilizes real-time data from two locations: the Sassafras River in the United States Chesapeake Bay and Lake Okeechobee in Florida, USA. These locations have experienced frequent HABs due to factors like chemical runoff and nutrient-rich conditions. By analyzing the collected data, the paper identifies and selects the most important features to optimize the prediction models\u2019 accuracy. Preliminary results demonstrate promising accuracy in predicting algal growth and identifying key characteristics associated with HABs. These findings contribute to a better understanding of algal blooms and pave the way for effective mitigation strategies to combat this global environmental challenge.","In recent years, the interpretive this looks like that structure has gained significant attention. It refers to the human tendency to break down images into key parts and make classification decisions by comparing them to pre-existing concepts in their minds. However, most existing prototypical-based models assign prototypes directly to each category without considering that key parts with the same meaning may appear in images from different categories. To address this issue, we propose dividing prototypes with the same meaning into the same latent space (referred to as Basic Feature Domain) since different category parts only slightly affect the corresponding prototype vectors. This process of integrating prototypes based on the feature domain is referred to as prototype alignment. Additionally, we introduce the concept of part-aware optimization, which prioritizes prototypical parts of images over simple category labels during optimizing prototypes. Moreover, we present two feature aggregation methods, by row and by cluster, for the basic feature domain. We demonstrate competitive results compared to other state-of-the-art prototypical part methods on the CUB-2011-200 dataset and Stanford Cars dataset using our proposed self-explanatory part-aware proto-aligned network (PaProtoPNet).","Highlights\n\u2022\nA new retraining methodology for SVMs is proposed\n\u2022\nThe new concept of Support Subsets is introduced\n\u2022\nMethodology proposal reduces complexity computation for SVM training\n\u2022\nImbalance datasets produce balanced Support Subsets\n\u2022\nThe proposal compares successfully to well-known retraining techniques\nAbstract\nThe availability of new data in previously trained Machine Learning (ML) models usually requires retraining and adjustment of the model. Support Vector Machines (SVMs) are widely used in ML because of their strong mathematical foundations and flexibility. However, SVM training is computationally expensive, both in time and memory. Hence, the training phase might be a limitation in problems where the model is updated regularly. As a solution, new methods for training and updating SVMs have been proposed in the past. In this paper, we introduce the concept of Support Subset and a new retraining methodology for SVMs. A Support Subset is a subset of the training set, such that retraining a ML model with this subset and the new data is equivalent to training with all the data. The performance of the proposal is evaluated in a variety of experiments on simulated and real datasets in terms of time, quality of the solution, resultant support vectors, and amount of employed data. The promising results provide a new research line for improving the effectiveness and adaptability of the proposed technique, including its generalization to other ML models.","In multi-dimensional classification (MDC), each training example is associated with multiple class variables from different class spaces. However, it is rather costly to collect labeled MDC examples which have to be annotated from several dimensions (class spaces). To reduce the labeling cost, we attempt to deal with the MDC problem under the semi-supervised learning setting. Accordingly, a novel MDC approach named PLAP is proposed to solve the resulting semi-supervised MDC problem. Overall, PLAP works under the label propagation framework to utilize unlabeled data. To further consider dependencies among class spaces, PLAP deals with each class space in a progressive manner, where the previous propagation results will be used to initialize the current propagation procedure, and all processed class spaces and the current one will be regarded as an entirety. Experiments validate the effectiveness of the proposed approach.","The significant rise in the amount of satellite images that have become available in recent years makes large-scale analysis of this data difficult. To make meaningful inferences from such images, one must have a thorough comprehension of the information they convey. Deep learning advances recently make it possible to train powerful machine learning models that can recognise several objects in a shot regardless of their attributes or distinct points of view. In this study, it is investigated if it's possible to identify monuments in satellite pictures using deep learning models. More specifically, the TensorFlow and Keras packages are used to build a model based on Indian monuments. Using Google Colab, the VGG16 model is trained on a variety of offline and online images. In order to enhance the performance of the model, the data augmentation technique is also used. Overall accuracy for VGG16 is 100 percent, which is extremely good. With the help of data augmentation on the model, a marginal improvement in accuracy is achieved. Lastly, this study's findings show that deep learning can be used to train a model, resulting in quite good outcomes even when trained on small and low-quality data sets.","Emotion plays a dominant role in speech. The same utterance with different emotions can lead to a completely different meaning. The ability to perform various of emotion during speaking is also one of the typical characters of human. In this case, technology trends to develop advanced speech emotion classification algorithms in the demand of enhancing the interaction between computer and human beings. This paper proposes a speech emotion classification approach based on the paralinguistic and spectral features extraction. The Mel-frequency cepstral coefficients (MFCC) are extracted as spectral feature, and openSMILE is employed to extract the paralinguistic feature. The machine learning techniques multi-layer perceptron classifier and support vector machines are respectively applied into the extracted features for the classification of the speech emotions. We have conducted experiments on the Berlin database to evaluate the performance of the proposed approach. Experimental results show that the proposed approach achieves satisfied performances. Comparisons are conducted in clean condition and noisy condition respectively, and the results indicate better performance of the proposed scheme.","Increasing privacy concerns have led to decentralized and federated machine learning techniques that allow individual clients to consult and train models collaboratively without sharing private information. Some of these applications, such as medical and healthcare, require the final decisions to be interpretable. One common form of data in these applications is multivariate time series, where deep neural networks, especially convolutional neural networks based approaches, have established excellent performance in their classification tasks. However, promising results and performance of deep learning models are a black box, and their decisions cannot always be guaranteed and trusted. While several approaches address the interpretability of deep learning models for multivariate time series data in a centralized environment, less effort has been made in a federated setting. In this work, we introduce FLAMES2Graph, a new horizontal federated learning framework designed to interpret the deep learning decisions of each client. FLAMES2Graph extracts and visualizes those input subsequences that are highly activated by a convolutional neural network. Besides, an evolution graph is created to capture the temporal dependencies between the extracted distinct subsequences. The federated learning clients only share this temporal evolution graph with the centralized server instead of trained model weights to create a global evolution graph. Our extensive experiments on various datasets from well-known multivariate benchmarks indicate that the FLAMES2Graph framework significantly outperforms other state-of-the-art federated methods while keeping privacy and augmenting network decision interpretation.","In this work, we address the United Nations Sustainable Development Goal 13: Climate Action by focusing on identifying public attitudes toward climate change on social media platforms such as Twitter. Climate change is threatening the health of the planet and humanity. Public engagement is critical to address climate change. However, climate change conversations on Twitter tend to polarize beliefs, leading to misinformation and fake news that influence public attitudes, often dividing them into climate change believers and deniers. Our paper proposes an approach to classify the attitude of climate change tweets (believe/deny/ambiguous) to identify denier statements on Twitter. Most existing approaches for detecting stances and classifying climate change tweets either overlook deniers\u2019 tweets or do not have a suitable architecture. The relevant literature suggests that emotions and higher levels of toxicity are prevalent in climate change Twitter conversations, leading to a delay in appropriate climate action. Therefore, our work focuses on learning stance detection (main task) while exploiting the auxiliary tasks of recognizing emotions and offensive utterances. We propose a multimodal multitasking framework MEMOCLiC that captures the input data using different embedding techniques and attention frameworks, and then incorporates the learned emotional and offensive expressions to obtain an overall representation of the features relevant to the stance of the input tweet. Extensive experiments conducted on a novel curated climate change dataset and two benchmark stance detection datasets (SemEval-2016 and ClimateStance-2022) demonstrate the effectiveness of our approach.","The automatic, sensor-based assessment of challenging behavior of persons with dementia is an important task to support the selection of interventions. However, predicting behaviors like apathy and agitation is challenging due to the large inter- and intra-patient variability. Goal of this paper is to improve the recognition performance by making use of the observation that patients tend to show specific behaviors at certain times of the day or week. We propose to identify such segments of similar behavior via clustering the distributions of annotations of the time segments. All time segments within a cluster then consist of similar behaviors and thus indicate a behavioral predisposition (BPD). We utilize BPDs by training a classifier for each BPD. Empirically, we demonstrate that when the BPD per time segment is known, activity recognition performance can be substantially improved.","A fuzzy rule-based classification system (FRBCS) is one of the most popular approaches used in pattern classification problems. One advantage of a fuzzy rule-based system is its interpretability. However, we\u2019re faced with some challenges when generating the rule-base. In high dimensional problems, we can not generate every possible rule with respect to all antecedent combinations. In this paper, by making the use of some data mining concepts, we propose a method for rule generation, which can result in a rule-base containing rules of different lengths. As the next phase, we use rule-weight as a simple mechanism to tune the classifier and propose a new method of rule-weight specification for this purpose. Through computer simulations on some data sets from UCI repository, we show that the proposed scheme achieves better prediction accuracy compared with other fuzzy and non-fuzzy rule-based classification systems proposed in the past.","Due to numerous deaths, colon cancer treatment and diagnosis are viewed as societal and financial challenges. The most severe reason for death worldwide is colorectal cancer. The classification of colon cancer tissues through images is presented in this paper as a multifaceted task. Classifying an illness at a premature stage increases its chances of existence, as late detection can be mortal which results in metastasis and a poor prognosis. The microscopic examination and classification of infected colon tissue sample images is a complex task. Also, the failure to manually detect the abnormality in the tissue by a pathologist might increase the severity of the disease. With the aid of intelligent machines, and automated diagnosis the classification of tissues from images can be done in much less time. These algorithms can learn by analyzing the patterns in the images and support the pathologist in completing the task with greater accuracy. In this research article, we proposed a tuned machine learning model, with the application of five machine learning techniques (K-Nearest Neighbor, Decision Trees, Random Forest, Categorical Boosting, and Gaussian Naive Bayes) for accurately classifying histopathological colon cancer tissues images of National Center for Tumor diseases Bank. The results demonstrate that the Categorical Boosting model has the best performance and is the most viable approach (accuracy: 0.9067, F1-Score: 0.9053, specificity: 0.9739, and sensitivity: 0.9757).","Highlights\n\u2022\nMultiple factors contributed to duration of flower induction (DFI) were explored.\n\u2022\nContribution of inherent tree and external environmental factors to DFI were classified.\n\u2022\nOptimal combination of timescales for air temperature and humidity was determined.\n\u2022\nRobust implicit and explicit machine learning predictive models for DFI were constructed.\nAbstract\nThe flower induction is a critical physiological change during which vegetative buds transit to floral buds. The duration of flower induction (DFI) for litchi plays determinative role for the success and the quality of flowering. It is hard to be reliably predicted because multiple factors including ages of trees, variety and dynamically environmental and climate variables have important impacts. Here we predicted the DFI for four litchi varieties using random forest (RF) implicit and stepwise regression (STR) explicit machine learning models. These models were trained and validated from the data consisting phenological phases from the mature of the last autumn shoots to the flower shedding, and the corresponding meteorological factors from 2009 to 2020. The DFI predictive models consider timescales from 1 h up to 10 days, and the determination coefficients from the 5-fold cross validation achieves 0.96 to 0.99. The high accuracy was maintained in the blind test, with the determination coefficients of 0.97\u20130.98 for the data in 2019 and 0.78\u20130.88 for 2020. The reliability is still sufficient to guide the cultivation of litchi although the dramatic climate change weaken the inherent multiple correlations for DFI. From these robust predictive models, we found ten important features that can be used to determine the DFI with consistent positive or negative contributions. They are the minimum cooling rate, the maturity time of the last autumn shoot, the maximum heating rate, the mean of minimum temperature, the mean of maximum temperature, the amount of heat accumulation at hot day with the minimum daily temperature above 22 \u00b0C, the maximum daily temperature below 26 \u00b0C, the minimum daily temperature below 6 \u00b0C, age, and the atmospheric relative humidity.","Graph Neural Networks (GNNs) have revolutionized graph learning through efficiently learned node embeddings and achieved promising results in various graph-related tasks such as node and graph classification. Within GNNs, a pooling operation reduces the size of the input graph by grouping nodes that share commonalities intending to generate more robust and expressive latent representations. For this reason, pooling is a critical operation that significantly affects downstream tasks. Existing global pooling methods mostly use readout functions like max or sum to perform the pooling operations, but these methods neglect the hierarchical information of graphs. Clique-based hierarchical pooling methods have recently been developed to overcome global pooling issues. Such clique pooling methods perform a hard partition between nodes, which destroys the topological structural relationship of nodes, assuming that a node should belong to a single cluster. However, overlapping clusters widely exist in many real-world networks since a node can belong to more than one cluster. Here we introduce a new hierarchical graph pooling method to address this issue. Our pooling method, named Quasi-CliquePool, builds on the concept of a quasi-clique, which generalizes the notion of cliques to extract dense incomplete subgraphs of a graph. We also introduce a soft peel-off strategy to find the overlapping cluster nodes to keep the topological structural relationship of nodes. For a fair comparison, we follow the same procedure and training settings used by state-of-the-art pooling techniques. Our experiments demonstrate that combining the Quasi-Clique Pool with existing GNN architectures yields an average improvement of 2% accuracy on four out of six graph classification benchmarks compared to other existing pooling methods.","Abundant spectral information of hyperspectral images (HSI) provides rich information for HSI classification, which often brings high dimensional data resulting in the dilemma between the demand for fine data and the limited resources such as computation, storage as well as transmission band-width. To address this issue, we propose a deep hierarchical feature representation model based on three-dimensional adaptive sampling and improved iterative shrinkage-threshold algorithm (ISTA) for HSI classification. Due to the adaptive sampling, we improve ISTA with deep learning network for spectral\u2013spatial feature representation since the ISTA is no longer applicable for the sampled data reconstruction. Through end-to-end joint learning, the proposed method can not only effectively reduce the required data, but also learn discriminative features for HSI classification, which will be meaningful for the HSI\u2019s transmission from the space satellites and fast classification. Experimental results demonstrate the effectiveness and superiority of the proposed method on three public HSI datasets.","Randomized-based Feedforward Neural Networks approach regression and classification (binary and multi-class) problems by minimizing the same optimization problem. Specifically, the model parameters are determined through the ridge regression estimator of the patterns projected in the hidden layer space (randomly generated in its neural network version) for models without direct links and the patterns projected in the hidden layer space along with the original input data for models with direct links. The targets are encoded for the multi-class classification problem according to the 1-of-J encoding (J the number of classes), which implies that the model parameters are estimated to project all the patterns belonging to its corresponding class to one and the remaining to zero. This approach has several drawbacks, which motivated us to propose an alternative optimization model for the framework. In the proposed optimization model, model parameters are estimated for each class so that their patterns are projected to a reference point (also optimized during the process), whereas the remaining patterns (not belonging to that class) are projected as far away as possible from the reference point. The final problem is finally presented as a generalized eigenvalue problem. Four models are then presented: the neural network version of the algorithm and its corresponding kernel version for the neural networks models with and without direct links. In addition, the optimization model has also been implemented in randomization-based multi-layer or deep neural networks. The empirical results obtained by the proposed models were compared to those reported by state-of-the-art models in the correct classification rate and a separability index (which measures the degree of separability in projection terms per class of the patterns belonging to the class of the others). The proposed methods show very competitive performance in the separability index and prediction accuracy compared to the neural networks version of the comparison methods (with and without direct links). Remarkably, the model provides significantly superior performance in deep models with direct links compared to its deep model counterpart.\nHighlights\n\u2022\nA RFNN model with bias in the output layer.\n\u2022\nOptimization of the reference points per class in the RFNN models (RVFL and ELM).\n\u2022\nRFNN model with an optimization that solves a generalized eigenvalue problem.\n\u2022\nExperimental and statistical validation in classification problems.\n\u2022\nCompetitive CCR performance in RVFL and ELM, and Deep models using RVFL.","Recently, MLP-based architectures achieved impressive results in image classification against CNNs and ViTs. However, there is an obvious limitation in that their parameters are related to image sizes, allowing them to process only fixed image sizes. Therefore, they cannot directly adapt dense prediction tasks (e.g., object detection and semantic segmentation) where images are of various sizes. Recent methods tried to address it but brought two new problems, long-range dependencies or important visual cues are ignored. This paper presents a new MLP-based architecture, Region-aware MLP (RaMLP), to satisfy various vision tasks and address the above three problems. In particular, we propose a well-designed module, Region-aware Mixing (RaM). RaM captures important local information and further aggregates these important visual clues. Based on RaM, RaMLP achieves a global receptive field even in one block. It is worth noting that, unlike most existing MLP-based architectures that adopt the same spatial weights to all samples, RaM is region-aware and adaptively determines weights to extract region-level features better. Impressively, our RaMLP outperforms state-of-the-art ViTs, CNNs, and MLPs on both ImageNet-1K image classification and downstream dense prediction tasks, including MS-COCO object detection, MS-COCO instance segmentation, and ADE20K semantic segmentation. In particular, RaMLP outperforms MLPs by a large margin (around 1.5% Apb or 1.0% mIoU) on dense prediction tasks. The training code could be found at https://github.com/xiaolai-sqlai/RaMLP.","Highlights\n\u2022\nUnbalanced residential energy consumption data classification problem is tackled.\n\u2022\nUrban vs rural energy consumption is classified in original and reduced feature space.\n\u2022\nMonte Carlo under-sampling is used to obtain distribution of classification performance metrics.\n\u2022\nSix family of classifiers were compared using 6 classification metrics for unbalanced data.\n\u2022\nHyperparameter tuning and optimal classifier pipeline were obtained using the genetic programming optimizer.\nAbstract\nEnergy consumer locations are required for framing effective energy policies. However, due to privacy concerns, it is becoming increasingly difficult to obtain the locational data of the consumers. Machine learning (ML) based classification strategies can be used to find the locational information of the consumers based on their historical energy consumption patterns. The ML methods in this paper are applied to the Residential Energy Consumption Survey 2009 dataset. In this dataset, the number of consumers in the urban area is higher than the rural area, thus making the classification problem unbalanced. The unbalanced classification problem has been solved in original and transformed or reduced feature space using Monte Carlo based under-sampling of the majority class datapoints. The hyperparameters for each classification algorithm family is represented as an optimized pipeline, obtained using the genetic programming (GP) optimizer. The classification performance metrics are then obtained for different algorithm families on the original and transformed feature spaces. Performance comparisons have been reported using univariate and bivariate distributions of the classification metrics viz. accuracy, geometric mean score (GMS), F1 score, precision, area under the curve (AUC) of receiver operator characteristics (ROC). The energy policy aspects for the urban and rural residential consumers based on the classification results have also been discussed.","X-ray nondestructive testing means are widely used in the inspection process of internal defects of parts. In practical inspection, defects are generally determined and rated by manual inspection based on X-ray images, which is inefficient and cannot meet the requirements of high-volume automatic inspection. This paper proposes an automatic defect classification model based on an improved U-Net. First, a classifier is added behind the encoder of U-Net. The encoder and classifier are connected in series to form the main branch of the model to complete the classification task. Second, the decoder of U-Net is improved by adding an attention module. The encoder and the improved decoder form the auxiliary branch of the model, which completes the segmentation task. The model proposed in this paper has two advantages. First, for small defects in images, the segmentation-based auxiliary task enables the model to focus on these small targets during the learning process and learn features with more representational power. Second, the introduction of an attention mechanism in the decoder can suppress the interference information, retain the effective location information, focus on the learning of defective regions, adjust the different feature mapping weights, and improve the performance of the model. The method has been validated on the self-built dataset and the dataset collected from X-ray inspection industrial sites and compared with typical image classification methods. The results show that the proposed method in this paper has higher defect recognition accuracy than other classical deep network models and can effectively identify multiple types of defect features while providing assurance and reference for the quality and safety performance of the parts to be inspected.","Skip Abstract Section\nAbstract\nAir quality prediction is considered one of complex problems. This is due to volatility, dynamic nature, and high variability in space and time of particulates and pollutants. Meanwhile, designing an automated model for monitoring and predicting air quality becomes more and more relevant, particularly in urban regions. Air pollution can significantly affect the environment and eventually citizens\u2019 health. In this paper, one of the popular machine learning algorithms, the neural network algorithm, is employed to classify different species of air pollutants. To boost the performance of the traditional neural network, the war strategy optimization algorithm tunes the neural network\u2019s parameters. The experimental results demonstrate that the proposed optimized neural network based on the war strategy algorithm can accurately classify air pollutant species.","For a long time, images have proved perfect at both storing and conveying rich semantics, especially human emotions. A lot of research has been conducted to provide machines with the ability to recognize emotions in photos of people. Previous methods mostly focus on facial expressions but fail to consider the scene context, meanwhile scene context plays an important role in predicting emotions, leading to more accurate results. In addition, Valence-Arousal-Dominance (VAD) values offer a more precise quantitative understanding of continuous emotions, yet there has been less emphasis on predicting them compared to discrete emotional categories. In this paper, we present a novel Multi-Branch Network (MBN), which utilizes various source information, including faces, bodies, and scene contexts to predict both discrete and continuous emotions in an image. Experimental results on EMOTIC dataset, which contains large-scale images of people in unconstrained situations labeled with 26 discrete categories of emotions and VAD values, show that our proposed method significantly outperforms state-of-the-art methods with 28.4% in mAP and 0.93 in MAE. The results highlight the importance of utilizing multiple contextual information in emotion prediction and illustrate the potential of our proposed method in a wide range of applications, such as effective computing, human-computer interaction, and social robotics.","Highlights\n\u2022\nCoincident misclassification points produced by several ML algorithms for false alarms.\n\u2022\nRobustness and accuracy compared to the results of applying the algorithms individually.\n\u2022\nSupport vector machine, decision tree model, k-nearest neighbors, and ensemble tree.\n\u2022\nSCADA and alarm log from a real case study based on three real wind turbines.\n\u2022\nAccuracy of 99%, misclassified points detects 6, 15% of false alarms.\nAbstract\nWind energy is one of the most relevant renewable energy sources and it is expected to play a major role in the global energy production. The supervisory control and data acquisition system collects large datasets from wind turbines generating large amounts of alarms. These alarms are false in several cases, producing unnecessary downtimes and costs that affect the competitiveness of the industry. Operators are requiring new data analysis techniques to ensure the detection of false alarm and proper operation and maintenance tasks. It is presented a novel approach based on the analysis of coincident misclassification points produced by several machine learning algorithms for false alarms detection in wind turbines. The proposed algorithms are support vector machine, decision tree model, k-nearest neighbors, and ensemble trees. The use of this methodology demonstrates an increase in robustness and accuracy compared to the results of applying the algorithms individually. These methods classify intervals with alarms by analyzing the dataset of supervisory control and data acquisition system and validated by holdout. A real case study based on three real wind turbines is presented to test the methodology, and the results showed an accuracy of 99% for the classification algorithms. The analysis of the misclassified points detected 6, 15% of false alarms in the case study, indicating that this approach is promising to detect false alarms of wind turbines.","There are many people with disabilities; it is estimated that 39 million people are blind and 246 million have limited vision, giving 285 million visually impaired people. Information and communication technologies can help disabled people achieve greater independence, quality of life, and inclusion in social activities by increasing, maintaining, or improving their functional capacities. This paper presents a significant evaluation of local and deep features for an automatic methodology for identifying banknotes. To determine the best local features, we evaluated a set of four point-of-interest detectors, two descriptors, seven ways of generating the image signature, and six classification methodologies. To define the deep features, we extract features using three pre-trained well-known CNNs. Additionally, we evaluated using a hybrid approach formed by combining local and deep features. In this situation, the features were selected according to their gain ratios and used as input to the classifier. Experiments performed on US dollar (USD), euro (EUR), and Brazilian real banknotes (BRL) obtained accuracy rates of 99.96%, 99.12%, and 96.92%, respectively.","Multi-label optimal margin distribution machine (mlODM) is an efficient algorithm for multi-label classification. Although it can achieve great generalization performance, it is inefficient for large-scale datasets due to the huge number of label pairs. Motivated by its sparse solution, in this paper, we propose a two-stage gap safe screening rule for accelerating mlODM, termed as TSSR. First, a sequential safe screening rule (SSSR) based on gap is designed to screen out part of redundant label pairs prior to training, which reduces the scale of mlODM. Compared with the previous DVI rule, our method ensures absolute safety without destroying efficiency. In the second stage, to further speed up the solving process, a dynamic safe screening rule (DSSR) is embedded into the solving algorithm DCDM when training the simplified mlODM. More importantly, the feasible solution generated in the first stage can promote the efficiency of DSSR. To the best of our knowledge, this is the first attempt to create a hybrid screening rule for multi-label model. Our TSSR can greatly reduce the cost and achieve exactly the same accuracy. Experimental results on seven multi-label benchmark datasets and two real-world learning problems including movie genres classification and hypoglycemic drugs prediction verify the superiority of the proposed methods.","Siamese Neural Networks (SNN) provide a robust mechanism to learn similarities/dissimilarities between objects of different classes. The distinguishing features learnt by SNNs make them a good candidate for multi-class classification as well. However, the potential of an SNN to create a classification space that has, both higher accuracy and lower inference time, needs to be exploited further. In this paper, we present a novel multi-class classification approach using SNNs by drawing concepts from the Immune Network theory. This bio-inspired strategy aids in injecting class specific characteristics into the SNN architecture, thereby enhancing the classification process. Experimental results conducted on three benchmark datasets indicate that the approach consistently provides higher accuracies and lesser inference times as compared to recent SNN based multi-class classification approaches, indicating its efficacy.","Face attribute classification (FAC) is a high-profile problem in biometric verification and face retrieval. Although recent research has been devoted to extracting more delicate image attribute features and exploiting the inter-attribute correlations, significant challenges still remain. Wavelet scattering transform (WST) is a promising non-learned feature extractor. It has been shown to yield more discriminative representations and outperforms the learned representations in certain tasks. Applied to the image classification task, WST can enhance subtle image texture information and create local deformation stability. This paper designs a scattering-based hybrid block, to incorporate frequency-domain (WST) and image-domain features in a channel attention manner (Squeeze-and-Excitation, SE), termed WS-SE block. Compared with CNN, WS-SE achieves a more efficient FAC performance and compensates for the model sensitivity of the small-scale affine transform. In addition, to further exploit the relationships among the attribute labels, we propose a learning strategy from a causal view. The cause attributes defined using the causality-related information can be utilized to infer the effect attributes with a high confidence level. Ablative analysis experiments demonstrate the effectiveness of our model, and our hybrid model obtains state-of-the-art results in two public datasets.","Tree augmented na\u00efve Bayes classifier (TAN) has been widely used in machine learning and data mining. To improve the flexibility and classification performance of TAN, this paper proposes a Flexible Tree Augmented Na\u00efve Bayes classifier (FTAN). In the FTAN, the mutual information contribution rate is used to measure the dependencies between attributes in the process of building the maximum weighted spanning tree. Then, a flexible filtering method is adopted to filter out edges with weak dependencies between attributes by dynamically adjusting the threshold. A range of experiments on UCI datasets reveals that the FTAN exhibits considerable advantages over other popular algorithms in terms of the 0-1 loss and class probability root mean square error. The FTAN is used to solve the problem of favourable distribution area prediction for the remaining oil and gas resources of the Jurassic Sangonghe Formation in the Junggar Basin. The application results show the effectiveness and superiority of the FTAN method, favourable areas of oil and gas resources are selected based on the FTAN prediction results. This provides a decision-making basis for optimising drilling strategies and oil and gas exploration targets.","We present a simple domain generalization baseline, which wins second place in both the common context generalization track and the hybrid context generalization track respectively in NICO CHALLENGE 2022. We verify the founding in recent literature, domainbed, that ERM is a strong baseline compared to recent state-of-the-art domain generalization methods and propose SimpleDG which includes several simple yet effective designs that further boost generalization performance. Code is available at https://github.com/megvii-research/ SimpleDG.","The Gene Ontology (GO) project is a major bioinformatics initiative with the aim of standardizing the representation of gene and gene product attributes across species and databases. The classes in GO are hierarchically structured in the form of a directed acyclic graph (DAG), what makes its prediction more complex. This work proposes an adapted Learning Classifier Systems (LCS) in order to predict protein functions described in the GO format. Hence, the proposed approach, called HLCS (Hierarchical Learning Classifier System) builds a global classifier to predict all classes in the application domain and its is expressed as a set of IF-THEN classification rules, which have the advantage of representing more comprehensible knowledge. The HLCS is evaluated in four different ion-channel data sets structured in GO terms and compared with a Ant Colony Optimisation algorithm, named hAnt-Miner. In the tests realized the HLCS outperformed the hAnt-Miner in two out of four data sets.","Lightweight CNN models aim to extend the application of deep learning from conventional image classification to mobile edge device-based image classification. However, the accuracy of lightweight CNN models currently is not as comparable as traditional large CNN models. To improve the accuracy of mobile platform-based image classification, we propose MobileACNet, a novel ACNet-based lightweight model based on MobileNetV3 (a popular lightweight CNN for image classification on mobile platforms). Our model adopts a similar idea to ACNet: consider global inference and local inference adaptively to improve the classification accuracy. We improve the MobileNetV3 by replacing the inverted residual block with our proposed adaptive inverted residual module (AIR). Experimental results show that our proposed MobileACNet can effectively improve the image classification accuracy by providing additional adaptive global inference on three public datasets, i.e., Cifar-100 dataset, Tiny ImageNet dataset, and a large-scale dataset ImageNet, for mobile-platform-based image classification.","In many real-world application domains, e.g., text categorization and image annotation, objects naturally belong to more than one class label, giving rise to the multi-label learning problem. The performance of multi-label learning greatly relies on the quality of available features, whereas the data generally involve a lot of irrelevant, redundant, even noisy features. This fact has led to that a surge of research on feature selection methods that select significant features for multi-label learning. Nevertheless, most of the previous approaches suffer from the deficiency that label-specific features are not taken into account, and they are also inefficient in exploiting labeling information such as local label correlations. Moreover, these methods lack interpretability, which can only find a feature subset for all labels, however, cannot show how features are related to different labels. Based on this, we present a new group-preserving label-specific feature selection (GLFS) framework for multi-label learning, which simultaneously considers the features special to the labels in the same group and specific features owned by each label to execute feature selection. In addition, we further consider to learn label-group and instance-group correlations for the exploitation of labeling information, and make a collaborative use of them to improve the model generalization. Extensive experiments validate the advantages of the proposed GLFS method.\nHighlights\n\u2022\nA group-preserving framework is designed for label-specific feature selection.\n\u2022\nLabel-group and instance-group correlations are used to enhance the generalization.\n\u2022\nAn alternating minimization algorithm is presented to seek the optimal solution.\n\u2022\nEmpirical studies are conducted to validate the advantages of the proposed method.","Based on the network structure and training methods of extreme learning machines, extreme learning machine combining hidden-layer feature weighting and batch training (ELM-WB) is proposed to make full use of representation-level features for object images and human action videos classification. To solve the problem of insufficient fusion of multiple representation-level features in most classification methods, a double hidden layer structure in which the input layer and the second hidden layer are directly connected is designed. A loop training method of weighting coefficients and output weights is proposed based on the advantages of this structure. The proposed network structure and training method are combined to construct an extreme learning machine combining hidden-layer feature weighting (ELM-W), which can effectively fuse representation-level features to enhance the classification ability of representation-level features. On this basis, the principle of online sequential ELM (OS-ELM) is introduced to update the loop training formula of the two weights to reduce the memory consumption during the operation of the overall algorithm. ELM-WB is proposed by combining the loop training formula of two weight matrices with batch training. In order to test the feasibility of ELM-WB, experiments are conducted on Caltech 101, MSRC, UCF11 and UCF101 databases. Experimental results prove that the proposed ELM-WB can improve classification accuracy by fusing representation-level features. At the same time, ELM-WB can be used to perform classification tasks on databases of any size in a general-purpose computer without specific hardware.","Many proteins are composed of two or more subunits, each associated with different polypeptide chains. The number and the arrangement of subunits forming a protein are referred to as quaternary structure. The quaternary structure of a protein is important, since it characterizes the biological function of the protein when it is involved in specific biological processes. Unfortunately, quaternary structures are not trivially deducible from protein amino acid sequences. In this work, we propose a protein quaternary structure classification method exploiting the functional domain composition of proteins. It is based on a nearest neighbor condensation technique in order to reduce both the portion of dataset to be stored and the number of comparisons to carry out. Our approach seems to be promising, in that it guarantees an high classification accuracy, even though it does not require the entire dataset to be analyzed. Indeed, experimental evaluations show that the method here proposed selects a small dataset portion for the classification (of the order of the 6.43%) and that it is very accurate (97.74%).","With the development of scientific research techniques, drug discovery has shifted from the serendipitous approach of the past to more targeted models based on an understanding of the underlying biological mechanisms of disease. However, there are hundreds or more of mechanism of action (MoA) data in the known drugs, which makes this process faced with complicated multi-label classification of text data. Traditional multi-label text classification algorithms will increase the complexity of the model and reduce the accuracy as the number of labels increases. Although deep learning algorithms can solve the problem of model complexity, they are currently only suitable for processing image format data. To overcome these problems, this study proposes a multi-label classification method based on Bayesian deep learning, which can convert non-image data format into image data, making it suitable for Convolutional neural network algorithm requirements. Then in the PyTorch environment, the Bayesian deep learning algorithm and the EfficientNet convolutional neural network are perfectly combined using the BLiTZ library to construct the Bayesian convolutional neural network model which named BCNNM. Not only improves the classification efficiency, this method also solves the problem of imbalanced classification of multi-label data, and fully considers the uncertainty in the neural network. In the process of drug development, this method has important practical significance for processing the multi-label classification of MoA data.","To address the problem of low classification accuracy of liquid dangerous goods in daily security screening technology, we propose a two-layer feature extraction classification algorithm based on Ultra-Wideband centimeter wave detection, which is composed of shallow Wavelet Transform-Autoencoder (WT-AE) and deep Attention-Gated Recurrent Unit (Attention-GRU) network. In order to abstract the best description feature, the shallow autoencoder adds a classification constraint. In the classification stage, the deep algorithm Attention-GRU algorithm can further abstract the sequence composed of shallow features into deep features to improve the accuracy of classification. The experimental results show that the WT-AE algorithm with shallow constraint is more suitable for feature extraction of UWB centimeter-wave signals in this experimental scene than PCA and ICA feature extraction algorithms. Compared with KNN, Linear kernel SVM, Gaussian kernel SVM and decision tree algorithms for sequence processing, Attention-GRU has better processing effect and higher accuracy of classification. By comparing the test accuracy of other algorithms, the double-layer feature classification algorithm performs better in this experimental scene. The final test accuracy can reach 95.8%.","In this paper, machine learning (ML) modeling is proposed for the detection and classification of global positioning system (GPS) spoofing in unmanned aerial vehicles (UAVs). Three testing scenarios are implemented in an outdoor yet controlled setup to investigate static and dynamic attacks. In these scenarios, authentic sets of GPS signal features are collected, followed by other sets obtained while the UAV is under spoofing attacks launched with a software-defined radio (SDR) transceiver module. All sets are standardized, analyzed for correlation, and reduced according to feature importance prior to their exploitation in training, validating, and testing different multiclass ML classifiers. The resulting performance evaluation of these classifiers shows a detection rate (DR), misdetection rate (MDR), and false alarm rate (FAR) better than 92%, 13%, and 4%, respectively, together with a sub-millisecond detection time. Hence, the proposed modeling facilitates accurate real-time GPS spoofing detection and classification for UAV applications.","In this work, we present a novel, fast clustering scheme for codebook generation from local features for object class recognition. It relies on a sequential data analysis and creates compact clusters with low variance. We compare our algorithm to other commonly used algorithms with respect to cluster statistics and classification performance. It turns out that our algorithm is the fastest for codebook generation, without loss in classification performance, when using the right matching scheme. In this context, we propose a well suited matching scheme for assigning data entries to cluster centers based on the sigmoid function.","Random forest is an efficient and accurate classification model, which makes decisions by aggregating a set of trees, either by voting or by averaging class posterior probability estimates. However, tree outputs may be unreliable in presence of scarce data. The imprecise Dirichlet model (IDM) provides workaround, by replacing point probability estimates with interval-valued ones. This paper investigates a new tree aggregation method based on the theory of belief functions to combine such probability intervals, resulting in a cautious random forest classifier. In particular, we propose a strategy for computing tree weights based on the minimization of a convex cost function, which takes both determinacy and accuracy into account and makes it possible to adjust the level of cautiousness of the model. The proposed model is evaluated on 25 UCI datasets and is demonstrated to be more adaptive to the noise in training data and to achieve a better compromise between informativeness and cautiousness.\nHighlights\n\u2022\nA cautious classifier based on random forests and belief functions is proposed.\n\u2022\nTree weights are automatically learned from data using a new cost function.\n\u2022\nThe cautiousness of the model is tuned using a single parameter.\n\u2022\nExtensive experiments demonstrate the interests of the proposed method.","Of all the terminal cancers that plague men, prostate cancer remains one of the most prevalent and ubiquitous. Data shows prostate cancer is the second leading cause of cancer death worldwide among men. About 11% of men have prostate cancer at some time during their lives. As it happens, we have dedicated our entire research to developing an approach that can improve the existing precision of prostate cancer diagnosis. In our research, we have dedicated a Transfer Learning approach for the Deep Learning model to compare the accuracy in results using Machine Learning classifiers. In addition, we evaluated individual performance in classifications with different evaluation measures using a Deep Learning pre-trained network, VGG16. During our evaluation, we assessed several performance metrics such as Precision, Recall, F1 Score, and Loss Vs. Accuracy for performance analysis. Upon implementing the Transfer Learning approach, we recorded the optimum performance using the VGG16 architecture compared to other popular Deep learning models such as MobileNet and ResNet. It is important to note that we have used the convolutional block and dense layers of VGG16 architecture to extract features from our image dataset. Afterward, we forwarded those features to Machine Learning classifiers to tabulate the final classification result. Upon successful tabulation, we have secured significant accuracy in prognostication using the Deep Machine Learning method in our research.","Mental health is a critical societal issue and early screening is vital to enabling timely treatment. The rise of text-based communications provides new modalities that can be used to passively screen for mental illnesses. In this paper we present an approach to screen for anxiety and depression through reply latency of text messages. We demonstrate that by constructing machine learning models with reply latency features. Our models screen for anxiety with a balanced accuracy of 0.62 and F1 of 0.73, a notable improvement over prior approaches. With the same participants, our models likewise screen for depression with a balanced accuracy of 0.70 and F1 of 0.80. We additionally compare these results to those of models trained on data collected prior to the COVID-19 pandemic. Finally, we demonstrate generalizability for screening by combining datasets which results in comparable accuracy. Latency features could thus be useful in multimodal mobile mental illness screening.","Active learning has achieved remarkable success in minimizing labeling costs for classification tasks with all data samples drawn from known classes. However, in real scenarios, most active learning methods fail when encountering open-set annotation (OSA) problem, i.e., numerous samples from unknown classes. The main reason for such failure comes from existing query strategies that are unavoidable to select unknown class samples. To tackle such problem and select the most informative samples, we propose a novel active learning framework named OSA-CQ, which simplifies the detection work of samples from known classes and enhances the classification performance with an effective contrastive query strategy. Specifically, OSA-CQ firstly adopts an auxiliary network to distinguish samples using confidence scores, which can dynamically select samples with the highest probability from known classes in the unlabeled set. Secondly, by comparing the predictions between auxiliary network, classification, and feature similarity, OSA-CQ designs a contrastive query strategy to select these most informative samples from unlabeled and known classes set. Experimental results on CIFAR10, CIFAR100 and Tiny-ImageNet show the proposed OSA-CQ can select samples from known classes with high information, and achieve higher classification performance with lower annotation cost than state-of-the-art active learning algorithms.","The dependence of the classification error on the size of a bagging ensemble can be modeled within the framework of Monte Carlo theory for ensemble learning. These error curves are parametrized in terms of the probability that a given instance is misclassified by one of the predictors in the ensemble. Out of bootstrap estimates of these probabilities can be used to model generalization error curves using only information from the training data. Since these estimates are obtained using a finite number of hypotheses, they exhibit fluctuations. This implies that the modeled curves are biased and tend to overestimate the true generalization error. This bias becomes negligible as the number of hypotheses used in the estimator becomes sufficiently large. Experiments are carried out to analyze the consistency of the proposed estimator.","With the rapid advancement in technology, the constant emergence of new applications and services has resulted in a drastic increase in Internet traffic, making it increasingly challenging for network analysts to maintain network security and classify traffic, especially when encrypted or tunneled. To address this issue, the proposed strategy aims to distinguish between regular traffic and traffic tunneled through a virtual private network and characterize traffic from seven different applications. The proposed approach utilizes various ensemble machine learning techniques, which are efficient and accurate and consume minimal computational time for training and prediction compared to conventional machine and deep learning models. These models were applied for both the classification and characterization of network traffic, deriving efficient results. The extreme and light gradient boosting algorithms performed well in multiclass classification, while AdaBoost and Light GBM performed well in binary classification. However, when all the datasets were merged and categorized into two classes and various feature engineering methods were applied, the proposed system achieved an accuracy of more than 99%, with minimal error scores using light GBM with min\u2013max scaling over stratified fivefold, thereby outperforming all existing approaches. This research highlights the efficiency and potential of the proposed model in detecting network traffic.","This paper investigates the effectiveness of different deep learning HTR families, including LSTM, Seq2Seq, and transformer-based approaches with self-supervised pretraining, in recognizing ciphered manuscripts from different historical periods and cultures. The goal is to identify the most suitable method or training techniques for recognizing ciphered manuscripts and to provide insights into the challenges and opportunities in this field of research. We evaluate the performance of these models on several datasets of ciphered manuscripts and discuss their results. This study contributes to the development of more accurate and efficient methods for recognizing historical manuscripts for the preservation and dissemination of our cultural heritage.","Deep Neural Network (DNN) inferences have been proven highly susceptible to carefully engineered adversarial perturbations, presenting a pivotal hindrance to real-world Computer Vision tasks. Most of the existing defenses have poor generalization ability due to their dependence on relatively limited Adversarial Examples (AE). Furthermore, the existing adversarial training necessitates continually retraining a target network with the sort of attack required to be repelled. The defense strategies that are primarily based on processing the perturbed image eventually fall short when pitted against constantly developing threats. Protection of DNN against adversarial attacks remains a difficult challenge on challenging datasets such as Fashion MNIST and CIFAR10. This paper proposes a GAN-based two-stage adversarial training model named Globally Connected and Trainable Hierarchical Fine Attention (GCTHFA). The first stage of the proposed GCTHFA GAN is to create a reconstructed image that is a purified version of an adversarial example. The proposed approach has used a trainable and globally linked attention map to teach the Generator about the different types of representations an image might have in different convolutional layers located at different levels in a network. The discriminator\u2019s reliance on feature vectors produced by transfer learning eliminates the traditional dependency on standard image pixels. The second step involves adversarial training of a target classifier to provide resistance to such attacks. Extensive testing on the MNIST, Fashion MNIST, and CIFAR10 datasets with different classifiers and attacks show that the proposed model can handle adversarial attack settings for various target models. The proposed model uses only one type of adversarial training, with no requirement for retraining based on the type of attack.","We show that standard machine learning algorithms may be trained to predict certain invariants of low genus arithmetic curves. Using datasets of size around 105, we demonstrate the utility of machine learning in classification problems pertaining to the BSD invariants of an elliptic curve (including its rank and torsion subgroup), and the analogous invariants of a genus 2 curve. Our results show that a trained machine can efficiently classify curves according to these invariants with high accuracies (&gt;0.97). For problems such as distinguishing between torsion orders, and the recognition of integral points, the accuracies can reach 0.998.","Ensemble learning consists of combining the prediction of different learners to obtain a final output. One key step for their success is the diversity among the learners. In this paper, we propose to reach the diversity in terms of the classification complexity by guiding the sampling of instances in the Bagging algorithm with complexity measures. The proposed Complexity-driven Bagging algorithm complements the classic Bagging algorithm by considering training samples of different complexity to cover the complexity space. Besides, the algorithm admits any complexity measure to guide the sampling. The proposal is tested in 28 real datasets and for a total of 9 complexity measures, providing satisfactory and promising results and revealing that training with samples of different complexity, ranging from easy to hard samples, is the best strategy when sampling based on complexity.","We present a methodology to analyze Multiple Classifiers Systems (MCS) performance, using the disagreement concept. The goal is to define an alternative approach to the conventional recognition rate criterion, which usually requires an exhaustive combination search. This approach defines a Distance-based Disagreement (DbD) measure using an Euclidean distance computed between confusion matrices and a soft-correlation rule to indicate the most likely candidates to the best classifiers ensemble. As case study, we apply this strategy to two different handwritten recognition systems. Experimental results indicate that the method proposed can be used as a low-cost alternative to conventional approaches.","Highlights\n\u2022\nThe current study extracts higher-order features from fire images.\n\u2022\nA new set of data is correctly labelled for classifying fire and non-fire.\n\u2022\nInformation-theoretic feature selection is adopted to minimize computational cost.\n\u2022\nThe SVM performs classification with an RBF kernel.\n\u2022\nThe model draws an overall accuracy of 96,21%, and a specificity of 97,99%.\n\u2022\nThe model draws an f-measure and g-mean values of 96,13% and 96,19% respectively.\nAbstract\nIn recent years, wildfires and forest fires have ravaged millions of hectares of the forest all over the world. Recent technological breakthroughs have increased interest in computer vision-based fire classification that classifies fire and non-fire pixels from image or video datasets. Fire pixels from an image or video can be classified using either a traditional machine learning approach or a deep learning approach. Presently, the deep learning approach is the mainstream in forest fire detection studies. Although deep learning algorithms can handle vast amounts of data, they ignore the variation in complexity among training samples and as a result, their training model performance is limited. Furthermore, deep learning approaches with little data and features perform poorly in real-world challenging fire scenarios. As a result, the current study adopts a machine learning technique to extract higher-order features from the processed images from the publicly available datasets: Corsican dataset and FLAME, and a private dataset: Firefront_Gestosa, for classifying fire and non-fire pixels. It should be emphasized that in machine learning applications, handling multidimensional data to train a model is challenging. Feature selection is used to overcome this problem by removing redundant or irrelevant data that has an impact on the model's performance. In this paper, information-theoretic feature selection approaches are used to choose the most important features for classification while minimizing the computational cost. The traditional machine classifier, Support Vector Machine (SVM) is adopted in the present work, that works on the discriminative features input selected from the feature selection technique. The SVM performs the classification of fire and non-fire pixels with a Radial Basis Function (RBF) kernel, and the model's performance is measured using assessment measures such as overall accuracy, sensitivity, specificity, precision, recall, F-measure, and G-mean. The model draws an overall accuracy of 96,21%, a sensitivity of 94,42%, a specificity of 97,99%, a precision of 97,91%, a recall of 94,42%, an f-measure and g-mean values of 96,13% and 96,19% respectively.","The core objective of this research is to develop a methodology for selecting a supervised machine learning classification technique based on the specific categories of objects that need to be classified. The study focuses on product categories extracted from Amazon's Product Reviews database, which are utilized to evaluate the subjectivity of post-purchase feedback. The primary supervised machine learning methods are utilized to efficiently perform the classification task. The resulting insights will enable the prioritization and choice of the best approach based on the selected categories. In the context of accelerated technological adoption due to the COVID-19 pandemic, this research contributes by showcasing how AI/ML can play a pivotal role in enhancing decision-making processes across various sectors and highlighting the significance of adapting to emerging technologies for sustainable growth.","The convolution neural network (CNN) not only has high fault tolerance but also has high computing capacity. The image classification performance of CNN has an important relationship with its network depth. The network depth is deeper, and the fitting ability of CNN is stronger. However, a further increase in the depth of CNN will not improve the accuracy of the network but will produce higher training errors, which will reduce the image classification performance of CNN. In order to solve the above problems, this paper proposes a feature extraction network, AA-ResNet with an adaptive attention mechanism. The residual module of the adaptive attention mechanism is embedded for image classification. It consists of a feature extraction network guided by the pattern, a generator trained in advance, and a complementary network. The feature extraction network guided by the pattern is used to extract different levels of features to describe different aspects of an image. The design of the model effectively uses the image information of the whole level and the local level, and the feature representation ability is enhanced. The whole model is trained as a loss function, which is about a multitask problem and has a specially designed classification, which helps to reduce overfitting and make the model focus on easily confused categories. The experimental results show that the method in this paper performs well in image classification for the relatively simple Cifar-10 dataset, the moderately difficult Caltech-101 dataset, and the Caltech-256 dataset with large differences in object size and location. The fitting speed and accuracy are high.","We present an action recognition scheme that integrates multiple modality of cues that include shape, motion and depth to recognize human gesture in the video sequences. In the proposed approach we extend classification framework that is commonly used in 2D object recognition to 3D spatio-temporal space for recognizing actions. Specifically, a boosting-based classifier is used that learns spatio-temporal features specific to target actions where features are obtained from temporal patterns of shape contour, optical flow and depth changes occuring at local body parts. The individual features exhibit different strength and sensitivity depending on many factors that include action, underlying body parts and background. In the current method, the multiple cues of different modalities are combined optimally by fisher linear discriminant to form a strong feature that preserve strength of individual cues. In the experiment, we apply the integrated action classifier on a set of target actions and evaluate its performance by comparing with single cue-based cases and present qualitative analysis of performance gain.","In clusters analysis, a problem of great interest is having methods that allow the representation of the topology of input space without the need to know additional information about it. This gives rise to growing competitive neural methods which are capable of determining the structure of the network autonomously during the process of training. This work proposes a variation of the Growing Neural Gas (GNG) algorithm, calling GNGwith post-pruning (GNG-PP), and a method of clustering based on the search for topological neighborhoods generated by the former. These were combined in a three-phase process to clustering the S&amp;P100 set, which belongs to the macroeconomic field. This problem has a high dimensionality in the characteristics space. Its results are compared to those obtained by SOM, Growing CellStructures (GCS), and a non-neural method. Evaluation of the results was made by means of the kappa coefficient, using as evaluation set the GICS industrial classification. The results show that when using the proposed methods the best clustering are generated, obtaining a kappa coefficient of 0.5643 in the GICS classification.","In this paper, we consider the problem of Novel Class Discovery (NCD) in Open Set Recognition (OSR). Given a labeled and an unlabeled set for training, NCD aims to discover the novel categories in the unlabeled set with prior knowledge learned from the labeled set. Existing approaches tackle the NCD problems under a close-set setting, where only the existing categories from the labeled set and the novel categories from the unlabeled set will occur during the inference. This paper considers a more realistic open-set scenario. In the open-set setting, in addition to the existing and novel categories, some unknown categories absent from the training could be present during inference. To address NCD in the open-set scenario, we propose the General Inter-Intra (GII) loss, a unified approach for learning representations from both labeled and unlabeled samples. The proposed approach discovers novel categories in the training set (NCD) meanwhile recognizes the unknown categories (OSR). We evaluate GII with image and graph datasets, and the results indicate that our proposed approach is more effective than other NCD and OSR approaches.","The problem of long-tailed recognition (LTR) has received attention in recent years due to the fundamental power-law distribution of objects in the real-world. Most recent works in LTR use softmax classifiers that are biased in that they correlate classifier norm with the amount of training data for a given class. In this work, we show that learning prototype classifiers addresses the biased softmax problem in LTR. Prototype classifiers can deliver promising results simply using Nearest-Class-Mean (NCM), a special case where prototypes are empirical centroids. We go one step further and propose to jointly learn prototypes by using distances to prototypes in representation space as the logit scores for classification. Further, we theoretically analyze the properties of Euclidean distance based prototype classifiers that lead to stable gradient-based optimization which is robust to outliers. To enable independent distance scales along each channel, we enhance Prototype classifiers by learning channel-dependent temperature parameters. Our analysis shows that prototypes learned by Prototype classifiers are better separated than empirical centroids. Results on four LTR benchmarks show that Prototype classifier outperforms or is comparable to state-of-the-art methods. Our code is made available at https://github.com/saurabhsharma1993/prototype-classifier-ltr.","Detecting COVID-19 as early as possible and quickly is one way to stop the spread of COVID-19. Machine learning development can help to diagnose COVID-19 more quickly and accurately. This report aims to find out how far research has progressed and what lessons can be learned for future research in this sector. By filtering titles, abstracts, and content in the Google Scholar database, this literature review was able to find 19 related papers to answer two research questions, i.e. what medical images are commonly used for COVID-19 classification and what are the methods for COVID-19 classification. According to the findings, chest X-ray were the most commonly used data to categorize COVID-19 and transfer learning techniques were the method used in this study. Researchers also concluded that lung segmentation and use of multimodal data could improve performance.","Automatically classifying cough sounds is one of the most critical tasks for the diagnosis and treatment of respiratory diseases. However, collecting a huge amount of labeled cough dataset is challenging mainly due to high laborious expenses, data scarcity, and privacy concerns. In this work, our aim is to develop a framework that can effectively perform cough classification even in situations when enormous cough data is not available, while also addressing privacy concerns. Specifically, we formulate a new problem to tackle these challenges and adopt few-shot learning and federated learning to design a novel framework, termed F2LCough, for solving the newly formulated problem. We illustrate the superiority of our method compared with other approaches on COVID-19 Thermal Face &amp; Cough dataset, in which F2LCough achieves an average F1-Score of 86%. Our results show the feasibility of few-shot learning combined with federated learning to build a classification model of cough sounds. This new methodology is able to classify cough sounds in data-scarce situations and maintain privacy properties. The outcomes of this work can be a fundamental framework for building support systems for the detection and diagnosis of cough-related diseases.","One of the major difficulties in face recognition while comparing photographs of individuals of different ages is the influence of age progression on their facial features. As a person ages, the face undergoes many changes, such as geometrical changes, changes in facial hair, and the presence of glasses, among others. Although biometric markers like computed face feature vectors should ideally remain unchanged by such factors, face recognition becomes less reliable as the age range increases. Therefore, this investigation was carried out to examine how the use of Embedded Prototype Subspace Classifiers could improve face recognition accuracy when dealing with age-related variations using face feature vectors only.","This is Part II of the two-part comprehensive survey devoted to a computing framework most commonly known under the names Hyperdimensional Computing and Vector Symbolic Architectures (HDC/VSA). Both names refer to a family of computational models that use high-dimensional distributed representations and rely on the algebraic properties of their key operations to incorporate the advantages of structured symbolic representations and vector distributed representations. Holographic Reduced Representations [321, 326] is an influential HDC/VSA model that is well known in the machine learning domain and often used to refer to the whole family. However, for the sake of consistency, we use HDC/VSA to refer to the field.\nPart I of this survey [222] covered foundational aspects of the field, such as the historical context leading to the development of HDC/VSA, key elements of any HDC/VSA model, known HDC/VSA models, and the transformation of input data of various types into high-dimensional vectors suitable for HDC/VSA. This second part surveys existing applications, the role of HDC/VSA in cognitive computing and architectures, as well as directions for future work. Most of the applications lie within the Machine Learning/Artificial Intelligence domain; however, we also cover other applications to provide a complete picture. The survey is written to be useful for both newcomers and practitioners.","The fraud detection literature unanimously shows that the use of a cardholder\u2019s transaction history as context improves the classification of the current transaction. Context representation is usually performed through either of two approaches. The first, manual feature engineering, is expensive, restricted, and hard to maintain as it relies on human expertise. The second, automatic context representation, removes the human dependency by learning new features directly on the fraud data with an end-to-end neural network. The LSTM and the more recent Neural Feature Aggregate Generator (NAG) are examples of such an approach. The architecture of the NAG is inspired by manual feature aggregates and addresses several of their limitations, primarily because it is automatic. However, it still has several drawbacks that we aim to address in this paper. In particular, we propose to extend the NAG in the following two main manners: (1) By expanding its expressiveness to model a larger panel of functions and constraints. This includes the possibility to model time constraints and additional aggregation functions. (2) By better aligning its architecture with the domain expert intuition on feature aggregates. We evaluate the different extensions of the NAG through a series of experiments on a real-world credit-card dataset consisting of over 60 million transactions. The extensions show comparable performance to the NAG on the fraud-detection task, while providing additional benefits in terms of model size and interpretability.","The imbalanced dataset\u2019s existing classification methods have low prediction accuracy for the minority class because of the little information present. Using over- and under-sampling techniques, we can improve the minority\u2019s ability to forecast outcomes. However, the minority class\u2019s accuracy of prediction is negatively impacted by the two methods due to the loss of vital information or the addition of irrelevant details for classification. SVM kernels have great abilities to handle asymmetric data, but when we need to use SVM kernels alone or as part of the ensemble technique for an unbalanced dataset, we don\u2019t have a strong reason to choose which kernel to use, and also how a particular kernel will act depends a lot on the data set. In this paper, we present a framework in which several kernel SVM (Linear, Polynomial, Sigmoid, RBF) classifiers were utilized as the base learners and one of the kernels (say RBF kernel) as meta learner using the Stacking Ensembles technique, which shows that stacked generalization of SVM kernels gives similar results as best performing kernel for an imbalanced dataset of software change proneness, using AUC, ROC, MCC, and BAS as an evaluation matrix.","Federated Learning (FL), also known as collaborative learning, is a distributed machine learning approach that collaboratively learns a shared prediction model without explicitly sharing private data. When dealing with sensitive data, privacy measures need to be carefully considered. Optimizers have a massive role in accelerating the learning process given the high dimensionality and non-convexity of the search space. The data partitioning in FL can be assumed to be either IID (independent and identically distributed) or non-IID. In this paper, we experiment with the impact of applying different adaptive optimization methods for FL frameworks in both IID and non-IID setups. We analyze the effects of label and quantity skewness, learning rate, and local client training on the learning process of optimizers as well as the overall performance of the global model. We evaluate the FL hyperparameter settings on biomedical text classification tasks on two datasets ADE V2 (Adverse Drug Effect: 2 classes) and Clinical-Trials (Reasons to stop trials: 17 classes).","Social play is essential in human interactions, increasing social bonding, mitigating stress, and relieving anxiety. With advancements in robotics, social robots can employ this role to assist in human-robot interaction scenarios for clinical and healthcare purposes. However, robotic intelligence still needs further development to match the wide spectrum of social behaviors and contexts in human interactions. In this paper, we present our robotic intelligence framework with a mutual learning paradigm in which we apply deep learning based on emotion recognition and behavior perception, through which the robot learns human movements and contexts through the interactive game of charades. Furthermore, we designed a gesture-based social game to provide a more empathetic and engaging social robot for the user. We also created a custom behavior database containing contextual behaviors for the proposed social games. A pilot study was conducted with participants ranging in age from 12 to 19 for a preliminary evaluation.","In many practical binary classification applications, such as financial fraud detection or medical diagnosis, it is crucial to optimize a model's performance on high-confidence samples whose scores are higher than a specific threshold, which is calculated by a given false positive rate according to practical requirements. However, the proportion of high-confidence samples is typically extremely small, especially in long-tailed datasets, which can lead to poor recall results and an alignment bias between realistic goals and loss. To address this challenge, we propose a novel loss reweighting framework called Momentum Threshold-Oriented Loss (MTOL) for binary classification tasks and propose two instantiated losses of it. Given a limited FPR range, MTOL aims to improve the recall of binary classification models at that FPR range by incorporating a batch memory queue and momentum estimation mechanisms. The MTOL adaptively estimates thresholds of FPR during the model training iterations and up-weights the loss of samples in the threshold range, with little consumption of storage and computation. Our experimental results on various datasets, including CIFAR-10, CIFAR-100, Tiny-ImageNet, demonstrate the significant effect of MTOL in improving the recall at low FPR especially in class imbalance settings. These results suggest that MTOL is a promising approach in scenarios where the model's performance in the low FPR range is of utmost importance.","Acute myocardial infarction (AMI) is the leading cause of hospital admissions and death all over the world and chest pain is the most common presenting complaint of AMI. Therefore, this paper proposes a machine learning (ML)-based prediction model for the in-hospital mortality in AMI patients with typical chest pain. To understand the principle of the black-box prediction model, a Shapley additive explanations (SHAP) method is applied to the ML-based prediction model. The experimental framework mainly includes three steps. First, we extract the experimental data from the Korea Acute Myocardial Infarction Registry National Institutes of Health (KAMIR-NIH), and then preprocess the selected data with missing value imputation, data normalization, and splitting. Thereafter, two kinds of data sampling methods such as synthetic minority oversampling techniques (SMOTE) and Adaptive Synthetic (ADASYN), are applied to handle the class imbalance problem on the experimental data. Second, different ML models such as decision tree, random forest, extreme gradient boosting (XGBoost), support vector machine, and logistic regression, are trained and evaluated on the preprocessed AMI patient data. Finally, the SHAP method is used to explain the best ML-based prediction model. The experimental results showed that the logistic regression with the ADASYN approach achieved the highest performance. Moreover, the SHAP technique enhanced the transparency of the ML model and can be a good reference for doctors to support their decisions in real life.","As a fast and inexpensive machining method applicable for creating a wide range of shapes and producing large batches, sheet metal punching is widely used e.g., in automotive, aerospace, electronics, and construction industries. A significant downside of sheet metal punching is the punching tool wear in use. A worn punch tool may impact the quality of the end product by causing imperfections and reduce the efficiency of the manufacturing process through increased scrap and by slowing down the production. Effective monitoring of punching tool wear is therefore essential for an efficient and cost-effective production of high-quality parts. The monitoring can be based on acceleration measurement which produces large amounts of raw data, making edge processing ideal as only the indication of the tool condition needs to be sent forward for decision support. Classification models for tool wear identification were built and compared in this study. The models are based on measured acceleration data. Two different open-source methods for time series feature extraction, namely TSFEL and MiniRocket, were tested and the classification results based on them compared. All methods used for building the models are computationally light and therefore applicable for real-time data processing at the edge. According to the results the MiniRocket algorithm is suitable for the task and superior compared to the TSFEL method. The classification accuracies based on the MiniRocket features are at best over 96.5 % and at worst around 84 %, whereas the corresponding accuracies are between 35 and 56 % for TSFEL feature based models. The use of the MiniRocket algorithm in building a model for punch tool monitoring shows very promising results. However, the dataset used was very limited. Therefore, further investigation is required based on an ampler dataset.","Detecting out-of-distribution (OOD) data is a task that is receiving an increasing amount of research attention in the domain of deep learning for computer vision. However, the performance of detection methods is generally evaluated on the task in isolation, rather than also considering potential downstream tasks in tandem. In this work, we examine selective classification in the presence of OOD data (SCOD). That is to say, the motivation for detecting OOD samples is to reject them so their impact on the quality of predictions is reduced. We show under this task specification, that existing post-hoc methods perform quite differently compared to when evaluated only on OOD detection. This is because it is no longer an issue to conflate in-distribution (ID) data with OOD data if the ID data is going to be misclassified. However, the conflation within ID data of correct and incorrect predictions becomes undesirable. We also propose a novel method for SCOD, Softmax Information Retaining Combination (SIRC), that augments softmax-based confidence scores with feature-agnostic information such that their ability to identify OOD samples is improved without sacrificing separation between correct and incorrect ID predictions. Experiments on a wide variety of ImageNet-scale datasets and convolutional neural network architectures show that SIRC is able to consistently match or outperform the baseline for SCOD, whilst existing OOD detection methods fail to do so. Code is available at https://github.com/Guoxoug/SIRC.","Dynamic functional connectivity network (DFCN) derived from resting-state functional magnetic resonance imaging (rs-fMRI), which characterizes the dynamic interaction between brain regions, has been applied to classification of brain diseases. However, existing studies usually focus on dynamic changes of low-order (i.e., pairwise) correlation of brain regions, thus neglecting their high-order dynamic information that could be important for brain disease diagnosis. Therefore, in this paper, we first propose a novel sparse learning based high-order DFCNs construction method, and then build a novel learning framework to extract high-level and high-order temporal features from the constructed high-order DFCNs for brain disease classification. The experimental results on 174 subjects from from the Alzheimer\u2019s Disease Neuroimaging Initiative (ADNI) demonstrate the effectiveness of our proposed method in comparison with state-of-the-art methods.","Cardiac magnetic resonance imaging (MRI) may suffer from motion-related artifacts resulting in non-diagnostic quality images. Therefore, image quality assessment (IQA) is essential for the cardiac MRI analysis. The CMRxMotion challenge aims to develop automatic methods for IQA. In this paper, given the limited amount of training data, we designed three special data augmentation techniques to enlarge the dataset and to balance the class ratio. The generated dataset was used to pre-train the model. We then randomly selected two multi-channel 2D images from one 3D volume to mimic sample inspection and introduced ResNet as the backbone to extract features from those two 2D images. Meanwhile, a channel-based attention module was used to fuse the features for the classification. Our method achieved a mean accuracy of 0.75 and 0.725 in 4-fold cross validation and the held-out validation dataset, respectively. The code can be found here ( https://github.com/xsunn/CMRxMotion).","Graph-based multi-view learning has attracted much attention due to the efficacy of fusing the information from different views. However, most of them exhibit high computational complexity. We propose an anchor-based bipartite graph embedding approach to accelerate the learning process. Specifically, different from existing anchor-based methods where anchors are obtained from key samples by clustering or weighted averaging strategies, in this article, the anchors are learned in a principled fashion which aims at constructing a distance-preserving embedding for each view from samples to their representations, whose elements are the weights of the edges linking corresponding samples and anchors. In addition, the consistency among different views can be explored by imposing a low-rank constraint on the concatenated embedding representations. We further design a concise yet effective feature collinearity guided feature selection scheme to learn tight multi-label classifiers. The objective function is optimized in an alternating optimization fashion. Both theoretical analysis and experimental results on different multi-label image datasets verify the effectiveness and efficiency of the proposed method.","This paper proposes a multi-domain sample classification method based on Baidu API's general object recognition function. we used three datasets in the experiment,including CIFAR-10, CIFAR-100, and Mini-ImageNet. For an unknown sample belonging to these three datasets, we first predict which domain it may belong to by using the output results of Baidu API, and then obtain the label of the sample by training a model on that domain. Compared with existing methods, our method reduces the number of high-performance models that need to be trained and reduces the computational difficulty. Experimental results show that our method is more convenient and accurate.","Currently, semi-supervised twin support vector machine based on Laplacian regularization (LapTSVM) have received extensive attention and research in many fields of machine learning. Unfortunately, Laplacian regularization has a constant null space, so the solution is often a constant function and cannot well maintain the local topology of the samples. Aiming the above urgent problems, this paper, we first construct a Hessian scatter regularization (HSR) term. HSR has two major advantages: (1) HSR prefers linear variation in function values along of the geodesic distance and maintains the local manifold structure of the samples well. (2) HSR tries to find the projection from the original space to the feature space to maximize the inter-class scatter and minimize the intra-class scatter of the samples; the scatter is regarded as the discriminative information (structural information) of samples. Secondly, by introducing HSR, we propose a Hessian scatter regularized twin support vector machine (HSR-TSVM). Compared with LapTSVM, HSR-TSVM uses the global and local structure information of the sample to overcome the shortcomings of insufficient extrapolation caused by Laplacian regularization, while retaining almost all the advantages of the classic LapTSVM. Furthermore, to improve the computational efficiency of HSR-TSVM, the least-squares version of HSR-TSVM, namely HSR-LSTSVM, is proposed, and the conjugate gradient method is used to solve it. Experimental results on four synthetic datasets, ten UCI datasets, and four image datasets show that the proposed methods are competitive with semi-supervised learning methods based on Laplacian regularization.\nHighlights\n\u2022\nThe Hessian Scatter Regularization (HSR) is constructed.\n\u2022\nThe Hessian Scatter Regularized TSVM (HSR-TSVM) is developed.\n\u2022\nThe least squares version of HSR-TSVM is proposed (HSR-LSTSVM).\n\u2022\nExtensive experiments on multiple datasets show that the our methods are competitive with other methods.","Power quality disturbances (PQDs) can lead to significant operational and financial losses in power systems. Accurate detection and classification of PQDs are essential for maintaining power quality and preventing power system failures. This research article introduces an innovative approach for the precise detection and classification of single- and multiple-state power quality disturbances (PQDs) using the Stockwell transform (ST) and a random forest classifier. To create realistic PQD signals, seventeen distinct classes are generated in accordance with IEEE Standard 1159, employing mathematical equations implemented in MATLAB software. The ST is employed to extract relevant features from the PQD signals, which are subsequently utilized as input for the random forest classifier. The classifier employs bootstrapping sampling to generate multiple training sets from the original dataset. Each training set is used to construct a decision tree by recursively partitioning the data based on significant features. To mitigate overfitting and enhance robustness, a random subset of features is selected at each node of the decision tree, thereby reducing tree correlation. The performance of the random forest classifier is compared with other widely utilized machine learning classifiers. The results exhibit the efficacy of the proposed approach in accurately detecting and classifying PQ events, highlighting its superiority over alternative methods.","Highlights\n\u2022\nFinite training sets lead to classifier error rates above the Bayes error rate.\n\u2022\nThe training set size required in a practical setting is a relevant information.\n\u2022\nMost approaches are purely experimental or consider just two classes.\n\u2022\nA theoretical expression for the relative excess over the Bayes error rate is provided.\nAbstract\nIn this paper, a theoretical learning curve is derived for the multi-class Bayes classifier. This curve fits general multivariate parametric models of the class-conditional probability density. The derivation uses a proxy approach based on analyzing the convergence of a statistic which is proportional to the posterior probability of the true class. By doing so, the curve depends only on the training set size and on the dimension of the feature vector; it does not depend on the model parameters. Essentially, the learning curve provides an estimate of the reduction in the excess of the probability of error that can be obtained by increasing the training set size. This makes it attractive in order to deal with the practical problems of defining appropriate training set sizes.","Highlights\n\u2022\nWe disclosed that only balancing the sample size cannot ensure fairness.\n\u2022\nWe propose a resampling method that dynamically minimizes the loss discrepancy between groups.\n\u2022\nWe introduce a new metric to evaluate the trade-off performance between fairness and accuracy.\n\u2022\nIt achieves the best trade-off performances between fairness and accuracy in various settings.\nAbstract\nExisting classification models often output discriminatory results since they learn the target attribute without addressing data imbalance with respect to the protected attributes (e.g., gender). The models tend to focus on learning toward demographic groups containing the larger number of training samples, which consequently leads to training loss discrepancy between the groups. Our work focuses on addressing the occurrence of training loss discrepancy between the groups to improve the model\u2019s fairness. To this end, we firstly define the target-protected group using the target and protected attribute labels and observe the group-wise training loss in terms of previous fairness approaches. From the observation, we figure out that balancing the total loss across all the groups allows to mitigate fairness issue significantly, and meanwhile, only considering the sample size of each group to obtain a balanced mini-batch is not enough for mitigating fairness. Motivated by the observations, we propose a fairness-aware batch sampling scheme that adaptively updates batch sampling probability (BSP) and constructs a fairness-aware mini-batch from the model\u2019s point of view. Our key idea is to balance the training losses via training with fairness-aware mini-batch. Through extensive experiments on two facial attribute benchmark datasets and one tabular dataset, our simple and effective sampling strategy achieves superior improvement in terms of two standard fairness metrics. We validate our algorithm with various experimental settings (e.g, multi-attribute classification, binary classification with multiple protected attributes). Moreover, we introduce a new metric for measuring the trade-off between fairness and classification performance. On this metric, our algorithm also achieves the best trade-off performance.","The Product Line Architecture (PLA) is one of the most important artifacts of a Software Product Line (SPL). PLA design can be formulated as an interactive optimization problem with many conflicting factors. Incorporating Decision Makers\u2019 (DM) preferences during the search process may help the algorithms find more adequate solutions for their profiles. Interactive approaches allow the DM to evaluate solutions, guiding the optimization according to their preferences. However, this brings up human fatigue problems caused by the excessive amount of interactions and solutions to evaluate. A common strategy to prevent this problem is limiting the number of interactions and solutions evaluated by the DM. Machine Learning (ML) models were also used to learn how to evaluate solutions according to the DM profile and replace them after some interactions. Feature selection performs an essential task as non-relevant and/or redundant features used to train the ML model can reduce the accuracy and comprehensibility of the hypotheses induced by ML algorithms. This work aims to select features of an ML model used to prevent human fatigue in an interactive search-based PLA design approach. We applied four selectors and through results we were able to reduce 30% of features, obtaining an accuracy of 99%.","To tackle the challenge of ineffective sentiment prediction using current sentiment classification methods, this paper introduces a method social network text sentiment classification. The method leverages a bidirectional short and long-term memory model (AT-BiLSTM), specifically designed for a big data environment. First, a vectorized representation of text is realized by introducing a pre-trained BERT model, and the classification results are dynamically adjusted according to the semantic information of the words. Then, the BiLSTM combined with the attention mechanism performs aspect-level sentiment analysis, and the corresponding model AT-BiLSTM is formulated. Finally, the BERT model randomly selects input tags for information masking and pre-trains the proposed model. The proposed method was evaluated against three alternative methods using an identical dataset. The results show that the novel method achieved the highest accuracy, recall, and F1-score, reaching 93.72%, 93.91%, and 92.38%, respectively. Consequently, the proposed method demonstrates superior performance compared to the other three methods evaluated.","Highlights\n\u2022\nWAFs diagnosis model of mine ventilation system based on SVM is constructed.\n\u2022\nThe influence law of sample attributes on WAFs diagnosis model is obtained.\n\u2022\nProvide technical support for realizing intelligent mine ventilation.\nAbstract\nRapid diagnosis and mitigation of windage alteration faults (WAFs) in the mine ventilation system is of great significance to create a good working environment and ensure safe production. The complexity of the ventilation system state evolution in time and space, and the high dimensionality of the feature set characterizing the system state leads to an exceptionally complex fault diagnosis. Therefore, this paper proposes to build a WAFs diagnosis model based on support vector machine (SVM). Meanwhile, the generalization performance of the model is significantly affected by the sample attributes. Therefore, the generalization errors of the model under the combination of four factors, i.e., sample dispersion, sample numbers, input features, and feature numbers, are further analyzed. The results show that the prediction effect of the model leads to a law that with the increase of the degree of sample dispersion, the number of samples, and the number of input features, it improves at the beginning and becomes stable in the end. Furthermore, the best feature for fault diagnosis is found as the combination of wind volume and wind pressure features. Finally, the correctness of the conclusion is verified by comparative experiments. This research provides useful guidance and valuable reference for how to build machine learning (ML) models for mine ventilation system fault diagnosis.","Extreme Classification (XC) seeks to tag data points with the most relevant subset of labels from an extremely large label set. Performing deep XC with dense, learnt representations for data points and labels has attracted much attention due to its superiority over earlier XC methods that used sparse, hand-crafted features. Negative mining techniques have emerged as a critical component of all deep XC methods, allowing them to scale to millions of labels. However, despite recent advances, training deep XC models with large encoder architectures such as transformers remains challenging. This paper notices that memory overheads of popular negative mining techniques often force mini-batch sizes to remain small and slow training down. In response, this paper introduces NGAME, a light-weight mini-batch creation technique that offers provably accurate in-batch negative samples. This allows training with larger mini-batches offering significantly faster convergence and higher accuracies than existing negative sampling techniques. NGAME was found to be up to 16% more accurate than state-of-the-art methods on a wide array of benchmark datasets for extreme classification, as well as 3% more accurate at retrieving search engine queries in response to a user webpage visit to show personalized ads. In live A/B tests on a popular search engine, NGAME yielded up to 23% gains in click-through-rates. Code for NGAME is available at https://github.com/Extreme-classification/ngame","Text data available on the Web are generally unstructured. Text classification, a machine learning technique, has proven to be a great alternative to structure textual data in a cost-effective, faster, and scalable manner. This study examines the feature space of Multilayer ELM (ML-ELM) for the classification of text data with the help of a novel feature selection technique termed as Correlation-based Feature Selection (CRFS). Experimental results show that the feature space of ML-ELM is better for text classification compared to the traditional vector space.","Highlights\n\u2022\nPropose logistic-BWE model with heterogeneous balancing and weighting effects.\n\u2022\nBuild sub models on data sets with different imbalance ratios.\n\u2022\nA dynamic weighting method is proposed to enhance the flexibility and accuracy.\n\u2022\nThe model can improve the prediction performance especially for default samples.\n\u2022\nThe model is robustness while maintaining the interpretability of results.\nAbstract\nThe logistic regression model is widely used in credit scoring practice due to its strong interpretability of results, but its recognition performance for default samples which are minority in real-world imbalanced data sets need to be improved. This paper designs a novel ensemble model based on logistic regression as the logistic-BWE model. It first carries out data preprocessing, then applying sample balancing algorithm to generate several training sub data sets with different imbalance ratios and constructing sub models respectively, finally according to the performance of each sub model in the validation stage, the weight of predicted results for different class of each sub model is dynamically calculated. The empirical results indicate that compared with ten representative credit scoring models on six public data sets, the logistic-BWE model has the strongest ability to recognize default samples, and has the best generalization ability on most data sets while maintaining the interpretability. Further tests demonstrate that the performance superiority of the logistic-BWE model is statistically significant, and it also has excellent robustness when it contains a sufficient number of sub models.","Multi-label learning deals with a kind of problem that the given samples areassociated with multiple labels simultaneously. Recently, multi-label learning has become a populartopic in the literatures of machine learning and has attracted lots of researches. In this paper, we propose a new multi-view multi-label learning method by considering the label correlation, which is called ELSMML. Based on the high-order strategy, we construct a crafted label correlation matrix to describe the relationships among labels. We further utilize multi-view learning and dimension reduction to exploit the high-level latent semantic label information and the latent feature information, so as to build a classifier in the low dimensional space. In addition, we apply manifold regularization terms to make the data samples in the low dimensional space have the same intrinsic structure as the original data. After that, we put forward the accelerated proximal gradient method to optimize the ELSMML model and obtain thepredictive classifier. Besides, we conduct convergence analysis and computational complexity analysis for ELSMML method. In the experiments, the ELSMML method can achieve better performance on the evaluation metrics compared with other baselines.","The emergence of novel types of communication, such as email, has been brought on by the development of the internet, which radically concentrated the way in that individuals communicate socially and with one another. It is now establishing itself as a crucial aspect of the communication network which has been adopted by a variety of commercial enterprises such as retail outlets. So in this research paper, we have built a unique spam-detection methodology based on email-body sentiment analysis. The proposed hybrid model is put into practice and preprocessing the data, extracting the properties, and categorizing data are all steps in the process. To examine the emotive and sequential aspects of texts, we use word embedding and a bi-directional LSTM network. this model frequently shortens the training period, then utilizes the Convolution Layer to extract text features at a higher level for the BiLSTM network. Our model performs better than previous versions, with an accuracy rate of 97\u201398%. In addition, we show that our model beats not just some well-known machine learning classifiers but also cutting-edge methods for identifying spam communications, demonstrating its superiority on its own. Suggested Ensemble model\u2019s results are examined in terms of recall, accuracy, and precision","Compositional zero-shot learning (CZSL) refers to recognizing unseen compositions of known visual primitives, which is an essential ability for artificial intelligence systems to learn and understand the world. While considerable progress has been made on existing benchmarks, we suspect whether popular CZSL methods can address the challenges of few-shot and few referential compositions, which is common when learning in real-world unseen environments. To this end, we study the challenging reference-limited compositional zero-shot learning (RL-CZSL) problem in this paper, i.e., given limited seen compositions that contain only a few samples as reference, unseen compositions of observed primitives should be identified. We propose a novel Meta Compositional Graph Learner (MetaCGL) that can efficiently learn the compositionality from insufficient referential information and generalize to unseen compositions. Besides, we build a benchmark with two new large-scale datasets that consist of natural images with diverse compositional labels, providing more realistic environments for RL-CZSL. Extensive experiments in the benchmarks show that our method achieves state-of-the-art performance in recognizing unseen compositions when reference is limited for compositional learning.","External knowledge (a.k.a. side information) plays a critical role in zero-shot learning (ZSL) which aims to predict with unseen classes that have never appeared in training data. Several kinds of external knowledge, such as text and attribute, have been widely investigated, but they alone are limited with incomplete semantics. Some very recent studies thus propose to use Knowledge Graph (KG) due to its high expressivity and compatibility for representing kinds of knowledge. However, the ZSL community is still in short of standard benchmarks for studying and comparing different external knowledge settings and different KG-based ZSL methods. In this paper, we proposed six resources covering three tasks, i.e., zero-shot image classification (ZS-IMGC), zero-shot relation extraction (ZS-RE), and zero-shot KG completion (ZS-KGC). Each resource has a normal ZSL benchmark and a KG containing semantics ranging from text to attribute, from relational knowledge to logical expressions. We have clearly presented these resources including their construction, statistics, data formats and usage cases w.r.t. different ZSL methods. More importantly, we have conducted a comprehensive benchmarking study, with a few classic and state-of-the-art methods for each task, including a method with KG augmented explanation. We discussed and compared different ZSL paradigms w.r.t. different external knowledge settings, and found that our resources have great potential for developing more advanced ZSL methods and more solutions for applying KGs for augmenting machine learning. All the resources are available at https://github.com/China-UK-ZSL/Resources_for_KZSL.","With the continuous development of network technology, the volume of encrypted traffic from unknown applications rises sharply, posing a significant challenge to conventional traffic classification methods. While these methods achieve a certain level of success in recognizing specific application traffic, they fail to classify unknown traffic, especially encrypted traffic. Existing traffic classification methods are usually constrained by the assumption that classes encountered in testing are also present in training, which is not consistent with the open environment of the real world. In this paper, we propose a novel data skew-based classification method for Transport Layer Security (TLS) application unknown traffic (DSCU) to achieve consistent classification of TLS applications. First, DSCU constructs skew data, and then the one-class classifiers generated based on the skew data limit the input space scope of the known class and reserve space for the unknown class. This enables DSCU to separate known flows (i.e., flows from applications contained in the training set) from unknown flows (i.e., flows without any application information regarding them during training). After separation, the fine-grained classification of known flows can improve the accuracy of known flow classification. Three groups of experiments conducted on a real-world dataset covering 25 applications show that DSCU reliably achieves outstanding performance on TLS flow classification.\nHighlights\n\u2022\nA multi-objective encrypted classify both unknown and known flows.\n\u2022\nSkew data are used to generate one-class classifiers.\n\u2022\nOvercome the need to reset thresholds owing to changes in unknown ratios.","Learning unbiased node representations for imbalanced samples in the graph has become a more remarkable and important topic. For the graph, a significant challenge is that the topological properties of the nodes (e.g., locations, roles) are unbalanced (topology-imbalance), other than the number of training labeled nodes (quantity-imbalance). Existing studies on topology-imbalance focus on the location or the local neighborhood structure of nodes, ignoring the global underlying hierarchical properties of the graph, i.e., hierarchy. In the real-world scenario, the hierarchical structure of graph data reveals important topological properties of graphs and is relevant to a wide range of applications. We find that training labeled nodes with different hierarchical properties have a significant impact on the node classification tasks and confirm it in our experiments. It is well known that hyperbolic geometry has a unique advantage in representing the hierarchical structure of graphs. Therefore, we attempt to explore the hierarchy-imbalance issue for node classification of graph neural networks with a novelty perspective of hyperbolic geometry, including its characteristics and causes. Then, we propose a novel hyperbolic geometric hierarchy-imbalance learning framework, named HyperIMBA, to alleviate the hierarchy-imbalance issue caused by uneven hierarchy-levels and cross-hierarchy connectivity patterns of labeled nodes. Extensive experimental results demonstrate the superior effectiveness of HyperIMBA for hierarchy-imbalance node classification tasks.","In recent years, the increasing use of online surveys for course evaluation in schools has led to an outpouring of evaluation texts. These texts, with their emotional polarity, can give schools the most direct feedback. Emotional analysis on course evaluation, therefore, has great implications. However, the not-so-rigid text grammar and rich text content pose a challenge for sentiment analysis in Chinese course evaluation. To solve this problem, this paper proposes a sentiment classification model BiLSTM-GCN-Att (BGAN). Here, BiLSTM is used to extract the features of the text and output the hidden state vector. Then, the deep biaffine attention mechanism is used to analyze the dependence of the text and generate a dependency matrix. Next, input the hidden state vector to the GCN. Finally, the softmax function is used as the output layer of the model to perform sentiment classification. The model proves effective and experimental results, showing that the BGAN achieved a maximum improvement of 11.02% and 14.47% in precision and F1-score respectively compared with the classical models.","This paper introduces a novel method for classifier pool generation in which a two-level strategy explores diversity in both data complexity and classifier decision spaces. The rationale is to induce pool members using data subsets representing subproblems with different difficulties while promoting diversity in classifiers\u2019 decisions. Two possible variants of the proposed method with a focus on maximum dispersion and maximum accuracy are presented. These differ in the property used to define the best pool of classifiers provided by an optimization process. A robust experimental protocol encompassing 28 classification datasets shows that the proposed pool generation provided the best accuracy on 327 over 336 experiments (97.3%) when compared to well-known pool generation methods to provide multiple classifier systems with and without dynamic selection.\nHighlights\n\u2022\nClassifier pool creation using diversity in the data complexity and decision spaces.\n\u2022\nTraining of classifiers on subsets of data showing different levels of difficulty.\n\u2022\nPool generation appropriated to dynamic selection of classifiers and ensembles.","Automated machine learning (AutoML) technologies offer powerful methods to automate the choice of meta-parameters and the instantiations of components of the machine learning training pipelines, such as an optimum form of data preprocessing or a suitable strength of model regularization. Besides given training data, AutoML relies on a suitable learning objective or scoring function and a search space in which to optimize the choices. Currently, most AutoML technologies focus on a single objective, which is related to the expected accuracy of the found model as evaluated according to the chosen learning objective. Additional desired characteristics such as model sparsity for an increased model efficiency and interpretability can be integrated as additional penalty terms in the objective function. Yet, this leads to one solution only, and does not mediate in between accuracy and sparsity as two usually contradicting objectives. In this contribution, we are interested in AutoML technologies which explore the full Pareto-front of sparse versus accurate models rather than a single average only. Since it is not guaranteed that architectural and meta-parameter choices stay constant along the full Pareto-front, averaging the two objectives is not necessarily optimal in this realm. Hence we propose how to treat this challenge by a novel iterative pipeline, which combines an AutoML method with feature selection technologies. We compare this technology to alternatives including baselines in two relevant modeling tasks, classification and outlier detection. We demonstrate the performance of these strategies for a couple of benchmark tasks. This contribution has been invited as a considerably extended version of the conference contribution [1]. The latter restricts to the scenario of classification only.","In order to improve the effect of glaucoma fundus image classification, a new algorithm based on decision tree and UNet++ was proposed. Firstly, the image is divided into three channels of RGB, and the extracted green channel image is enhanced with the Butterworth parameter function of the fusion power function. Then the improved UNet++ network model is used to extract the texture features of the fundus image, and the residual module is used to enhance the texture features. The results of the experiment show that the average accuracy, the average specificity and the average sensitivity of the improved algorithm increase by 9.2%, 6.4% and 6.5% respectively. The improved algorithm is effective in glaucoma fundus image classification.","This paper focuses on the impact of rule representation in Michigan-style Learning Fuzzy-Classifier Systems (LFCSs) on its classification performance. A well-representation of the rules in an LFCS is crucial for improving its performance. However, conventional rule representations frequently need help addressing problems with unknown data characteristics. To address this issue, this paper proposes a supervised LFCS (i.e., Fuzzy-UCS) with a self-adaptive rule representation mechanism, entitled Adaptive-UCS. Adaptive-UCS incorporates a fuzzy indicator as a new rule parameter that sets the membership function of a rule as either rectangular (i.e., crisp) or triangular (i.e., fuzzy) shapes. The fuzzy indicator is optimized with evolutionary operators, allowing the system to search for an optimal rule representation. Results from extensive experiments conducted on continuous space problems demonstrate that Adaptive-UCS outperforms other UCSs with conventional crisp-hyperrectangular and fuzzy-hypertrapezoidal rule representations in classification accuracy. Additionally, Adaptive-UCS exhibits robustness in the case of noisy inputs and real-world problems with inherent uncertainty, such as missing values, leading to stable classification performance.","Support Vector Machines (SVM) are an efficient alternative for supervised classification. In the soft margin SVM model, two different objectives are optimized and the set of alternative solutions represent a Pareto-front of points, each one of them representing a different classifier. The performance of these classifiers can be evaluated and compared through some performance metrics that follow from the confusion matrix. Moreover, when the SVM includes feature selection, the model becomes hard to solve. In this paper, we present an alternative SVM model with feature selection and the performance of the new classifiers is compared to those of the classical soft margin model through some performance metrics based on the confusion matrix: the area under the ROC curve, Cohen\u2019s Kappa coefficient and the F-Score. Both the classical soft margin SVM model with feature selection and our proposal have been implemented by metaheuristics, given the complexity of the models to solve.\nHighlights\n\u2022\nTwo different models for the SVM with feature selection are compared.\n\u2022\nThe models have been implemented by metaheuristics.\n\u2022\nDifferent metrics have been used to compare the models.","Environmental sound classification (ESC) is the most trending research areas. The sounds in the surroundings such as screaming, air conditioners, and rain droplets can help in the development of context-aware applications. It is complex to process the envi- ronmental sounds as compared to speech and music due to the unstructured essence of environmental sounds. In the past, certain preprocessing techniques, feature extraction, and classification algorithms are used for ESC. Several researchers have applied ma- chine learning classifiers for ESC and certain ensemble classifiers are also used but the accuracy can be increased if instead of combining homogeneous classifiers, heterogeneous classifiers can be ensembled. In this paper, a hybrid ensemble classifier is used for ESC on the UrbanSound8k dataset and cepstral features Mel Frequency Cepstral Coefficients are used. Five different machine learning classifiers- Decision Tree, Support Vector Machine, Logistic Regression, K- Nearest Neighbour, and Naive Bayes are used to develop a hybrid ensemble model. The highest accuracy is obtained when all the five classifiers are combined. The proposed approach gives an accuracy of 79.4% and is compared with the benchmark results using individual classifiers and the former out- performs the latter. The results of the hybrid ensemble model on the UrbanSound8K dataset are also compared with the dataset ESC-10.","International Classification of Disease (ICD) coding is to assign standard codes, which describe the state of a patient, to a clinical note. It is challenging given the complexity and the number of codes. The ICD taxonomy is hierarchically organized with several level codes (chapter, category, subcategory and its subdivision). However, most existing studies focus on the prediction of the fine-grained subcategory codes, neglecting the hierarchical relations of ICD codes. Those models pay less attention to common features related to sibling subcategories. The common features could be helpful for rare sample prediction and could be captured in the task of coarse-grained code prediction. In this paper, we propose a multi-task learning model, which explicitly trains multiple classifiers for different code levels. Simultaneously, we capture the relations between finer-grained and coarser-grained labels through a reinforcement mechanism. Extensive experiments on an English and a Chinese dataset show that our approach achieves competitive performance compared with baseline models, especially on Macro-F1 results.","Electroencephalography (EEG) signals are crucial data to understand brain activities. Thus, many papers have been proposed about EEG signals. In particular, machine learning techniques have been used/presented to extract information from EEG signals. However, there are limited works on sentence classification using this data. To fill this gap, we propose an automated EEG signal classification model. In this model, we have presented a new molecular-based feature extractor, which utilizes a graph of the testosterone molecular structure. The proposed testosterone graph-based pattern is a nature-inspired pattern. The motivation is to show the feature extraction capability of the chemical-based graphs. Hence, we presented a hand-modeled EEG classification architecture. Our architecture uses wavelet packet decomposition (WPD) to generate wavelet bands to extract low and high-level features. The statistical feature extraction function has been used to generate statistical features, and our proposed testosterone pattern (TesPat) generates textural features. A feature selector has been used to choose the most informative features (neighborhood component analysis). Channel-wise results have been calculated by deploying a shallow classifier (k nearest neighbors). Majority voting has been conducted to create general results, and our proposed model selects the best-resulted predicted labels vector. Our proposed model attained a classification accuracy of &gt;97% with 10-fold cross-validation (CV) and &gt;91% with leave-one subject out (LOSO) CV. Our high classification results demonstrate that our presented system is an accurate and robust sentence classification model. The novelty of this work is the development of an accurate testosterone-based learning model using three EEG sentence datasets.","Highlights\n\u2022\nImbalance can be mitigated by rebalancing (costs, population) or ensemble learning.\n\u2022\nAsymmetric label switching creates diversity in ensemble learning.\n\u2022\nRebalancing and switching can be combined in a principled way.\n\u2022\nOptimum decision thresholds for these combinations are analytically derived.\n\u2022\nA gating network aggregating the learners contributions improves performance.\nAbstract\nAsymmetric label switching is an effective and principled method for creating a diverse ensemble of learners for imbalanced classification problems. This technique can be combined with other rebalancing mechanisms, such as those based on cost policies or class proportion modifications. In this study, and under the Bayesian theory framework, we specify the optimal decision thresholds for the combination of these mechanisms. In addition, we propose using a gating network to aggregate the learners contributions as an additional mechanism to improve the overall performance of the system.","Domain Generalization (DG) requires a model to learn a hypothesis from multiple distributions that generalizes to an unseen distribution. Recent explorations show that, for neural networks, the choice of hyper-parameters and model architecture significantly affects DG performance, and making the right choice is non-trivial. In this paper, we show evidence suggesting that the models that perform better at DG, might be implicitly learning a low dimensional representation in the feature space. Furthermore, we take forward this idea and employ explicit feature learning to improve DG. To this end, we propose a DG specific supervised contrastive loss. We show how this performance improvement correlates to reduced dimensionality of the representation. Our work establishes new state-of-the-art on five different DG benchmarks, compared against over two dozen existing approaches in DomainBed. We show how this performance improvement correlates to reduced dimensionality of the representation.","We introduce Robust Training with Trust Scores (RT2S), a framework to train machine learning classifiers with potentially noisy labels. RT2S calculates a trust score for each training sample, which indicates the quality of its corresponding label. These trust scores are employed as sample weights during training and optionally during threshold optimization. The trust scores are generated from two sources: (i) the model's confidence in the observed label, leveraging out-of-fold prediction scores to detect anomalous labels in the training data, and (ii) the probability of the correct label, ascertained by a Large Language Model with the ability to identify biased label noise. We evaluate RT2S by training machine learning models on 6 product classification datasets that utilize low-quality labels generated by a rule-based classification engine acting as a surrogate labeler. Our experimental findings indicate that RT2S outperforms all baselines, and achieves an average accuracy improvement of 4.38% (max 7.18%) over rule-based classifiers in particular.","The rawly collected training data often comes with separate noisy labels collected from multiple imperfect annotators (e.g., via crowdsourcing). A typical way of using these separate labels is to first aggregate them into one and apply standard training methods. The literature has also studied extensively on effective aggregation approaches. This paper revisits this choice and aims to provide an answer to the question of whether one should aggregate separate noisy labels into single ones or use them separately as given. We theoretically analyze the performance of both approaches under the empirical risk minimization framework for a number of popular loss functions, including the ones designed specifically for the problem of learning with noisy labels. Our theorems conclude that label separation is preferred over label aggregation when the noise rates are high, or the number of labelers/annotations is insufficient. Extensive empirical results validate our conclusions.","We present part of Huawei's efforts in building a Product Knowledge Graph (PKG). We want to identify which product attributes (i.e. properties) are relevant and important in terms of shopping decisions to product categories (i.e. classes). This is particularly challenging when the attributes and their values are mined from online product catalogues, i.e. HTML pages. These web pages contain semi-structured data, which do not follow a concerted format and use diverse vocabulary to designate the same features. We propose a system for key attribute identification (KATIE) based on fine-tuning pre-trained models (e.g., DistilBERT) to predict the applicability and importance of an attribute to a category. We also propose an attribute synonyms identification module that allows us to discover synonymous attributes by considering not only their labels' similarities but also the similarity of their values sets. We have evaluated our approach to Huawei categories taxonomy and a set of internally mined attributes from web pages. KATIE guarantees promising performance results compared to the most recent baselines.","In the multi-label text classification task, a text usually corresponds to multiple label categories, and the labels have correlation and hierarchical structure. However, when the label hierarchy is unknown, the number of various labels is not balanced, which makes it difficult for the model to classify low-frequency labels. At the same time, due to the existence of similar labels, the model will be difficult to distinguish similar labels. In this paper, we propose a multi-label text classification model based on multi-level constraint augmentation and label association attention. Compared with traditional methods, our method has two contributions: (1) In order to alleviate the problem of unbalanced number of different label categories and ensure the rationality of sample generation, we propose a data augmentation method based on multi-level constraints. In the process of sample generation, this method uses historical generation information, sample original text information and sample topic to constrain the generated text. (2) In order to make the model recognize the associated labels accurately, we propose an interaction mechanism based on label association attention and filter gate. This method combines text information and label weight information. At the same time, our classification model considers the important weights of text sentences and effectively utilizes the co-occurrence relationship between labels. Experimental results on three benchmark datasets show that our model outperforms state-of-the-art methods on all main evaluation metrics, especially on low-frequency label prediction with sparse samples.","Recently, Convolution Neural Networks (CNN) have achieved excellent performance in some areas of computer vision, including face recognition, character recognition, and autonomous driving. However, there are still many CNN-based models that cannot be deployed in real-world scenarios due to poor robustness. In this paper, focusing on the classification task, we attempt to evaluate and optimize the robustness of CNN-based models from a new perspective: the convolution kernel. Inspired by the discovery that the root cause of the model decision error lies in the wrong response of the convolution kernel, we propose a convolution kernel robustness evaluation metric based on the distribution of convolution kernel responses. Then, we devise the Convolution Kernel Robustness Calibrator, termed as CKR-Calibrator, to optimize key but not robust convolution kernels. Extensive experiments demonstrate that CKR-Calibrator improves the accuracy of existing CNN classifiers by 1%\u20134% in clean datasets and 1%\u20135% in corrupt datasets, and improves the accuracy by about 2% over SOTA methods. The evaluation and calibration source code is open-sourced at https://github.com/cym-heu/CKR-Calibrator.","Classification of long sequential data is an important Machine Learning task and appears in many application scenarios. Recurrent Neural Networks, Transformers, and Convolutional Neural Networks are three major techniques for learning from sequential data. Among these methods, Temporal Convolutional Networks (TCNs) which are scalable to very long sequences have achieved remarkable progress in time series regression. However, the performance of TCNs for sequence classification is not satisfactory because they use a skewed connection protocol and output classes at the last position. Such asymmetry restricts their performance for classification which depends on the whole sequence. In this work, we propose a symmetric multi-scale architecture called Circular Dilated Convolutional Neural Network (CDIL-CNN), where every position has an equal chance to receive information from other positions at the previous layers. Our model gives classification logits in all positions, and we can apply a simple ensemble learning to achieve a better decision. We have tested CDIL-CNN on various long sequential datasets. The experimental results show that our method has superior performance over many state-of-the-art approaches. The model and experiments are available at (https://github.com/LeiCheng-no/CDIL-CNN).","This paper presents an extensive systematic literature review with the aim of identifying and classifying issues in the information classification process. The classification selected uses human and organizational factors for grouping the identified issues. The results reveal that policy-related issues are most commonly described, but not necessarily the most crucial ones. Furthermore, gaps in the research field are identified in order to outline paths for further research.","In many real applications, the data are always collected from different types and they are subjected to obtain partial labeling information of objects. Such data are referred to as partially labeled mixed-type data. There is currently few work on feature selection approaches for these data. Motivated by this issue, this paper aims at selecting the informative feature subset from partially labeled mixed-type data. At first, to improve the classification performance, an improved label propagation algorithm based on K-nearest neighbor is proposed, which marks the decision labels of unlabeled objects by making use of the information between unlabeled objects and labeled objects. On this basis, a feature multi-criteria measure based on the dependency, information entropy and information granulation is proposed for selecting candidate features. Finally, the corresponding semi-supervised feature selection algorithm is developed to select the feature subset for the partially labeled mixed-type data. Experimental results on UCI data sets demonstrate the effectiveness of the proposed feature selection algorithm and the superiority in terms of the classification accuracy compared with other algorithms.","Convolutional neural networks (CNN) have enabled major advances in image classification through convolution and pooling. In particular, image pooling transforms a connected discrete grid into a reduced grid with the same connectivity and allows reduction functions to take into account all the pixels of an image. However, a pooling satisfying such properties does not exist for graphs. Indeed, some methods are based on a vertex selection step which induces an important loss of information. Other methods learn a fuzzy clustering of vertex sets which induces almost complete reduced graphs. We propose to overcome both problems using a new pooling method, named MIVSPool. This method is based on a selection of vertices called surviving vertices using a Maximal Independent Vertex Set (MIVS) and an assignment of the remaining vertices to the survivors. Consequently, our method does not discard any vertex information nor artificially increase the density of the graph. Experimental results show an increase in accuracy for graph classification on various standard datasets.","Severe limitations in data and technological availability have vastly affected NLP research into African languages. With Africa having over 2000 languages, the lack of NLP research is a massive flaw within the NLP field. African languages can hold the key to the next significant advancement in NLP research because some researchers suggest that 30% of current-day languages are derived from African languages. With Sentiment Analysis being a foundational part of NLP research, the release of the AfriSenti-SemEval Shared Task 12, hosted as a part of The 17th International Workshop on Semantic Evaluation, has provided 14 new annotated datasets for Sentiment Analysis on African languages. We utilize these datasets to evaluate our approach: Delta TF-IDF features with conventional machine learning models. Delta TF-IDF results showed that our approach could provide promising results with the low resource task of sentiment analysis on African Languages. Since it utilized a significantly less data than its transformer counter parts.","Sentence-level relation classification is a technique for classifying the relation between the head entity and the tail entity in a sentence. Currently, it is popularly used to realize relation classification based on deep learning methods. However, these methods rely heavily on large-scale annotated data, and the role of head and tail entities\u2019 information is not fully explored. In response to the above problems, we propose a prototypical networks model based on entity convolution for relation classification, which deforms the head entity and tail entity vectors encoded by BERT into multiple different convolution kernels and then performs convolution operations on the original sentence. Thus we can extract the features related to the entities in the sentence and classify the extracted features by using prototypical networks to realize relation classification. Experimental results strongly demonstrate that our model achieves state-of-the-art results compared with baseline models.\nHighlights\n\u2022\nWe utilize a few-shot learning method based on the prototypical network to implement relation classification, thereby alleviating the insufficient data in the new domain.\n\u2022\nWe propose a new method for entity convolution to alleviate the problem about the under-utilization of text information.\n\u2022\nWe evaluate our method on FewRel 1.0 and 2.0 datasets, and experimental results demonstrate that our model achieves state-of-the-art results compared with baseline models.","Yellow rust is a devastating disease that causes significant losses in wheat production worldwide and significantly affects wheat quality. It can be controlled by cultivating resistant cultivars, applying fungicides, and appropriate agricultural practices. The degree of precautions depends on the extent of the disease. Therefore, it is critical to detect the disease as early as possible. The disease causes deformations in the wheat leaf texture that reveals the severity of the disease. The gray-level co-occurrence matrix(GLCM) is a conventional texture feature descriptor extracted from gray-level images. However, numerous studies in the literature attempt to incorporate texture color with GLCM features to reveal hidden patterns that exist in color channels. On the other hand, recent advances in image analysis have led to the extraction of data-representative features so-called deep features. In particular, convolutional neural networks (CNNs) have the remarkable capability of recognizing patterns and show promising results for image classification when fed with image texture. Herein, the feasibility of using a combination of textural features and deep features to determine the severity of yellow rust disease in wheat was investigated. Textural features include both gray-level and color-level information. Also, pre-trained DenseNet was employed for deep features. The dataset, so-called Yellow-Rust-19, composed of wheat leaf images, was employed. Different classification models were developed using different color spaces such as RGB, HSV, and L*a*b, and two classification methods such as SVM and KNN. The combined model named CNN-CGLCM_HSV, where HSV and SVM were employed, with an accuracy of 92.4% outperformed the other models.","Leaders clustering method is a fast one and can be used to derive prototypes called leaders from a large training set which can be used in designing a classifier. Recently nearest leader based classifier is shown to be a faster version of the nearest neighbor classifier, but its performance can be a degraded one since the density information present in the training set is lost while deriving the prototypes. In this paper we present a generalized weighted k-nearest leader based classifier which is a faster one and also an on-par classifier with the k-nearest neighbor classifier. The method is to find the relative importance of each prototype which is called its weight and to use them in the classification. The design phase is extended to eliminate some of the noisy prototypes to enhance the performance of the classifier. The method is empirically verified using some standard data sets and a comparison is drawn with some of the earlier related methods.","Unexpected failures of equipment in sequential and continuous manufacturing processes such as semiconductor production can result in a serious deterioration of productivity. This directly affects the interests of equipment operators and indirectly reduces the credibility of the equipment manufacturers, resulting in serious potential harm to operators and manufacturers of equipment. Hence, the demand for predictive maintenance has increased to maximize the efficiency of equipment operations by predicting faults and fault timing beyond preemptive preventive maintenance, which depends on periodic inspections while operating equipment. However, since existing studies on predictive maintenance mainly utilize data collected during the equipment operation stage to predict equipment faults and fault timing, predictive maintenance research has only been conducted from the perspective of equipment operating companies. Accordingly, we propose a machine learning method in this paper that predicts the existence of faults and the fault timing of equipment using pre-shipment inspection data from equipment manufacturers. Importantly, this can be performed by an equipment manufacturer without access to operational data. This is the first predictive maintenance study to be conducted from the perspective of an equipment manufacturing company. We used tree-based ensemble models to identify important variables for faults and proceeded to predict the existence of faults and the first fault timing of equipment. Moreover, we verified the method\u2019s performance using real data from a leading semiconductor equipment manufacturer in South Korea. The contribution of this study can be summarized as follows. (1) We propose an initial predictive maintenance method that can be performed by equipment manufacturing companies. (2) As a result of an experiment using real data from the selected manufacturer, the proposed method yielded an accuracy of up to 94% for fault existence prediction and reduced the prediction error by more than 50% for the first fault timing compared to where a prediction model was not implemented. (3) It was demonstrated that the initial state of a piece of equipment has a great influence on future faults by deriving high performance using pre-shipment inspection data rather than operation data. (4) By using tree-based ensemble models, we identified major factors related to equipment faults.\nHighlights\n\u2022\nA fault detection and timing prediction model is proposed.\n\u2022\nThe proposed model is trained using pre-shipment inspection data without using operation data.\n\u2022\nThe proposed model yields at most 92.9% of fault detection F1 score.\n\u2022\nThe proposed model improves fault timing accuracy over 74.5% in terms of RMSE.","In the recent decade, plant disease classification using convolution neural networks has proven to be superior because of its ability to extract key features. Obtaining the optimum feature subset with the necessary discriminant information is challenging. The main objective of this paper is to design an efficient hybrid plant disease feature selection approach and validate it on standard image datasets. The raw input image features were transformed into 8192 learned features by employing the VGG16. To reduce the training time and enhance classification accuracy, the dimensionality reduction technique Principal Component Analysis (PCA) is integrated with the big bang-big crunch (BBBC) optimization algorithm. The PCA-BBBC feature selection method reduces computing time by eliminating unnecessary and redundant features. The proposed approach was evaluated on plant diseases and benchmarked image datasets. Experimental results reveal that the Artificial Neural Network (ANN) classifier integrated with the VGG16-PCA-BBBC approach enhanced the performance of the classifier. The proposed approach outperformed the VGG16-PCA-ANN method and other popular image classification techniques. For the rice disease dataset, the proposed hybrid approach reduced the VGG16 extracted 8192 deep features to 200 relevant principal components. The recommended reduced features were used for training ANN. The test dataset was classified by ANN with an accuracy of 99.12%. Experimental results demonstrate that the proposed approach improved the performance of the classifier and accurately labeled image and plant diseases datasets aiding farmers to adopt remedial measures.","Post-myocardial infarction (MI) patients are at risk of major adverse cardiac events (MACE), with risk stratification primarily based on global image-based biomarkers, such as ejection fraction, in current clinical practice. However, these metrics neglect more subtle and localized shape differences in 3D cardiac anatomy and function, which limit predictive accuracy. In this work, we propose a novel geometric deep learning approach to directly predict MACE outcomes within 1 year after the infarction event from high-resolution 3D cardiac anatomy meshes. Its architecture is specifically designed for direct and efficient processing of surface mesh data with a hierarchical, multi-scale structure to enable both local and global feature learning. We evaluate the binary MACE prediction capabilities of the proposed mesh classification network on a multi-center dataset of post-MI patients. Our results show that the proposed method outperforms corresponding clinical benchmarks by\n\u223c\n16% and\n\u223c\n6% in terms of area under the receiver operating characteristic (AUROC) curve for 3D shape and 3D contraction inputs, respectively. Furthermore, we visually analyze both 3D cardiac shapes and 3D contraction patterns with regards to their MACE predictability and demonstrate how task-specific information learned by the network on a balanced dataset successfully generalizes to increasing levels of class imbalance. Finally, we compare our approach to both clinical and machine learning benchmarks on our original highly-imbalanced dataset of post-MI patients and find average improvements in AUROC scores of\n\u223c\n9% and\n\u223c\n3%, respectively.","With more than 350 million active domain names and at least 200,000 newly registered domains per day, it is technically and economically challenging for Internet intermediaries involved in domain registration and hosting to monitor them and accurately assess whether they are benign, likely registered with malicious intent, or have been compromised. This observation motivates the design and deployment of automated approaches to support investigators in preventing or effectively mitigating security threats. However, building a domain name classification system suitable for deployment in an operational environment requires meticulous design: from feature engineering and acquiring the underlying data to handling missing values resulting from, for example, data collection errors. The design flaws in some of the existing systems make them unsuitable for such usage despite their high theoretical accuracy. Even worse, they may lead to erroneous decisions, for example, by registrars, such as suspending a benign domain name that has been compromised at the website level, causing collateral damage to the legitimate registrant and website visitors.\nIn this paper, we propose novel approaches to designing domain name classifiers that overcome the shortcomings of some existing systems. We validate these approaches with a prototype based on the COMAR (COmpromised versus MAliciously Registered domains) system focusing on its careful design, automated and reliable ground truth generation, feature selection, and the analysis of the extent of missing values. First, our classifier takes advantage of automatically generated ground truth based on publicly available domain name registration data. We then generate a large number of machine-learning models, each dedicated to handling a set of missing features: if we need to classify a domain name with a given set of missing values, we use the model without the missing feature set, thus allowing classification based on all other features. We estimate the importance of features using scatter plots and analyze the extent of missing values due to measurement errors.\nFinally, we apply the COMAR classifier to unlabeled phishing URLs and find, among other things, that 73% of corresponding domain names are maliciously registered. In comparison, only 27% are benign domains hosting malicious websites. The proposed system has been deployed at two ccTLD registry operators to support their anti-fraud practices.","In many practical applications of machine learning, there are a large number of partially labeled categorical data due to the high cost of labeling data. The semi-supervised learning algorithm is needed to deal with such data. This paper studies the label prediction of partially labeled categorical data and considers semi-supervised attribute reduction in a partially labeled categorical decision information system (p-CDIS) with predicted labels. The labels of unlabeled data are first predicted by means of the conditional probability. Then, uncertainty measurement for a p-CDIS with predicted labels is studied, and the dependence and conditional information entropy (CIE) are defined. Next, based on the dependence and CIE, two attribute reduction algorithms are designed. In addition, the effect of label deletion rate (LDR) on the dependence, CIE and reduction results are also studied. Finally, the results of experiments and statistical tests on 16 categorical UCI datasets show that the designed algorithms are statistically better than some state-of-the-art algorithms in classification accuracy.","Example-dependent cost classification is a special case of pattern classification where the costs are specific for each individual pattern. Most of the practical applications related to this kind of classification problem exhibit class imbalance in the available data, thus including an additional difficulty to the classification task. This problem has high practical importance because it appears intrinsically in relevant application fields, such as Finance or Health. We propose to use a 2-step Bayesian methodology to solve this problem because its formulation allows the inclusion of the individual example costs in the classification and takes into account the class probabilities. In particular, the main contribution is to apply principled rebalancing classification algorithms in the first step: We propose 3 Neural Network based learning machines, WR-MLP, WSR-MLPE and WSR-DNN, to provide the estimates of the required conditional probabilities for the Bayesian test. Unlike some similar approaches in the literature that use heuristic methods in the first step, which in most cases require calibration mechanisms to compensate for the estimation biases, the consistency of the proposed estimates is theoretically supported, thus providing a clear potential advantage. Experiments with seven real-world datasets show that the proposed methods are competitive against eleven state-of-the-art benchmarks, and provide an advantage in the less favourable situations: cases with a strong imbalance and highly nonlinear classification borders.\nHighlights\n\u2022\nExample-dependent cost classification problems are usually imbalanced.\n\u2022\nThe state-of-the-art methods are not principled and their performance is not robust.\n\u2022\nWe propose to apply principled rebalancing classification algorithms.\n\u2022\nWe propose principled mixed rebalancing techniques to combat imbalance.\n\u2022\nThe proposed methods improve the state of the art in performance and robustness.","The article provides a description of the most frequent bigrams and trigrams obtained using the n-gram analysis technique on a representative sample of Russian spoken language. N-gram analysis allows identifying frequent lists of sequences consisting of n graphical words, which is important for describing corpus material in various theoretical and applied aspects. The source data for applying this technique was a sample of 388 episodes of everyday speech communication from the ORD corpus (about 110 hours of audio). The results of the n-gram analysis in the form of frequency lists of word sequences allow constructing a typology of the most common bigrams and trigrams in Russian oral communication and lead the study equally to the levels of grammar, pragmatics, lexicon, and phraseology. The list of the most frequent bigrams and trigrams contains grammatical structures (U TEBYA, YA NE PONIMAYU, MNE KAZHETSYA), idioms (in a broad sense of the term) (VSYO RAVNO, TO ZHE SAMOE), introductory units (TAK SKAZAT\u2019, S DRUGOY STORONY), as well as a number of sequences typical only for oral speech, such as one-word pragmatic markers (NU VOT, KAK BY, NU V OBSHCEM), amplifications (DA-DA, TAK-TAK-TAK), and hesitations-vocalizations (E-E, M-M-M). The obtained frequency lists can be useful for solving many modern applied natural language processing tasks.","Abstract: In the data stream learning scenario, the whole picture of the data can't be observed, and the data may change dynamically, thus increasing the complexity and imbalance of the data. Aiming at the characteristics of data class changes (appearance, disappearance, and reappearance) of multi-class data stream, a Matthews Adaptive XGBoost algorithm based on One-Vs-Rest strategy is proposed. For the three cases of class change, the One-Vs-Rest strategy is used to build the model, and a binary classification model is created for each class. Aiming at the problem that the model gradually forgets the knowledge of the class after the class disappears, a management mechanism based on class frequency is proposed. In view of the problem that the restarted model affects the overall performance of the ensemble after the class is reproduced, the sliding window is used to initialize it. Aiming at the problem of ensemble classifiers, a mechanism to adapt the number of base classifiers is proposed. Experiments on real and synthetic datasets show that the improved algorithm improves Kappa and PMAUC indicators by 1.03% and 0.82%, respectively.\nCCS CONCEPTS \u2022 Computing methodologies \u2022 Machine learning \u2022 Machine learning algorithms","Since classical deep learning (DL) techniques are hungry for massive data and suffer from domain shift, domain adaptation (DA) methods are broadly adopted in prognostics and health management (PHM) to align source and target domains. However, DA relies on target datasets collected in advance, which are not always available in practice. In this paper, a domain generalization (DG) approach, which learns from multiple source domains and generalizes well to unseen domains, is introduced for remaining useful life (RUL) prediction of bearings under unseen operating conditions. Specifically, we propose an adversarial out-domain augmentation (AOA) framework to generate pseudo-domains, thereby increasing the diversity of available samples. Hence, a generator is trained in an adversarial manner to generate augmented pseudo-domains by maximizing the domain discrepancy of the latent representations. In addition, we add manifold and semantic regularization to its objective function to ensure the consistency of the pseudo-domains. Trained with these available domains, a task predictor can improve the generalization in inaccessible target domain. Based on this, we provide a specific implementation of AOA-based RUL prediction for DG and validate its effectiveness and superiority using experimental datasets.","Few-shot learning datasets contain a large number of classes with only a few examples in each. Existing datasets may contain thousands of classes, but very simple images (e.g. handwritten characters) such that a naive baseline can perform very well. Or they may be so complex, with large within-class variation and distracting background, that they are too difficult to enable meaningful learning with so few examples. To construct a customizable dataset of consistent natural images, we assemble a new dataset with each class containing a small subset from each of the 1000 classes of ImageNet-1k. To select subsets with clear within-class consistency we use an evolutionary approach to minimize the pairwise cosine-distance between features generated by a pre-trained VGG model. We train a classifier on these evolved image cliques and find that our evolved dataset provides a greater challenge than hand written digits, but not the extreme difficulty of a non-evolved subset of ImageNet. We find that pre-training our classifier on these evolved prototypical classes significantly improves performance on classifying random subsets ImageNet (relative to pre-training on similar random-class subsets), and conjecture that these prototypical classes may be beneficial for seeding concept learning. Dataset and code is publicly available at: https://github.com/lfrati/OmnImage","Learning with Noisy Labels (LNL) has become an appealing topic, as imperfectly annotated data are relatively cheaper to obtain. Recent state-of-the-art approaches employ specific selection mechanisms to separate clean and noisy samples and then apply Semi-Supervised Learning (SSL) techniques for improved performance. However, the selection step mostly provides a medium-sized and decent-enough clean subset, which overlooks a rich set of clean samples. To fulfill this, we propose a novel LNL framework ProMix that attempts to maximize the utility of clean samples for boosted performance. Key to our method, we propose a matched high confidence selection technique that selects those examples with high confidence scores and matched predictions with given labels to dynamically expand a base clean sample set. To overcome the potential side effect of excessive clean set selection procedure, we further devise a novel SSL framework that is able to train balanced and unbiased classifiers on the separated clean and noisy samples. Extensive experiments demonstrate that ProMix significantly advances the current state-of-the-art results on multiple benchmarks with different types and levels of noise. It achieves an average improvement of 2.48% on the CIFAR-N dataset.","Deoxyribonucleic acid (DNA) can be considered as one of the most useful biometrics. It has effectively been used for recognizing persons. However, it seems that there is still a need to propose a new approach for verifying humans, especially after the recent big wars, where too many people lost and die. This approach should have the capability to provide high personal verification performance. In this paper, a personal recognition approach based on artificial intelligence is proposed. This approach is called the artificial DNA algorithm for recognition (ADAR). It utilizes a unique identity for each person acquired from DNA nucleotides, and it can verify individuals efficiently with high performance. The ADAR has been designed and applied to multiple datasets, namely, the DNA classification (DC), sample DNA sequence (SDS), human DNA sequences (HDS), and DNA sequences (DS). For all datasets, a low value of 0% is achieved for each of the false acceptance rate (FAR) and false rejection rate (FRR).","Convolutional neural newtorks have been presenting great results regarding image classification in different contexts. As known in the literature, this kind of network demands a great volume of labeled samples to reach a good convergence during the training process. However, in real scenarios labeled samples are scarce because the labeling process is costly and time consuming. Moreover, it is highly susceptible to errors when accomplished by human specialists. This fact impairs in a great extent the applicability of convolutional neural networks in several contexts where there is a lack of labeled samples. Thus, in this work we focus on mitigate this issue. To do so, we apply the semi-supervised paradigm to the convolutional neural networks. We proposed the aggregation of two semi-supervised techniques from literature with this kind of deep learning networks and analyse their behavior. We performed an extensive assessment of this aggregation. We also considered the transfer learning approach in the process to verify its generalization under the semi-supervised paradigm. Our experiments, with three public datasets, testify that our proposed aggregation obtained better results, gains of up to 88% in accuracy, when compared with the supervised paradigm.","This paper ideates, inspired by LBP and its variants, a novel local feature extraction operator for texture classification, referred to as Multi-scale Ternary and Septenary Pattern (MTSP). MTSP is a histogram-based feature re\u1e55resentation that is composed of two single-scale STP and SSP (single-scale ternary and septenary patterns, respectively) encoders designed according to a novel set theory based pattern encoding scheme that integrates the concepts of both LQP\u2019s and LTP\u2019s operators. The essence of STP and SSP is to compute several virtual pixels based on various local and global image statistics and progressively encode interactions between local and non-local pixels by examining the directional information and differential excitation according to relationships between adjacent pixels rearranged in a variety of spatial arrangements. Unlike various parametric state-of-the-art texture operators that perform thresholding based on static thresholds, MTSP incorporates dynamic thresholds estimated automatically. MTSP descriptor has good ability as faithfully as possible to capture more detailed image information via complementary texture information generated from the fusion of both STP and SSP encoders. Experimental results show that MTSP ensures reliable performance stability over ten texture datasets and against several recent representative methods. In addition, the performance of MTSP is further proved statistically via the Wilcoxon signed rank test demonstrating thus that MTSP is a good candidate for texture modeling.","Among the causes of reduced production is a chicken disease, which can negatively affect consumer health. With the advancement of computer vision technology and profound innovations in the field of research, it has become increasingly important to analyze disease images collected by sensors in chickens to analyze the possibility of infection conveniently and efficiently. Consequently, research proposes to identify lesions using the Autoencoder and Yolov6 model to classify and detect diseases in chicken flocks. This model is suitable for different chicken breeds from many countries and regions. This method helps improve and enhance image recognition accuracy by incorporating the data enhancement method in the data preprocessing step. The results show that the value of val/mAP (average accuracy) obtained by the method proposed in this paper is 99.15%. Moreover, hit over 90% on the test dataset. This method can be applied to the early detection of disease-carrying chickens in the captive population, ensuring a quality food source for humans.","License Plate Recognition (LPR) plays a critical role in various applications, such as toll collection, parking management, and traffic law enforcement. Although LPR has witnessed significant advancements through the development of deep learning, there has been a noticeable lack of studies exploring the potential improvements in results by fusing the outputs from multiple recognition models. This research aims to fill this gap by investigating the combination of up to 12 different models using straightforward approaches, such as selecting the most confident prediction or employing majority vote-based strategies. Our experiments encompass a wide range of datasets, revealing substantial benefits of fusion approaches in both intra- and cross-dataset setups. Essentially, fusing multiple models reduces considerably the likelihood of obtaining subpar performance on a particular dataset/scenario. We also found that combining models based on their speed is an appealing approach. Specifically, for applications where the recognition task can tolerate some additional time, though not excessively, an effective strategy is to combine 4\u20136 models. These models may not be the most accurate individually, but their fusion strikes an optimal balance between accuracy and speed.","Hypertension (HPT) is a lethal medical disorder in which the blood vessels unusually have high pressure for an extended period. It may raise the risk of various health complications. Ballistocardiography (BCG) is an emerging tool to diagnose various heart-related diseases and is used to depict the repetitive vibrations in the human body induced by the sudden evacuation of blood into the major arteries with each heartbeat. This article presents the multi-resolution analysis of BCG signals for the screening of HPT patients using integrated tunable Q-factor wavelet transform (ITQWT). The TQWT decomposes an input BCG signal into various sub-bands (SBs). Specifying a predetermined accurate basis function for optimal decomposition utilizing TQWT is a difficult task. Therefore, in this study, for the first time, a multi-verse optimization (MVO) algorithm is integrated with TQWT for selecting optimum tuning parameters to decompose the input BCG signals into more representative SBs. To detect hypertensive BCG signals eleven statistical features are evaluated from each SB. Among them, a set of seven statistically-significant features are selected by applying the Kruskal\u2013Wallis test and fed to a K-nearest neighbor (K-NN) classifier with six different kernels using a 10-fold validation scheme. The highest classification accuracy of 92.21%, sensitivity of 92.96%, and specificity of 91.60% are achieved using a weighted K-NN classifier. This paper presents, a non-parameterized approach for the optimal decomposition of BCG data to detect HPT more accurately. The primary benefit of the proposed support system is that it can detect HPT patients with high accuracy by reducing the clinician\u2019s workload.\nGraphical abstract\nDisplay Omitted\nHighlights\n\u2022\nHypertension is a common health problem.\n\u2022\nMVO algorithm is integrated with TQWT for selecting optimum tuning parameters.\n\u2022\nStatistical features are evaluated from ITQWT SBs.\n\u2022\nK-NN classifier is employed to classify hypertensive BCG signals.","Data classification is a core component of many intelligent systems, and it has been almost monopolizing the interest of the scientific community for decades. However, developing classifiers capable of explaining how a classification result was derived, in a way that is compatible to the human perception, is still a challenge. Steps towards this direction have been made via interpretable fuzzy classifiers capable of extracting linguistically expressible rules from data. Motivated by the need for interpretable data classification, this study proposes a novel interpretable fuzzy classification framework based on Fuzzy Similarity Phrases (FSPs), aiming to provide an improved tradeoff between interpretability and classification performance. This framework incorporates the following novel components: a) a generic vocabulary-based feature extraction scheme which considers similarity features and fuzzy sets to construct FSPs; b) a rule generation methodology based on FSPs; c) an embedded feature selection methodology targeting to keep both the number of extracted rules and their length low; and an interpretable class prediction mechanism based on the interpretation of the FSPs. The experimental evaluation of the proposed framework on various publicly available datasets, in comparison with well-known and state-of-the-art classifiers, validate that it can provide a better average classification performance with fewer rules.","Pathological complete response (pCR) after neoadjuvant che-motherapy (NAC) in patients with breast cancer was found to improve survival, and it has a great prognostic value in the aggressive tumor subtype. This study aims to predict pCR before NAC treatment with a radiomic feature-based ensemble learning model using both positron emission tomography/computed tomography (PET/CT) images taken from the online QIN-Breast dataset. It studies the problem of constructing an end-to-end classification pipeline that includes a large-scale radiomic feature extraction, a hybrid iterative feature selection and a heterogeneous weighted ensemble classification. The proposed hybrid feature selection procedure can identify significant radiomic predictors out of 2153 features extracted from delineated tumour regions. The proposed weighted ensemble approach aggregates the outcomes of four weak classifiers (Decision tree, Naive Bayes, K-nearest neighbour, and Logistics regression) based on their importance. The empirical study demonstrates that the proposed feature selection-cum-ensemble classification method has achieved 92% and 88.4% balanced accuracy in PET and CT, respectively. The PET/CT aggregated model performed better and achieved 98% balanced accuracy and 94.74% F1-score. Furthermore, this study is the first classification work on the online QIN-Breast dataset.","Current techniques in deep learning are still unable to train adversarially robust classifiers which perform as well as non-robust ones. In this work, we continue to study the space of loss functions, and show that the choice of loss can affect robustness in highly nonintuitive ways.\nSpecifically, we demonstrate that a surprising choice of loss function can in fact improve adversarial robustness against some attacks. Our loss function encourages accuracy on adversarial examples, and explicitly penalizes accuracy on natural examples. This is inspired by the theoretical and empirical works suggesting a fundamental tradeoff between standard accuracy and adversarial robustness. Our method, NAturally Penalized (NAP) loss, achieves 61.5% robust accuracy on CIFAR-10 with\n\u03b5\n=\n8\n/\n255\nperturbations in\n\u2113\n\u221e\n(against a PGD-60 adversary with 20 random restarts). This improves over the standard PGD defense by over 3%, against other loss functions proposed in the literature. Although TRADES performs better on CIFAR-10 against Auto-Attack, our approach gets better results on CIFAR-100. Our results thus suggest that significant robustness gains are possible by revisiting training techniques, even without additional data.","Legal judgment prediction (LJP) is a significant task in legal intelligence, which aims to assist the judges and determine the judgment result based on the case's fact description. The judgment result consists of law articles, charge, and prison term. The law articles serve as the basis for the charge and the prison term, which can be divided into two types, named as charge-related law article and term-related law article, respectively. Recently, many methods have been proposed and made tremendous progress in LJP. However, the existing methods only focus on the prediction of the charge-related law articles, ignoring the term-related law articles (e.g., laws about lenient treatment), which limits the performance in the prison term prediction. In this paper, following the actual legal process, we expand the law article prediction as a multi-label classification task that includes both the charge-related law articles and term-related law articles and propose a novel multi-law aware LJP (ML-LJP) method to improve the performance of LJP. Given the case's fact description, firstly, the label (e.g., law article and charge) definitions in the Code of Law are used to transform the representation of the fact into several label-specific representations and make the prediction of the law articles and the charge. To distinguish the similar content of different label definitions, contrastive learning is conducted in the training. Then, a graph attention network (GAT) is applied to learn the interactions among the multiple law articles for the prediction of the prison term. Since numbers (e.g., amount of theft and weight of drugs) are important for LJP but often ignored by conventional encoders, we design a corresponding number representation method to locate and better represent these effective numbers. Extensive experiments on real-world dataset show that our method achieves the best results compared to the state-of-the-art models, especially in the task of prison term prediction where ML-LJP achieves a 10.07% relative improvement over the best baseline.","Imbalanced datasets affect the performance of machine learning algorithms adversely. To cope with this problem, several resampling methods have been developed recently. In this article, we present a case study approach for investigating the effects of data balancing approaches. The case study concerns the discrimination between growth hormone treated and non-treated animals using Liquid Chromatography-High Resolution Mass Spectrometry (LC-HRMS) data. Our LC-HRMS dataset contains 1241 bovine urine samples, of which only 65 specimens were from animal studies and guaranteed to contain growth-stimulating hormones while the rest has been reported to be untreated, making it a \u223c5% imbalanced dataset. In this research, classification algorithms, combined with resampling strategies and dimensionality reduction methods, were investigated to find a prediction model to correctly identify the samples of treated animals. Furthermore, to cope with a large number of missing data points in the given dataset, a replacement with random low values strategy was applied. Our results showed that the replacement method was effective, and LogisticRegression combined with the oversampling algorithms SMOTE or ADASYN, GaussianProcessClassifier with the oversampling algorithm SMOTE, and LinearDiscriminantAnalysis were the best performing models after log transformation of the dataset was followed by Recursive Feature Elimination.\nHighlights\n\u2022\nThe model discriminating between growth-hormone treated and non-treated animals.\n\u2022\nNew dataset using Liquid Chromatography-High Resolution Mass Spectrometry data.\n\u2022\nMany experiments with different algorithms including data balancing approaches.","The success of Hollywood cinema is partially attributed to the notion that Hollywood film-making constitutes both an art and an industry: an artistic tradition based on a standardized approach to cinematic narration. Film theorists have explored the narrative structure of movies and identified forms and paradigms that are common to many movies - a latent narrative structure. We raise the challenge of understanding and formulating the movie story structure and introduce a novel story-based labeled dataset-the Movie Narrative Dataset (MND). The dataset consists of 6,448 scenes taken from the manual annotation of 45 cinema movies, by 119 distinct annotators. The story-related function of each scene was manually labeled by at least six different human annotators as one of 15 possible key story elements (such as Set-Up, Debate, and Midpoint) defined in screenwriting guidelines. To benchmark the task of scene classification by their narrative function, we trained an XGBoost classifier that uses simple temporal features and character co-occurrence features to classify each movie scene into one of the story beats. With five-fold cross-validation over the movies, the XGBoost classifier produced an F1 measure of 0.31 which is statistically significant above a static baseline classifier. These initial results indicate the ability of machine learning approaches to detect the narrative structure in movies. Hence, the proposed dataset should contribute to the development of story-related video analytics tools, such as automatic video summarization and movie recommendation systems.","This paper explores how machine learning can help classify aid activities by sector using the OECD Creditor Reporting System (CRS). The CRS is a key source of data for monitoring and evaluating aid flows in line with the United Nations Sustainable Development Goals (SDGs), especially SDG17 which calls for global partnership and data sharing. To address the challenges of current labor-intensive practices of assigning the code and the related human inefficiencies, we propose a machine learning solution that uses ELECTRA to suggest relevant five-digit purpose codes in CRS for aid activities, achieving an accuracy of 0.9575 for the top-3 recommendations. We also conduct qualitative research based on semi-structured interviews and focus group discussions with SDG experts who assess the model results and provide feedback. We discuss the policy, practical, and methodological implications of our work and highlight the potential of AI applications to improve routine tasks in the public sector and foster partnerships for achieving the SDGs.","Stroke, categorized under cardiovascular and circulatory diseases, is considered the second foremost cause of death worldwide, causing approximately 11% of deaths annually. Stroke diagnosis using a Computed Tomography (CT) scan is considered ideal for identifying whether the stroke is hemorrhagic or ischemic. However, most methods for stroke classification are based on a single slice-level prediction mechanism, meaning that the most imperative CT slice has to be manually selected by the radiologist from the original CT volume. This paper proposes an integration of Convolutional Neural Network (CNN), Vision Transformers (ViT), and AutoML to obtain slice-level predictions as well as patient-wise prediction results. While the CNN with inductive bias captures local features, the transformer captures long-range dependencies between sequences. This collaborative local-global feature extractor improves upon the slice-wise predictions of the CT volume. We propose stroke-specific feature extraction from each slice-wise prediction to obtain the patient-wise prediction using AutoML. While the slice-wise predictions helps the radiologist to verify close and corner cases, the patient-wise predictions makes the outcome clinically relevant and closer to real-world scenario. The proposed architecture has achieved an accuracy of 87% for single slice-level prediction and an accuracy of 92% for patient-wise prediction. For comparative analysis of slice-level predictions, standalone architectures of VGG-16, VGG-19, ResNet50, and ViT were considered. The proposed architecture has outperformed the standalone architectures by 9% in terms of accuracy. For patient-wise predictions, AutoML considers 13 different ML algorithms, of which 3 achieve an accuracy of more than 90%. The proposed architecture helps in reducing the manual effort by the radiologist to manually select the most imperative CT from the original CT volume and shows improvement over other standalone architectures for classification tasks. The proposed architecture can be generalized for volumetric scans aiding in the patient diagnosis of head and neck, lungs, diseases of hepatobiliary tract, genitourinary diseases, women\u2019s imaging including breast cancer and various musculoskeletal diseases. Code for proposed stroke-specific feature extraction with the pre-trained weights of the trained model is available at: https://github.com/rishiraj-cs/StrokeViT_With_AutoML.\nGraphical abstract\nDisplay Omitted\nHighlights\n\u2022\nMost methods for stroke classification are based on slice-level prediction mechanism.\n\u2022\nAn integration of CNN and ViT is proposed for slice-level prediction.\n\u2022\nFor patient-wise predictions, stroke-specific feature extraction is proposed.\n\u2022\nAutoML is trained on the proposed domain-specific features extracted.","Virtual reality technology provides a strong sense of immersion and interactivity. It is widely used in the fields of anxiety relief, fear therapy, and depression regulation. However, objectively evaluating the emotional intervention effect of virtual reality technology is a difficult problem. The main purpose of this paper is to explore the use of EEG signals to identify individual emotional states in virtual reality scenarios and to improve the computational efficiency and recognition accuracy of emotional valence. To induce the target emotional state of the participants, we established a relatively standard emotion-induced virtual reality video library. The EEG data of the participants were collected synchronously as they watched the virtual reality video. The results show that the emotion recognition performance of multiple features (energy spectrum, differential entropy, differential asymmetry, and rational asymmetry) is better than that of a single feature. The radial basis function neural network (RBFNN) performed better than the deep belief network (DBN). RBFNN achieves the highest average classification accuracy of 91.1%. By combining the feature selection (F-test) method with the RBFNN, an ideal classification performance can be maintained with computational efficiency improvements. Furthermore, it is demonstrated that the features extracted from the theta band outperform features extracted from other bands in emotional valence decoding. These results may contribute to the application of EEG-based affective computing technology in the field of psychological rehabilitation and assessment.","As the state-of-the-art graph learning models, the message passing based neural networks (MPNNs) implicitly use the graph topology as the \"pathways\" to propagate node features. This implicit use of graph topology induces the MPNNs' over-reliance on (node) features and high inference latency, which hinders their large-scale applications in industrial contexts. To mitigate these weaknesses, we propose the Graph Explicit Neural Network (GENN) framework. GENN can be flexibly applied to various MPNNs and improves them by providing more efficient and accurate inference that is robust in feature-constrained settings. Specifically, we carefully incorporate recent developments in network embedding methods to efficiently prioritize the graph topology for inference. From this vantage, GENN explicitly encodes the topology as an important source of information to mitigate the reliance on node features. Moreover, by adopting knowledge distillation (KD) techniques, GENN takes an MPNN as the teacher to supervise the training for better effectiveness while avoiding the teacher's high inference latency. Empirical results show that our GENN infers dramatically faster than its MPNN teacher by 40x-78x. In terms of accuracy, GENN yields significant gains (more than 40%) for its MPNN teacher when the node features are limited based on our explicit encoding. Moreover, GENN outperforms the MPNN teacher even in feature-rich settings thanks to our KD design.","In this paper we address the Class Incremental Learning (CIL) problem, characterized by sequences of data batches in which examples of different classes occur at different times. From a theoretical point of view, we propose a new approach that we call hierarchical sequencing and prove that any CIL task can be sequenced into simple incremental classification tasks by means of the hierarchical sequencing. From a practical point of view, we propose the HILAND method for image classification, which combines the hierarchical sequencing with transfer learning. In our experiments, the HILAND method has obtained state-of-the-art results for the CIL problem, but with far less training effort through transfer learning.\nHighlights\n\u2022\nIncremental learning can be sequenced into simpler tasks by hierarchical sequencing\n\u2022\nOur class incremental method combines transfer learning and hierarchical sequencing\n\u2022\nOur method attains state-of-the-art results with far less training effort","In this paper we propose a novel boosting based classification algorithm, SODA-Boosting (where SODA stands for Second Order Discriminant Analysis). Unlike the conventional AdaBoost based algorithms widely applied in computer vision, SODA-Boosting does not involve time consuming procedures to search a huge feature pool in every iteration during the training stage. Instead, in each iteration SODA-Boosting efficiently computes discriminative weak classifiers in closed-form, based on reasonable hypotheses on the distribution of the weighted training samples. As an application, SODA-Boosting is employed for image based gender recognition. Experimental results on publicly available FERET database are reported. The proposed algorithm achieved accuracy comparable to state-of-the-art approaches, and demonstrated superior performance to relevant boosting based algorithms.","Deep neural networks (DNNs) achieve top performance through costly training on large datasets. Such resources may not be available in some scenarios, like IoT or healthcare. Extreme learning machines (ELMs) aim to alleviate this problem using single-layered networks, requiring fewer training resources. Current investigations have found that DNNs are prone to security and privacy threats, where malfunction of the network or training data extraction can be performed.\nDue to the increasing attention to ELMs and their lack of security investigations, we research the security implications of this type of network. Precisely, we investigate backdoor attacks in ELMs. We created a comprehensive experimental setup to evaluate their security in various datasets and scenarios. We conclude that ELMs are vulnerable to backdoor attacks with up to 97% attack success rate. Additionally, we adapt and evaluate the usage fine-pruning to ELMs.","Node classification is the task of predicting the labels of unlabeled nodes in a graph. State-of-the-art methods based on graph neural networks achieve excellent performance when all labels are available during training. But in real-life, models are often applied on data with new classes, which can lead to massive misclassification and thus significantly degrade performance. Hence, developing open-set classification methods is crucial to determine if a given sample belongs to a known class. Existing methods for open-set node classification generally use transductive learning with part or all of the features of real unseen class nodes to help with open-set classification. In this paper, we propose a novel generative open-set node classification method, i.e., G2Pxy, which follows a stricter inductive learning setting where no information about unknown classes is available during training and validation. Two kinds of proxy unknown nodes, inter-class unknown proxies and external unknown proxies are generated via mixup to efficiently anticipate the distribution of novel classes. Using the generated proxies, a closed-set classifier can be transformed into an open-set one, by augmenting it with an extra proxy classifier. Under the constraints of both cross entropy loss and complement entropy loss, G2Pxy achieves superior effectiveness for unknown class detection and known class classification, which is validated by experiments on benchmark graph datasets. Moreover, G2Pxy does not have specific requirement on the GNN architecture and shows good generalizations.","Taxonomy plays a vital role in identifying different mosquito species. Studies show that though not all mosquitoes threaten humanity, specific species exist in less fortunate areas that immensely disrupt people\u2019s lives. As identified, researchers discovered that deficiency in identifying between a vector mosquito that carries a lethal disease apart from non-vectors led to people becoming susceptible. Recently, studies proposed automating these mosquitoes\u2019 classification so that people who lack awareness can soon obtain assistance from an intelligent system. However, most solutions still require expensive computations and specialized resources to operate and even reproduce, making the most vulnerable areas or groups of people unable to benefit from them. Therefore, this work solves this problem with a lightweight model built by compressing, duplicating, and fusing a Deep Convolutional Neural Network model (DCNN), adding a modified residual block, and training it through Knowledge Distillation (KD). Upon assessment, results yielded significant performance improvements, as the proposed model reached 99.22% overall accuracy that only requires 0.33 GFLOPs to operate and consumes only 437 KB of disk space. In addition, results also showed the benefits of KD in saliency. Compared to most studies, previous and current state-of-the-art DCNNs, this work shows promising viability to solve the problem pragmatically.\nGraphical abstract\nDisplay Omitted\nHighlights\n\u2022\nThe proposed model classified six mosquito species better than most solutions.\n\u2022\nIt attained improvements via compression, fusion, residual learning and knowledge distillation.\n\u2022\nIt has better cost-efficiency than the existing solutions with only 40 K parameters.","In this paper, a new method of supervised classification of documents is proposed. It utilizes discrete trasforms to extract features from classified objects and adopts adaptive potential active hypercontours (APAH) for document classification. The idea of APAH generalizes classic contour methods of image segmentation. It has two main advantages: it can use almost any knowledge during the search for an optimal classification function and it can operate in a feature space where only metric is defined. Here, both of them are utilized - the first one by using expert knowledge about significance of documents from training set and the second one by inducing new metrics in feature spaces. The method has been evaluated on the subset of open directory project (ODP) database and compared with k-NN, the well known classification technique.","Ensemble learning also named ensemble of multiple classifiers is one of the hot topics in machine learning. Ensemble learning can improve not only the accuracy but also the efficiency of the classification system. Constructing the component classifiers in ensemble learning is crucial, because it has direct influence on the performance of the classification system. In the construction of component classifiers, it should be guaranteed that the constructed component classifiers possess certain accuracy and diversity. Based on the confidence degree of classifier, this paper presents an approach consisting of three steps to dynamically integrate rough set reducts. Firstly, multiple reducts are computed. Secondly, multiple component classifiers with certain diversity are trained on the different reducts. Finally, these component classifiers are integrated by adopting dynamic integration strategy. The experimental results show that the proposed algorithm is efficient and feasible.","In this paper, we propose a multi-class classification method using kernel supports and a dynamical system under differential privacy. For small datasets, kernel methods, such as kernel support vector machines (SVMs), show good generalization performance with high-dimensional feature mapping. However, kernel SVMs have a fundamental weakness in achieving differential privacy because they construct decision functions based on a subset of the training data called support vectors. Furthermore, multi-class SVMs must decompose the training data into a binary class, which requires multiple accesses to the same training data. To address these limitations, we develop a two-phase classification algorithm based on support vector data description (SVDD). We first generate and prove a differentially private SVDD (DP-SVDD) by perturbing the sphere center in a high-dimensional feature space. Next, we partition the input space using a dynamical system and classify the partitioned regions using a noisy count. The proposed method results in robust, fast, and user-friendly multi-class classification, even on small-sized datasets, where differential privacy performs poorly.","In this study, we show the advantages of incorporating multi-source knowledge from publicly available sources, such as ChatGPT and Wikipedia, into existing datasets to enhance the performance of machine learning models for routine tasks, such as classification. specifically, we propose the utilization of supplementary data from external sources and demonstrate the utility of widely accessible knowledge in the context of the Forest Cover Type Prediction task launched by the Roosevelt National Forest of Northern Colorado. Additionally, we exhibit an improvement in classification accuracy for the Isolated Letter Speech Recognition dataset when incorporating information on regional accents in the prediction of spoken English letter names.","Deep learning has shown promise in accurate medical image analysis, but challenges remain. Data privacy concerns hinder the availability of large, high-quality medical datasets. Traditional deep learning approaches are computationally intensive and lack efficiency. This paper proposes the use of Federated Learning (FL) with ResNet-18, a deep neural network architecture. ResNet-18 addresses gradient issues using residual blocks and skip connections. FL enables collaborative training while preserving data privacy. The training process utilizes stochastic gradient descent and techniques such as data augmentation and regularization for improved model performance.","Faults in photovoltaic (PV) systems significantly reduce their efficiency and can pose safety risks. Nevertheless, most residential PV systems are not actively monitored, because existing methods often require expensive sensors, which are only cost-effective for large PV systems. Therefore, we propose a graph neural network (GNN) to monitor a group of nearby PV systems without relying on dedicated sensors. Instead, the GNN compares 24 h of current and voltage measurements obtained from the inverters. Four GNN variants are experimentally compared using simulated data of six different PV systems in Colorado. Results show that all GNN variants outperform a state-of-the-art PV fault diagnosis method based on gradient boosted trees. Moreover, some GNN variants can even generalize to PV systems which were not in the training data, enabling monitoring of new PV systems without retraining.","A common method to perform the object classification with different images being taken at different views is to extract the features from each image without performing the fusion. On the other hand, this paper proposes a multivariate two dimensional singular spectrum analysis (M2DSSA) based approach to fuse the features in different images together to perform the object classification. First, a four channel two dimensional signal is formed using four images taken at four different views. Second, the M2DSSA is applied to the four channel two dimensional signal. Next, the histogram of the oriented gradient (Hog) is computed on each channel of each M2DSSA component. Then, the selection of the M2DSSA components is performed based on the correlation coefficients among these Hogs and the fusion of these images is performed via the M2DSSA. Next, the Hog of each reconstructed image is recomputed and these Hogs are employed as the features for the support vector machine to perform the object classification. Our proposed method yields the classification accuracies at 92.5925% and 97.8723% for the images in the first dataset and the second dataset, respectively. Since the information of the objects in different images is fused together, the computer numerical simulation results show that the classification accuracies of our proposed method are higher than those of the baseline method without performing the fusion and those of the other fusion methods.","Detection and classification of a brain tumour in medical images is always a challenging task, as the treatment may lead to Radiosurgery depending upon the exact shape, size, and position of a tumour. To provide a complete characterization of glioma and the degree of malignancy, classification and grading are vital. In some complicated cases, localization of the tumour, comparison of tumour tissues with adjacent regions, to make it clear for detection, and finally classification without human intervention is need of the hour. Out of the available numerous imaging techniques to detect and classify brain tumours, MRI is the most suitable non-invasive technique and has superior image quality. The edge over other techniques is its superior soft-tissue resolution and the ability to acquire different images employing contrast-enhanced agents. This research work classifies Low-Grade Glioma (LGG) and High-Grade Glioma(HGG) tumours on the basis of features obtained from extracted tumor region. The results of the classification of HGG and LGG tumours using the most prominent features are validated by five-fold cross-validation. Satisfactory and encouraging classification results are obtained using large and publicly available clinical datasets. Using the extracted features, a computer-assisted algorithm is developed that uses a machine-learning algorithm along with Gradient-Based Kernel Selection Graph Cut to provide binary classification with an accuracy of 94.6% with T1ce sequence of MRI. The proposed models can be employed to assist physicians and radiologists in the early pathological detection and classification of gliomas. The proposed framework also can be a model for validating brain tumours and their initial screening for their grading classification.","The aim of this study is to classify structural cartographic objects in high-resolution satellite images. The target classes have an important intra-class variability because the class definitions belong to high-level concepts. Structural attributes seem to be the most plausible cues for the classification task. We propose an Adaboost learning method using edge-based features as weak learners. Multi-scale sub-pixel edges are converted to geometrical primitives as potential evidences of the target object. A feature vector is calculated from the primitives and their perceptual groupings, by the accumulation of combinations of their geometrical and spatial attributes. A classifier is constructed using the feature vector. The main contribution of this paper is the usage of structural shape attributes in a statistical learning method framework.\nWe tested our method on CNES dataset prepared for the ROBIN Competition and we obtained promising results.","The paper concerns the authorship recognition in a collection of Polish literary texts from the late 19th and early 20th centuries, consisting of 99 novels from 33 authors. The authors divide the books into smaller parts and analyze the classification based on a book part. To mimic the real task of testing an unknown book, the data set has been divided in such a way that parts of the same book do not appear simultaneously in the training and test set. The authors compare the approaches by working with raw texts, and, to avoid the semantic features of the text, they represent texts in the form of a sequence of grammatical class bigrams. In the case of raw text analysis, classical TF-IDF, supervised fastText, and contemporary transformer-based BERT are analyzed. In the case of grammatical classes, only TF-IDF and fastText are applied. In addition, the authors propose a sequence averaging method that works by dividing the text into smaller parts, classifying each part separately, and making the final classification based on averaging results from each part of the text. The study suggests that the TF-IDF on the raw text outperforms other methods and the sequence averaging improves the classification results for most of the analyzed schemas. Surprisingly, the BERT based method is the worst. This phenomenon is carefully analyzed and explained.","Deep learning has been increasingly incorporated into various computational pathology applications to improve its efficiency, accuracy, and robustness. Although successful, most previous approaches for image classification have crucial drawbacks. There exist numerous tasks in pathology, but one needs to build a model per task, i.e., a task-specific model, thereby increasing the number of models, training resources, and cost. Moreover, transferring arbitrary task-specific model to another task is still a challenging problem. Herein, we propose a task-agnostic generative and general pathology image classifier, so called GPC, that aims at learning from diverse kinds of pathology images and conducting numerous classification tasks in a unified model. GPC, equipped with a convolutional neural network and a Transformer-based language model, maps pathology images into a high-dimensional feature space and generates pertinent class labels as texts via the image-to-text classification mechanism. We evaluate GPC on six datasets for four different pathology image classification tasks. Experimental results show that GPC holds considerable potential for developing an effective and efficient universal model for pathology image analysis.","Highlights\n\u2022\nPotential features that affect stock returns are comprehensively studied.\n\u2022\nPerformance of 14 ensemble classifiers in stock returns prediction are compared.\n\u2022\nA feature selection method based on filters and voting techniques are proposed.\n\u2022\nThe most important features controlling the Iran stock market are identified.\nAbstract\nStock market prediction is considered as an important yet challenging aspect of financial analysis. The difficulty of forecasting arises from volatile and non-linear nature of stock market, which is affected by varied uncertain factors, ranging from financial ratios to macroeconomic indicators. Recent advances in machine learning, particularly ensembles, have made it possible for academic researchers and financial practitioners to forecast the stock market more efficiently. The novelty of this work is to evaluate how stock return in an oil-dependent country (i.e., Iran), which has been facing stagflation for a long time due to economic and political issues, is affected by fundamental and macroeconomic indicators. Our main objectives are to (1) find the most important fundamental and macroeconomic indicators that control the stock returns of companies listed on the Tehran Stock Exchange (TSE); (2) compare the performance of newly developed bagging- and boosting-based ensembles in predicting annual real stock returns of the TSE; and (3) develop multiclass classification models to forecast stock returns. Prior studies mainly focused on developing binary classification models, which simply predict whether stock returns will be positive or negative in the future. We, however, design multiclass classification models to provide more information for the investors and reduce the uncertainties associated with the prediction. To this end, we first provide a comprehensive list of 57 potential features affecting the stock returns. Next, the data are carefully preprocessed and fed to 14 different bagging- and boosting-based ensembles (e.g., Random Forest, LightGBM, XGBoost, Extra-Trees, AdaBoost, CatBoost) to predict the stock returns. The performance of ensembles is evaluated through different measures (e.g., accuracy, F-score, G-mean). We then propose a novel feature selection method to identify the most contributing features to the stock returns. Our proposed model identifies nearly 65% of 57 original features as redundancy, resulting in 20 most significant features. The selected features are fed to the mentioned ensembles to re-predict the stock returns. Finally, the performance of stock returns forecasts with and without selected features is compared. To design the ensembles, we employ the data from listed companies on the TSE for a 15-year period, spanning between 2005 and 2020. Results suggest that boosting ensembles, in general, outperform bagging-based methods. Among the boosting ensembles, XGBoost and AdaBoost provide the best and worst predictive performance, respectively. Among the bagging-like ensembles, Rotation Forest is the most accurate one, whereas Random Patches performs the worst. Further, our proposed feature selection approach effectively identifies the most representative features for stock returns prediction and can be used as a reliable framework for future investment decisions.","Humans effortlessly classify and recognize complex patterns even if their attributes are imprecise and often inconsistent. It is not clear how the brain processes uncertain visual information. We have recorded single cell responses to various visual stimuli in area V4 of the monkey\u2019s visual cortex. Different visual patterns are described by their attributes (condition attributes) and placed, together with the decision attributes, in a decision table. Decision attributes are divided into several classes determined by the strength of the neural responses. Small cell responses are classified as class 0, medium to strong responses are classified as classes 1 to n-1 (min(n)=3 ), and the strongest cell responses are classified as class n. The higher the class of the decision attribute the more preferred is the stimulus. Therefore each cell divides stimuli into its own family of equivalent objects.\nBy comparing responses of different cells we have found related concept classes. However, many different cells show inconsistency between their decision rules, which may suggest that parallel different decision logics may be implemented in the brain.","Occupant presence and their behaviour in the built environment significantly impact energy consumption in buildings. Currently, building systems are often operated based on assumed occupancy and fixed schedules, or occupants manually adjust them for their comfort, which sometimes leads to energy wastage. Also, the inherent complexity and unpredictability of occupancy patterns can contribute to disparities between simulated and actual energy consumption, underscoring the importance of selecting suitable methods for occupancy prediction. Various methods, such as occupancy cameras, thermal imagers, Passive Infrared Sensors, and Radio-Frequency Identification, can collect occupancy information. However, they often come with limitations including cost, complexity, and invasiveness. Hence, there is a growing interest in creating occupancy prediction models using an indirect approach based on indoor air quality (IAQ) data. However, this indirect approach must be further explored within the building simulation field. In this study, we apply an approach based on the novel QLattice algorithm for occupancy detection, utilizing a minimal sensing strategy with a comprehensive set of IAQ data. Furthermore, we compare the QLattice algorithm\u2019s performance with that of traditional machine learning (ML) algorithms such as Support Vector Machines (SVM), Decision Trees (DT), and XGBoost using metrics including accuracy, precision, recall, F1 score, AUC-ROC values, and computational time. The QLattice algorithm outperforms traditional ML algorithms in all evaluation metrics, achieving over 90% accuracy on the test dataset. Additionally, compared to traditional black-box ML algorithms, QLattice stands out for its explainability and interpretability, providing insights useful in decision-making.","This paper introduces a novel approach to address the challenges of transfer learning, which aims to efficiently train a classifier for a new domain using supervised information from similar domains. Traditional transfer learning methods may fail to maintain the discriminative features of the target domain due to the scarcity of labelled data and the use of irrelevant source domain data distribution subspace, resulting in poor metrics. To overcome these challenges, the proposed approach, called KDADP, transforms the data distribution of both the source and target domains into a lower-dimensional subspace while preserving their discriminatory information. The KDADP model maximizes between-class variance and minimizes within-class variance with L1 penalization, enabling the recovery of the most useful characteristics and reducing the model\u2019s complexity. Experimental results on three real-world domain adaptation datasets demonstrate that the proposed KDADP model significantly improves classification performance and outperforms state-of-the-art primitive, shallow, and deeper domain adaptation methods.","Support vector classification (SVC) is a well-known statistical technique for classification problems in machine learning and other fields. An important question for SVC is the selection of covariates (or features) for the model. Many studies have considered model selection methods. As is well-known, selecting one winning model over others can entail considerable instability in predictive performance due to model selection uncertainties. This paper advocates model averaging as an alternative approach, where estimates obtained from different models are combined in a weighted average. We propose a model weighting scheme and provide the theoretical underpinning for the proposed method. In particular, we prove that our proposed method yields a model average estimator that achieves the smallest hinge risk among all feasible combinations asymptotically. To remedy the computational burden due to a large number of feasible models, we propose a screening step to eliminate the uninformative features before combining the models. Results from real data applications and a simulation study show that the proposed method generally yields more accurate estimates than existing methods.","Graph neural networks (GNNs) have emerged as a powerful tool for analyzing graph data, where data are represented by nodes and edges. However, the conventional methods have limitations in analyzing graphs with diverse attributes and preserving crucial information during the graph embedding. As a result, there is a possibility of losing crucial information during the integration of individual nodes. To address this problem, we propose an attention-based readout with subgraphs for graph embedding that partitions the graph according to unique node attributes. This method ensures that important attributes are retained and prevents dilution of distinctive node features. The adjacency matrices and node feature matrices for the partitioned graphs go into a graph isomorphism network (GIN) to aggregate the features, where the attention mechanism merges the partitioned graphs to construct the whole graph embedding vector. Extensive experiments on six graph datasets demonstrate that the proposed method captures various local patterns and produces superior performance against the state-of-the-art methods for graph classification. Especially, on the challenging IMDB-MULTI dataset, our method achieves a significant performance gain of 27.87%p over the best method called MA-GCNN.","Identifying the severity of dysarthria is considered a diagnostic step in monitoring the patient\u2019s progress and a beneficial step in the transcription of dysarthric speech. In this paper, the effectiveness of using the multi-head attention mechanism (MHA) and the multi-task learning approach is explored for automated dysarthria severity level classification. Dysarthric speech utterances are represented by mel spectrograms and fed to a residual convolutional neural network for effective feature learning. Then the MHA module is added to identify the salient severity-highlighting periods. At the classification end, gender, age, and disorder-type identifications are employed as auxiliary tasks to share mutual information and leverage the severity classification. The performance of the proposed method is evaluated on the Universal Access Speech database. By giving a gain of 11.51% classification accuracy over the baseline system under the speaker-dependent scenario and 11.58% under the speaker-independent scenario, the proposed system demonstrates its potential for the dysarthria severity classification.\nHighlights\n\u2022\nSpectrograms of dysarthric speech encoded by a residual convolutional neural network.\n\u2022\nMulti-head attention identifies the severity-highlighting periods in spectrograms.\n\u2022\nMulti-task learning shares mutual information for improved severity classification.\n\u2022\nGender, age and disorder type identifications are employed as the auxiliary tasks.","Sentiment uncertainty is a key problem of sentiment classification. In this paper, we mainly focus on two issues with sentiment uncertainty, i.e., context-dependent sentiment classification and topic-dependent sentiment classification. This is the first work that applies three-way decisions to sentiment classification from the perspective of the decision-theoretic rough set model. We discuss the relationship between sentiment classification rules and thresholds involved in three-way decisions and then prove it. The experiment results on real data sets validate that our methods are satisfactory and can achieve better performance.","Agriculture is the underlying occupation of the vast people in India and it is a major economic contribution. Soil is prime for the vital nutrient supply to the crops and its yield. Determination of the type of soil which comprises of the clay, sand and silt particles in the respective proportion is indeed significant for the suitable crop selection and to identify the weeds growth. The most commonly utilized soil determination methods were International Pipette method and Pressure-plate apparatus method. In this research work, multiclass soil classification using machine learning and deep learning models for the appropriate determination of the soil type as Multi-Stacking ensemble model and a novel feature selection algorithm Q-HOG is proposed; since the Artificial Intelligence has led to furtherance in the smart agriculture. Besides, the images are collected from the exploration site vriddhachalam along with the soil datasets will increase the classification accuracy. The deep learning models Recurrent Neural Network(RNN), Long Short Term Memory(LSTM), Gated Recurrent Unit(GRU) and VGG16 are considered and the comprehensive evaluation of these different deep learning architectures and also the machine learning algorithms such as Na\u00efve-bayes, KNN, SVM are carried out and the obtained results are tabulated. Multi-stacking ensemble model for multi-classification is proposed with the Machine learning and deep learning algorithms and evaluated the performance with increased computation time. Among these models the proposed model outperformed in soil classification in-terms of accuracy as 98.96 percent, achieved precision as 96.14 percent, recall as 99.65 percent and the achieved F1-Score is 97.87 percent.","In this work, a fast k most similar neighbor (k-MSN) classifier for mixed data is presented. The k nearest neighbor (k-NN) classifier has been a widely used nonparametric technique in Pattern Recognition. Many fast k-NN classifiers have been developed to be applied on numerical object descriptions, most of them based on metric properties to avoid object comparisons. However, in some sciences as Medicine, Geology, Sociology, etc., objects are usually described by numerical and non numerical features (mixed data). In this case, we can not assume the comparison function satisfies metric properties. Therefore, our classifier is based on search algorithms suitable for mixed data and non-metric comparison functions. Some experiments and a comparison against other two fast k-NN methods, using standard databases, are presented.","With the enhancement of people's living standards and the rapid evolution of cyber-physical systems, residential environments are becoming smart and well-connected, causing a significant raise in overall energy consumption. As household appliances are major energy consumers, their accurate recognition becomes crucial to avoid unattended usage and minimize peak-time load on the smart grids, thereby conserving energy and making smart environments more sustainable. Traditionally, an appliance recognition model is trained at a central server (service provider) by collecting electricity consumption data via smart plugs from the clients (consumers), causing a privacy breach. Besides that, the data are susceptible to noisy labels that may appear when an appliance gets connected to a non-designated smart plug. While addressing these issues jointly, we propose a novel federated learning approach to appliance recognition, called FedAR+, enabling decentralized model training across clients in a privacy-preserving way even with mislabeled training data. FedAR+ introduces an adaptive noise handling method, essentially a joint loss function incorporating weights and label distribution, to empower the appliance recognition model against noisy labels. By deploying smart plugs in an apartment complex, we collect a labeled dataset that, along with two existing datasets, are utilized to evaluate the performance of FedAR+. Experimental results show that our approach can effectively handle up to 30% concentration of noisy labels while outperforming the prior solutions by a large margin on accuracy.","Often pieces of information are received sequentially over time. When did one collect enough such pieces to classify? Trading wait time for decision certainty leads to early classification problems that have recently gained attention as a means of adapting classification to more dynamic environments. However, so far results have been limited to unimodal sequences. In this pilot study, we expand into early classifying multimodal sequences by combining existing methods. Spatial-temporal transformers trained in the supervised framework of Classifier-Induced Stopping outperform exploration-based methods. We show our new method yields experimental AUC advantages of up to 8.7%.","Occupancy sensing and estimation in large commercial buildings has become a significant problem to be solved, with applications ranging from occupancy-based HVAC control to space planning, and security, etc. Thermal sensing is a promising technology to solve this problem, being easy to deploy in practice and allowing an actual occupancy count in a particular room without violating the data and privacy concerns. While initial strides have been made to solve this problem with thermal arrays, there are many problems that remain unsolved, including accuracy performance, overlapping of sensing areas that lead to under/over-counting, and data training requirements for different zones.\nIn this paper, we introduce TODOS 1, a novel system for estimating occupancy in intelligent buildings. TODOS uses a low-cost, low-power thermal sensor array along with a passive infrared sensor. We introduce a novel data processing pipeline that allows us to automatically extract features from the thermal images using an artificial neural network. Through an extensive experimental evaluation2, we show that TODOS provides occupancy detection accuracy of 98% to 100% under different scenarios. In addition, it solves the issue of occupancy over/under-counting by overlapping sensing areas when using multiple thermal sensors in large rooms. This is done by treating the entire area as a single input thermal image instead of partitioning the area into multiple thermal images individually processed. Furthermore, TODOS introduces a data augmentation technique that allows the generation of training data for rooms of different sizes and shapes, without requiring specific training data from each room. Using these data, TODOS can train specifically designed neural networks optimized for any room size and shape, and achieve almost the same level of occupancy detection accuracy in rooms where experimental labeled training data is available, making it a viable solution that generalizes to the different rooms in large buildings.","Highlights\n\u2022\nAn intelligent framework is provided for fNIRS mental arithmetic classification.\n\u2022\nThe matching pursuit analysis of the fNIRS has not been examined so far.\n\u2022\nA simple subject-specific channel selection methodology is proposed.\n\u2022\nA cascade feature selection approach is presented.\n\u2022\nExamining different classifiers, a maximum accuracy of 86.2% was achieved.\nAbstract\nRecently, a relatively new and emerging neuroimaging tool, functional near-infrared spectroscopy (fNIRS), has captured the attention of scientific disciplines, especially in designing a brain-computer interface. This technique can provide measures of the hemodynamic changes in the brain. Several attempts have been made to characterize the signals in different conditions. However, they mainly utilized conventional statistical approaches to describe the data. To our knowledge, the matching pursuit analysis of the fNIRS signals has not been examined so far. The current study analyzed and merged the matching pursuit-based indices of oxy-, deoxy-, and total-hemoglobin concentrations to classify mental tasks and rest conditions. It took advantage of a simple subject-specific channel selection methodology and a cascade feature selection approach. Maximum accuracy of 86.2% was achieved by examining different classifiers. In conclusion, the results of the proposed framework sketch promising future directions in the field.","Over the last two decades automatic facial expression recognition has become an active research area. Facial expressions are an important channel of non-verbal communication, and can provide cues to emotions and intentions. This paper introduces a novel method for facial expression recognition, by assembling contour fragments as discriminatory classifiers and boosting them to form a strong accurate classifier. Detection is fast as features are evaluated using an efficient lookup to a chamfer image, which weights the response of the feature. An Ensemble classification technique is presented using a voting scheme based on classifiers responses. The results of this research are a 6-class classifier (6 basic expressions of anger, joy, sadness, surprise, disgust and fear) which demonstrate competitive results achieving rates as high as 96% for some expressions. As classifiers are extremely fast to compute the approach operates at well above frame rate. We also demonstrate how a dedicated classifier can be consrtucted to give optimal automatic parameter selection of the detector, allowing real time operation on unconstrained video.","Abstract: The electromyogram (EMG), also known as an EMG, is used to assess nerve impulses in motor nerves, sensory nerves, and muscles. EMS is a versatile tool used in various biomedical applications. It is commonly employed to determine physical health, but it also finds utility in evaluating emotional well-being, such as through facial electromyography. Classification of EMG signals has attracted the interest of scientists since it is crucial for identifying neuromuscular disorders (NMDs). Recent advances in the miniaturization of biomedical sensors enable the development of medical monitoring systems. This paper presents a portable and scalable architecture for machine learning modules designed for medical diagnostics. In particular, we provide a hybrid classification model for NMDs. The proposed method combines two supervised machine learning classifiers with the discrete wavelet transform (DWT). During the online testing phase, the class label of an EMG signal is predicted using the classifiers\u2019 optimal models, which can be identified at this stage. The simulation results demonstrate that both classifiers have an accuracy of over 98%. Finally, the proposed method was implemented using an embedded CompactRIO-9035 real-time controller.","Skip Background and Objective Section\nBackground and Objective\nPrenatal fetal monitoring, which can monitor the growth and health of the fetus, is very vital for pregnant women before delivery. During pregnancy, it is crucial to judge whether the fetus is abnormal, which helps obstetricians carry out early intervention to avoid fetal hypoxia and even death. At present, clinical fetal monitoring widely used fetal heart rate monitoring equipment. Fetal heart rate and uterine contraction signals obtained by fetal heart monitoring equipment are important information to evaluate fetal health status.\nSkip Methods Section\nMethods\nThis paper is based on 1D-CNN (One Dimension Convolutional Neural Network) and GRU (Gate Recurrent Unit). We preprocess the obtained data and enhances them, to make the proportion of number of instances in different class in the training set is same.\nSkip Results Section\nResults\nIn model performance evaluation, standard evaluation indicators are used, such as accuracy, sensitivity, specificity, and ROC (receiver operating characteristic). Finally, the accuracy of our model in the test set is 95.15%, the sensitivity is 96.20%, and the specificity is 94.09%.\nSkip Conclusions Section\nConclusions\nIn fetal heart rate monitoring, this paper proposes a 1D-CNN and bidirectional GRU hybrid models, and the fetal heart rate and uterine contraction signals given by monitoring are used as input feature to classify the fetal health status. The results show that our approach is effective in evaluating fetal health status and can assists obstetricians in clinical decision-making. And provide a baseline for the introduction of 1D-CNN and bidirectional GRU hybrid models into the evaluation of fetal health status.","Performance of the pseudo-label (PL)-based self-supervised training depends greatly on the quality of estimated PLs. Recent studies have shown that label noise can remarkably impact downstream performance. Recently, research has demonstrated that mixup regularization is effective against noise memorization. In this work, we extend this previous study by exploring several recent forms of mixup, namely 2-step interpolation double mixup to enhance model robustness, mixup over speech frames for better recognition at the frame-level, moment exchange mixup to encourage utilization of moment information of speaker speech as they can reveal speaker style, and virtual mixup training to regularize the areas in-between training points to be locally-Lipschitz and enforce consistent predictions. We analyze their effect on the generalization of some state-of-the-art speaker verification (SV) systems and explore their combination via different multi-task learning-based approaches. Our results show that the proposed mixup formulations are aligned with the SV task and that our proposed multi-task learning-based approach can be beneficial to improve the performance and robustness of SV systems.","Detection of therapeutic peptide is a major research direction in the current biopharmaceutical field. However, traditional biochemical experimental detection methods take a lot of time. As supplementary methods for biochemical experiments, the computational methods can improve the efficiency of therapeutic peptide detection. Currently, most machine learning-based therapeutic peptide identification algorithms do not consider the processing of noisy samples. We propose a therapeutic peptide classifier, called weighted echo state networks based on subspace projection (WESN-SP), which reduces the bias caused by high-dimensional noisy features and noisy samples. WESN-SP is trained by sparse Bayesian learning algorithm (SBL) and introduces a weight coefficient for each sample by kernel dependence maximization-based subspace projection. The experimental results show that WESN-SP has better performance than other existing methods.","A novel method for creating diverse ensembles of image classifiers is proposed. The idea is that, for each base image classifier in the ensemble, a random image transformation is generated and applied to all of the images in the labeled training set. The base classifiers are then learned using features extracted from these randomly transformed versions of the training data, and the result is a highly diverse ensemble of image classifiers. This approach is evaluated on a benchmark pedestrian detection dataset and shown to be effective.","Once novel malware is detected, threat reports are written by security companies that discover it. The reports often vary in the terminology describing the behavior of the malware making comparisons of reports of the same malware from different companies difficult. To aid in the automated discovery of novel malware, it was recently proposed that novel malware could be detected by identifying behaviors. This assumes that a core set of behaviors are present in most, if not all, malware variants. However, there is a lack of malware datasets that are labeled with behaviors. Motivated by a need to label malware with a common set of behaviors, this work examines automating the process of labeling malware with behaviors identified in malware threat reports despite the variability of terminology. To do so, we examine several techniques from the natural language processing (NLP) domain. We find that most state-of-the-art word embedding NLP methods require large amounts of data and are trained on generic corpora of text data\u2014missing the nuances related to information security. To address this, we use simple feature selection techniques. We find that simple feature selection techniques generally outperform word embedding methods and achieve an increase of 6% in the F.5-score over prior work when used to predict MITRE ATT&amp;CK tactics in threat reports. Our work indicates that feature selection, which has commonly been overlooked by sophisticated methods in NLP tasks, is beneficial for information security related tasks, where more sophisticated NLP methodologies are not able to pick out relevant information security terms.","Conversation disentanglement aims to identify and group utterances from a conversation into separate threads. Existing methods primarily focus on disentangling multi-party conversations with three or more speakers, explicitly or implicitly incorporating speaker-related feature signals to disentangle. Most existing models require a large amount of human annotated data for model training, and often focus on pairwise relations between utterances, not accounting much for the conversational context. In this work, we propose a multi-task learning approach with a contrastive learning objective, DiSC, to disentangle conversations between two speakers -- a user and a virtual speech assistant, for a novel domain of e-commerce. We analyze multiple ways and granularities to define conversation \"threads''. DiSC jointly learns the relation between pairs of utterances, as well as between utterances and their respective thread context. We train and evaluate our models on multiple multi-threaded conversation datasets that were automatically created, without any human labeling effort. Experimental results on public datasets as well as real-world shopping conversations from a commercial speech assistant show that DiSC outperforms state-of-the-art baselines by at least 3%, across both automatic and human evaluation metrics. We also demonstrate how DiSC improves downstream dialog response generation in the shopping domain.","This paper presents a Prolog-based reasoning module to generate counterfactual explanations given the predictions computed by a black-box classifier. Our approach comprises four well-defined stages that can be applied to any structured pattern classification problem. Firstly, we pre-process the given dataset by imputing missing values and normalizing the numerical features. Secondly, we transform numerical features into symbolic ones using fuzzy clustering such that extracted fuzzy clusters are mapped to an ordered set of predefined symbols. Thirdly, we encode instances as a Prolog rule using the nominal values, the predefined symbols, the decision classes, and the confidence values. Fourthly, we compute the overall confidence of each Prolog rule using fuzzy-rough set theory to handle the uncertainty caused by transforming numerical quantities into symbols. This step comes with an additional theoretical contribution to a new similarity function to compare the previously defined Prolog rules involving confidence values. Finally, we implement a chatbot as a proxy between humans and the Prolog-based reasoning module to resolve natural language queries and generate counterfactual explanations. During the numerical simulations using synthetic datasets, we study the performance of our system when using different fuzzy operators and similarity functions.","Abstract\u2014It is important for schools and online learning platforms to mine student attitudes and opinions from student reviews so teachers know what needs to be improved. Previous studies have only focused on the semantic features of sentences, ignoring syntactic dependency relations in sentences. To address this problem, we build a graph convolutional network (GCN) on the syntactic dependency tree of sentences to exploit syntactic information for the first time in the field of online student reviews. Furthermore, in order to exploit positional relations and alleviate over-smoothing, we add relative aspect distance relations and stochastic edge elimination. On this basis, a novel multi-channel graph convolution model (MCGCN) is proposed, which constructs three GCN channels for syntactic dependency relations, relative aspect distance relations and stochastic edge elimination to extract relational features. Finally we use average pooling to fuse three relational features. Experiments on the CR23k dataset demonstrate that the overall performance of the model outperforms the state-of-the-art methods.","Skip BACKGROUND: Section\nBACKGROUND:\nParkinson\u2019s disease (PD) is a chronic neurodegenerative disorder characterized by motor impairments and various other symptoms. Early and accurate classification of PD patients is crucial for timely intervention and personalized treatment. Inertial measurement units (IMUs) have emerged as a promising tool for gathering movement data and aiding in PD classification.\nSkip OBJECTIVE: Section\nOBJECTIVE:\nThis paper proposes a Convolutional Wavelet Neural Network (CWNN) approach for PD classification using IMU data. CWNNs have emerged as effective models for sensor data classification. The objective is to determine the optimal combination of wavelet transform and IMU data type that yields the highest classification accuracy for PD.\nSkip METHODS: Section\nMETHODS:\nThe proposed CWNN architecture integrates convolutional neural networks and wavelet neural networks to capture spatial and temporal dependencies in IMU data. Different wavelet functions, such as Morlet, Mexican Hat, and Gaussian, are employed in the continuous wavelet transform (CWT) step. The CWNN is trained and evaluated using various combinations of accelerometer data, gyroscope data, and fusion data.\nSkip RESULTS: Section\nRESULTS:\nExtensive experiments are conducted using a comprehensive dataset of IMU data collected from individuals with and without PD. The performance of the proposed CWNN is evaluated in terms of classification accuracy, precision, recall, and F1-score. The results demonstrate the impact of different wavelet functions and IMU data types on PD classification performance, revealing that the combination of Morlet wavelet function and IMU data fusion achieves the highest accuracy.\nSkip CONCLUSION: Section\nCONCLUSION:\nThe findings highlight the significance of combining CWT with IMU data fusion for PD classification using CWNNs. The integration of CWT-based feature extraction and the fusion of IMU data from multiple sensors enhance the representation of PD-related patterns, leading to improved classification accuracy. This research provides valuable insights into the potential of CWT and IMU data fusion for advancing PD classification models, enabling more accurate and reliable diagnosis.","A classification is a problem of identifying the category (or the class) of an unknown-class observation using past historical data. One important issue in a classification is a class imbalanced problem which typically finds in a classification where the proportion of the target class is significantly smaller than others. A traditional classifier normally misclassifies an instance from this target class, called the minority class, as noise due to the small number of instances. Modification of the classification algorithm to handle a class imbalanced problem is a challenging task, especially for a random forest. In the random forest algorithm, the bootstrapping step is used to generate several subsets from a training data by random sampling uniformly with replacement. Most bootstrapping subsets may not even contain instances from the minority class which guarantee decision tree components to misclassify instances from the minority class. A random tree algorithm that needs to generate the bootstrapping subsets for each decision tree must assure the distribution of minority instances. This paper proposes a random forest algorithm using quartile-pattern bootstrapping by leveraging mass-ratio-variance outlier factor and minority condensation decision tree to handle this problem. The mass-ratio-variance outlier factor is a score assigned to each instance that will give a large value to an outlier and give a low value to instances surrounded by other instances in the same class. To evaluate the performance of this proposed algorithm, two synthesized datasets are used in the experiments. The experimental results show significant improvement when a dataset is imbalanced. The performance from the test dataset via F1 with the proposed algorithm is better than the performance from the traditional random forest algorithm.","Support vector machine (SVM), as a useful tool of classification, has been widely applied in many fields. However, it may incur computationally infeasibility on very large sample datasets. To handle this problem, this paper presents a novel sparse and robust SVM model with truncated Huber loss, which is known as L t h-SVM. To this end, we establish a first-order optimality condition of L t h-SVM based on the newly introduced P-stationary point, which allows us to define L t h support vectors and working set of L t h-SVM and proves that all of the L t h support vectors are a small portion of the whole training set. Furthermore, we develop a novel efficient alternating direction method of multipliers with working set (dubbed as L t h-ADMM) to address L t h-SVM, which is proven that sequence produced by L t h-ADMM is proved to be a local minimizer of L t h-SVM and enjoys a relatively low computational complexity if the size of the working set is very small. We compare L t h-ADMM with other nine state-of-the-art solvers on both synthetic and real datasets. The extensive numerical experiments demonstrate that L t h-ADMM is capable of delivering higher prediction accuracy, presenting a smaller number of support vectors and running quickly, especially, in large-scale datasets setting.\nHighlights\n\u2022\nWe present a new loss function, which enables us to construct a new sparse and robust truncated Huber loss SVM model.\n\u2022\nWe define the support vectors of truncated Huber loss SVM, which proves that the support vectors are a small portion of the training set.\n\u2022\nTo address the truncated Huber loss SVM, we propose a new alternating direction method of multipliers with working set.\n\u2022\nThe extensive numerical experiments demonstrate that our proposed algorithm is capable of delivering higher prediction accuracy.","Deep Learning (DL) has enabled considerable increases in the accuracy of classification tasks in several domains, including Human Activity Recognition (HAR). It is well-known that when data distribution changes between the training and test datasets, the accuracy can drop, sometimes significantly. However, some variability sources in HAR, such as sensor orientation, are only sometimes considered when evaluating these models. Therefore, we must understand how much such changes could impact current DL architectures. In this paper, under an orientation variability scenario, we evaluate three common DL architectures, DeepConvLSTM, TinyHAR, and Attend-and-Discriminate, to quantify the performance drop attributed to this shift. Our results show that all architectures show performance drops on average, as expected, but participants are affected differently from them, so they would fall short for some in classification accuracy in real-life settings where orientation can change across the wearing sessions of one participant or across participants. The performance change is related to the difference in distribution distance.","This paper presents a method for combining classifiers that uses k-nearest localized templates. The localized templates are estimated from a training set using C-means clustering algorithm, and matched to the decision profile of a new incoming sample by a similarity measure. The sample is assigned to the class which is most frequently represented among the k most similar templates. The appropriate value of k is determined according to the characteristics of the given data set. Experimental results on real and artificial data sets show that the proposed method performs better than the conventional fusion methods.","Although Placement of students is an intrinsic requirement for the students worldwide majorly for professional as well as for higher degree courses. However, often the students are not aware about their skill levels that are considered as the industry readiness parameters. The Institutions often take care of the students to upgrade the skill level to meet the requirement of the Industry. In order to analyze the students based on the several parameters of industry readiness intelligent methods are required to assess them. In this research work, different classification methods are applied on the existing placement data to evaluate whether a student will get a job or not. Accuracies of these methods are compared using a real life data set. This analysis will help the Institutes to take the decision how long the training programs will continue. The result of the classification methods have been improved further using Random oversampling.","Automated Machine Learning (AutoML) is a promising direction for democratizing AI by automatically deploying Machine Learning systems with minimal human expertise. The core technical challenge behind AutoML is optimizing the pipelines of Machine Learning systems (e.g. the choice of preprocessing, augmentations, models, optimizers, etc.). Existing Pipeline Optimization techniques fail to explore deep interactions between pipeline stages/components. As a remedy, this paper proposes a novel neural architecture that captures the deep interaction between the components of a Machine Learning pipeline. We propose embedding pipelines into a latent representation through a novel per-component encoder mechanism. To search for optimal pipelines, such pipeline embeddings are used within deep-kernel Gaussian Process surrogates inside a Bayesian Optimization setup. Furthermore, we meta-learn the parameters of the pipeline embedding network using existing evaluations of pipelines on diverse collections of related datasets (a.k.a. meta-datasets). Through extensive experiments on three large-scale meta-datasets, we demonstrate that pipeline embeddings yield state-of-the-art results in Pipeline Optimization.","The Support Vector Machine (SVM) is a widely used algorithm for batch classification with a run and memory efficient counterpart given by the Core Vector Machine (CVM). Both algorithms have nice theoretical guarantees, but are not able to handle data streams, which have to be processed instance by instance. We propose a novel approach to handle stream classification problems via an adaption of the CVM, which is also able to handle multiclass classification problems. Furthermore, we compare our Multiclass Core Vector Machine (MCCVM) approach against another existing Minimum Enclosing Ball (MEB)-based classification approach. Finally, we propose a real-world streaming dataset, which consists of changeover detection data and has only been analyzed in offline settings so far.","Computer vision is augmented in various manufacturing industries to perform automated inspection operations accurately and efficiently. It has been observed that the performance of vision-based inspection approaches degrades considerably upon utilizing images captured under shop-floor conditions. This work proposes utilizing Histogram Equalization and adversarial training through Neural Structure Learning (NSL) for developing a robust vision-based Surface Defect Classification framework. A novel deep neural network architecture obtains adversarial samples in the extracted feature space instead of obtaining the same in the original input image space. The architecture can be easily integrated and employed with various machine learning models. A commonly employed steel surface defect dataset (NEU) with practical relevance to industrial cases is selected for the model training and experimental studies. The robustness of the proposed approach is evaluated over the Extended Diversity Enhanced (ENEU) dataset derived by simulating image acquisition variations similar to shop floor conditions. The results reveal that the proposed approach enhances the recognition accuracy of the baseline method from 87.7% to 92.4% over ENEU. The prediction accuracy of the proposed approach is considerably better than the traditional methods and deep learning competitors over ENEU. The qualitative and quantitative comparison of results obtained using the present approach with methods reported in the literature demonstrates the effectiveness of adversarial training in improving the generalization abilities of machine learning models.","Imbalanced datasets are commonly observed in various real-world applications, presenting significant challenges in training classifiers. When working with large datasets, the imbalanced issue can be further exacerbated, making it exceptionally difficult to train classifiers effectively. To address the problem, over-sampling techniques have been developed to linearly interpolating data instances between minorities and their neighbors. However, in many real-world scenarios such as anomaly detection, minority instances are often dispersed diversely in the feature space rather than clustered together. Inspired by domain-agnostic data mix-up, we propose generating synthetic samples iteratively by mixing data samples from both minority and majority classes. It is non-trivial to develop such a framework, the challenges include source sample selection, mix-up strategy selection, and the coordination between the underlying model and mix-up strategies. To tackle these challenges, we formulate the problem of iterative data mix-up as a Markov decision process (MDP) that maps data attributes onto an augmentation strategy. To solve the MDP, we employ an actor-critic framework to adapt the discrete-continuous decision space. This framework is utilized to train a data augmentation policy and design a reward signal that explores classifier uncertainty and encourages performance improvement, irrespective of the classifier's convergence. We demonstrate the effectiveness of our proposed framework through extensive experiments conducted on seven publicly available benchmark datasets using three different types of classifiers. The results of these experiments showcase the potential and promise of our framework in addressing imbalanced datasets with diverse minorities.","In this poster, we present a Machine Learning (ML) technique to predict the number of iterations needed for a Pathfinder-based FPGA router to complete a routing problem. Given a placed circuit, our technique uses features gathered on each routing iteration to predict if the circuit is routable and how many more iterations will be required to successfully route the circuit. This enables early exit for routing problems which are unlikely to be completed in a target number of iterations. Such early exit may help to achieve a successful route within tractable time by allowing the user to quickly retry the circuit compilation with a different random seed, a modified circuit design, or a different FPGA. We demonstrate our predictor in the VTR 8 framework; compared to VTR's predictor, our ML predictor incurs lower prediction errors on the Koios Deep Learning benchmark suite. This corresponds with an approximate time saving of 48% from early rejection of unroutable FPGA designs while also successfully completing 5% more routable designs and having a 93% shorter early exit latency.","Search engines are tools used to find information on the Internet. Since the web has a plethora of websites, the engine queries the majority of active sites and builds a database organized according to keywords utilized in the search. Because of this, when a user types a few descriptive words on the home page of the search engine, the search function lists websites corresponding to these keywords. However, there are some problems with this search approach. For instance, if a user wants information about the word Jaguar, most search results are animals and cars. This is a polysemic problem that forces search engines to always provide the most popular but not the most relevant results. This article presents a study of using sentiment technology to help news classification and categorization and improve the classification accuracy. We have introduced a smart search function embedded into a search engine to tackle polysemic issues and record relevant results to determine their sentimentality. Therefore, this study presents a topic that involves several aspects of natural language processing (NLP) and sentiment analysis for news categorization and classification. A web crawler was used to collect British Broadcasting Corporation (BBC) news across the Internet, carried out preprocessing of text by using NLP, and applied sentiment analysis methods to determine the polarity of the processed text data. The sentimentality represents negative, positive, or neutral polarities assigned by the sentiment analysis algorithms. The research utilized the BBC news site to collect different information using a web crawler and a database to explore the sentimentality of BBC news. The natural language toolkit (NLTK) and BM25 indexed and preprocessed patterns in the database. The experimental results depict the proposed search function surpassing normal search with an accuracy rate of 85%. Moreover, the results show a negative polarity of BBC news using the Sentistrength algorithm. Furthermore, the Valence Aware Dictionary and sEntiment Reasoner (VADER) was the best-performing sentiment analysis model for news classification. This model obtained an accuracy of 85% using data collected with the proposed smart function.","Aspect-based sentiment analysis is a fine-grained sentiment classification task. The task analysis determines the corresponding sentiment polarity based on the aspect words. Recent approaches perform aspect-based sentiment analysis outstandingly by using graph convolution networks over dependency trees, but the approach relies heavily on the accuracy of dependency analysis results and is limited by the complexity of informal online comment expression. To overcome these challenges, in this paper, we explore two kinds of sentence information to enrich the expression of the sentences, i.e., syntactic information obtained from the dependency trees and semantic information obtained by the attention mechanism. On this basis, we propose a dual cooperative graph attention networks(D-CoGAT) model for cooperatively learning sentence information. We utilize the attention mechanism for the dependency matrix generated by the dependency tree. The attention mechanism is also introduced for the embedding of sentences. Furthermore, we propose adaptive modules to adaptively scale the features from grammar and semantics, and design pyramid modules to enhance the fitting and extraction of both features. We conducted experiments on three publicly available datasets, and extensive experimental results demonstrated the effectiveness of our method.","Leukemia is a form of blood cancer that damages the cells in the blood and bone marrow of the human body. It produces cancerous blood cells that disturb the human's immune system and significantly affect bone marrow's production ability to effectively create different varieties of blood cells like red blood cells (RBCs) and white blood cells (WBC), and platelets. Different kinds of manual methods have been used, but all these techniques are slow, labour-intensive, inaccurate, and need a lot of human experience and dedication. To deal with such manual methods, different researchers used different machine learning algorithms to classify the cells into normal and blast cells. However, still, the problem is complex blood characteristics. In this paper, we have proposed a robust diagnosis system to classify leukemia and its subtypes. Acute lymphocytic leukemia (ALL) is classified into subtypes based on FAB classification, such as L1, L2 and L3 types with better performance. Our model outperformed as compared to other state-of-the-art approaches.","Enormous researches have been accomplished to look at cost-effective and economical tourist's places of interest and sightseeing conditions of a place using free and near-real-time information resources such as social media, WebPages, and Maps. This paper shows all the possible information for the development of a tourist or trip planning system are gathered by using the geographic information mining based on their tourist place preferences and gathered information in the selected region. The dynamic review clusters created using support vector machine classifiers are used to recognise the tourist places of rural and urban areas in India that make sure about the accessibility of basic necessary facilities like clear routes, shutting places, etc. Support vector machine is considered as most efficient algorithm to recognise such kind of clusters. Finally, this paper gives valuable suggestions for tourists or tourism planners by using WebPages or social media data.","Infertility is a global issue. Total fertility rate declined from over 5 live births per woman in 1950\u20131955 to 2.5 births per woman in 2010\u20132015. Female infertility is 37% globally and 12.5% in India. Female infertility can be reduced by identifying and treating the underlying cause early. Ovulatory diseases contribute 25% globally to female infertility. Various female pelvic imaging methods can diagnose ovulatory problems. Diagnostic ultrasound is chosen because it is radiation- and contrast-free and cost-effective. The intensity-based grouping and textural data were used for the detection of follicles and cysts in the ovary, which is based on machine learning (ML). Ovarian diagnosis was given a major boost thanks to the application of machine-learning algorithms, which permitted a success rate of 97% and significantly improved the overall quality of the process. Standard machine-learning strategies have been looked into for the purpose of ovarian classification. In order to determine which method of classification produces the most accurate findings, we constructed three distinct models utilising artificial neural networks, discriminant classifiers, and support vector machines. When the results of the various created classifiers were compared, it was discovered that SVM had the highest accuracy (98.5%). This ingenious tool, which may be classified as a decision support system, will assist the attending physician in reaching the appropriate determination and preventing an error in his or her interpretation.","Highlights\n\u2022\nWe present a mathematical framework to learn invariance to certain image transformations.\n\u2022\nThe method is computationally efficient, data-efficient, and has no hyper-parameters.\n\u2022\nThe method is robust where the training and testing distributions are different.\n\u2022\nFreely available software implementing the proposed method is provided.\nAbstract\nDeep convolutional neural networks (CNNs) are broadly considered to be state-of-the-art generic end-to-end image classification systems. However, they are known to underperform when training data are limited and thus require data augmentation strategies that render the method computationally expensive and not always effective. Rather than using a data augmentation strategy to encode invariances as typically done in machine learning, here we propose to mathematically augment a nearest subspace classification model in sliced-Wasserstein space by exploiting certain mathematical properties of the Radon Cumulative Distribution Transform (R-CDT), a recently introduced image transform. We demonstrate that for a particular type of learning problem, our mathematical solution has advantages over data augmentation with deep CNNs in terms of classification accuracy and computational complexity, and is particularly effective under a limited training data setting. The method is simple, effective, computationally efficient, non-iterative, and requires no parameters to be tuned. Python code implementing our method is available at https://github.com/rohdelab/mathematical_augmentation. Our method is integrated as a part of the software package PyTransKit, which is available at https://github.com/rohdelab/PyTransKit.","To provide researcher and operation personnel with recommendations on the condition of equipment so as to ensure the safe, reliable, and smooth operation of the EAST-NBI (Experimental Advanced Superconducting Tokamak Neutral Beam Injection) power system, we propose a memory attention mechanism MacBERT (MLM as correction Bidirectional Encoder Representation from Transformers) text classification model. Firstly, we use MacBERT to generate word vectors containing context, which alleviates the masking differences in pre-training and fine-tuning phases. Secondly, the deep features are further extracted and important parts are highlighted through the memory attention module. Finally, the Linear layer and Softmax layer are used to find the label with the highest predicted probability. Experimental results show that the proposed model performs well in the classification of maintenance records for the EAST-NBI power system, which shows certain research significance.","Classical supervised machine learning (ML) follows the assumptions of closed-world learning. However, this assumption does not work in an open-world dynamic environment. Therefore, the automated systems must be able to discover and identify unseen instances. Open-world ML can deal with unseen instances and classes through a two-step process: (1) discover and classify unseen instances and (2) identify novel classes discovered in step (1). Most existing research on open-world machine learning (OWML) only focuses on step 1. However, performing step 2 is required to build intelligent systems. The proposed framework comprises three different but interconnected modules that discover and identify unseen classes. Our in-depth performance evaluation establishes that the proposed framework improves open accuracy by up to 8% compared to the state-of-the-art models.\nHighlights\n\u2022\nProposed framework presents a novel model InSim for open text classification.\n\u2022\nWe introduce neighborhood blending that improves open accuracy over existing models.\n\u2022\nGives better results for both unseen instances discovery and unseen class identification.\n\u2022\nIdentified novel classes from unseen data are close to the ground truth.\n\u2022\nImproved accuracy in the open world can help AI machines to become robust &amp; realistic.","Non-Small Cell Lung Cancer (NSCLC) exhibits intrinsic heterogeneity at the molecular level that aids in distinguishing between its two prominent subtypes \u2014 Lung Adenocarcinoma (LUAD) and Lung Squamous Cell Carcinoma (LUSC). This paper proposes a novel explainable AI (XAI)-based deep learning framework to discover a small set of NSCLC biomarkers. The proposed framework comprises three modules \u2014 an autoencoder to shrink the input feature space, a feed-forward neural network to classify NSCLC instances into LUAD and LUSC, and a biomarker discovery module that leverages the combined network comprising the autoencoder and the feed-forward neural network. In the biomarker discovery module, XAI methods uncovered a set of 52 relevant biomarkers for NSCLC subtype classification. To evaluate the classification performance of the discovered biomarkers, multiple machine-learning models are constructed using these biomarkers. Using 10-Fold cross-validation, Multilayer Perceptron achieved an accuracy of 95.74% (\u00b11.27) at 95% confidence interval. Further, using Drug-Gene Interaction Database, we observe that 14 of the discovered biomarkers are druggable. In addition, 28 biomarkers aid the prediction of the survivability of the patients. Out of 52 discovered biomarkers, we find that 45 biomarkers have been reported in previous studies on distinguishing between the two NSCLC subtypes. To the best of our knowledge, the remaining seven biomarkers have not yet been reported for NSCLC subtyping and could be further explored for their contribution to targeted therapy of lung cancer.\nHighlights\n\u2022\nDiscovered 52 NSCLC biomarkers using a novel XAI-based deep learning framework.\n\u2022\nAchieved improved accuracy of 95.7% in subtype classification of NSCLC.\n\u2022\n45 out of 52 biomarkers have been identified as lung cancer biomarkers in literature.\n\u2022\n14 out of 52 biomarkers are found potentially druggable.\n\u2022\nThe XAI-based feature selection outperforms six popular feature selection methods.","Ransomware attacks are one of the most dangerous related crimes in the coin market. To increase the challenge of fighting the attack, early detection of ransomware seems necessary. In this article, we propose a high-performance Bitcoin transaction predictive system that investigates Bitcoin payment transactions to learn data patterns that can recognize and classify ransomware payments for heterogeneous bitcoin networks into malicious or benign transactions. The proposed approach makes use of three supervised machine learning methods to learn the distinctive patterns in Bitcoin payment transactions, namely, logistic regression (LR), random forest (RF), and Extreme Gradient Boosting (XGBoost). We evaluate these ML-based predictive models on the BitcoinHeist ransomware dataset in terms of classification accuracy and other evaluation measures such as confusion matrix, recall, and F1-score. It turned out that the experimental results recorded by the XGBoost model achieved an accuracy of 99.08%. As a result, the resulting model accuracy is higher than many recent state-of-the-art models developed to detect ransomware payments in Bitcoin transactions.","Large-scale natural soundscapes are remarkably complex and offer invaluable insights into the biodiversity and health of ecosystems. Recent advances have shown promising results in automatically classifying the sounds captured using passive acoustic monitoring. However, the accuracy performance and lack of transferability across diverse environments remains a challenge. To rectify this, we propose a robust and flexible ecoacoustics sound classification grid search-based framework using optimised machine learning algorithms for the analysis of large-scale natural soundscapes. It consists of four steps: pre-processing including the application of spectral subtraction denoising to two distinct datasets extracted from the Australian Acoustic Observatory, feature extraction using Mel Frequency Cepstral Coefficients, feature reduction, and classification using a grid search approach for hyperparameter tuning across classifiers including Support Vector Machine, k-Nearest Neighbour, and Artificial Neural Networks. With 10-fold cross validation, our experimental results revealed that the best models obtained a classification accuracy of 96% and above in both datasets across the four major categories of sound (biophony, geophony, anthrophony, and silence). Furthermore, cross-dataset validation experiments using a pooled dataset highlight that our framework is rigorous and adaptable, despite the high variance in possible sounds at each site.","In the era of big data, the data in many business scenarios are characterized by a small number of labelled samples and a large number of unlabelled samples. It is quite difficult to classify and identify such data and provide effective decision support for a business. A commonly employed processing method in this kind of data scenario is the disagreement-based semisupervised learning method, i.e., exchanging high-confidence samples among multiple models as pseudolabel samples to improve each model\u2019s classification performance. As such pseudolabel samples inevitably contain label noise, they may interfere with the subsequent model learning and damage the robustness of the ensemble model. To solve this problem, a semisupervised classification algorithm based on noise learning theory and a disagreement cotraining framework is proposed. In this model, first, the probably approximately correct (PAC) estimation theory under label noise conditions is applied, the relationship between the label noise level and model robust estimation in the process of multiround cotraining is discussed, and a disagreement elimination algorithm framework based on multiple-model (feature argument and select (FANS) algorithm and L1 penalized logistics regression (PLR) algorithm) cotraining is constructed based on this theoretical relationship. The experimental results show that the algorithm proposed in this paper gives not only a high-confidence sample set that meets the upper bound constraint of the label noise level but also a robust ensemble model capable of resisting sampling bias. The work performed in this paper provides a new research perspective for semisupervised learning theory based on disagreement.","Few-shot open-set recognition (FSOSR) has become a great challenge, which requires classifying known classes and rejecting the unknown ones with only limited samples. Existing FSOSR methods mainly construct an ambiguous distribution of known classes from scarce known samples without considering the latent distribution information of unknowns, which degrades the performance of open-set recognition. To address this issue, we propose a novel loss function called multirelation margin (MRM) loss that can plug in few-shot methods to boost the performance of FSOSR. MRM enlarges the margin between different classes by extracting the multi-relationship of paired samples to dynamically refine the decision boundary for known classes and implicitly delineate the distribution of unknowns. Specifically, MRM separates the classes by enforcing a margin while concentrating samples of the same class on a hypersphere with a learnable radius. In order to better capture the distribution information of each class, MRM extracts the similarity and correlations among paired samples, ameliorating the optimization of the margin and radius. Experiments on public benchmarks reveal that methods with MRM loss can improve the unknown detection of AUROC by a significant margin while correctly classifying the known classes.","This paper presents a discriminative model of multivariate pattern classification, based on functional magnetic resonance imaging (fMRI) and anatomical template. As a measure of brain function, Regional homogeneity (ReHo) is calculated voxel by voxel, and then a widely used anatomical template is applied on ReHo map to parcelate it into 116 brain regions. The mean and standard deviation of ReHo values in each region are extracted as features. Pseudo-Fisher Linear Discriminant Analysis (PFLDA) is performed for training samples to generate discriminative model. Classification experiments have been carried out in 48 schizophrenia patients and 35 normal controls. Under a full leave-one-out (LOO) cross-validation, correct prediction rate of 80% is achieved. Anatomical parcellation process is proved useful to improve classification rate by a control experiment. The discriminative model shows its ability to reveal abnormal brain functional activities and identify people with schizophrenia.","The traditional convolutional neural network (CNN) only focuses on the features of the last layer of the network, ignoring those of other layers. The detailed information of the shallow network can improve the accuracy of classification to some extent. With the deepening of the network, the problems such as gradient disappearance and explosion are obvious. In order to alleviate this problem, a method for residual network image classification with multi-scale feature fusion is proposed in the paper. First of all, the Random Image Cropping and Patching (RICAP), a data augmentation method, is adopted to cut and splice the new training samples from the training set, based on which feature maps of different sizes are obtained by residual modules. Secondly, after reducing the dimension of high-level feature maps, all the feature maps with the same dimension are processed by the multi-scale fusion. Finally, these feature vectors are input into the Softmax classifier for training and classification, and the method of learning rate decay is used in the training process. The experiment results indicate that this method has a good performance on image classification based on the public datasets of MNIST and CIFAR-10.","Speech emotion recognition (SER) is a crucial aspect of affective computing and human-computer interaction, yet effectively identifying emotions in different speakers and languages remains challenging. This paper introduces SER-Fuse, a multi-modal SER application that is designed to address the complexities of multiple speakers and languages. Our approach leverages diverse audio/speech embeddings and text embeddings to extract optimal features for multi-modal SER. We subsequently employ multi-feature fusion to integrate embedding features across modalities and languages. Experimental results archived on the English-Chinese emotional speech (ECES) dataset reveal that SER-Fuse attains competitive performance in the multi-lingual approach compared to the single-lingual approaches. Furthermore, we provide the implementation of SER-Fuse for download at https://github.com/nhattruongpham/SER-Fuse to support reproducibility and local deployment.","The support vector classification-regression machine for k-class classification (K-SVCR) is a novel multi-class classification approach based on the \u201c1-versus-1-versus-rest\u201d structure. In this work, we suggested an efficient model by proposing the p-norm\n(\n0\n&lt;\np\n&lt;\n1\n)\ninstead of the 2-norm for the regularization term in the objective function of K-SVCR that can be used for feature selection. We derived lower bounds for the absolute value of nonzero entries in every local optimal solution of the p-norm based model. Also, we provided upper bounds for the number of nonzero components of the optimal solutions. We explored the link between solution sparsity, regularization parameters, and the p-choice.","Breast cancer is the most common and deadly type of cancer in the world. Based on machine learning algorithms such as XGBoost, random forest, logistic regression, and K-nearest neighbor, this paper establishes different models to classify and predict breast cancer, so as to provide a reference for the early diagnosis of breast cancer. Recall indicates the probability of detecting malignant cancer cells in medical diagnosis, which is of great significance for the classification of breast cancer, so this article takes recall as the primary evaluation index and considers the precision, accuracy, and F1-score evaluation indicators to evaluate and compare the prediction effect of each model. In order to eliminate the influence of different dimensional concepts on the effect of the model, the data are standardized. In order to find the optimal subset and improve the accuracy of the model, 15 features were screened out as input to the model through the Pearson correlation test. The K-nearest neighbor model uses the cross-validation method to select the optimal k value by using recall as an evaluation index. For the problem of positive and negative sample imbalance, the hierarchical sampling method is used to extract the training set and test set proportionally according to different categories. The experimental results show that under different dataset division (8\u2009:\u20092 and 7\u2009:\u20093), the prediction effect of the same model will have different changes. Comparative analysis shows that the XGBoost model established in this paper (which divides the training set and test set by 8\u2009:\u20092) has better effects, and its recall, precision, accuracy, and F1-score are 1.00, 0.960, 0.974, and 0.980, respectively.","Support vector machine constructs an optimal classification hyperplane by support vectors. While samples near the boundary are overlapped seriously, it not only increases the burden of computation but also decreases the generalization ability. An improved SVM: NN-SVM algorithm was proposed to solve the above problems in literature [1]. NN-SVM just reserves or deletes a sample according to whether its nearest neighbor has same class label with itself or not. However, its generalization ability will be decreased by samples intermixed in another class. Therefore, in this paper, we present an improved NN-SVM algorithm: it prunes a sample according to its nearest neighbor\u2019s class label as well as distances between the sample and its k congener nearest neighbors. Experimental results show that the improved NN-SVM is better than NN-SVM in accuracy of classification and the total training and testing time is comparative to that of NN-SVM.","Word vector is an important tool for natural language processing (NLP) tasks such as text classification. However, existing static language models such as Word2vec cannot solve the polysemy problem, leading to a decline in text classification performance. To solve this problem, this paper proposes a method for making Chinese word vector dynamic (MCWVD). The part of speech (POS) is used to solve the ambiguity problem caused by different POS. The POS structure graph is constructed and the syntactic structure information of POS features is extracted by GCN (Graph Convolutional Network). POS vector and word vector are concatenated into PW (POS-Word) vector. Parametric matrix is added to improve the fusion effect of POS and word features. Multilayer attention is used to distinguish the importance of different features and further update the vector expression of word vectors about the current context. Experiments on Chinese datasets THUCNews and SogouNews show that MCWVD effectively improves the accuracy of text classification and achieves better performance than CoVe (Context Vectors) and ELMo (Embeddings from Language Models). MCWVD also achieves similar performance to BERT and GPT-1 (Generative Pre-Training), but with a much lower computational cost and only 4% of BERT parameters.","Machine learning methods have become an effective strategy commonly used in quantitative hedge funds, which can maximize profits and reduce investment risks to a certain extent. Traditional stock forecasting systems are usually based on a single attribute of stock data. There are two main challenges in this process: 1) Use suitable processing methods to deal with highly nonlinear time series data such as stocks. 2) Using a single class of stock data for training does not capture the correlation between other related data and the training data. Based on RBF neural network, this research introduces view weighting and collaborative learning mechanism, and proposes a MV-RBF model. It mainly includes the following contributions: 1) By comparing the experimental results of MV-RBF model with the single-view model, its effectiveness and feasibility are verified. 2) The MV-RBF model was compared with other commonly used classification models to analyze its advantages and disadvantages. 3) Study the relevant parameters, stability and other indicators of MV-RBF model. The experimental results show that compared with the single view model and most common classification models, MV-RBF has certain improvement in the prediction accuracy.","Sleep is a state that coordinates all the human body's major functions. Irregularity of sleep or its deprivation disrupts the physical and mental state of a person. An automated method that can analyze sleep stages can help sleep physicians identify and treat various sleep-related ailments is the need of the hour. The property of the Synchrosqueezed transform to concentrate the time\u2013frequency representation of a signal around the ridges is used to extract discriminatory information. EEG signal is divided into seven sub-bands, and features for six sleep stages are extracted. These features include Normalized Sub-band Power, Inter-quartile Range, Mean Absolute Deviation, Movement, and Fourier Synchrosqueezed Transform based features for the six sleep stages over the seven EEG sub-bands. Sleep-EDF, Sleep EDF Extended, and DREAMS databases have marked sleep stages following the Rechtschaffen and Kales (R&amp;K) and the American Academy of Sleep Medicine (AASM) criterion, are used to extract the proposed features. The performance of the SVM classifier is evaluated based on 10-fold cross-validation on the three databases for 2 to 6 class problems. The comparative performance with the recently published results using the same database shows improved performance of the proposed work. Further, the effect of the gamma band is also studied, and its inclusion shows improvement in the proposed model performance.","Skip Abstract Section\nAbstract\nThe earth\u2019s ecology is well balanced and protected by forests. On the other hand, forest fires affect forest resources, thus causing both economical and ecological losses. Hence, preserving forest resources from fires is very essential to reduce environmental disasters. Controlling forest fire at an early stage is necessary to control their spread. This requirement enforces the necessity of fast and reliable fire detection algorithms. In this paper, a color models aware dynamic feature extraction for forest fire detection using machine learning classifiers is proposed to achieve early detection of fire and reduced false alarm rate. The proposed algorithm extracts fire detection index, wavelet energy, and gray level co-occurrence matrix features from RGB, L*a*b*, and YCbCr color models respectively to train the machine learning classifiers. The performance of the proposed model is analysed using various machine learning algorithms and the standard classification metrics. The proposed color-aware feature extraction gives precision, recall, F1-score, and accuracy of 99, 95, 94, and 97% respectively for the K-nearest neighbourhood model. The support vector machine model delivers 98, 95, 93, and 96.5% respectively. The accuracy of the proposed model is improved by a minimum of 3%, and a maximum of 11% than other color models. Similarly, the false rate reduction is a minimum of 5% and a maximum of 17% than other models.","When employing supervised machine learning to analyze network traffic, the heart of the task often lies in developing effective features for the ML to leverage. We develop GGFAST, a unified, automated framework that can build powerful classifiers for specific network traffic analysis tasks, built on interpretable features. The framework uses only packet sizes, directionality, and sequencing, facilitating analysis in a payload-agnostic fashion that remains applicable in the presence of encryption.\nGGFAST analyzes labeled network data to identify n-grams (\"snippets\") in a network flow's sequence-of-message-lengths that are strongly indicative of given categories of activity. The framework then produces a classifier that, given new (unlabeled) network data, identifies the activity to associate with each flow by assessing the presence (or absence) of snippets relevant to the different categories.\nWe demonstrate the power of our framework by building---without any case-specific tuning---highly accurate analyzers for multiple types of network analysis problems. These span traffic classification (L7 protocol identification), finding DNS-over-HTTPS in TLS flows, and identifying specific RDP and SSH authentication methods. Finally, we demonstrate how, given ciphersuite specifics, we can transform a GGFAST analyzer developed for a given type of traffic to automatically detect instances of that activity when tunneled within SSH or TLS.","No abstract available.","Tropical Cyclone (TC) intensity estimation is a continuous label classification problem, which aims to build a mapping relationship from TC images to intensities. Due to the similar visual appearance of TCs in adjacent intensities, the discriminative image representation plays an important role in TC intensity estimation. Existing works mainly revolve around the continuity of intensity which may result in a crowded feature distribution and perform poorly at distinguishing the boundaries of categories. In this paper, we focus on jointly learning category-level and instance-level representations from tropical cyclone images. Specially, we propose a general framework containing a CI-extractor and a classifier, inside which the CI-extractor is used to extract an instance-separable and category-discriminative representation between images. Meanwhile, an inter-class distance consistency (IDC) loss is applied on top of the framework which can lead to a more uniform feature distribution. In addition, a non-parameter smoothing algorithm is proposed to aggregate temporal information from the image sequence. Extensive experiments demonstrate that our method, with the result of 7.35 knots at RMSE, outperforms the state-of-the-art TC intensity estimation method on the TCIR dataset.","Breast cancer is main reason for mortality in woman. Prediction of breast cancer is a challenging task in medical data analysis. Doctors and pathologist required some automated tools to take decision and to differentiate between malignant and benign tumour. A machine learning (ML) algorithm helps lot to take decisions and to perform diagnosis from the data collected by medical field. Various researches show that ML techniques are helpful for decision making in breast cancer prediction. In this paper, we used various ML Classification techniques: Na\u00efve Bayes(NB), Logistic regression (LR),Support vector machine(SVM),K-Nearest Neighbor (KNN), Decision Tree(DT), and ensemble techniques: Random forest(RF), Adaboost, XGBoost on breast cancer dataset and evaluated by using different performance measure. It has been found that both decision tree and XGBoost classifier has highest accuracy 97% among all model and highest AUC 0.999 obtained for XGBoost classifier.","The complex problems of multiclass imbalance, virtual or real concept drift, concept evolution, high-speed traffic streams and limited label cost budgets pose severe challenges in network traffic classification tasks. In this paper, we propose a multiclass imbalanced and concept drift network traffic classification framework based on online active learning (MicFoal), which includes a configurable supervised learner for the initialization of a network traffic classification model, an active learning method with a hybrid label request strategy, a label sliding window group, a sample training weight formula and an adaptive adjustment mechanism for the label cost budget based on a periodic performance evaluation. In addition, a novel uncertain label request strategy based on a variable least confidence threshold vector is designed to address the problems of a variable multiclass imbalance ratio or even the number of classes changing over time. Experiments performed based on eight well-known real-world network traffic datasets demonstrate that MicFoal is more effective and efficient than several state-of-the-art learning algorithms.\nHighlights\n\u2022\nAn online active learning framework is proposed for multiclass imbalanced network traffic with concept drift.\n\u2022\nA novel uncertain label request strategy based on the variable least confidence threshold vector is designed.\n\u2022\nA sample weight formula is proposed, which tends to assign larger weights to indistinguishable and minority class samples.\n\u2022\nAn adaptive adjustment mechanism of the label cost budget based on periodic performance evaluation.\n\u2022\nThe proposed framework is composed of several separate stages that can be easily modified or tuned.","Developing fair deep learning models for identity-sensitive applications (e.g., face attribute recognition) has gained increasing attention from the research community. Indeed, it has been observed that deep models can easily overfit to the bias of the training set, resulting in discriminative performance against certain demographic groups during test time. Motivated by the observation that a biased classifier could result in different adversarial robustness among training samples in different demographic groups (robustness bias), we argue that such adversarial robustness information of individual training samples could imply whether the training data distribution is fair among different demographic groups. In other words, under a fair classifier, the training samples from different demographic groups are expected to show similar or comparable adversarial robustness. Therefore, in this work, we propose to re-weight the training loss of individual training samples using their adversarial robustness, and provide the fairness-awareness in the training process. Extensive experimental results on CelebA dataset show that the face attribute classifiers could learn significantly greater demographic fairness under our proposed training objective and outperform other state-of-the-art re-weighting fairness algorithms on different face recognition applications. Moreover, our proposed method also reduces the non-trivial robustness bias among different demographic groups, preventing the under-represented demographic groups from higher adversarial threats.","Diversity is a key characteristic to obtain advantages of combining predictors. In this paper, we propose a modification of bagging to explicitly trade off diversity and individual accuracy. The procedure consists in dividing the bootstrap replicates obtained at each iteration of the algorithm in two subsets: one consisting of the examples misclassified by the ensemble obtained at the previous iteration, and the other consisting of the examples correctly recognized. A high individual accuracy of a new classifier on the first subset increases diversity, measured as the value of the Q statistic between the new classifier and the existing classifier ensemble. A high accuracy on the second subset on the other hand, decreases diversity. We trade off between both components of the individual accuracy using a parameter \u03bb\u2009\u2208\u2009[0,1] that changes the cost of a misclassification on the second subset. Experiments are provided using well-known classification problems obtained from UCI. Results are also compared with boosting and bagging.","In this paper we present the capability of the Median M-Type Radial Basis Function (MMRBF) Neural Network in image classification applications. The proposed neural network uses the Median M-type (MM) estimator in the scheme of radial basis function to train the neural network. Other RBF based algorithms were compared with our approach. From simulation results we observe that the MMRBF neural network has better classification capabilities","Large training data and expensive model tweaking are standard features of deep learning with images. As a result, data owners often utilize cloud resources to develop large-scale complex models, which also raises privacy concerns. Existing cryptographic solutions for training deep neural networks (DNNs) are too expensive, cannot effectively utilize cloud GPU resources, and also put a significant burden on client-side pre-processing. This article presents an image disguising approach: DisguisedNets, which allows users to securely outsource images to the cloud and enables confidential, efficient GPU-based model training. DisguisedNets uses a novel combination of image blocktization, block-level random permutation, and block-level secure transformations: random multidimensional projection (RMT) or AES pixel-level encryption (AES) to transform training data. Users can use existing DNN training methods and GPU resources without any modification to training models with disguised images. We have analyzed and evaluated the methods under a multi-level threat model and compared them with another similar method\u2014InstaHide. We also show that the image disguising approach, including both DisguisedNets and InstaHide, can effectively protect models from model-targeted attacks.","Technological advancement has made possible the collection of data from social media platforms at unprecedented speed and volume. Current methods for analyzing such data lack interpretability, are computationally intensive, or require a rigid data specification. Functional data analysis enables the development of a flexible, yet interpretable, modeling framework to extract lower-dimensional relevant features of a user's posting behavior on social media, based on their posting activity over time. The extracted features can then be used to discriminate a malicious user from a genuine one. The proposed methodology can classify a binary time series in a computationally efficient manner and provides more insights into the posting behavior of social media agents. Performance of the method is illustrated numerically in simulation studies and on a motivating Twitter data set. The developed methods are applicable to other social media data, such as Facebook, Instagram, Reddit, or TikTok, or any form of digital interaction where the user's posting behavior is indicative of their user class.","In the last few years, enormous strides have been made for object detection and data association, which are vital subtasks for one-stage online multi-object tracking (MOT). However, the two separated submodules involved in the whole MOT pipeline are processed or optimized separately, resulting in a complex method design and requiring manual settings. In addition, few works integrate the two subtasks into a single end-to-end network to optimize the overall task. In this study, we propose an end-to-end MOT network called joint detection and association network (JDAN) that is trained and inferred in a single network. All layers in JDAN are differentiable, and can be optimized jointly to detect targets and output an association matrix for robust multi-object tracking. What\u2019s more, we generate suitable pseudo-labels to address the data inconsistency between object detection and association. The detection and association submodules could be optimized by the composite loss function that is derived from the detection results and the generated pseudo association labels, respectively. The proposed approach is evaluated on two MOT challenge datasets, and achieves promising performance compared with classic and latest methods.","As a result of the digital transformation, the degree of automation in production environments is constantly increasing. The automation of logistics processes offers great potential for optimizing material flows within production. However, there are strong requirements regarding the reliability of automated guided vehicles (AGVs). This paper investigates if it is possible to identify the surface of an AGV based on control and measurement parameters compared to target parameters. The vehicle concept is based on differential drive and is used for the autonomous transport of goods within and between production halls. The information about the surface can be used to ensure a better and safer interaction with the environment. The creation of a dataset consisting of a low and a high friction surface is described. Furthermore, the pre-processing of the data which covers the time window length, resampling and data scaling is described. Finally, different algorithms for multivariable time series classification are benchmarked on the created dataset. Experiments show that the state of the art algorithm HIVE-COTE 2.0 provides the highest accuracy on the test dataset with 97.5%. Finally, a validation is performed by reviewing sequences with low confidence levels.","Multi-view data containing complementary and consensus information can facilitate representation learning by exploiting the intact integration of multi-view features. Because most objects in the real world often have underlying connections, organizing multi-view data as heterogeneous graphs is beneficial to extracting latent information among different objects. Due to the powerful capability to gather information of neighborhood nodes, in this article, we apply Graph Convolutional Network (GCN) to cope with heterogeneous graph data originating from multi-view data, which is still under-explored in the field of GCN. In order to improve the quality of network topology and alleviate the interference of noises yielded by graph fusion, some methods undertake sorting operations before the graph convolution procedure. These GCN-based methods generally sort and select the most confident neighborhood nodes for each vertex, such as picking the top-k nodes according to pre-defined confidence values. Nonetheless, this is problematic due to the non-differentiable sorting operators and inflexible graph embedding learning, which may result in blocked gradient computations and undesired performance. To cope with these issues, we propose a joint framework dubbed Multi-view Graph Convolutional Network with Differentiable Node Selection (MGCN-DNS), which is constituted of an adaptive graph fusion layer, a graph learning module, and a differentiable node selection schema. MGCN-DNS accepts multi-channel graph-structural data as inputs and aims to learn more robust graph fusion through a differentiable neural network. The effectiveness of the proposed method is verified by rigorous comparisons with considerable state-of-the-art approaches in terms of multi-view semi-supervised classification tasks, and the experimental results indicate that MGCN-DNS achieves pleasurable performance on several benchmark multi-view datasets.","With the exceeding advancement in technology, the sophistication of attacks is considerably increasing. Standard security methods fall short of achieving the security essentials of IoT against physical attacks due to the nature of IoTs being resource-constrained elements. Physical Unclonable Functions (PUFs) have been successfully employed as a lightweight memoryless solution to secure IoT devices. PUF is a device that exploits the integrated circuits\u2019 inherent randomness originated during the fabrication process to give each physical entity a unique identifier. Nevertheless, because PUFs are vulnerable to mathematical clonability, Feed-Forward Arbiter PUF (FF PUF) was introduced to withstand potential attack methods. Motivated by the necessity to expose a critical vulnerability of the standard FF PUFs design, we introduce a problem-tailored adversarial model to attack FF PUF design using a carefully engineered loop-specific neural network-based design calibrated and trained using FPGA-based in-silicon implementation data to exhibit real-world attacking scenarios posed on FF PUFs, in addition to applying simulated data. The empirical results show that the proposed adversarial model adds outperforming results to the existing studies in attacking FF PUFs, manifesting the improved efficiency in breaking FF PUFs. We demonstrate our high-performing results in numerical experiments of language modeling using the deep Neural Networks method.","Personalized federated learning is aimed at allowing numerous clients to train personalized models while participating in collaborative training in a communication-efficient manner without exchanging private data. However, many personalized federated learning algorithms assume that clients have the same neural network architecture, and those for heterogeneous models remain understudied. In this study, we propose a novel personalized federated learning method called federated classifier averaging (FedClassAvg). Deep neural networks for supervised learning tasks consist of feature extractor and classifier layers. FedClassAvg aggregates classifier weights as an agreement on decision boundaries on feature spaces so that clients with not independently and identically distributed (non-iid) data can learn about scarce labels. In addition, local feature representation learning is applied to stabilize the decision boundaries and improve the local feature extraction capabilities for clients. While the existing methods require the collection of auxiliary data or model weights to generate a counterpart, FedClassAvg only requires clients to communicate with a couple of fully connected layers, which is highly communication-efficient. Moreover, FedClassAvg does not require extra optimization problems such as knowledge transfer, which requires intensive computation overhead. We evaluated FedClassAvg through extensive experiments and demonstrated it outperforms the current state-of-the-art algorithms on heterogeneous personalized federated learning tasks.","This paper provides a comprehensive survey of bias mitigation methods for achieving fairness in Machine Learning (ML) models. We collect a total of 341 publications concerning bias mitigation for ML classifiers. These methods can be distinguished based on their intervention procedure (i.e., pre-processing, in-processing, post-processing) and the technique they apply. We investigate how existing bias mitigation methods are evaluated in the literature. In particular, we consider datasets, metrics and benchmarking. Based on the gathered insights (e.g., What is the most popular fairness metric? How many datasets are used for evaluating bias mitigation methods?), we hope to support practitioners in making informed choices when developing and evaluating new bias mitigation methods.","Classification of normal vs. pathological infant cry is a socially relevant and challenging problem. Many feature sets, such as Mel Frequency Cepstral Coefficients (MFCC), Linear Frequency Cepstral Coefficients (LFCC), and Constant Q Cepstral Coefficients (CQCC) have been used for this task. However, an effective representation of the spectral and pitch components of a spectrum together is not achieved leaving scope for improvement. Also, the infant cry can be considered a melodic sound implying that the fundamental frequency and timbre-based features also carry vital information. This work proposes Constant Q Harmonic Coefficients (CQHC), and Constant Q Pitch Coefficients (CQPC) extracted by the decomposition of the Constant Q Transform (CQT) spectrum for the infant cry classification. This work uses Convolutional Neural Network (CNN) as the classifier along with traditional classifiers, such as Gaussian Mixture Models (GMM) and Support Vector Machines (SVM). The results using the CNN classifier are compared by considering the MFCC, LFCC, and CQCC feature sets as the baseline features. The feature-level fusion of MFCC with log-CQHC and MFCC with log-CQPC achieved a 5-fold accuracy of 98.73% and 98.96% respectively, surpassing the baseline MFCC. Furthermore, the fusion of MFCC with log-CQHC and log-CQPC feature sets resulted in improved classification accuracy of 3%, 4.7%, and 5.85% when compared with the baseline MFCC, LFCC, and CQCC feature sets, respectively. Further, our intensive experiments using three classifiers structures, namely, GMM, SVM, and CNN indicate superior results using the proposed feature extraction techniques.","Multi-view learning aims to leverage data acquired from multiple sources to achieve better performance compared to using a single view. However, the performance of multi-view learning can be negatively impacted by noisy or corrupted views in certain real-world situations. As a result, it is crucial to assess the confidence of predictions and obtain reliable learning outcomes. In this paper, we introduce CALM, an enhanced encoding and confidence evaluation framework for trustworthy multi-view classification. Our method comprises enhanced multi-view encoding, multi-view confidence-aware fusion, and multi-view classification regularization, enabling the simultaneous evaluation of prediction confidence and the yielding trustworthy classifications. Enhanced multi-view encoding takes advantage of cross-view consistency and class diversity to improve the efficacy of the learned latent representation, facilitating more reliable classification results. Multi-view confidence-aware fusion utilizes a confidence-aware estimator to evaluate the confidence scores of classification outcomes. The final multi-view classification results are then derived through confidence-aware fusion. To achieve reliable and accurate confidence scores, multivariate Gaussian distributions are employed to model the prediction distribution. The advantage of CALM lies in its ability to evaluate the quality of each view, reducing the influence of low-quality views on the multi-view fusion process and ultimately leading to improved classification performance and confidence evaluation. Comprehensive experimental results demonstrate that our method outperforms other trusted multi-view learning methods in terms of effectiveness, reliability, and robustness.","The pervasiveness of the Internet and social media have enabled the rapid and anonymous spread of Hate Speech content on microblogging platforms such as Twitter. Current EU and US legislation against hateful language, in conjunction with the large amount of data produced in these platforms has led to automatic tools being a necessary component of the Hate Speech detection task and pipeline. In this study, we examine the performance of several, diverse text representation techniques paired with multiple classification algorithms, on the automatic Hate Speech detection and abusive language discrimination task. We perform an experimental evaluation on binary and multiclass datasets, paired with significance testing. Our results show that simple hate-keyword frequency features (BoW) work best, followed by pre-trained word embeddings (GLoVe) as well as N-gram graphs (NGGs): a graph-based representation which proved to produce efficient, very low-dimensional but rich features for this task. A combination of these representations paired with Logistic Regression or 3-layer neural network classifiers achieved the best detection performance, in terms of micro and macro F-measure.","Modern geological practices, in both industry and academia, rely largely on a legacy of observational data at a range of scales. However, widespread ambiguities in the petrographic description of rock facies reduce the reliability of descriptive data. Previous studies have demonstrated a great potential for the use of convolutional neural networks (CNNs) in the classification of facies from digital images; however, it remains to be determined which of the available CNN architectures performs best for a geological classification task. We evaluate the ability of top-performing CNNs to classify carbonate core images using transfer learning, systematically developing a performance comparison between these architectures on a complex geological dataset. Three datasets with orders of magnitude difference in data quantity (7000\u2013104,000 samples) were created that contain images across seven classes from the modified Dunham Classification for carbonate rocks. Following training of nine different CNNs of four architectures on these datasets, we find the Inception-v3 architecture to be most suited to this classification task, achieving 92% accuracy when trained on the larger dataset. Furthermore, we show that even when using transfer learning the size of the dataset plays a key role in the performance of the models, with those trained on the smaller datasets showing a strong tendency to overfit. This has direct implications for the application of deep learning in geosciences as many papers currently published use very small datasets of less than 5000 samples. Application of the framework developed in this research could aid the future of deep learning based carbonate classification, with further potential to be easily modified to suit the classification of cores originating from different formations and lithologies.\nGraphical abstract\nDisplay Omitted\nHighlights\n\u2022\nTop-performing CNNs are compared for application to geological classification tasks\n\u2022\nWe present results using the largest dataset of carbonate core images to date\n\u2022\nMost geological studies of deep learning use datasets smaller than 10,000 points\n\u2022\nEven transfer learning methods overfit on datasets smaller than 100,000 data points\n\u2022\nDifferent architectures are more appropriate depending on the size of dataset used","In this paper we study the application of Bayesian network models to classify multispectral and hyperspectral remote sensing images. Different models of Bayesian networks as: Naive Bayes (NB), Tree Augmented Naive Bayes (TAN) and General Bayesian Networks (GBN), are applied to the classification of hyperspectral data. In addition, several Bayesian multi-net models: TAN multi-net, GBN multi-net and the model developed by Gurwicz and Lerner, TAN-Based Bayesian Class-Matched multi-net (tBCM2) (see [1]) are applied to the classification of multispectral data. A comparison of the results obtained with the different classifiers is done.","Conventional active learning (AL) frameworks aim to reduce the cost of data annotation by actively requesting the labeling for the most informative data points. However, introducing AL to data hungry deep learning algorithms has been a challenge. Some proposed approaches include uncertainty-based techniques, geometric methods, implicit combination of uncertainty-based and geometric approaches, and more recently, frameworks based on semi/self supervised techniques. In this paper, we address two specific problems in this area. The first is the need for efficient exploitation/exploration trade-off in sample selection in AL. For this, we present an innovative integration of recent progress in both uncertainty-based and geometric frameworks to enable an efficient exploration/exploitation trade-off in sample selection strategy. To this end, we build on a computationally efficient approximate of Thompson sampling with key changes as a posterior estimator for uncertainty representation. Our framework provides two advantages: (1) accurate posterior estimation, and (2) tune-able trade-off between computational overhead and higher accuracy. The second problem is the need for improved training protocols in deep AL. For this, we use ideas from semi/self supervised learning to propose a general approach that is independent of the specific AL technique being used. Taken these together, our framework shows a significant improvement over the state-of-the-art, with results that are comparable to the performance of supervised-learning under the same setting. We show empirical results of our framework, and comparative performance with the state-of-the-art on four datasets, namely, MNIST, CIFAR10, CIFAR100 and ImageNet to establish a new baseline in two different settings.","Conventional rule learning algorithms aim at finding a set of simple rules, where each rule covers as many examples as possible. In this paper, we argue that the rules found in this way may not be the optimal explanations for each of the examples they cover. Instead, we propose an efficient algorithm that aims at finding the best rule covering each training example in a greedy optimization consisting of one specialization and one generalization loop. These locally optimal rules are collected and then filtered for a final rule set, which is much larger than the sets learned by conventional rule learning algorithms. A new example is classified by selecting the best among the rules that cover this example. In our experiments on small to very large datasets, the approach\u2019s average classification accuracy is higher than that of state-of-the-art rule learning algorithms. Moreover, the algorithm is highly efficient and can inherently be processed in parallel without affecting the learned rule set and so the classification accuracy. We thus believe that it closes an important gap for large-scale classification rule induction.","Diagnosing the fault type accurately from a variety of faults is very essential to ensure a stable electricity supply when a short-circuit fault occurs. In this paper, a hybrid classification model combining the one-dimensional convolutional neural network (1D-CNN) and the bidirectional long short-term memory network (BiLSTM) is proposed for the classification of cable short-circuit faults to improve the accuracy of fault diagnosis. Sample sets of the current signal for single-phase grounding short circuit, two-phase grounding short circuit, two-phase to phase short circuit, and three-phase grounding short-circuit are obtained by the simulink model, and the signal is input to this network model. The local features of the cable fault signals are extracted using 1D-CNN and the fault signal timing information is captured using BiLSTM, which enables the diagnosis of cable faults based on the automatically extracted features. The experimental results of the simulation show that the model can obtain a good recognition performance and can achieve an overall accuracy of 99.45% in classifying the four short-circuit faults with 500 iterations. In addition, the analysis of loss function curves and accuracy curves shows that the method performs better than networks with only temporal feature extraction, such as 1D-CNN and LSTM.","The expeditious proliferation of web services over the current web-cyberspace becomes a significant challenge for the web service classification. Recent reports suggest a deep learning algorithm namely, gated recurrent unit, performs well. But the performance of the GRU classifier hinges on structure of the GRU classifier namely, hidden neural network (HNN) and training data. Hence, it is significant to predict optimal HNN and training dataset. The recent studies revealed that genetic algorithm (GA) is most suited for this optimisation problem. However, GA is inefficient in terms of convergence rate and generation of prudent solution. For that, domain knowledge is incorporated in GA operator in order to accelerate convergence rate called as knowledge-based genetic algorithm (KBGA). The research work adopts KBGA to optimise the structure of GRU, so that the classifier outperforms the classification process. Extensive analysis and statistical hypothesis test have been done to show the efficacy of KBGA.","Deep learning-based brain tumor classification from brain magnetic resonance imaging (MRI) is a significant research problem. The research problem encounters a major challenge. The training datasets used to develop deep learning algorithms could be imbalanced with significantly more samples for one type of tumor than others. This imbalance in the training dataset affects the performance of tumor classification using deep learning models as the classifier performance gets biased towards the majority class. The article addresses the challenge of training data imbalance by proposing a novel class-weighted focal loss and studies the effects of weighted loss functions on feature learning by convolutional neural networks (CNN). However, finding optimal class weights is a challenge and the predictions of CNN trained using weighted functions could be biased. The article presents two approaches to improve the performance of the expert system: deep feature fusion and majority voting on classifier predictions. In the first approach, the deep feature fusion concerns the fusion of deep features extracted from CNN models trained using separate loss functions. The fused deep features are classified using proven models, such as support vector machine (SVM) and k-nearest neighbours (KNN). In the other approach, a majority voting is performed on the predictions for three different feature sets extracted from CNN models trained using separate loss functions. The majority voting uses the same classifier upon three different feature sets. The proposed approaches show a significant improvement in brain tumor predictions over a state of the art method based on CNN trained using cross-entropy loss. The classification errors between the majority class and the minority class samples are reduced considerably in the proposed strategies. The experiments are evaluated using the Figshare dataset, and the performance improved for the metrics: accuracy, precision, recall, balanced accuracy and F-scores.","Aiming at the problem that the load identification accuracy of non-intrusive load monitoring (NILM) is greatly affected by the power of loads and the number of background loads, a non-intrusive load identification method based on the current complex spectrum and support vector machine (SVM) is proposed. Through the high-frequency sampling of the load's voltage and current, the complex spectrum of the current is extracted by the fast Fourier transform (FFT), and the multi-class SVM load identification model is established and optimized to realize the non-intrusive load identification. The algorithm is verified using the PLAID datasets, and the load identification accuracy of the algorithm is compared with SVM classifiers based on total harmonic distortion rate (THD), harmonic component ratio and harmonic amplitude. The results of the experiments show that the proposed method not only improves the identification accuracy of low-power loads, but also has higher identification accuracy and better identification robustness of switching load in multi-load scenarios.","Nowadays assuring that search and recommendation systems are fair and do not apply discrimination among any kind of population has become of paramount importance. This is also highlighted by some of the sustainable development goals proposed by the United Nations. Those systems typically rely on machine learning algorithms that solve the classification task. Although the problem of fairness has been widely addressed in binary classification, unfortunately, the fairness of multi-class classification problem needs to be further investigated lacking well-established solutions. For the aforementioned reasons, in this paper, we present the Debiaser for Multiple Variables (DEMV), an approach able to mitigate unbalanced groups bias (i.e., bias caused by an unequal distribution of instances in the population) in both binary and multi-class classification problems with multiple sensitive variables. The proposed method is compared, under several conditions, with a set of well-established baselines using different categories of classifiers. At first we conduct a specific study to understand which is the best generation strategies and their impact on DEMV\u2019s ability to improve fairness. Then, we evaluate our method on a heterogeneous set of datasets and we show how it overcomes the established algorithms of the literature in the multi-class classification setting and in the binary classification setting when more than two sensitive variables are involved. Finally, based on the conducted experiments, we discuss strengths and weaknesses of our method and of the other baselines.\nHighlights\n\u2022\nWe highlight the strengths and weaknesses of established bias mitigation methods.\n\u2022\nWe revise our work by generalizing the instance generation and removal strategies.\n\u2022\nWe evaluate the effectiveness of different instance-generation strategies in DEMV.\n\u2022\nWe evaluate DEMV by considering several datasets, methods and sensitive variables.\n\u2022\nWe observe how DEMV overcomes the baselines in almost all the experiments.","Cardiac Magnetic Resonance Imaging (MRI) plays an important role in the analysis of cardiac function. However, the acquisition is often accompanied by motion artefacts because of the difficulty of breath-hold, especially for acute symptoms patients. Therefore, it is essential to assess the quality of cardiac MRI for further analysis. Time-consuming manual-based classification is not conducive to the construction of an end-to-end computer aided diagnostic system. To overcome this problem, an automatic cardiac MRI quality estimation framework using ensemble and transfer learning is proposed in this work. Multiple pre-trained models were initialised and fine-tuned on 2-dimensional image patches sampled from the training data. In the model inference process, decisions from these models are aggregated to make a final prediction. The framework has been evaluated on CMRxMotion grand challenge (MICCAI 2022) dataset which is small, multi-class, and imbalanced. It achieved a classification accuracy of 78.8% and 70.0% on the training set (5-fold cross-validation) and a validation set, respectively. The final trained model was also evaluated on an independent test set by the CMRxMotion organisers, which achieved the classification accuracy of 72.5% and Cohen\u2019s Kappa of 0.6309 (ranked top 1 in this grand challenge). Our code is available on Github: https://github.com/ruizhe-l/CMRxMotion.","In machine learning, the term \u201dclass imbalanced\u201d is frequently used. This is a crucial part of the field of machine learning. It is quite important in the classification process and has a significant impact on performance. That is why researchers are concentrating on it to overcome this difficulty. Various researchers have devised numerous methods till now. The approaches to addressing this imbalance issue found so far can be broadly categorized into three categories, which are the data-level approach, algorithm-level approach, and hybrid-level approach. To evaluate the most recent developments in resolving the negative effects of class imbalance, this study provides a comparative analysis of research that has been published within the last 5 years with an emphasis on high-class imbalance. In this study, an attempt has been made to provide a concise overview of what imbalance classification is, how it is created, and what the inconveniences are due to it. We have tried to provide a summary of several studies that have been published in the last few years and along with that a comparative analysis of all these approaches has been done.","Breast cancer was the most diagnosed form of cancer in 2020. Early diagnosis of breast cancer results in a significant improvement in long-term survival rates. Current methods require consultation with experts, which is expensive and time-consuming and thus may not be accessible to all. This paper seeks to train and evaluate supervised machine learning models for the accurate and efficient detection of breast cancer. The Wisconsin Breast Cancer Database dataset describes 30 attributes of cell nuclei, including, but not limited to, their radius, texture, and concavity. It contains 569 instances, 212 of which are malignant tumors. The Random Forest algorithm outperforms other algorithms in classifying breast tumors as either malignant or benign and is thus selected as our primary model. It is trained on two different subsets of the dataset having 16 and 8 features, respectively, identified with the help of multiple feature selection methods. The Random Forest models are tested post hyperparameter tuning on a holdout set, and accuracies of 100% and 99.30% respectively. The models are also compared with four other machine learning classification algorithms: Support Vector Machine (SVM), Decision Tree, Multilayer Perceptron, and K-Nearest Neighbors. The results confirm that Random Forest is the superior method for breast cancer diagnosis.","Highlights\n\u2022\nThis research presents a customized PDICNet model for plant leaf disease identification and classification.\n\u2022\nInitially, ResNet-50 is used to extract multiple features from plant leaf images with colour, texture properties.\n\u2022\nModified red deer optimization algorithm is implemented as an optimal feature selection algorithm to obtain optimized and salient features with reduced size of the MRDOA.\n\u2022\nFurther, deep learning convolutional neural network classifier model is utilized to achieve enhanced classification performance.\nAbstract\nThe manual inspections of plant diseases resulted in low accuracy with high time consumption and unable to predict the multiple diseases of plants. To address these difficulties, it is necessary to develop automated systems that are capable of effectively classifying. Therefore, this article presents a customized PDICNet model for plant leaf disease identification and classification. Initially, ResNet-50 is used to extract multiple features from plant leaf images with colour and texture properties. In addition, the modified Red Deer optimization algorithm (MRDOA) is implemented as an optimal feature selection algorithm to obtain optimized and salient features with a reduced size of the MRDOA. Further, a deep learning convolutional neural network (DLCNN) classifier model is utilized to achieve enhanced classification performance. Obtained simulation outcome discloses the superiority of proposed PDICNet model with an accuracy and F1-score of 99.73%, and 99.78%, respectively for PlantVillage dataset and 99.68%, and 99.71% for Rice Plant dataset.\nGraphical abstract\nDisplay Omitted","Multiple Sclerosis (MS) is an inflammatory autoimmune disease of the Central Nervous System, characterized by development of lesions that cause interference in the communication between brain and the rest of the body. Some techniques using numeric algorithms based on mathematical and probabilistic theories are generally used in order to obtain lesions detection. In this paper we describe an innovative approach for lesions recognition to be applied after segmentation of brain tissues from quantitive evaluation of MR studies. Knowledge about MS lesions is formalized through an ontology and a set of rules: integrating them, automatic inferences can be realized to point out lesions, starting from data about potentially brain abnormal white matter.","We propose an automatic COVID1-19 diagnosis framework from lung CT-scan slice images using double BERT feature extraction. In the first BERT feature extraction, A 3D-CNN is first used to extract CNN internal feature maps. Instead of using the global average pooling, a late BERT temporal pooing is used to aggregate the temporal information in these feature maps, followed by a classification layer. This 3D-CNN-BERT classification network is first trained on sampled fixed number of slice images from every original CT scan volume. In the second stage, the 3D-CNN-BERT embedding features are extracted for every 32 slice images sequentially, and these features are divided into fixed number of segments. Then another BERT network is used to aggregate these features into a single feature followed by another classification layer. The classification results of both stages are combined to generate final outputs. On the validation dataset, we achieve macro F1 score 92.05%; and on the testing dataset, we achieve macro F1 84.43%.","As the huge dimensionality of textual data restrains the classification accuracy, it is essential to apply feature selection (FS) methods as dimension reduction step in text classification (TC) domain. Most of the FS methods for TC contain several number of probabilities. In this study, we proposed a new FS method named as Extensive Feature Selector (EFS), which benefits from corpus-based and class-based probabilities in its calculations. The performance of EFS is compared with nine well-known FS methods, namely, Chi-Squared (CHI2), Class Discriminating Measure (CDM), Discriminative Power Measure (DPM), Odds Ratio (OR), Distinguishing Feature Selector (DFS), Comprehensively Measure Feature Selection (CMFS), Discriminative Feature Selection (DFSS), Normalised Difference Measure (NDM) and Max\u2013Min Ratio (MMR) using Multinomial Naive Bayes (MNB), Support-Vector Machines (SVMs) and k-Nearest Neighbour (KNN) classifiers on four benchmark data sets. These data sets are Reuters-21578, 20-Newsgroup, Mini 20-Newsgroup and Polarity. The experiments were carried out for six different feature sizes which are 10, 30, 50, 100, 300 and 500. Experimental results show that the performance of EFS method is more successful than the other nine methods in most cases according to micro-F1 and macro-F1 scores.","Convolutional Neural Network (CNN) is a widely used neural network in deep learning, and Graph Convolutional Network (GCN) is one of the most effective semi -supervised methods. It spread node information in a conversion way. In this article, we have studied the differences between CNN and GCN in the classification of high -spectrum image. Because the traditional GCN algorithm needs to build an adjacent matrix on all data, the calculation cost is very high, especially in large -scale remote sensing problems. MinigCNS uses a small -batch learning method to solve the problem of CCN calculation costs, and then it has not solved the problem of low efficiency of single model classification. This article studies the advantages of minigcns and CNN, and proposes a weighted fusion network FU-W, which weighs the minigcns and CNN weighted integration to break the bottleneck of single model performance. We experimented with the fusion algorithm on the two high -spectrum data sets, and its overall accuracy can reach 88.8%in Indian Pines. The experiment proves the superiority of the fusion strategy for a single CNN or GCN model.","Ensembles are among the state-of-the-art in many machine learning applications. With the ongoing integration of ML models into everyday life, e.g., in the form of the Internet of Things, the deployment and continuous application of models become more and more an important issue. Therefore, small models that offer good predictive performance and use small amounts of memory are required. Ensemble pruning is a standard technique for removing unnecessary classifiers from a large ensemble that reduces the overall resource consumption and sometimes improves the performance of the original ensemble. Similarly, leaf-refinement is a technique that improves the performance of a tree ensemble by jointly re-learning the probability estimates in the leaf nodes of the trees, thereby allowing for smaller ensembles while preserving their predictive performance. In this paper, we develop a new method that combines both approaches into a single algorithm. To do so, we introduce L1 regularization into the leaf-refinement objective, which allows us to jointly prune and refine trees at the same time. In an extensive experimental evaluation, we show that our approach not only offers statistically significantly better performance than the state-of-the-art but also offers a better accuracy-memory trade-off. We conclude our experimental evaluation with a case study showing the effectiveness of our method in a real-world setting.","Recent solutions to object classification have focused on the decomposition of objects into representative parts. However, the vast majority of these methods are based on single visual cue measurements. Psychophysical evidence suggests that humans use multiple visual cues to accomplish recognition. In this paper, we address the problem of integrating multiple visual information for object recognition. Our contribution in this paper is twofold. First, we describe a new probabilistic integration model of multiple visual cues at different spatial locations across the image. Secondly, we use the cue integration framework to classify images of objects by combining two-dimensional and three-dimensional visual cues. Classification results obtained using the method are promising.","The number of malware infected machines from all over the world has been growing day by day. New malware variants appear in the wild to evade the malware detection and classification systems and may infect with ransomware or crypto miners for adversary financial gain. A recent colonial pipeline ransomware attack is an example of these attacks that impacted daily human activities, and the victim had to pay ransom to restore their operations. Windows-based systems are the most adopted systems across different industries for running applications. They are prone to get targeted by installing the malware. In this paper, we propose a Deep Learning (DL)-based Convolutional Neural Network (CNN) model to perform the malware classification on Portable Executable (PE) binary files using the fusion feature set approach. We present an extensive performance evaluation of various DL model architecture and Machine Learning (ML) classifier i.e. Support Vector Machine (SVM), on multi-aspect feature sets covering the static, dynamic, and image features to select the proposed CNN model. We further leverage the CNN-based architecture for effective classification of the malware using different combinations of feature sets and compare the results with the best-performed individual feature set. Our performance evaluation of the proposed model shows that the model classifies the malware or benign files with an accuracy of 97% when using fusion feature sets. The proposed model is robust and generalizable and showed similar performances on completely unseen two malware datasets. In addition, the embedding features of the CNN model are visualized, and various visualization methods are employed to understand the characteristics of the datasets. Further, large-scale learning and stacked classifiers were employed after the penultimate layer to enhance the CNN classification performance.","This paper presents a methodology for developing a volcano-seismic event classification system using a multi-station deep learning approach to support monitoring the Nevados del Chill\u00e1n Volcanic Complex, which has been active since 2017. A convolutional network of multiple inputs processes the information from an event recorded up to five seismic stations. Each record is represented by its normalized spectrogram; thus, the network may receive from one to five spectrograms as input. The design includes entering additional information into the network, like the stations configuration and the event duration, information not provided by the spectrograms. Finally, this work includes the design and implementation of a relational database to access the continuous traces of events, showing different subsets of data quickly and efficiently. The results show that the classification of an event recorded up to five stations is substantially more effective than a single-station strategy. However, incorporating additional information of the signal does not significantly improve the classification performance.","In the foodservice industry, time is a crucial factor that impacts both consumers and management. Machine learning (ML) is increasingly used to improve the quality of services through prediction. In this study, we aim to develop a model for predicting meal duration using Random Forest Classification algorithm. The study uses data from the Point-of-Sale (POS) system of a full-service Thai hotpot restaurant, with a focus on two commercial areas in Bangkok. The variables that we used include the branch, the number of customers, the number of items, the day of the week, and the time of the day. As a result, the overall accuracy of the model was 86% and the F1-score was 0.81. The discussion of the potential use of this approach in connection with the existing system in a restaurant could also be beneficial, aiding the restaurant in planning management more efficiently and gaining a better understanding of consumer behavior. This study will discuss the results of the model along with additional perspectives for future work.","An essential part of making shopping purchase decisions is to compare and contrast products based on key differentiating features, but doing this manually can be overwhelming. Prior methods offer limited product comparison capabilities, e.g., via pre-defined common attributes that may be difficult to understand, or irrelevant to a particular product or user. Automatically generating an informative, natural-sounding, and factually consistent comparative text for multiple product and attribute types is a challenging research problem. We describe HCPC (Human Centered Product Comparison), to tackle two kinds of comparisons for online shopping: (i) product-specific, to describe and compare products based on their key attributes; and (ii) attribute-specific comparisons, to compare similar products on a specific attribute. To ensure that comparison text is faithful to the input product data, we introduce a novel multi-decoder, multi-task generative language model. One decoder generates product comparison text, and a second one generates supportive, explanatory text in the form of product attribute names and values. The second task imitates a copy mechanism, improving the comparison generator, and its output is used to justify the factual accuracy of the generated comparison text, by training a factual consistency model to detect and correct errors in the generated comparative text. We release a new dataset (https://registry.opendata.aws/) of ~15K human generated sentences, comparing products on one or more attributes (the first such data we know of for product comparison). We demonstrate on this data that HCPC significantly outperforms strong baselines, by ~10% using automatic metrics, and ~5% using human evaluation.","In the classification task, many improved algorithms have been developed based on Support Vector Machines (SVM). Since the recently proposed Fast Support Vector Classifier (FSVC) can handle large scale datasets, this paper improves FSVC by quantile, called QSVC, which uses quantile rather than average value of samples to represent all samples. The advantages of QSVC are as follows: 1) QSVC performs better on the skewed distribution; 2) and the robustness of quantile is better. Experiments show that our QSVC performs better in accuracy and speed than FSVC. For example,Table 2 shows that the average accuracy of QSVC is higher than that of FSVC and Liblinear, and it takes less time.","Imbalanced binary classification plays an important role in many applications. Some popular classifiers, such as logistic regression (LR), usually underestimate the probability of the minority class. Therefore, in this paper, we introduce two novel methods under distribution uncertainty, the idea of which is to modify the predicted probability with an additional uncertainty estimation. We develop the mean-uncertain method and the volatility-uncertain method, respectively, by assuming that the disturbance term follows the maximal and the G-normal distributions, which are the most important distributions within a sublinear expectation framework. Experiments on the simulated dataset and 10real-life datasets are conducted to compare the newly proposed approaches to several existing ones, including two resampling methods and two regression-based methods. The results of experiments show that our methods outperform most of the others in common evaluation metrics, especially the accuracy of the minority class.","The edited k-nearest neighbor consists of the application of the k-nearest neighbor classifier with an edited training set, in order to reduce the classification error rate. This edited training set is a subset of the complete training set in which some of the training patterns are excluded. In recent works, genetic algorithms have been successfully applied to generate edited sets. In this paper we propose three improvements of the edited k-nearest neighbor design using genetic algorithms: the use of a mean square error based objective function, the implementation of a clustered crossover, and a fast smart mutation scheme. Results achieved using the breast cancer database and the diabetes database from the UCI machine learning benchmark repository demonstrate the improvement achieved by the joint use of these three proposals.","In this paper, we propose a technique for hierarchical yoga pose classification in a multi-tasking framework. Novelty lies in the proposed supervised contrastive combined loss function. We propose the usage of linear combination of three loss functions: cross entropy, self-supervised contrastive loss and supervised contrastive loss in a multi-tasking manner. We introduce radial and cosine margin into the formulation of self-supervised and supervised contrastive loss to pull feature embeddings of same classes closer together compared to feature embeddings of different classes. We use a two stage transfer learning based end to end training methodology trained over novel supervised contrastive multi-tasking combined loss function in the first stage and later fine tune over cross entropy multi-tasking loss in the second stage. We apply our methodology on the publicly available Yoga-82 large-scale dataset. We report peak Top-1 yoga pose classification accuracy of 94.86% over 6 pose classes (Yoga-6), 91.9% over 20 pose classes (Yoga-20) and 87.17% over 82 pose classes (Yoga-82). Our proposed method achieves 5% improvement over Top-1 classification accuracy in Yoga-6, 7.3% improvement in Yoga-20 and 8.1% improvement in Yoga-82 in comparison with state of the art (SOTA) methodology. We achieve SOTA accuracies in all three hierarchies.","While utilizing machine learning models, one of the most crucial aspects is how bias and fairness affect model outcomes for diverse demographics. This becomes especially relevant in the context of machine learning for medical imaging applications as these models are increasingly being used for diagnosis and treatment planning.\nIn this paper, we study biases related to sex when developing a machine learning model based on brain magnetic resonance images (MRI). We investigate the effects of sex by performing brain age prediction considering different experimental designs: model trained using only female subjects, only male subjects and a balanced dataset. We also perform evaluation on multiple MRI datasets (Calgary-Campinas(CC359) and CamCAN) to assess the generalization capability of the proposed models.\nWe found disparities in the performance of brain age prediction models when trained on distinct sex subgroups and datasets, in both final predictions and decision making (assessed using interpretability models). Our results demonstrated variations in model generalizability across sex-specific subgroups, suggesting potential biases in models trained on unbalanced datasets. This underlines the critical role of careful experimental design in generating fair and reliable outcomes.","In order to increase the yield in agricultural stock, its speed of delivery &amp; production plays a crucial role in economic growth of a county. Conventionally, the rice variety classification process were costly, requires intense manual labor and are quite prone to human made error in identification; thereby resulting in inconsistent and slow process. This type of process can be greatly automated with the help of computer vision to result in a non-destructive, quick &amp; nondestructive technique. In this research, we have presented a neural model based semantic segmentation method for classification of agro-morphological characteristics to differentiate the rice varieties &amp; secondly to extract the spikelets per panicle for prediction of its yield. The advantage of using segmentation base method is that it takes in account of shape, color, and texture properties after manually annotating the rice\u2019s agro-morphological images with over 15,000 images of crops for 10 varieties of rice. This research will serve as a major tool for botanist, industrial farmers and food processing industry for rapid rice classification, yield estimation with the help of our presented computer vision technology.","Multi-label image classification aims to predict all possible labels in an image. It is usually formulated as a partial-label learning problem, given the fact that it could be expensive in practice to annotate all labels in every training image. Existing works on partial-label learning focus on the case where each training image is annotated with only a subset of its labels. A special case is to annotate only one positive label in each training image. To further relieve the annotation burden and enhance the performance of the classifier, this paper proposes a new partial-label setting in which only a subset of the training images are labeled, each with only one positive label, while the rest of the training images remain unlabeled. To handle this new setting, we propose an end-to-end deep network, PLMCL (Partial-Label Momentum Curriculum Learning), that can learn to produce confident pseudo labels for both partially-labeled and unlabeled training images. The novel momentum-based law updates soft pseudo labels on each training image with the consideration of the updating velocity of pseudo labels, which help avoid trapping to low-confidence local minimum, especially at the early stage of training in lack of both observed labels and confidence on pseudo labels. In addition, we present a confidence-aware scheduler to adaptively perform easy-to-hard learning for different labels. Extensive experiments demonstrate that our proposed PLMCL outperforms many state-of-the-art multi-label classification methods under various partial-label settings on three different datasets.","Most existing short text classification methods treat each phrase as an independent homogeneous distribution, thus losing the association information between sentences. To solve this problem, we propose a heterogeneous GCN for short text classification. The heterogeneous text graphs are constructed using word nodes to take advantage of the structural features of corpus graph and sentence graph. Integrating different topologies of text graphs captures more neighbourhood information, thus addressing the sparsity of short text. To better fuse topological features, this paper extracts text graph structure features with the help of GCN, constructs hypernode to capture global information of text, and deeply interacts and fuses with local information captured by CNN. Experiments demonstrate that the model outperforms the other models on the three benchmark datasets. The model was tested on a fault text classification dataset provided by an automotive company, thus enabling effective validation of the model in a specific industry domain.","We consider a multilabel all-relevant feature selection task which is more general than the classical minimal-optimal subset task. Whereas the goal of the minimal-optimal methods is to find the smallest subset of features allowing accurate prediction of labels, the objective of the all-relevant methods is to identify all the features that are related to the target labels, including strongly and all weakly relevant features. The all-relevant task has received much interest in the fields where discovering the dependency structure between features and target variables is more important than the prediction itself, e.g., in medical and bioinformatics applications. In this paper, we formally describe the all-relevant problem for multi-label classification using an information-theoretic approach. We propose a relevancy score and an efficient method of its calculation based on the lower bounds of conditional mutual information. Another practical issue is how to separate the relevant features from irrelevant ones. To find a threshold, we propose a testing procedure based on a permutation scheme. Finally, empirical evaluation of all-relevant methods requires a specific approach. We consider a large variety of simulated datasets representing different dependency structures and containing various types of interactions. Empirical results on simulated datasets and a large clinical database demonstrate that the proposed method can successfully identify relevant features.\nHighlights\n\u2022\nWe describe the all-relevant feature selection in multi-label data analysis.\n\u2022\nWe propose a general measure of feature relevancy using the information theory.\n\u2022\nWe propose a feature selection using relevancy measure and permutation distribution.\n\u2022\nWe design simulation models representing different multi-label structures.\n\u2022\nWe perform experiments on a real-world dataset.","Surface-defect detection has attracted extensive attention in the field of industrial inspection but remains challenging, owing to the rare occurrence and the various appearance of the defects. Promising results have been obtained by supervised methods but they require a large number of pixel-level annotations which are very costly to obtain. This paper proposes a memory-attended multi-inference network (MaMiNet) for image-level defect detection. MaMiNet integrates image classification with saliency detection and can accommodate a variable number of samples with pixel-label annotations along with image-level annotation. Considering the various defect appearance, a memory attention feature enhancement module is exploited to capture the attention information not only within one sample but across whole samples and seek better representation ability for the defective regions. A multi-inference aware aggregation module is proposed to fuse features with different inference hints and obtain more comprehensive features. The proposed method is extensively validated on four datasets and better experimental results are obtained compared with other state-of-the-art methods, especially in weak supervision mode without any pixel-level annotation. The efficacy of the proposed modules is validated through ablation studies.\nHighlights\n\u2022\nA mixed-supervised image-level surface defect detection method is proposed.\n\u2022\nMemory attention helps to learn discriminative features of defect across samples.\n\u2022\nThe representation ability is improved through the saliency detection module.\n\u2022\nBetter results are obtained on four datasets compared with SOTA methods.","The main difficulty in the binary object classification field lays in dealing with a high variability of symbol appearance. Rotation, partial occlusions, elastic deformations, or intra-class and inter-class variabilities are just a few problems. In this paper, we introduce a novel object description for this type of symbols. The shape of the object is aligned based on principal components to make the recognition invariant to rotation and reflection. We propose the Blurred Shape Model (BSM) to describe the binary objects. This descriptor encodes the probability of appearance of the pixels that outline the object\u2019s shape. Besides, we present the use of this descriptor in a system to improve the BSM performance and deal with binary objects multi-classification problems. Adaboost is used to train the binary classifiers, learning the BSM features that better split object classes. Then, the different binary problems learned by the Adaboost are embedded in the Error Correcting Output Codes framework (ECOC) to deal with the muti-class case. The methodology is evaluated in a wide set of object classes from the MPEG07 repository. Different state-of-the-art descriptors are compared, showing the robustness and better performance of the proposed scheme when classifying objects with high variability of appearance.","There is a demand for flowers globally all year round, more particularly roses, necessitating increased production for flowers. Demand for roses has increased due to their year-long availability as well as its uses in cosmetic, perfume, medicinal products, food raw materials and decoration industry. Rose plants are prone to drastic fluctuations in temperature, drought stress damages, and low precipitations. These resulted to an increase in greenhouse production to generate optimum supply to meet growing demands as controlled environment provides several advantages. In this study, four machine learning models (random forest, support vector machine, multinomial logistic regression, and artificial neural networks) were applied to roses greenhouse cultivation dataset. The study aims to classify the most suitable greenhouse environment to upgrade the roses state leading to the optimal production of roses. Four model configurations corresponding to the pre-processing techniques were tested. These were scaling only, scaling plus removal of outliers, scaling plus SMOTE, and scaling with removal of outliers plus SMOTE. Random forest with all pre-processing steps applied to the dataset obtained the best performance with the highest weighted F1- scores, weighted-average precision, weighted-average recall, and Cohen's kappa statistic. This indicates that machine learning models can predict corrective actions leading to improved conditions of roses. The notable contribution of this research is to find valid and reliable classification models that assist growers in predicting the best greenhouse micro-environment.","According to the former contributions, the authors present a novel fingerprint classification method based on analysis of singularities and geometric framework. First, a robust pseudoridges extraction algorithm on fingerprints is adopted to extract the global geometric shape of fingerprint ridges of pattern area. Then, by use of the detected singularities and with the help of the analysis of the global geometric shape of fingerprint ridges of pattern area, the fingerprint image is classified into different pre-specified classes. This algorithm has been tested on the NJU fingerprint database which contains 2500 images. For the 1000 images in this database, the classification accuracy is 92.2%.","Mixing data augmentation methods have been widely used in text classification recently. However, existing methods do not control the quality of augmented data and have low model explainability. To tackle these issues, this paper proposes an explainable text classification solution based on attentive and targeted mixing data augmentation, ATMIX. Instead of selecting data for augmentation without control, ATMIX focuses on the misclassified training samples as the target for augmentation to better improve the model's capability. Meanwhile, to generate meaningful augmented samples, it adopts a self-attention mechanism to understand the importance of the subsentences in a text, and cut and mix the subsentences between the misclassified and correctly classified samples wisely. Furthermore, it employs a novel dynamic augmented data selection framework based on the loss function gradient to dynamically optimize the augmented samples for model training. In the end, we develop a new model explainability evaluation method based on subsentence attention and conduct extensive evaluations over multiple real-world text datasets. The results indicate that ATMIX is more effective with higher explainability than the typical classification models, hidden-level, and input-level mixup models.","Recently, incremental learning has attracted a lot of interest in both research communities and industries. Generally, given a series of data sets sequentially, it tries to achieve good performance on the new data set while maintaining not bad performance on the old ones. Despite the recent success of incremental learning, existing works mainly assume that the coming data set is from the feature space of old ones, i.e., homogeneous feature space. And they adopt one feature extractor to forcibly project different feature spaces into one space. However, this assumption is hard to hold in real-world scenarios. Especially, the attributes of tables may sequentially increase in tabular learning. Thus, classic incremental learning models may hinder their effectiveness. In this paper, we propose a new method, incremental tabular learning on heterogeneous feature space (ILEAHE) to solve this issue. We first propose the ideas that feature extractors should be decomposed into shared and specific extractors to process the shared and specific features across different data sets respectively. Then, we propose a novel measurement named discriminative ability to measure specific extractors. Thus, two kinds of extractors can be discriminated and the specific extractor will more focus on those domain-specific features. We further demonstrate the effectiveness of ILEAHE through empirical studies.","As a convenient biochemical detection technology, colloidal gold strip lateral flow immunochromatography has been widely used in disease detection and diagnosis, food safety and other fields. In order to achieve high accuracy and efficient test strip detection, the artificial intelligence-based image classification method was applied to HIV colloidal gold test strip detection in this paper. An unsupervised HIV colloidal gold strip detection algorithm based on K-means++ and machine learning is proposed, and key technologies such as strip image preprocessing, K-means++ image segmentation method, data enhancement and K value selection are studied. Three image classifiers KNN(K nearest neighbor classification algorithm), SVC(support vector machine) and GaussianNB(Gaussian Bayes classifier) were used to compare the classification effect. Experiments show that the classification effect of the proposed algorithm is better than that of the deep learning yolox network. The classification accuracy of the unsupervised detection algorithm based on the combination of K-means++ and KNN can reach 94%, the sensitivity is 98%, and the specificity is 80%, which can well solve the misjudgment problem caused by the insignificant T-line of weak positive test strips.","Multi-class classification can be solved by decomposing it into a set of binary classification problems according to some encoding rules, e.g., one-vs-one, one-vs-rest, error-correcting output codes. Existing works solve these binary classification problems in the original feature space, while it might be suboptimal as different binary classification problems correspond to different positive and negative examples. In this paper, we propose to learn label-specific features for each decomposed binary classification problem to consider the specific characteristics containing in its positive and negative examples. Specifically, to generate the label-specific features, clustering analysis is respectively conducted on the positive and negative examples in each decomposed binary data set to discover their inherent information and then label-specific features for one example are obtained by measuring the similarity between it and all cluster centers. Experiments clearly validate the effectiveness of learning label-specific features for decomposition-based multi-class classification.","Reducing the occurrence of gear failures and extending their service life is a vital issue in industrial production. To solve the problem that the method of gear fault detection with Convolution Neural Network (CNN) is difficult to extract the temporal features of the vibration data, an improved Convolutional-LSTM (Conv-LSTM) gear fault detection method was proposed. First, the raw data was fed into the convolutional layer, followed by the pooling and LSTM layers. A batch normalisation layer (BN) was added after the convolutional layer to speed up convergence. Second, to reduce the complexity of the model, a Global Maximum Pooling layer (GMP) was used to replace the flattened layer, and the Hinge functions are used as loss functions. Finally, classification is carried out by the Softmax classifier. The overall accuracy of model architecture could reach 99.64% on the University of Connecticut gear fault dataset. The results show that the proposed method is effective and can meet gear fault diagnosis's accuracy and timeliness requirements.","Nowadays, medical diseases are one of the primary causes of death, and it is one the major concerns of developed countries. So, the disease identification process needs a lot of attention since if the diseases are idenfied at the early stage, the rate of death can be decreased. Machine learning techniques is one of the popular approaches that is used for identifying the diseases at the early stage. In this paper, two machine learning techniques, namely Naive Bayes classification algorithm and Laplace smoothing technique are used to predict the heart disease. Here, many medical details are used, such as gender, age, fasting blood sugar, blood pressure, cholesterol, etc. to predict the hearth disease of a patient. The proposed decision system supports avoiding unnecessary diagnosis test, which can be highly beneficial to start the treatment quickly. Thus, both time and money can be saved. Both the performance analysis and the experimental results show the efficiency of the proposed scheme over the existing schemes.","Highlights\n\u2022\nA measure defined to show the effectiveness of each two features in classification.\n\u2022\nCollaboration Graph (CG) represents the measure as an edge between each two features.\n\u2022\nCommunity detection is used on CG to specify informative feature subsets.\n\u2022\nThe approach has been tested successfully on real and synthetic data.\nAbstract\nThe curse of dimensionality of features in data classification is still an open issue. An approach to solve this problem is to partition features into several sub-sets of features hence the data classification task for every subset is performed. Then, an ensemble of these classifications are reported as the result of the classification problem. However, the feature set partitioning into sub-sets of features is still an area of research interest. Thus, in this paper, an innovative framework is proposed in which, first, a collaboration measure between each two features is defined and measured. Then, the collaboration graph, consisted of features as nodes and measured collaborations as edges\u2019 weights, is generated according to the collaboration measures calculated. After that, a community detection method is used to find the graph communities. The communities are considered as the feature subsets and a base classifier is trained for each subset based on the corresponding training data of the subsets. Then, the ensemble classifier is created by a combination of base classifiers according to the AdaBoost Aggreagation. The simulation results of the proposed approach over the real and synthetic datasets indicate that the proposed approach considerably increases the classification accuracy in comparison to previous methods.","Although Graph Neural Networks (GNNs) have been successful in node classification tasks, their performance heavily relies on the availability of a sufficient number of labeled nodes per class. In real-world situations, not all classes have many labeled nodes and there may be instances where the model needs to classify new classes, making manual labeling difficult. To solve this problem, it is important for GNNs to be able to classify nodes with a limited number of labeled nodes, known as few-shot node classification. Previous episodic meta-learning based methods have demonstrated success in few-shot node classification, but our findings suggest that optimal performance can only be achieved with a substantial amount of diverse training meta-tasks. To address this challenge of meta-learning based few-shot learning (FSL), we propose a new approach, the Task-Equivariant Graph few-shot learning (TEG) framework. Our TEG framework enables the model to learn transferable task-adaptation strategies using a limited number of training meta-tasks, allowing it to acquire meta-knowledge for a wide range of meta-tasks. By incorporating equivariant neural networks, TEG can utilize their strong generalization abilities to learn highly adaptable task-specific strategies. As a result, TEG achieves state-of-the-art performance with limited training meta-tasks. Our experiments on various benchmark datasets demonstrate TEG's superiority in terms of accuracy and generalization ability, even when using minimal meta-training data, highlighting the effectiveness of our proposed approach in addressing the challenges of meta-learning based few-shot node classification. Our code is available at the following link: https://github.com/sung-won-kim/TEG","Cancer classification using high-throughput mass spectrometry data for early disease detection and prevention has recently become an attractive topic of research in bioinformatics. Recently, several studies have shown that the synergy of proteomic technology and pattern classification techniques is promising for the predictive diagnoses of several cancer diseases. However, the extraction of some effective features that can represent the identities of different classes plays a critical factor for any classification problems involving the analysis of complex data. In this paper we present the concept of a fuzzy fractal dimension that can be utilized as a novel feature of mass spectrometry (MS) data. We then apply vector quantization (VQ) to model the class prototyes using the fuzzy fractal dimensions for classification. The proposed methodology was tested with an MS-based ovarian cancer dataset. Using a simple VQ-based classification rule, the overall average classification rates of the proposed approach were found to be superior to some other methods.","Electroencephalography (EEG) motor imagery (MI) signals has recently attracted a great deal of attention as these signals encrypt a person's desire of executing a command. MI signals are used to assist disabled people and even for autonomous driving through some control devices like wheelchairs just by thinking about it. Therefore, an accurate MI tasks classification from EEG signals is cricial to get a reliable Brain Computer Interface (BCI) system. In this paper, we proposed a new method of classifying MI tasks based on Convolutional Neural Network (CNN) methods. We applied a simple preprocessing to the data followed by a feature extraction step using Common Spatial Pattern (CSP) to extract spatial features and Wavelet Packet Decomposition (WPD) to extract frequency-time features. We then tested our four proposed models: CNN, CNN+LSTM, CNN-SVM and CNN+LSTM-SVM using BCI Competition IV 2a dataset. The obtained experimental results show that the proposed CNN-SVM gives the best results. Our results are really promising achieving interesting accuracy, precision, recall, and F1 score of 64.33%, 65.05%, 66.11%, et 64.11%, respectively.","Sentiment analysis is an essential task in understanding human-generated textual documents. While most research into sentiment analysis focuses on monolingual sentences, in multilingual communities, a significant proportion of social media text contains a mixture of languages or code-switching. Thus, it has become vital to research and build models that handle code-switched data. However, despite significant research and custom expert neural architectures proposed, the current literature is mainly limited to modeling single language pairs. To expand on existing work and baseline performance for this particular task, we perform multiple experiments: fine-tuning pre-trained multilingual models and fine-tuning monolingual BERT models on sentence and word-level translations. The experiments are performed across five datasets where English is code-switched with Spanish, Tamil, Telugu, Hindi, and Malayalam. Our best model outperforms the current best single model that works with multiple code-switched language pairs on standard classification metrics on a binary sentiment classification task. We further expand our experiment with a ternary sentiment classification task and produce results comparable to single language-pair-specific models.","Arbitrary, inconsistent, or faulty decision-making raises serious concerns, and preventing unfair models is an increasingly important challenge in Machine Learning. Data often reflect past discriminatory behavior, and models trained on such data may reflect bias on sensitive attributes, such as gender, race, or age. One approach to developing fair models is to preprocess the training data to remove the underlying biases while preserving the relevant information, for example, by correcting biased labels. While multiple label noise correction methods are available, the information about their behavior in identifying discrimination is very limited. In this work, we develop an empirical methodology to systematically evaluate the effectiveness of label noise correction techniques in ensuring the fairness of models trained on biased datasets. Our methodology involves manipulating the amount of label noise and can be used with fairness benchmarks but also with standard ML datasets. We apply the methodology to analyze six label noise correction methods according to several fairness metrics on standard OpenML datasets. Our results suggest that the Hybrid Label Noise Correction [20] method achieves the best trade-off between predictive performance and fairness. Clustering-Based Correction [14] can reduce discrimination the most, however, at the cost of lower predictive performance.","A new approach is proposed for the data-based identification of transparent fuzzy rule-based classifiers. It is observed that fuzzy rule-based classifiers work in a similar manner as kernel function-based support vector machines (SVMs) since both model the input space by nonlinearly maps into a feature space where the decision can be easily made. Accordingly, trained SVM can be used for the construction of fuzzy rule-based classifiers. However, the transformed SVM does not automatically result in an interpretable fuzzy model because the SVM results in a complex rule-base, where the number of rules is approximately 40-60% of the number of the training data. Hence, reduction of the SVM-initialized classifier is an essential task. For this purpose, a three-step reduction algorithm is developed based on the combination of previously published model reduction techniques. In the first step, the identification of the SVM is followed by the application of the Reduced Set method to decrease the number of kernel functions. The reduced SVM is then transformed into a fuzzy rule-based classifier. The interpretability of a fuzzy model highly depends on the distribution of the membership functions. Hence, the second reduction step is achieved by merging similar fuzzy sets based on a similarity measure. Finally, in the third step, an orthogonal least-squares method is used to reduce the number of rules and re-estimate the consequent parameters of the fuzzy rule-based classifier. The proposed approach is applied for the Wisconsin Breast Cancer, Iris and Wine classification problems to compare its performance to other methods.","Recently, the Text-to-SQL task has received much attention. Many sophisticated neural models have been invented that achieve significant results. Most current work assumes that all the inputs are legal and the model should generate an SQL query for any input. However, in the real scenario, users are allowed to enter the arbitrary text that may not be answered by an SQL query. In this article, we focus on the issue\u2013answerability classification for the Text-to-SQL system, which aims to distinguish the answerability of the question according to the given database schema. Existing methods concatenate the question and the database schema into a sentence, then fine-tune the pre-trained language model on the answerability classification task. In this way, the database schema is regarded as sequence text that may ignore the intrinsic structure relationship of the schema data, and the attention that represents the correlation between the question token and the database schema items is not well designed. To this end, we propose a relational Question-Schema graph framework that can effectively model the attention and relation between question and schema. In addition, a conditional layer normalization mechanism is employed to modulate the pre-trained language model to generate better question representation. Experiments demonstrate that the proposed framework outperforms all existing models by large margins, achieving new state of the art on the benchmark TRIAGESQL. Specifically, the model attains 88.41%, 78.24%, and 75.98% in Precision, Recall, and F1, respectively. Additionally, it outperforms the baseline by approximately 4.05% in Precision, 6.96% in Recall, and 6.01% in F1.","Dimensionality reduction is an important step in increasing the performance of machine learning algorithms while decreasing the processing time. From feature reduction approaches, feature extraction is aimed to achieve better data representation while feature selection is meant to discard redundant features. The main contribution of this manuscript is to propose a hybrid feature selection and extraction approach which performs these tasks in a unified framework, simultaneously. Also, the proposed approach aims to maintain the manifold of data using locally alignment constrains. The main goal of these suggestions is to discriminate different classes while reducing the redundancy of the samples specially for the imbalanced data classification problems. The proposed approach optimizes a hybrid objective function that tries to both increase the between class discrimination and decrease the within class distribution while minimizing the information loss. Also, we have embedded a weighting factor into the objective function to achieve a basis for feature importance measurement and feature selection. To evaluate the proposed approach, a multiclass linear SVM classifier is applied on the reduced data in a k-fold cross-validation scheme, and the accuracy, as well as F-score, is used as the performance measure. Comparisons of the proposed method with some recent approaches on 12 datasets of UCI, show the superiority of the proposed method over previously proposed approaches. Also, Friedman and Nemenyi tests are applied which show significant improvement in the proposed approach.","Highlights\n\u2022\nProcessing Complex-valued PolSAR Data Using Complex-valued Convolutional Neural Network (CV-CNN).\n\u2022\nUses a Gaussian-type activation function (GTAF) that preserves the integrity of complex-valued operations.\n\u2022\nIntroduces learnable Gaussian parameters for GTAF, and designs two multi-channel activation methods.\n\u2022\nThe classification accuracy is better than that of existing state-of-the-art methods in three datasets.\nAbstract\nTo process complex-valued information such as SAR signals conveniently, the complex-valued convolutional neural network (CV-CNN) has been proposed in recent years, and it has achieved great success in SAR image recognition. This paper proposes an activation function with learnable parameters based on the Gaussian-type activation function (GTAF) in CV-CNN to improve the utilization of information in the real and imaginary parts of the neuro. For the multi-channel input of the feature map, this paper discusses two ways to set the parameters of the Gaussian-type activation function. One is that all channels share the same parameters, called the channel-sharing Gaussian-type activation function (CSGTAF). The other is that each channel has its independent parameters, called the channel-exclusive Gaussian-type activation function (CEGTAF). In addition, this paper derives the backpropagation formula of both CSGTAF and CEGTAF in detail for the training process of CV-CNN. This paper performs experimental analysis on three L-band standard PolSAR datasets. The experimental results show that, compared with the traditional method and the Gaussian activation function with fixed parameters, both CSGTAF and CEGTAF achieve higher recognition accuracy, and the difference in the recognition effect of different targets in the same dataset is little. Both show good recognition performance and have good stability and versatility.","The classification of materials is a research hotspot. These methods generally focus on the classification of flat materials and do not consider the influence of polishing and convex surfaces. We develop a classification algorithm of polishing and convex surface objects, and derive the photon accumulation point spread function (PAPSF) of material from the imaging model of a binocular pulsed time-of-flight (ToF) camera as the classification feature, which consists of depth distortion, the indirect reflection photon cumulant and the indirect reflection photon cumulant. We design a one-versus-all support vector machine (SVM) classifier to classify materials of polishing and convex surfaces objects. We conduct classification experiments on four plastics and four metal materials with a similar appearance. Our method in flat and raw material classification has the same classification accuracy as the latest method based on a continuous-wave- modulation ToF camera, but also our method achieved accuracies of 91.0% in flat and polishing material classification, 93.0% in different convex surface and fixed polishing material classification, 91.5% in fixed convex surface and different polishing material classification and 90.2% in polishing and convex surface material classification.","Partial-label learning (PLL) relies on a key assumption that the true label of each training example must be in the candidate label set. This restrictive assumption may be violated in complex real-world scenarios, and thus the true label of some collected examples could be unexpectedly outside the assigned candidate label set. In this paper, we term the examples whose true label is outside the candidate label set OOC (Out-Of-Candidate) examples, and pioneer a new PLL study to learn with OOC examples. We consider two types of OOC examples in reality, i.e., the closed-set/open-set OOC examples whose true label is inside/outside the known label space. To solve this new PLL problem, we first calculate the wooden cross-entropy loss from candidate and non-candidate labels respectively, and dynamically differentiate the two types of OOC examples based on specially designed criteria. Then, for closed-set OOC examples, we conduct reversed label disambiguation in the non-candidate label set; for open-set OOC examples, we leverage them for training by utilizing an effective regularization strategy that dynamically assigns random candidate labels from the candidate label set. In this way, the two types of OOC examples can be differentiated and further leveraged for model training. Extensive experiments demonstrate that our proposed method outperforms state-of-the-art PLL methods.","This paper presents methods of prediction of casting mechanical parameters based on direct microstructure image analysis using deep neural networks and graphite forms recognition and classification. These methods are applied to predict tensile strength of iron-carbon alloys based on microstructure photos taken with the light-optical microscopy technique, but are general and can be adapted to other applications. In the first approach EfficientNet architecture is used. In the second approach graphite structures are separated, recognized using VGG19 network, counted and classified using support vector machines, decision trees, random forest, logistic regression, multi-layer perceptron and AdaBoost. Accuracy of the first approach is better. However, the second allows to create a classifier, for which the accuracy is also high, and can be easily analyzed by human expert.","We employed a granular support vector Machines(GSVM) for prediction of soluble proteins on over expression in Escherichia coli . Granular computing splits the feature space into a set of subspaces (or information granules) such as classes, subsets, clusters and intervals [14]. By the principle of divide and conquer it decomposes a bigger complex problem into smaller and computationally simpler problems. Each of the granules is then solved independently and all the results are aggregated to form the final solution. For the purpose of granulation association rules was employed. The results indicate that a difficult imbalanced classification problem can be successfully solved by employing GSVM.","Mis-translation or dropping of proper nouns reduces the quality of machine translation or speech translation output. In this paper, we propose a method to build a proper noun dictionary for the systems which use class-based language models. The method consists of two parts: training data building part and word classifier training part. The first part uses bilingual corpus which contain proper nouns. For each proper noun, the first part finds out the class which gives the highest sentence-level automatic evaluation score. The second part trains CNN-based word class classifier by using the training data yielded by the first step. The training data consists of source language sentences with proper nouns and the proper nouns\u2019 classes which give the highest scores. The CNN is trained to predict the proper noun class given the source side sentence. Although, the proposed method does not require the manually annotated training data at all, the experimental results on a statistical machine translation system show that the dictionary made by the proposed method achieves comparable performance to the manually annotated dictionary.","Gradient Boosting Decision Tree (GBDT) has achieved remarkable success in a wide variety of applications. The split finding algorithm, which determines the tree construction process, is one of the most crucial components of GBDT. However, the split finding algorithm has long been criticized for its bias towards features with a large number of potential splits. This bias introduces severe interpretability and overfitting issues in GBDT. To this end, we provide a fine-grained analysis of bias in GBDT and demonstrate that the bias originates from 1) the systematic bias in the gain estimation of each split and 2) the bias in the split finding algorithm resulting from the use of the same data to evaluate the split improvement and determine the best split. Based on the analysis, we propose unbiased gain, a new unbiased measurement of gain importance using out-of-bag samples. Moreover, we incorporate the unbiased property into the split finding algorithm and develop UnbiasedGBM to solve the overfitting issue of GBDT. We assess the performance of UnbiasedGBM and unbiased gain in a large-scale empirical study comprising 60 datasets and show that: 1) UnbiasedGBM exhibits better performance than popular GBDT implementations such as LightGBM, XGBoost, and Catboost on average on the 60 datasets and 2) unbiased gain achieves better average performance in feature selection than popular feature importance methods. The codes are available at https://github.com/ZheyuAqaZhang/UnbiasedGBM.","The coronavirus has caused havoc on billions of people worldwide. The Reverse Transcription Polymerase Chain Reaction(RT-PCR) test is widely accepted as a standard diagnostic tool for detecting infection, however, the severity of infection can't be measured accurately with RT-PCR results. Chest CT Scans of infected patients can manifest the presence of lesions with high sensitivity. During the pandemic, there is a dearth of competent doctors to examine chest CT images. Therefore, a Guided Gradcam based Explainable Classification and Segmentation system (GGECS) which is a real-time explainable classification and lesion identification decision support system is proposed in this work. The classification model used in the proposed GGECS system is inspired by Res2Net. Explainable AI techniques like GradCam and Guided GradCam are used to demystify Convolutional Neural Networks (CNNs). These explainable systems can assist in localizing the regions in the CT scan that contribute significantly to the system's prediction. The segmentation model can further reliably localize infected regions. The segmentation model is a fusion between the VGG-16 and the classification network. The proposed classification model in GGECS obtains an overall accuracy of 98.51 % and the segmentation model achieves an IoU score of 0.595.","The deployment of Deep Neural Networks (DNNs) on edge devices is hindered by the substantial gap between performance requirements and available computational power. While recent research has made significant strides in developing pruning methods to build a sparse network for reducing the computing overhead of DNNs, there remains considerable accuracy loss, especially at high pruning ratios. We find that the architectures designed for dense networks by differentiable architecture search methods are ineffective when pruning mechanisms are applied to them. The main reason is that the current methods do not support sparse architectures in their search space and use a search objective that is made for dense networks and does not focus on sparsity.\nThis paper proposes a new method to search for sparsity-friendly neural architectures. It is done by adding two new sparse operations to the search space and modifying the search objective. We propose two novel parametric SparseConv and SparseLinear operations in order to expand the search space to include sparse operations. In particular, these operations make a flexible search space due to using sparse parametric versions of linear and convolution operations. The proposed search objective lets us train the architecture based on the sparsity of the search space operations. Quantitative analyses demonstrate that architectures found through DASS outperform those used in the state-of-the-art sparse networks on the CIFAR-10 and ImageNet datasets. In terms of performance and hardware effectiveness, DASS increases the accuracy of the sparse version of MobileNet-v2 from 73.44% to 81.35% (+7.91% improvement) with a 3.87\u00d7 faster inference time.","Idiomatic expressions are important natural parts of all languages and prominent parts of our daily speech. Idioms cannot be interpreted from the words that they are formed with directly and people may not understand the meaning. From past literature, it was noted that idiom affects Natural Language Processing research like machine translation, semantic analysis, and sentiment analysis. Other languages like English, Chinese, and Indian idioms are recognized through different methods in different research. As there is no standard method and research to identify Amharic idioms, this study is aimed to build a model to identify idioms for the Amharic language using a supervised machine learning approach. The study used 800 labeled expressions for training and 200 expressions for testing from Amharic idiom books \u201c\u12e8\u12a0\u121b \u1228\u129b \u1348\u120a\u1326\u127d\u201d and different Amharic documents. To measure the performance of the model, we used accuracy, precision, recall, and F-score. Finally, a 97.5% accuracy result was achieved from the testing dataset showing a promising result. The study contributes to the information systems discourse about improving the awareness and knowledge of researchers on Amharic idioms.","Weighted nearest neighbors (WNN) classifiers are popular non-parametric classifiers. Despite the significant progress in WNN, most existing WNN classifiers are designed for traditional supervised learning problems where both training samples and test samples are assumed to be independent and identically distributed. However, in many real applications, it could be difficult or expensive to obtain training samples from the distribution of interest. Therefore, data collected from some related distributions are often used as supplementary training data for the classification task under the distribution of interest. It is essential to develop effective classification methods that could incorporate both training samples from the distribution of interest (if they exist) and supplementary training samples from a different but related distribution. To address this challenge, we propose a novel Transfer learning weighted Nearest Neighbors (TNN) classifier. As a WNN classifier, TNN determines the weights on the class labels of training samples for different test samples adaptively by minimizing an upper bound on the conditional expectation of the estimation error of the regression function. It puts decreasing weights on the class labels of the successive more distant neighbors. To accommodate the difference between training samples from the distribution of interest and supplementary training samples, TNN adds a non-negative offset to the distance between each supplementary training sample and the test sample, and thus constrains the excessive influence of the supplementary training samples on the prediction. Our theoretical studies show that, under certain conditions, TNN is consistent and minimax optimal (up to a logarithmic factor) in the covariate shift setting. In the posterior drift or the more general setting where both covariate shift and posterior drift exist, the excess risk of TNN depends on the maximum posterior discrepancy between the distribution of the supplementary training samples and the distribution of interest. Both our simulation studies and an application to the land use/land cover mapping problem in geography demonstrate that TNN outperforms other existing methods. It can serve as an effective tool for transfer learning.","Plant diseases and pests are primary factors that can negatively affect crop yield, quality, and profitability. Therefore, the accurate and automatic identification of pests is crucial for the agricultural industry. However, traditional methods of pest classification are limited, as they face difficulties in identifying pests with subtle differences and dealing with sample imbalances. To address these issues, we propose a pest classification model based on data enhancement and multi-feature learning. The model utilizes Mobile Inverted Residual Bottleneck Convolutional Block (MBConv) modules for multi-feature learning, enabling it to learn diverse and rich features of pests. To improve the model\u2019s ability to capture fine-grained details and address sample imbalances, data enhancement techniques such as random mixing of pictures and mixing after region clipping are used to augment the training data. Our model demonstrated excellent performance not only on the large-scale pest classification IP102 dataset but also on smaller pest datasets.","Different researches suggest that inner facial features are not the only discriminative features for tasks such as person identification or gender classification. Indeed, they have shown an influence of features which are part of the local face context, such as hair, on these tasks. However, object-centered approaches which ignore local context dominate the research in computational vision based facial analysis. In this paper, we performed an analysis to study which areas and which resolutions are diagnostic for the gender classification problem. We first demonstrate the importance of contextual features in human observers for gender classification using a psychophysical \u201dbubbles\u201d technique. The success rate achieved without internal facial information convinced us to analyze the performance of an appearance-based representation which takes into account facial areas and resolutions that integrate inner features and local context.","Highlights\n\u2022\nEffective: PLFace significantly improves the accuracy of masked face recognition while maintaining the performance of normal face recognition on several face recognition benchmarks, including mask-free and masked datasets.\n\u2022\nEasy: PLFace can be easily migrated to the existing loss functions of face recognition, e.g., CosFace, ArcFace, CurricularFace.\n\u2022\nEfficient: PLFace only adds negligible computational complexity during the training process, and has the same cost as the backbone model during the inference process.\nAbstract\nThe outbreak of the COVID-19 coronavirus epidemic has promoted the development of masked face recognition (MFR). Nevertheless, the performance of regular face recognition is severely compromised when the MFR accuracy is blindly pursued. More facts indicate that MFR should be regarded as a mask bias of face recognition rather than an independent task. To mitigate mask bias, we propose a novel Progressive Learning Loss (PLFace) that achieves a progressive training strategy for deep face recognition to learn balanced performance for masked/mask-free faces recognition based on margin losses. Particularly, our PLFace adaptively adjusts the relative importance of masked and mask-free samples during different training stages. In the early stage of training, PLFace mainly learns the feature representations of mask-free samples. At this time, the regular sample embeddings shrink to the corresponding prototype, which represents the center of each class while being stored in the last linear layer. In the later stage of training, PLFace converges on mask-free samples and further focuses on masked samples until the masked sample embeddings are also gathered in the center of the class. The entire training process emphasizes the paradigm that normal samples shrink first and masked samples gather afterward. Extensive experimental results on popular regular and masked face benchmarks demonstrate that our proposed PLFace can effectively eliminate mask bias in face recognition. Compared to state-of-the-art competitors, PLFace significantly improves the accuracy of MFR while maintaining the performance of normal face recognition.","ALVOT is a supervised classification model based on partial precedences. These classifiers work with databases having objects described simultaneously by numeric and nonnumeric features. In this paper a new object selection method based on the error per subclass is proposed for improving the accuracy, especially with noisy training matrixes. A comparative numerical experiment was performed with different methods of object selection. The experimental results show a good performance of the proposed method with respect to previously reported in the literature.","Smart buildings are generally equipped with thousands of heterogeneous sensors and control devices that impact the operation of their electrical systems. Analytical tools that aim to optimise the energy efficiency within such complex systems requires prior mapping or (classification) of diverse set of sensors according to a standard. Prior research primarily focuses on exploiting the similarities in sensor names (text metadata) to categorise them into identical classes (or groups). However, the sensors within and across buildings often follow distinct naming conventions by different vendors. In addition the definition of the classes or groups also varies significantly amongst researchers. This limits the usability and portability of prior techniques when applied across buildings. There are standard ontologies (Brick, Haystack etc.) that provide a set of standardized classes for the sensors in the buildings. The work herein follows a new avenue to address this challenging classification problem by (i) utilizing only time-series data of sensors and not text metadata, (ii) developing a simple, effective and hitherto unexplored Machine Learning (ML) model to classify the sensors into a set of standard Brick classes, and (iii) evaluating the model on a large proprietary dataset comprising of 129 buildings. Experimental results demonstrate promising performance of the presented data driven model, with average classification accuracy in terms of weighted F-score at 0.78 (\u00b10.14), and statistically significant improvements over prior methods.","In view of the current research seldom consider the multi-scale characteristics of network traffic, which may lead to an inaccurate classification of anomalies and a high false alarm rate. In this paper, a network traffic anomaly detection method based on the multi-scale residual classifier (MSRC) is proposed. We use sliding windows to divide the network traffic into subsequences with different observation scales, use the wavelet transform technology to obtain the time\u2013frequency information of each subsequence on multiple decomposition scales, design a stacked automatic encoder (SAE) to learn the distribution of input data, calculate the reconstruction error vector by using the constructed feature space, and learn the feature information of different scales in the reconstruction error vector by using the multipath residual group, and complete traffic anomaly detection through the lightweight classifier. Experimental results show that the detection performance of the proposed method for abnormal network traffic is improved compared with the traditional method. It is proved that large observation scales and more transformation scales have positive effects on discovering the potential diversity information in the original network traffic.","Uncontrolled proliferation of B-lymphoblast cells is a common characterization of Acute Lymphoblastic Leukemia (ALL). B-lymphoblasts are found in large numbers in peripheral blood in malignant cases. Early detection of the cell in bone marrow is essential as the disease progresses rapidly if left untreated. However, automated classification of the cell is challenging, owing to its fine-grained variability with B-lymphoid precursor cells and imbalanced data points. Deep learning algorithms demonstrate potential for such fine-grained classification as well as suffer from the imbalanced class problem. In this paper, we explore different deep learning-based State-Of-The-Art (SOTA) approaches to tackle imbalanced classification problems. Our experiment includes input, GAN (Generative Adversarial Networks), and loss-based methods to mitigate the issue of imbalanced class on the challenging C-NMC and ALLIDB-2 dataset for leukemia detection. We have shown empirical evidence that loss-based methods outperform GAN-based and input-based methods in imbalanced classification scenarios.\nGraphical abstract\nDisplay Omitted\nHighlights\n\u2022\nPerformed extensive analysis on state-of-the-art imbalance classification techniques.\n\u2022\nLoss based methods are more effective than input and GAN based methods.\n\u2022\nMCC loss works best in high imbalance scenarios (i.e. 1:100 ratio).","Existing studies have recognized the effect of noise of granulated datasets on classification performance. Whether this effect continues an aid in generating decisional rules for the tree-based learning models needs to be disclosed. This study conducts an experiment that investigates the effect of noisy data on rule generation performance (RGP). The unsupervised (equal-width interval, EWI) and 28 supervised (minimum description length, MDL) techniques were used to granulate datasets. The decision-tree based classification model that either included or did not include 24 EWI and 28 MDL noisy granulated datasets were used, followed by testing and comparison on classification accuracy and RGP. Main results are as follows. Removal of noisy granulated datasets with EWI is advantageous to decision tree generation when original classification accuracy (OCA) of datasets is higher than 90% or less than 70%, but not obvious between 70% and 90%. Contrariwise, those with MDL is neither highly related to improvement of generation rate nor simplicity for scales of both higher than 90% and less than 70%, but slightly related to those with OCA between 70% and 90%.","Changes to a software project are inevitable as the software requires continuous adaptations, improvements, and corrections throughout maintenance. Identifying the purpose and impact of changes made to the codebase is critical in software engineering. However, manually identifying and characterizing software changes can be a time-consuming and tedious process that adds to the workload of software engineers. To address this challenge, several attempts have been made to automatically identify and demystify intents of software changes based on software artifacts such as commit change logs, issue reports, change messages, source code files, and software documentation. However, these existing approaches have their limitations. These include a lack of data, limited performance, and an inability to evaluate compound changes. This paper presents a doctoral research proposal that aims to automate the process of identifying commit-level changes in software projects using software repository mining and code representation learning models. The research background, state-of-the-art, research objectives, research agenda, and threats to validity are discussed.","With advances in sensing technology, multi-modal data collected from different sources are increasingly available. Multi-modal classification aims to integrate complementary information from multi-modal data to improve model classification performance. However, existing multi-modal classification methods are basically weak in integrating global structural information and providing trustworthy multi-modal fusion, especially in safety-sensitive practical applications (e.g., medical diagnosis). In this paper, we propose a novel Dynamic Poly-attention Network (DPNET) for trustworthy multi-modal classification. Specifically, DPNET has four merits: (i) To capture the intrinsic modality-specific structural information, we design a structure-aware feature aggregation module to learn the corresponding structure-preserved global compact feature representation. (ii) A transparent fusion strategy based on the modality confidence estimation strategy is induced to track information variation within different modalities for dynamical fusion. (iii) To facilitate more effective and efficient multi-modal fusion, we introduce a cross-modal low-rank fusion module to reduce the complexity of tensor-based fusion and activate the implication of different rank-wise features via a rank attention mechanism. (iv) A label confidence estimation module is devised to drive the network to generate more credible confidence. An intra-class attention loss is introduced to supervise the network training. Extensive experiments on four real-world multi-modal biomedical datasets demonstrate that the proposed method achieves competitive performance compared to other state-of-the-art ones.","In recent years, numerous machine learning-based systems have actively propagated discriminatory effects and harmed historically disadvantaged groups through their decision-making. This undesired behavior highlights the importance of research topics such as fairness in machine learning, whose primary goal is to include fairness notions into the training process to build fairer models. In parallel, Differential Item Functioning (DIF) is a mathematical tool often used to identify bias in test preparation for candidate selection; DIF detection assists in identifying test items that disproportionately favor or disadvantage candidates solely because they belong to a specific sociodemographic group. This paper argues that transposing DIF concepts into the machine learning domain can lead to promising approaches for developing fairer solutions. As such, we propose DIF-SR, the first DIF-based Sample Reweighting method for weighting samples so that the assigned values help build fairer classifiers. DIF-SR can be seen as a data preprocessor that imposes more importance on the most auspicious examples in achieving equity ideals. We experimentally evaluated our proposal against two baseline strategies by employing twelve datasets, five classification algorithms, four performance measures, one multicriteria measure, and one statistical significance test. Results indicate that the sample weight computed by DIF-SR can guide supervised machine learning methods to fit fairer models, simultaneously improving group fairness notions such as demographic parity, equal opportunity, and equalized odds.","Dental caries is a common dental disease. According to statistics, about 90% of adults suffer from dental caries. Therefore, early detection and treatment of dental caries are crucial to dental health. According to the depth of carious lesions, dental caries can be classified into shallow, moderate, and deep caries. Among them, the accurate classification of moderate caries and deep caries is important to making the subsequent treatment plan. Clinically, doctors can make the diagnosis with the help of CBCT images. However, due to the spatial complexity of the 3D volume, the difficulty of labeling the carious lesion, and the insignificant difference between moderate and deep caries, there is still a great challenge to accurately identifying moderate and deep caries. And to the best of our knowledge, there is no study on automatic dental caries classification based on CBCT images. In this paper, we propose a feature patch based attention model to improve the classification accuracy of dental caries in CBCT images. We extract overlapping patches from the 3D feature maps and assign every patch with a corresponding weight computed by adaptive learning to achieve automatic screening of regions that are critical for classification. We collect a real-world dental dataset which includes 167 CBCT scans with moderate caries and 157 CBCT scans with deep caries. A series of experiments demonstrate that our algorithm achieves 92% accuracy on caries classification, which outperforms state-of-the-art methods by a large margin.","As brain-related research presents increasing importance, the requirement for automatic spike detection algorithms also emerges. Traditional spike detection algorithms, including amplitude thresholding and wavelet transformation, show several shortcomings that impede the practical application. Here, we propose an artificial neural network-assisted amplitude thresholding algorithm and conduct experiments with raw signals collected from the primary somatosensory cortex and primary motor cortex of macaques. Using F1 score as an evaluation index, artificial neural networks, as well as its lightweight version, effectively help the amplitude thresholding to achieve better performance, showing enormous potential for real-time spike detection application.","A time series is a sequence of sequentially ordered real values in time. Time series classification (TSC) is the task of assigning a time series to one of a set of predefined classes, usually based on a model learned from examples. Dictionary-based methods for TSC rely on counting the frequency of certain patterns in time series and are important components of the currently most accurate TSC ensembles. One of the early dictionary-based methods was WEASEL, which at its time achieved SotA results while also being very fast. However, it is outperformed both in terms of speed and accuracy by other methods. Furthermore, its design leads to an unpredictably large memory footprint, making it inapplicable for many applications. In this paper, we present WEASEL 2.0, a complete overhaul of WEASEL based on two recent advancements in TSC: Dilation and ensembling of randomized hyper-parameter settings. These two techniques allow WEASEL 2.0 to work with a fixed-size memory footprint while at the same time improving accuracy. Compared to 15 other SotA methods on the UCR benchmark set, WEASEL 2.0 is significantly more accurate than other dictionary methods and not significantly worse than the currently best methods. Actually, it achieves the highest median accuracy over all data sets, and it performs best in 5 out of 12 problem classes. We thus believe that WEASEL 2.0 is a viable alternative for current TSC and also a potentially interesting input for future ensembles.","Clinical text classification allows assigning labels to content-based data using machine learning algorithms. However, unlike other study domains, clinical texts present complex linguistic diversity, including abbreviations, typos, and numerical patterns that are difficult to represent by the most-used classification algorithms. In this sense, sequences of character strings and symbols, known as Regular Expressions (RegExs), offer an alternative to represent complex patterns from the texts and could be used jointly with the most commonly used classification algorithms for accurate text classification. Thus, a classification algorithm can label test texts when RegExs produce no matches. This work proposes a method that combines automatically-generated RegExs and supervised algorithms for classifying clinical texts. RegExs are automatically generated using alignment algorithms in a supervised manner, filtering out those that do not meet a minimum confidence threshold and do not contain specific keywords for the classification problem. At prediction time, our method assigns the class of the most confident RegEx that matches a test text. When no RegExs matches a test text, a supervised algorithm assigns a class. Three clinical datasets with textual information on obesity and smoking habits were used to assess the performance of four classifiers based on Random Forest (RF), Support Vector Machine (SVM), Naive Bayes (NB), and Bidirectional Encoder Representations from Transformers (BERT). Classification results indicate that our method, on average, improved the classifiers\u2019 performance by up to 12% in all performance metrics. These results show the ability of our method to generate confident RegExs that capture representative patterns from the texts for use with supervised algorithms.","Highlights\n\u2022\nA fault diagnosis method based on the ADTC model is proposed.\n\u2022\nThe ADTC model can extract and verify the features of unlabeled data.\n\u2022\nThe proposed model provides a method for solving the insufficient data problem.\n\u2022\nExperiment results demonstrate the high performance of the proposed method.\nAbstract\nFault diagnosis of mechanical equipment using data-driven machine learning methods has been developed recently as a promising technique for improving the reliability of industrial systems. However, these methods suffer from data sparsity due to the difficulty in data collection, which limits the feature extraction of anomalies. To solve this problem, we propose the mel spectrogram-based advanced deep temporal clustering (ADTC) model, which can extract and verify the features of unlabeled data through an unsupervised learning based autoencoder and the K-means. In addition, the ADTC model uses the proposed centroid based learning to obtain calibrated unsupervised learning data by minimizing the data point and target centroid distances for misclustered encoder output features in ensemble-based unsupervised learning. The classifier of the ADTC model uses a supervised learning based deep support vector machine network model, which is robust to nonlinear data, to diagnose the faults of the mechanical equipment. The proposed ADTC model was validated using mechanical equipment dataset with data augmentation to address the imbalanced dataset problem. During experiments, the mel spectrogram-based ADTC model exhibited the best performance in the various industrial environment with a prediction accuracy as high as 98.06%, outperforming other compared algorithms.","Anomaly detection is critical to ensure the security of cyber-physical systems (CPS). However, due to the increasing complexity of attacks and CPS themselves, anomaly detection in CPS is becoming more and more challenging. In our previous work, we proposed a digital twin-based anomaly detection method, called ATTAIN, which takes advantage of both historical and real-time data of CPS. However, such data vary significantly in terms of difficulty. Therefore, similar to human learning processes, deep learning models (e.g., ATTAIN) can benefit from an easy-to-difficult curriculum. To this end, in this paper, we present a novel approach, named digitaL twin-based Anomaly deTecTion wIth Curriculum lEarning (LATTICE), which extends ATTAIN by introducing curriculum learning to optimize its learning paradigm. LATTICE attributes each sample with a difficulty score, before being fed into a training scheduler. The training scheduler samples batches of training data based on these difficulty scores such that learning from easy to difficult data can be performed. To evaluate LATTICE, we use five publicly available datasets collected from five real-world CPS testbeds. We compare LATTICE with ATTAIN and two other state-of-the-art anomaly detectors. Evaluation results show that LATTICE outperforms the three baselines and ATTAIN by 0.906%-2.367% in terms of the F1 score. LATTICE also, on average, reduces the training time of ATTAIN by 4.2% on the five datasets and is on par with the baselines in terms of detection delay time.","Accurate and rapid situation analysis during humanitarian crises is critical to delivering humanitarian aid efficiently and is fundamental to humanitarian imperatives and the Leave No One Behind (LNOB) principle. This data analysis can highly benefit from language processing systems, e.g., by classifying the text data according to a humanitarian ontology. However, approaching this by simply fine-tuning a generic large language model (LLM) involves considerable practical and ethical issues, particularly the lack of effectiveness on data-sparse and complex subdomains, and the encoding of societal biases and unwanted associations. In this work, we aim to provide an effective and ethically-aware system for humanitarian data analysis. We approach this by (1) introducing a novel architecture adjusted to the humanitarian analysis framework, (2) creating and releasing a novel humanitarian-specific LLM called HumBERT, and (3) proposing a systematic way to measure and mitigate biases. Our results show the better performance of our approach on zero-shot and full-training settings in comparison with strong baseline models, while also revealing the existence of biases in the resulting LLMs. Utilizing a targeted counterfactual data augmentation approach, we significantly reduce these biases without compromising performance.","Highlights\n\u2022\nA new proposed algorithm for multi-class recognition with a one-step verification system.\n\u2022\nDetailed experimental analysis that identifies the effectiveness of the 13 developed deep CNNs models, as well as the hybrid ensemble model.\n\u2022\nA statistical approach for selecting only the most qualified models in recognizing patterns.\n\u2022\nA new method for approaching the deep learning intra-class refinement problem in multiclass recognition systems which substantially improves the accuracy for each class.\nAbstract\nResearchers often focus on building models that maximize overall predictive accuracy. In practice, however, it can be important for a model to yield good accuracy with each class value. Toward this end, a new recognition with a one-step verification methodology is proposed. It emphasizes the accuracy of each class value. The proposed discriminative system constructs an ensemble using several deep Convolutional Neural Networks (CNNs) with the help of statistical information.\nTo the best of our knowledge, this is the first ensemble model that combines many deep CNNs with a focus on maximizing the accuracy for each class, rather than just overall accuracy. Experimental results show that the demonstration models achieved accuracy in the range of 97.82% to 99.72% within only a few epochs, rivaling the state-of-the-art. These results indicate that the performance of the proposed approach substantially improves the intra-class correlation, leading to improved classification accuracy for each class.\nGraphical abstract\nDisplay Omitted","Ensembling is a popular and effective method for improving machine learning (ML) models. It proves its value not only in classical ML but also for deep learning. Ensembles enhance the quality and trustworthiness of ML solutions, and allow uncertainty estimation. However, they come at a price: training ensembles of deep learning models eat a huge amount of computational resources. A snapshot ensembling collects models in the ensemble along a single training path. As it runs training only one time, the computational time is similar to the training of one model. However, the quality of models along the training path is different: typically, later models are better if no overfitting occurs. So, the models are of varying utility. Our method improves snapshot ensembling by selecting and weighting ensemble members along the training path. It relies on training-time likelihoods without looking at validation sample errors that standard stacking methods do. Experimental evidence for Fashion MNIST, CIFAR-10, and CIFAR-100 datasets demonstrates the superior quality of the proposed weighted ensembles c.t. vanilla ensembling of deep learning models. The suggested approach spends on training of N models N times less time than classical ensemble and provides significantly higher quality compared to the single model.","Higher cognitive process efforts may result in mental exhaustion, poor performance, and long-term health issues. An EEG-based methods for detecting a pilot's mental state have recently been created utilizing machine learning algorithms. EEG signals include a significant noise component, and these approaches either ignore this or use a random mix of preprocessing techniques to reduce noise. In the absence of uniform preprocessing procedures for cleaning, it would be impossible to compare the efficacy of machine learning models across research, even if they employ data obtained from the same experiment. In this study, we intend to evaluate how preprocessing approaches affect the performance of machine learning models. To do this, we concentrated on fundamental preprocessing techniques, such as a band-pass filter and independent component analysis. Using a publicly accessible actual physiological dataset gathered from a pilot who was exposed to a variety of mental events, we explore the influence of these preprocessing strategies on two machine learning models, SVMs and ANNs. Our findings indicate that the performance of the models is unaffected by preprocessing techniques. Moreover, our findings indicate that the models were able to anticipate the mental states from merged data collected in two environments. These findings demonstrate the necessity for a standardized methodological framework for the application of machine learning models to EEG inputs.","Approval of credit application is one of the censorious business decisions the bankers usually take on a regular basis. The growing number of new credit applications and the enormous amount of outstanding credit card bills during the recent pandemic make this even more challenging nowadays. Some of the previous studies suggest automating the credit approval process to mitigate this challenge. However, we believe that trustworthy machine assistance could be more beneficial as the effectiveness of complete automation may depend on the richness of the training dataset and model efficiency. A novel classifier, named random wheel, interestingly provides a more interpretable output. In this work, we have proposed an enhanced version of random wheel to facilitate a trustworthy recommendation for credit approval process. It not only produces more accurate and precise recommendations but also provides an interpretable confidence measure. Besides, it explains the machine recommendation for each credit application as well. The availability of recommendation confidence and explanation could bring more trust in the machine provided intelligence which may, in turn, enhance the efficiency of the credit approval process.\nHighlights\n\u2022\nCredit approval is one of the critical business decision in the banking sector.\n\u2022\nIntelligence of expert system can be used to build a trustworthy approval process.\n\u2022\nThis work evaluates the effectiveness of random wheel classifier for this purpose.\n\u2022\nThis novel classifier provides more accurate and precise approval recommendation.\n\u2022\nThe additional confidence measure and explanation help in building the trust.","Classification is an important issue in data mining and knowledge discovery, and the attribute reduction has been proven to be effective in improving the classification accuracy in many applications. In this paper, we first apply rough set theory to reduce irrelative attribute and retain the important attributes, and the input neuron based on the important attributes can simplify the structure of BP-neuron network and improve classification accuracy. Then an efficient BP-neural network classification model based on attribute reduction is developed for high-dimensional data analysis. Finally, the experimental results demonstrate the efficiency and effectiveness of the proposed model."],"author":["NaN","Feng, Panpan and Fu, Jie and Wang, Ning and Zhou, Yanjie and Zhou, Bing and Wang, Zongmin","Sotero, Roberto C and Sanchez-Bornot, Jose M and Shaharabi-Farahani, Iman and Iturria-Medina, Yasser","Selvakumar, P. and Muthukumaran, G.","NaN","Xu, Changkai and Zhang, Chunjie and Yang, Yanwu and Yang, Huaizhi and Bo, Yijun and Li, Danyong and Zhang, Riquan","Jhansi Rani, Challapalli and Devarakonda, Nagaraju","Hu, Zhenda and Wang, Zhaoxia and Wang, Yinglin and Tan, Ah-Hwee","Senthil Kumar, Prathyusha","NaN","Shao, Mingwen and Hu, Zhiyong and Wu, Weizhi and Liu, Huan","Shi, Piao and Hu, Min and Shi, Xuefeng and Ren, Fuji","NaN","Lv, Yan and Yin, Yu Jia and Guo, Wenwen and Bai, Lan","NaN","Dantas, Cassio F. and Drumond, Thalita F. and Marcos, Diego and Ienco, Dino","Huang, Keke and Tang, Jing and Liu, Juncheng and Yang, Renchi and Xiao, Xiaokui","NaN","Zhang, Xuejun and Zhang, Susu and Bu, Zhaohui and Ma, Liangdi and Huang, Ju","Li, Yu and Parsan, Anisha and Wang, Bill and Dong, Penghao and Yao, Shanshan and Qin, Ruwen","Gupta, Soumyajit and Lee, Sooyong and De-Arteaga, Maria and Lease, Matthew","Mohiuddin, Karishma and Alam, Mirza Ariful and Alam, Mirza Mohtashim and Welke, Pascal and Martin, Michael and Lehmann, Jens and Vahdati, Sahar","Huang, Linqing and Fan, Jinfu and Zhao, Wangbo and You, Yang","Ueda, Ryosuke and Takeuchi, Koh and Kashima, Hisashi","Koyyada, Shiva prasad and Singh, Thipendra P","Noor, Khondaker Tasrif and Robles-Kelly, Antonio and Kusy, Brano","Tran, Viet Hong and Nguyen, Quan Hoang and Van Nguyen, Vinh","NaN","Han, Xiaoyu and Zhu, Xiubin and Pedrycz, Witold and Li, Zhiwu","Li, Ximing and Wang, Bing and Wang, Yang and Wang, Meng","Liu, Kun-Hong and Gao, Jie and Xu, Yong and Feng, Kai-Jie and Ye, Xiao-Na and Liong, Sze-Teng and Chen, Li-Yan","Shan, Jiajun and Zhang, Zhiqiang and Zeng, Yuwei and Ying, Yuyan and Wu, Haiyan and Song, Haiyu and Chen, Yanhong and Deng, Shengchun","Li, Tianjun and Wang, Yingxu and Liu, Licheng and Chen, Long and Chen, C.L. Philip","C R, Nagarathna and M, Kusuma and K, Seemanthini","Sun, Hao and Liu, Jiaqing and Chen, Yen-Wei and Lin, Lanfen","Abdeljaber, Osama and Habite, Tadios and Olsson, Anders","Xia, Tian and Shao, Yabin and Xia, Shuyin and Xiong, Yiping and Lian, Xiaoyu and Ling, Wu","NaN","Airao, Jay and Gupta, Abhishek and Saraf, Gaurav and Nirala, Chandrakant K","Wang, Depei and Cheng, Lianglun and Wang, Zhuowei and Li, Jin","Viana, Talles B. and Souza, Victor L.F. and Oliveira, Adriano L.I. and Cruz, Rafael M.O. and Sabourin, Robert","Radhakrishnan B. L. and Ezra, Kirubakaran and Jebadurai, Immanuel Johnraja","NaN","Kocadagli, Ozan and Ozer, Ezgi and Batista, Arnaldo G.","Lu, Ting and Ding, Kexin and Fu, Wei and Li, Shutao and Guo, Anjing","NaN","Sharma, Arnav and Sharma, Subhanjali and Bhardwaj, Utkarsh and Mistry, Sajib and Deb, Novarun and Krishna, Aneesh","Bisen, Tejasvee and Javed, Mohammed and Kirtania, Shashank and Nagabhushan, P.","Fu, Rui and Wu, Yuncheng and Xu, Quanqing and Zhang, Meihui","Lughofer, Edwin and Pratama, Mahardhika","Chai, Jiali and Wu, Ruixuan and Li, Aoyu and Xue, Chen and Qiang, Yan and Zhao, Juanjuan and Zhao, Qinghua and Yang, Qianqian","NaN","NaN","Yao, Jianping and Tran, Son N. and Sawyer, Samantha and Garg, Saurabh","Cao, Fengyun and Chen, Sijing and Zhong, Jin and Gao, Yikai and Gastaldo, Paolo","Jang, Jun-Gi and Shim, Sooyeon and Egay, Vladimir and Lee, Jeeyong and Park, Jongmin and Chae, Suhyun and Kang, U","Jo, Nathanael and Aghaei, Sina and Benson, Jack and Gomez, Andres and Vayanos, Phebe","Zhou, Xinlei and Chen, Sudong and Peng, Nianjiao and Zhou, Xinpeng and Wang, Xizhao","Chen, Mingqiang and Liu, Lizhe and Chen, Xiaohao and Zhu, Siyu","He, Jiahui and Zia, Haris Bin and Castro, Ignacio and Raman, Aravindh and Sastry, Nishanth and Tyson, Gareth","NaN","Abeyrathna, K. Darshana and Abouzeid, Ahmed A. O. and Bhattarai, Bimal and Giri, Charul and Glimsdal, Sondre and Granmo, Ole-Christoffer and Jiao, Lei and Saha, Rupsa and Sharma, Jivitesh and Tunheim, Svein A. and Zhang, Xuan","Purwar, Archana and Manju, Ms.","Yamaguchi, Akihiro and Ueno, Ken and Kashima, Hisashi","Dogan, Musa and Taspinar, Yavuz Selim and Cinar, Ilkay and Kursun, Ramazan and Ozkan, Ilker Ali and Koklu, Murat","Kuljeet Singh and Shastri, Sourabh and Kumar, Sachin and Mansotra, Vibhakar","Shao, Yuan-Hai and Lv, Xiao-Jing and Huang, Ling-Wei and Bai, Lan","Nancy, V. Auxilia Osvin and Prabhavathy, P. and Arya, Meenakshi S. and Ahamed, B. Shamreen","Han, Xiangmin and Wang, Jun and Ying, Shihui and Shi, Jun and Shen, Dinggang","Kang, Zhongfeng and Nielsen, Mads and Yang, Bo and Deng, Lihui and Lorenzen, Stephan Sloth","Paul, Jackson H. and Digh, Andy D.","Sun, Siqi and Wang, Yongyu","Roy, Debaditya and Lekssays, Ahmed and Girdzijauskas, Sarunas and Carminati, Barbara and Ferrari, Elena","Zhang, Kejun and Feng, Liwen and Yu, Xinying","Manwani, Naresh and Mitra, Suman K. and Joshi, M. V.","Kienitz, Daniel and Komendantskaya, Ekaterina and Lones, Michael","Romero, Miguel and Nakano, Felipe Kenji and Finke, Jorge and Rocha, Camilo and Vens, Celine","Wang, Rui-Qi and Zhu, Fei and Zhang, Xu-Yao and Liu, Cheng-Lin","Qi, Kai and Yang, Hu","Frick, Thomas and Antognini, Diego and Rigotti, Mattia and Giurgiu, Ioana and Grewe, Benjamin and Malossi, Cristiano","Langenecker, Sven and Sturm, Christoph and Schalles, Christian Schalles and Binnig, Carsten","Mahjoubi, Soroush and Ye, Fan and Bao, Yi and Meng, Weina and Zhang, Xian","Li, Chen and Shao, Yabin and Xia, Shuyin and Wang, Cheng and Xia, Tian and Peng, Xiaoli","Yin, Xiuye and Chen, Liyong","Dou, Yifeng and Meng, Wentao","Sreelakshmi, S. and Vinod Chandra, S. S.","Del Bimbo, Davide and Gemelli, Andrea and Marinai, Simone","Kulkarni, Atharva and Masud, Sarah and Goyal, Vikram and Chakraborty, Tanmoy","Chen, Mingcai and Zhao, Yu and Wang, Zhonghuang and He, Bing and Yao, Jianhua","Barzinji, Ala Othman and Ma, Jixin and Ma, Chaoying","NaN","Chang, Yung-Chia and Chang, Kuei-Hu and Chen, Wei-Ting","Zhao, Konghao and Bhandari, Sapan and Whitener, Nathan P and Grayson, Jason M and Khuri, Natalia","Shervegar, Vishwanath Madhava and Khan, Muhammad Attique and Dhiman, Gaurav and VE, Sathishkumar","B. Subbulakshmi and C. Deisy and Parthasarathy, S.","Sliwinski, J.T. and Mandl, M. and Stoll, H.M.","NaN","Twitchell, Douglas P. and Fuller, Christie M.","Kalb, Thorsten and Kushibar, Kaisar and Cintas, Celia and Lekadir, Karim and Diaz, Oliver and Osuala, Richard","Ahmed, Abdelfatah and Velayudhan, Divya and Hassan, Taimur and Bennamoun, Mohammed and Damiani, Ernesto and Werghi, Naoufel","Li, Junnan and Zhou, MingQiang and Zhu, Qingsheng and Wu, Quanwang","NaN","Kim, Seonjun and Kim, Minjae and Lee, Youngki","Marin-Castro, Heidy and Sucar, Enrique and Morales, Eduardo","Iqbal, Sehrish and Rakovic, Mladen and Chen, Guanliang and Li, Tongguang and Ferreira Mello, Rafael and Fan, Yizhou and Fiorentino, Giuseppe and Radi Aljohani, Naif and Gasevic, Dragan","Miftahushudur, Tajul and Sahin, Halil Mertkan and Grieve, Bruce and Yin, Hujun","Shi, Yu and Xu, Ning and Yuan, Hua and Geng, Xin","Verbiest, Nele and Ramentol, Enislay and Cornelis, Chris and Herrera, Francisco","NaN","Liu, Ruiqi and Xu, Ganggang and Shang, Zuofeng","Fan, Jinfu and Jiang, Zhencun and Xian, Yuanqing and Wang, Zhongjie","Asmita, Sharma and Pranshul, Lakhanpal and Marin, Litoiu and Lauren E., Sergio and Sumona, Mukhopadhyay","Mamdouh Farghaly, Heba and Abd El-Hafeez, Tarek","Chen, Jinqian and Zhu, Jihua and Zheng, Qinghai","Gago-Pallares, Yuridia and Fontenla-Romero, Oscar and Alonso-Betanzos, Amparo","NaN","Bhope, Rahul Atul and Jayaram, K. R. and Venkatasubramanian, Nalini and Verma, Ashish and Thomas, Gegi","Yu, Xiaohan and Gao, Yongsheng and Bennamoun, Mohammed and Xiong, Shengwu","Yan, Yuan-Ting and Zhang, Yan-Ping and Zhang, Yi-Wen","Mishra, Aakriti and Ramanathan, A. and Batta, Vineet and Malathy, C. and Kundu, Soumya Snigdha and Gayathri, M. and Vathana, D. and Kamineni, Srinath","Xu, Hao and Xiao, Hui and Hao, Huazheng and Dong, Li and Qiu, Xiaojie and Peng, Chengbin","Aljabar, Paul and Heckemann, R. and Hammers, Alexander and Hajnal, Joseph V. and Rueckert, Daniel","Chen, Xu and Marazopoulou, Katerina and Lee, Wesley and Agarwal, Christine and Sukumaran, Jason and Hofleitner, Aude","NaN","Chang, Hsin-Yun and Sun, Chung-Shan","Zhou, Jing-Wen and Fu, Shao-Feng and Li, Long-Hai and Dong, Jun-Zhe","Choudhary, Nurendra and Singh, Rajat and Bindlish, Ishita and Shrivastava, Manish","Deng, Liping and Xiao, MingQing","Amgoud, Leila and Muller, Philippe and Trenquier, Henri","Gupta, Rohan Kumar and Sinha, Rohit","NaN","Cao, Chunzheng and Liu, Xin and Cao, Shuren and Shi, Jian Qing","Zhou, Wei and Dou, Peng and Su, Tao and Hu, Haifeng and Zheng, Zhijie","Liao, Yi and Ning, Kuangfeng","Huang, Linqing and Zhao, Wangbo and Liew, Alan Wee-Chung and You, Yang","Koottungal, Akash and Pandey, Shailesh and Nambiar, Athira","Sanghvi, Diya and Fernandes, Laureen Maria and D\u2019Souza, Siona and Vasaani, Naxatra and Kavitha, K. M.","Yazdinejad, Abbas and Dehghantanha, Ali and Parizi, Reza M. and Epiphaniou, Gregory","Shen, Yuhao and Li, Bo and Xu, Xinlan and Luo, Bing and Zhang, Chao and Hao, Fei","Alkhalifa, Rabab and Kochkina, Elena and Zubiaga, Arkaitz","NaN","Behera, Adarsh Prasad and Morabito, Roberto and Widmer, Joerg and Champati, Jaya Prakash","Zhang, Zhiwang and He, Jing and Cao, Jie and Li, Shuqing","Singh, Gunjan and Nagpal, Arpita and Singh, Vijendra","Gillala, Rekha and Mishra, Anand Kumar and Tyagi, Amit Kumar","Shi, Shengnan and Li, Jie and Zhu, Dan and Yang, Fang and Xu, Yong","Mohapatra, Subasish and Maneesha, Sushree and Patra, Prashanta Kumar and Mohanty, Subhadarshini","Anwar, Zeba and Masood, Sarfaraz","Xu, Zhongguo and Jha, Naresh and Mehadi, Syed and Mandal, Mrinal","Kurniadi, Felix Indra and Rohmana, Rian Cahya and Taufani, Leon","Zhuang, Yuchen and Yu, Yue and Kong, Lingkai and Chen, Xiang and Zhang, Chao","Jaiswal, Dibyanshu and Chatterjee, Debatri and B s, Mithun and Ramakrishnan, Ramesh Kumar and Pal, Arpan","Gatto, Bernardo Bentes and Colonna, Juan Gabriel and dos Santos, Eulanda Miranda and Lameiras Koerich, Alessandro and Fukui, Kazuhiro","Sabitha, P. and Meeragandhi, G.","Ghosh, Bishwamittra","Shi, Lihua and Yan, Fang and Liu, Haihong","Chahi, Abderrazak and El-merabet, Youssef and Ruichek, Yassine and Touahni, Raja","Zhang, Han and Wang, Xinyu and Liu, Junxiu and Zhang, Lei and Ji, Lixia","Biswal, Tapaswini and Parida, S.K. and Mishra, Sanhita","Kumar, Tapesh and Mahrishi, Mehul and Sharma, Girish","Wang, Yujun and Guo, Jian and Xu, Li and Li, Kexin and Li, Zongming","Li, Hao-Tian and Wei, Tong and Yang, Hao and Hu, Kun and Peng, Chong and Sun, Li-Bo and Cai, Xun-Liang and Zhang, Min-Ling","Zhu, Xiaofei and Peng, Zhanwang and Guo, Jiafeng and Dietze, Stefan","Khosa, Ikramullah and Rahman, Abdur and Ali, Khurram and Akhtar, Jahanzeb and Armghan, Ammar and Arshad, Jehangir and Amentie, Melkamu Deressa and Sun, Kehui","Janouskova, Klara and Rigotti, Mattia and Giurgiu, Ioana and Malossi, Cristiano","NaN","NaN","Han, Feiyang and Miao, Yun and Sun, Zhaoyi and Wei, Yimin","Shrestha, Avantika and Tlachac, M. L. and Flores, Ricardo and Rundensteiner, Elke A","NaN","Hou, Junlin and Xu, Jilan and Zhang, Nan and Zhang, Yuejie and Zhang, Xiaobo and Feng, Rui","Gundawar, Atharva and Lodha, Srishti and Vijayarajan, V. and Iyer, Balaji and Prasath, V. B. Surya","Ma, Guanghui and Hu, Chunming and Ge, Ling and Zhang, Hong","Prabhakar, Sunil Kumar and Lee, Seong-Whan","Liu, Chang and Yang, Chun and Qin, Hai-Bo and Zhu, Xiaobin and Liu, Cheng-Lin and Yin, Xu-Cheng","Saranya, R. B. and Kesavan, Ramesh and Nisha Devi, K.","Alyami, Sarah and Luqman, Hamzah and Hammoudeh, Mohammad","Oh, Se Won and Jeong, Hyuntae and Chung, Seungeun and Lim, Jeong Mook and Noh, Kyoung Ju","NaN","Sahu, Sushanta Kumar and Chowdhury, Ananda S.","Li, Zongxi and Li, Xianming and Xie, Haoran and Wang, Fu Lee and Leng, Mingming and Li, Qing and Tao, Xiaohui","NaN","NaN","Kim, Taeheung and Lee, Jong-Seok","Fu, Saiji and Tian, Yingjie and Tang, Jingjing and Liu, Xiaohui","Liu, Lingzhi and Qiang, Baohua and Wang, Yuanchun and Yang, Xianyi and Tian, Jubo and Zhang, Shihao","Yi, Liping and Wang, Gang and Liu, Xiaoguang and Shi, Zhuan and Yu, Han","Grina, Fares and Elouedi, Zied and Lefevre, Eric","Asghar, Eram and Ratti, Andrea and Tolio, Tullio","Qawqzeh, Yousef K and Ashraf, Mahwish","Qi, Kai and Yang, Hu and Gupta, B. B.","NaN","Guo, Yangyang and Nie, Liqiang and Cheng, Harry and Cheng, Zhiyong and Kankanhalli, Mohan and Del Bimbo, Alberto","Zhang, Lingjun and Hu, Qinghua and Duan, Jie and Wang, Xiaoxue","Ma, Xi-Ao and Jiang, Wentian and Ling, Yun and Yang, Bailin","Sun, Pengbo and Zuo, Yi and Wang, Yudi","Zeng, Zhiqiang and Wang, Xiaodong and Li, Wei and Ye, Yuandi","Alshayeji, Mohammad H. and Sindhu, Silpa ChandraBhasi and Abed, Sa'ed","NaN","Nguyen, Tan-Sy and Luong, Marie and Kaaniche, Mounir and Ngo, Long H. and Beghdadi, Azeddine","Li, Chen and Polling, Marcel and Cao, Lu and Gravendeel, Barbara and Verbeek, Fons J.","Padmaja, T. Maruthi and Dhulipalla, Narendra and Krishna, P. Radha and Bapi, Raju S. and Laha, A.","Christen, Peter and Hand, David J. and Kirielle, Nishadi","Hu, Mingzhi and Zhang, Xin and Li, Yanhua and Zhou, Xun and Luo, Jun","Wang, Lin and Gjoreski, Hristijan and Ciliberto, Mathias and Lago, Paula and Murao, Kazuya and Okita, Tsuyoshi and Roggen, Daniel","Wang, Yijin and Peng, Yali and Liu, Shigang and Ge, Bao and Li, Jun","Payntar, Nicole D.","Hiramatsu, Makoto and Wakabayashi, Kei and Harashima, Jun","Hu, Boren and Zhu, Yun and Li, Jiacheng and Tang, Siliang","Liang, Chunhui and Ma, Wenqing","Wei, Chenhao and Xiao, Lu and Yu, Tingting and Chen, Xinyu and Wang, Xiao and Wong, Sunny and Clune, Abigail","Yin, Shao-Yu and Huang, Yu and Chang, Tien-Yu and Chang, Shih-Fang and Tseng, Vincent S.","NaN","Kai, Yang and Miao, Zhang and Peng, Sun and Miaomiao, Qi","Debnath, Chandrima and Guha, Debashree and Hait, Swati Rani and Guria, Soumita and Chakraborty, Debjani","Singhal, Abhishek and Sharma, Devendra Kumar","Lee, Eunji and Kim, Sihyeon and Kim, Sundong and Jung, Soyeon and Kim, Heeja and Cha, Meeyoung","Andersen, Lasse R. and Jacobsen, Lukas J. and Campos, David","Akritidis, Leonidas and Bozanis, Panayiotis","Birchha, Vijay and Nigam, Bhawna","Jayaraman, Ashok Kumar and Murugappan, Abirami and Trueman, Tina Esther and Ananthakrishnan, Gayathri and Ghosh, Ashish","Xie, Xijiong and Li, Yanfeng and Sun, Shiliang","Hazarika, Barenya Bikash and Gupta, Deepak","Ougiaroglou, Stefanos and Filippakis, Panagiotis and Fotiadou, Georgia and Evangelidis, Georgios","Philips, James and Tabrizi, Nasseh","Besharati Moghaddam, Fatemeh and Lopez, Angel J. and Van Gheluwe, Casper and De Vuyst, Stijn and Gautama, Sidharta","Vijayan, Bineetha and Soman, Gayathri and Vivek, M. V. and Judy, M. V.","Li, Mingxiang and Xing, Huange and Wang, Tengyun and Xiao, Kaiming","Hu, Guanxiang and He, Wei and Sun, Chao and Zhu, Hailong and Li, Kangle and Jiang, Li","Sloan, Derek and Dombay, Evelin and Sabiiti, Wilber and Mtafya, Bariki and Arandelovic, Ognjen and Zachariou, Marios","Chhabra, Parth and Neerkaje, Atula Tejaswi and Agarwal, Shivam and Sawhney, Ramit and Thakkar, Megh and Nakov, Preslav and Chava, Sudheer","Trudgian, David C. and Yang, Zheng Rong","Suciu, Mihai-Alexandru and Lung, Rodica-Ioana","Zhu, Yicheng and Qiu, Yiqiao and Wu, Qingyuan and Wang, Fu Lee and Rao, Yanghui","Yu, Ying and Wang, Yinglong","Gao, Yi and Xu, Miao and Zhang, Min-Ling","Varshney, Seema and Lakshmi, C. Vasantha and Patvardhan, C.","Song, Chengyu and Cai, Fei and Wang, Mengru and Zheng, Jianming and Shao, Taihua","Pittino, Federico and Dimitrievska, Vesna and Heer, Rudolf","NaN","Sharma, Rohit and Mahanti, Gautam Kumar and Chakraborty, Chinmay and Panda, Ganapati and Rath, Adyasha","Zhang, Jiayun and Zhang, Xiyuan and Zhang, Xinyang and Hong, Dezhi and Gupta, Rajesh K. and Shang, Jingbo","NaN","Corbara, Silvia and Moreo, Alejandro and Sebastiani, Fabrizio","Luo, Hairong and Gao, Ge and Huang, Han and Ke, Ziyi and Peng, Cheng and Gu, Ming","Tariq, Muhammad Arham and Sargano, Allah Bux and Iftikhar, Muhammad Aksam and Habib, Zulfiqar","NaN","Yalcin Kuzu, Serpil","Bond, Jacob and Gupta, Siddhartha and Elvitigala, Thanura","Zhu, Xiaoyan and Li, Jiaxuan and Ren, Jingtao and Wang, Jiayin and Wang, Guangtao","Yin, Yanling and Ji, Nan and Wang, Xipeng and Shen, Weizheng and Dai, Baisheng and Kou, Shengli and Liang, Chen","Gottlieb, Noam and Werman, Michael","Li, Kunpeng and Xu, Junjie and Zhao, Huimin and Deng, Wu","NaN","Blachnik, Marcin and undefinedciegienka, Piotr and Da\u0327browski, Daniel","NaN","Li, Ping and Shang, Lin and Li, Huaxiong","NaN","Niranjan, Ranjani and Rao, Sachit","Abdoli, Mahsan and Akbari, Mohammad and Shahrabi, Jamal","Hazarika, Devamanyu and Krishnamurthy, Gangeshwar and Poria, Soujanya and Zimmermann, Roger","NaN","NaN","Trieu, Nguyen Minh and Thinh, Nguyen Truong","Zhao, Dawei and Li, Hong and Lu, Yixiang and Sun, Dong and Zhu, De and Gao, Qingwei","Dada, Emmanuel Gbenga and Oyewola, David Opeoluwa and Joseph, Stephen Bassi and Emebo, Onyeka and Oluwagbemi, Olugbenga Oluseun and Ahsan, Mominul","Rozendo, Guilherme Botazzo and Roberto, Guilherme Freire and do Nascimento, Marcelo Zanchetta and Alves Neves, Leandro and Lumini, Alessandra","Zhou, Yu Sheng","Zeng, Meng and Zhang, Zhonglin","Wang, Junwen and Gao, Yongbin and Fang, Zhijun","Wahba, Yasmen and Madhavji, Nazim and Steinbacher, John","Zaboli, Hamidreza and Rahmati, Mohammad","Mao, Jun-Xiang and Wang, Wei and Zhang, Min-Ling","Dinakaran, S. and Thangaiah, P. Ranjit Jeba","Yumang, Analyn N and Banguilan, Dave Emilson S and Veneracion, Clark Kent S","Gu, Qiangqiang and Shaikh, Nazim and Lin, Ping-chang and Jayachandran, Srinath and Porwal, Prasanna and Li, Xiao and Nie, Yao","Khater, Tarek and Tawfik, Hissam and Singh, Balbir","Manisha and Clifford, William and McLaughlin, Eugene and Stynes, Paul","Pandurangan, Shalini and Papandrea, Michela and Gelsomini, Mirko","NaN","Calzavara, Stefano and Cazzaro, Lorenzo and Pibiri, Giulio Ermanno and Prezza, Nicola","Mehak, Shakra and Leva, Maria Chiara and Kelleher, Jhon De and Guilfoyle, Michael","Lv, Jia and Song, Kaikai and Ye, Qiang and Tian, Guangjian","NaN","NaN","Luo, Kaiwen and Wang, Xiaomin and Sun, Fan","Kansal, Liza and Pandey, Anoushka and Shukla, Sanidhya Madhav and Dhaliwal, Parneeta","Berkmans, T. John and Karthick, S.","NaN","Huti, Mohamed and Lee, Tiarna and Sawyer, Elinor and King, Andrew P.","Li, Weidong and Yu, Yongbo and Meng, Fanqian and Duan, Jinlong and Zhang, Xuehai","Murata, Kengo and Ito, Seiya and Ohara, Kouzou","Yang, Yang and Xue, Zhenxia and Ma, Jun and Chang, Xia","Xiong, Jie and Yu, Li and Niu, Xi and Leng, Youfang","La Cava, William G","Xia, Jing and Li, Xiaolong and Tan, Yongbin and Zhang, Wu and Li, Dajun and Xiong, Zhengkun","Ou, Liang and Do, Thomas and Tran, Xuan-The and Leong, Daniel and Chang, Yu-Cheng and Wang, Yu-Kai and Lin, Chin-Teng","Zhao, Shiman and Chen, Wei and Wang, Tengjiao","Patil, Hemant A. and Basu, T. K.","Xing, Changda and Duan, Chaowei and Wang, Zhisheng and Wang, Meiling","Zunino, Andrea and Murray, Christopher and Blythman, Richard and Murino, Vittorio","Li, Dantong and Li, Guixin and Li, Shuang and Bang, Ashley","Wang, Tianle and Wang, Zihan and Liu, Weitang and Shang, Jingbo","Ding, Hongwei and Sun, Yu and Wang, Zhenyu and Huang, Nana and Shen, Zhidong and Cui, Xiaohui","NaN","Zou, Yuliang and Choi, Jinwoo and Wang, Qitong and Huang, Jia-Bin","Nishy Reshmi, S. and Shreelekshmi, R.","NaN","Zhang, Wei and Zhang, Pengye and Zhang, Bo and Wang, Xingxing and Wang, Dong","NaN","Duman, Ekrem","Huang, Zijian and Gao, Xin and Chen, Wenli and Cheng, Yingying and Xue, Bing and Meng, Zhihang and Zhang, Guangyao and Fu, Shiyuan","Mirzaei, Sayeh","NaN","Li, Xiaoxu and Li, Yalan and Zheng, Yixiao and Zhu, Rui and Ma, Zhanyu and Xue, Jing-Hao and Cao, Jie","Flores, Ricardo and Tlachac, M. L. and Shrestha, Avantika and Rundensteiner, Elke","Lamboni, Matieyendou","Galesso, Silvio and Bravo, Maria Alejandra and Naouar, Mehdi and Brox, Thomas","NaN","NaN","Abdulnagimov, Ansaf and Lopukhova, Ekaterina and Alektorov, Gleb and Klyavlin, Nail","Kakooei, Mohammad and Baleghi, Yasser","Mougan, Carlos and Alvarez, Jose Manuel and Ruggieri, Salvatore and Staab, Steffen","Alves Ribeiro, Victor Henrique and Santana, Roberto and Reynoso-Meza, Gilberto","Melese, Tamiru and Berhane, Tesfahun and Mohammed, Abdu and Walelgn, Assaye and Hussain, Sadiq","Koga, Tsukasa and Maruyama, Osamu","Boeschoten, Sjoerd and Catal, Cagatay and Tekinerdogan, Bedir and Lommen, Arjen and Blokland, Marco","Sulaiman, Maisarah and Azaman, Aizreena","Wu, Jing and Smith, W. A. P. and Hancock, E. R.","Diep, Tuong-Nghiem and Tran, Thien-Phuc and Ho-Ngoc, Vinh-Phat and Yang, Tuan-Anh and Do, Trong-Le","Du, Yanrui and Yan, Jing and Chen, Yan and Liu, Jing and Zhao, Sendong and She, Qiaoqiao and Wu, Hua and Wang, Haifeng and Qin, Bing","Zhao, Yifeng and Yang, Liming","NaN","NaN","de Amorim, Lucas B.V. and Cavalcanti, George D.C. and Cruz, Rafael M.O.","Baser, Preeti and Saini, Jatinderkumar R. and Kotecha, Ketan","Xu, Xinlei and Wang, Zhe and Chi, Ziqiu and Yang, Hai and Du, Wenli","Rao, Talgan Kumar and Darapaneni, Narayana and Paduri, Anwesh Reddy and S, Amarnath G and Kumar, Arun and Ps, Guruprasad","Wu, ChienHsing","Zhang, Xingxuan and He, Yue and Wang, Tan and Qi, Jiaxin and Yu, Han and Wang, Zimu and Peng, Jie and Xu, Renzhe and Shen, Zheyan and Niu, Yulei and Zhang, Hanwang and Cui, Peng","Spinnato, Francesco and Guidotti, Riccardo and Monreale, Anna and Nanni, Mirco and Pedreschi, Dino and Giannotti, Fosca","Luo, Ke and Zhang, Xiujun","Zhou, Hao and Shan, Mengyi and Qin, Lu-Ping and Cheng, Gang","Xu, Ping and Zhao, Yang and Xue, Lingyun and Liu, Yian and Yan, Ming and Zhu, Lei and Weng, Lin and Hu, Shundi and Wen, Luhong","Fu, Siming and Chu, Huanpeng and He, Xiaoxuan and Wang, Hualiang and Yang, Zhenyu and Hu, Haoji","Sun, Yanjie and Xu, Kele and Liu, Chaorun and Dou, Yong and Qian, Kun","Shifman, Danit Abukasis and Cohen, Izack and Huang, Kejun and Xian, Xiaochen and Singer, Gonen","Han, Fenggang and Zhang, Xiao and He, Linjie and Kong, Liru and Chen, Yumin","Huang, Guochang and Wang, Yunhong","Akash, Bathini Sai and Kumar, Lov and Singh, Vikram and Patel, Anoop Kumar and Krishna, Aneesh","Cao, Yungui and Chen, Jiazhen and Huang, Liqing and Huang, Tianqian and Ye, Feng","Wang, Mingda and Yang, Canqian and Xu, Yi","Jang, Yehoon and Kim, Namgi and Lee, Byoung-Dai","Liao, Guobo and Zhang, Peng and Yin, Hongpeng and Deng, Xuanhong and Li, Yanxia and Zhou, Han and Zhao, Dandan","Site, Aditi and Nurmi, Jari and Lohan, Elena Simona","Negru, Vlad-Andrei and Lemnaru, Camelia and Potolea, Rodica","Caliskan, Abdullah and O'Brien, Conor and Panduru, Krishna and Walsh, Joseph and Riordan, Daniel","Roussinov, Dmitri and Sharoff, Serge and Puchnina, Nadezhda","Han, Aiyang and Chen, Songcan","Zheng, Guolin and Li, Zuoyong and Hu, Wenkai and Fan, Haoyi and Ching, Fum Yew and Yu, Zhaochai and Chen, Kaizhi","NaN","E, Nikhil and Padhi, Anshul and Parikh, Pulkit and Kanwal, Swati and Karlapalem, Kamalakar and Raman, Natraj","NaN","Fan, Xing and Pan, Wei and Huang, Qingqing","Wang, Hewen and Yang, Renchi and Huang, Keke and Xiao, Xiaokui","Tian, Yuchen and Wang, Jiacheng and Jin, Yueming and Wang, Liansheng","NaN","Zhao, Xinqiao and Xiao, Jimin and Zhang, Bingfeng and Zhang, Quan and Waleed, Al-Nuaimy","Pan, Haiyang and Xu, Haifeng and Zheng, Jinde and Tong, Jinyu","Paul, Sneha and Patterson, Zachary and Bouguila, Nizar","Yuan, Rui and Pourmousavi, S. Ali and Soong, Wen L. and Nguyen, Giang and Liisberg, Jon A.R.","Satapathy, Santosh Kumar and Kondaveeti, Hari Kishan and Sreeja, S R and Madhani, Hiral and Rajput, Nitinsingh and Swain, Debabrata","Koo, Jaehoon and Klabjan, Diego and Utke, Jean","Ali, Zakir and Alturise, Fahad and Alkhalifah, Tamim and Khan, Yaser Daanial and Zhou, Xiaolong","Weinshall, Daphna and Zamir, Lior","Roul, Rajendra Kumar and Satyanath, Gaurav","Cordeiro, Filipe R. and Sachdeva, Ragav and Belagiannis, Vasileios and Reid, Ian and Carneiro, Gustavo","Lal, Rohit and Bolla, Bharath Kumar and Sabeesh, E","Mcdonald, Denisa Qori and Sariyanidi, Evangelos and Zampella, Casey J. and Dejardin, Ellis and Herrington, John D. and Schultz, Robert T. and Tunc, Birkan","Zhao, Deji and Ning, Bo and Song, Shuangyong and Wang, Chao and Chen, Xiangyan and Yu, Xiaoguang and Zou, Bo","Fernandes, Anita Maria da Rocha and Cassaniga, Mateus Junior and Passos, Bianka Tallita and Comunello, Eros and Stefenon, Stefano Frizzo and Leithardt, Valderi Reis Quietinho","Yolina, Hanni and Rojali and Irwansyah, Edy","Xu, Zelin and Liu, Kangjun and Chen, Ke and Ding, Changxing and Wang, Yaowei and Jia, Kui","Gunaratna, Kalpa and Srinivasan, Vijay and Jin, Hongxia","Shati, Pouya and Cohen, Eldan and McIlraith, Sheila","NaN","Baci, Alkid and Heindorf, Stefan","Wang, Juanyan and Bilgic, Mustafa","Dihingia, Leena and Bannulmath, Prashant and Chowdhury, Amartya Roy and Prasanna, S.R.M and Deepak, K.T and Sheikh, Tehreem","Kou, Yi and Lin, Guoping and Qian, Yuhua and Liao, Shujiao","NaN","Gao, Xin and Meng, Zhihang and Jia, Xin and Liu, Jing and Diao, Xinping and Xue, Bing and Huang, Zijian and Li, Kangsheng","Kuralenok, I. and Nekrest'yanov, I.","Yao, Ruihan and Zhang, Yufeng and Li, Zhiyao and Zhu, Jingying","Simsek, Furkan and Pfitzmann, Brian and Raetz, Hendrik and Otholt, Jona and Yang, Haojin and Meinel, Christoph","Liang, Qiao and Hu, Cheng and Huang, Haiyan","Zheng, Huilin and Sherazi, Syed Waseem Abbas and Arif, Saba and Lee, Myung Jin and Lee, Jong Yun","Chen, Yumin and Zhang, Xiao and Zhuang, Ying and Yao, Bingyu and Lin, Bin","Shrihari, A. and Guha, Prithwijit and Kulkarni, Rishikesh Dilip","Tonkes, Vincent and Sabatelli, Matthia","NaN","Cao, Xianghai and Li, Chenguang and Feng, Jie and Jiao, Licheng","Skitalinskaya, Gabriella and Cardiff, John","Guan, Yunchuan and Liu, Yu and Zhou, Ke and Huang, Junyuan","Tasci, Gulay and Loh, Hui Wen and Barua, Prabal Datta and Baygin, Mehmet and Tasci, Burak and Dogan, Sengul and Tuncer, Turker and Palmer, Elizabeth Emma and Tan, Ru-San and Acharya, U. Rajendra","John, Lijo and Mahanta, Hridoy Jyoti and Soujanya, Y. and Sastry, G. Narahari","Olsson, Roland and Acharya, Shubodha","Anand, Sidharth and Devulapally, Naresh Kumar and Bhattacharjee, Sreyasee Das and Yuan, Junsong","Dhakshayani, J. and Surendiran, B.","Testa, Brian and Xiao, Yi and Sharma, Harshit and Gump, Avery and Salekin, Asif","Liu, Chang and Yang, Yuwen and Xie, Zhe and Lu, Hongtao and Ding, Yue","Ren, Jinjun and Wang, Yuping and Cheung, Yiu-ming and Gao, Xiao-Zhi and Guo, Xiaofang","Liu, Zhibin and Hu, Huijun","NaN","Ma, Yanbiao and Jiao, Licheng and Liu, Fang and Yang, Shuyuan and Liu, Xu and Li, Lingling","Casanova, Arianna and Benavoli, Alessio and Zaffalon, Marco","Hasthanasombat, Apinan and Ghosh, Abhirup and Spathis, Dimitris and Mascolo, Cecilia","Luo, Ke and Lu, Linlin and Xie, Yanhua and Chen, Fang and Yin, Fang and Li, Qingting","Wang, Xu and Yang, Guangxiang and Xiao, Zhenqi and Yu, Guangling","Mora-Rubio, Alejandro and Noga, Michelle and Punithakumar, Kumaradevan","Das, Sunanda and Imtiaz, Md. Samir and Neom, Nieb Hasan and Siddique, Nazmul and Wang, Hui","Li, Yanting and Wang, Shuai and Jin, Junwei and Philip Chen, C. L.","NaN","Dahiya, Kunal and Yadav, Sachin and Sondhi, Sushant and Saini, Deepak and Mehta, Sonu and Jiao, Jian and Agarwal, Sumeet and Kar, Purushottam and Varma, Manik","Yoshida, Ruriko and Takamori, Misaki and Matsumoto, Hideyuki and Miura, Keiji","Peng, Yingqi and Wulandari and Kondo, Naoshi and Fujiura, Tateshi and Suzuki, Tetsuhito and Yoshioka, Hidetsugu and Itoyama, Erina","Anjali Rajak and Rakesh Tripathi","Wang, Jinyang and Wang, Tao and Gan, Min and Hadjichristofi, George","NaN","Deepthi and Sankar, Deepa and Thomas, Tessamma","Poellhuber, Louis-Vincent and Poellhuber, Bruno and Desmarais, Michel and Leger, Christian and Roy, Normand and Manh-Chien Vu, Mathieu","Verma, Jaiprakash Vinodkumar and Savalia, Meshwa Rameshbhai","Wang, Lewen and Zhao, Haozhe and Feng, Cunguang and Liu, Weiqing and Huang, Congrui and Santoni, Marco and Cristofaro, Manuel and Jafrancesco, Paola and Bian, Jiang","Wang, Yifei and Zhou, Yiyang and Zhu, Jihua and Liu, Xinyuan and Yan, Wenbiao and Tian, Zhiqiang","Mior, Michael J","Mingote, Victoria and Miguel, Antonio and Ortega, Alfonso and Lleida, Eduardo","Rai, Prakhar and Gehlot, Shiv and Gupta, Ritu and Gupta, Anubha","Li, Shiyang and Xia, Guoen and Zhang, Xianquan","Gupta, Palak and Varshney, Anmol and Khan, Mohammad Rafeek and Ahmed, Rafeeq and Shuaib, Mohammed and Alam, Shadab","Rodrigues, Pedro and Fonseca, Micaela and Lopes, Phil","Syam, Vipin and Safal, Shivesh and Bhutia, Ongmu and Singh, Amit Kumar and Giri, Diksha and Bhandari, Samrat Singh and Panigrahi, Ranjit","Yu, Haitao and Xiong, Feng and Chen, Zuhui","Wu, Shengli and Li, Jinlong and Ding, Weimin","NaN","Kumar, Rachit and Romano, Joseph and Ritchie, Marylyn and Moore, Jason","NaN","Zhou, Jingyue and Tian, Ye and Luo, Jian and Zhai, Qianru","Jiang, Yuxuan and Wang, Yanfeng and Zhang, Ruipeng and Xu, Qinwei and Zhang, Ya and Chen, Xin and Tian, Qi","Mishra, Sanhita and Swain, Sarat Chandra and Biswal, Tapaswini and Routray, Aurobinda","Pavlovski, Martin and Ravindran, Srinath and Gligorijevic, Djordje and Agrawal, Shubham and Stojkovic, Ivan and Segura-Nunez, Nelson and Gligorijevic, Jelena","NaN","Wang, Ruiqi and Qi, Lei and Shi, Yinghuan and Gao, Yang","Huang, Jiande and Chen, Ping and Lu, Lijuan and Deng, Yuhui and Zou, Qiang","NaN","Zhao, Zhengyun and Tian, Yichen and Yuan, Zheng and Zhao, Peng and Xia, Feng and Yu, Sheng","Amjad, Maaz and Gelbukh, Alexander and Voronkov, Ilia and Saenko, Anna","Alishahi, Mina and Gast, Daan and Vermeiren, Sam","Yang, Han and Li, Jun","Suri, Manan and Semwal, Nalin and Chaudhary, Divya and Gorton, Ian and Kumar, Bijendra","Pandey, Suraj Kumar and Nair, Shivashankar B","NaN","Fang, Xiaoqi and Zhang, Guoyun and Zhang, Guifeng and Zhou, Xuhui and Wu, Jianhui and Zhao, Lin","Mo, Gao","Li, Xintian and Culotta, Aron","Zheng, Zhichao and Sun, Huaijiang and Zhou, Ying","Alexandrov, Mikhail and Skitalinskaya, Gabriella and Cardiff, John and Koshulko, Olexiy and Shushkevich, Elena","Jeon, Younghwan and Hwang, Ganguk","Huang, Jing and Liang, Kevin J. and Kovvuri, Rama and Hassner, Tal","Raja Sekaran, Sarmela Ap and Pang, Ying Han and Ooi, Shih Yin","Andersen, Jakob Smedegaard and Zukunft, Olaf","Ukanchanakitti, Phatsakorn and Winaichatsak, Nattapong and Cho, Natthawin and Sumetpipat, Kanes","Xie, Wentao and Liu, Qian and Su, Yongye and Yan, Yi and Huang, Shujun and Kuang, Qin and Hu, Pingzhao","Lu, Menglong and Huang, Zhen and Tian, Zhiliang and Zhao, Yunxiang and Fei, Xuanyu and Li, Dongsheng","Kimura, Keigo and Bao, Jiaqi and Kudo, Mineichi and Sun, Lu","YAN, Jiameng and TIAN, Lan and WANG, Xiaoyu and LIU, Junhui and LI, Meng","Mohan, Meera and Dhanalakshmi, P. and Kumar, R. Satheesh","Waqas, Muhammad and Tahir, Muhammad Atif and Khan, Salman A.","Dash, Amanda and Albu, Alexandra Branzan","Loizidou, Kosmia and Elia, Rafaella and Pitris, Costas","Xu, Xiaofeng and Bao, Xianglin and Lu, Xingyu and Zhang, Ruiheng and Chen, Xinquan and Lu, Guifu","Merchant, Arpit and Castillo, Carlos","Vasilev, Iulii and Petrovskiy, Mikhail and Mashechkin, Igor","NaN","Dutta, Suparna and Das, Monidipa","Madasu, Avinash and Rao, Vijjini Anvesh","NaN","Davoodi, Paria and Ezoji, Mehdi and Sadeghnejad, Naser","Lv, Xian-Long and Chiang, Hsiao-Dong and Wang, Bin and Zhang, Yong-Feng","Kolukisa, Burak and Bakir-Gungor, Burcu","Kim, Sang-Woon and Duin, Robert","Wei, Zhen and Zhang, Li and Zhao, Lei","Attieh, Joseph and Tekli, Joe","Sarlas, Athanasios and Kalafatelis, Alexandros and Alexandridis, Georgios and Kourtis, Michail-Alexandros and Trakadas, Panagiotis","Stanciu, Dan-Cristian and Ionescu, Bogdan","Wang, Tingting and Bian, Yinju and Zhang, Yixiao and Hou, Xiaolin","Cai, Chengyi and Liu, Jiaxin and Yu, Wendi and Guo, Yuchen","Biggs, Mikayla and Wang, Yaohua and Soni, Neetu and Priya, Sarv and Bathla, Girish and Canahuate, Guadalupe","NaN","Chandra, B. and Gupta, Manish and Gupta, M. P.","Shaik, Janbhasha and Bhavanam, S. Nagakishore","Wibawa, Made Satria and Lo, Kwok-Wai and Young, Lawrence S. and Rajpoot, Nasir","Ramasamy, Karthikeyan and Balakrishnan, Kiruthika and Velusamy, Durgadevi","Amaldi, Edoardo and Consolo, Antonio and Manno, Andrea","Schak, Monika and Gepperth, Alexander","Balamurugan, R. and Mohite, Saurabh and Raja, S. P.","Yang, Xiaohui and Wang, Zheng and Wu, Huan and Jiao, Licheng and Xu, Yiming and Chen, Haolin","Bunke, Horst and Riesen, Kaspar","Wang, Qi and Li, Xu","Gallo, Ignazio and Binaghi, Elisabetta","Gill, Daniel and Ritov, Ya\u2019acov and Dror, Gideon","John, Vijay and Kawanishi, Yasutomo","Shi, Linlin and Yang, Peiliang and Yu, Pengfei and Lai, Canxiong and Zhou, Zhenwei and Hong, Danni","Chen, Sentao and Wang, Lei and Hong, Zijie and Yang, Xiaowei","Hendro, Hendro and Shiddiqi, Ary Mazharuddin","Lyu, Huizi and Huang, Desen and Li, Sen and Ng, Wing W.Y. and Ma, Qianli","Sadafi, Ario and Hehr, Matthias and Navab, Nassir and Marr, Carsten","Ma, Ruiyi and Han, Tian and Lei, Wenxin","Hadjadj, Lies and Amini, Massih-Reza and Louhichi, Sana","Semwal, Vijay Bhaskar and Prajapat, Yogesh Kumar and Jain, Rahul","Xi, Liang and Liang, Yujia and Huang, Xunhua and Liu, Han and Li, Ao","NaN","Li, Jingyu and Ma, Haokai and Li, Xiangxian and Qi, Zhuang and Meng, Xiangxu and Meng, Lei","Kumari, Neha and Minz, Sonajharia","NaN","Roshan, SeyedEhsan and Tanha, Jafar and Hallaji, Farzad and Ghanbari, Mohammad-reza and Fiumara, Giacomo","Pan, Wei and She, Kun and Wei, Pengyuan and Zeng, Kai","Abdar, Moloud and Salari, Soorena and Qahremani, Sina and Lam, Hak-Keung and Karray, Fakhri and Hussain, Sadiq and Khosravi, Abbas and Acharya, U. Rajendra and Makarenkov, Vladimir and Nahavandi, Saeid","Inovero, Carlo Garing and Ditablan, Gil Jeremy P. and Reyes, John Ramil C. and Tajanlangit, Renzel E.","Hamed, Ahmed and Tahoun, Mohamed and Nassar, Hamed","El Balghiti, Othman and Elmachtoub, Adam N. and Grigas, Paul and Tewari, Ambuj","NaN","Albano, Alessandro and Sciandra, Mariangela and Plaia, Antonella","Li, Xin and Xu, Qi and Li, Xingxing and Xin, Hao and Yuan, Yilong and Shen, Zhiheng and Zhou, Yuxuan","Wang, Yu and Xiong, Gang and Li, Zhen and Cui, Mingxin and Gou, Gaopeng and Hou, Chengshang","Cao, Alexander and Klabjan, Diego and Luo, Yuan","Gupta, Manali and Sharma, Sanjay Kumar and Sampada, G. C. and Uddin, Ziya","Sheikh-Zadeh, Alireza and Scott, Marc A. and Enayaty-Ahangar, Forough","Tang, Phat Loi and Le Pham, Thuy-Dung and Dinh, Tien Ba","Obregon, Josue and Jung, Jae-Yoon","Solatorio, Aivin V.","Khlifi, Ghaith and Jenhani, Ilyes and Messaoud, Montassar Ben and Mkaouer, Mohamed Wiem","Liu, Kun and Tang, Chunming and Natella, Roberto","Zhang, Dell and Taneva-Popova, Bilyana","Dong, Tengjiao","Farshidvard, A. and Hooshmand, F. and MirHassani, S.A.","Ning, Zhihan and Jiang, Zhixing and Zhang, David","Yao, Xuanrong and Wang, Xin and Liu, Yue and Zhu, Wenwu","Piao, Chunyu and Wang, Nan and Yuan, Chaofeng and Gastaldo, Paolo","Gupta, Muktesh and Wadhvani, Rajesh and Rasool, Akhtar","Su, Yulin and Chen, Boan and Feng, Ziming and Yan, Junchi","Dhariyal, Bhaskar and Le Nguyen, Thach and Ifrim, Georgiana","Hirose, Kei and Miura, Kanta and Koie, Atori","Jyoti, Kumari and Sushma, Sai and Yadav, Saurabh and Kumar, Pawan and Pachori, Ram Bilas and Mukherjee, Shaibal","Huang, Xiaoming and Zhu, Peihu and Chen, Yuwen and Ma, Jian","Wang, Chenyu and Endo, Toshio and Hirofuchi, Takahiro and Ikegami, Tsutomu","Gordon, Lucia and Behari, Nikhil and Collier, Samuel and Bondi-Kelly, Elizabeth and Killian, Jackson A. and Ressijac, Catherine and Boucher, Peter and Davies, Andrew and Tambe, Milind","de Mello, Felipe Lopes and de Oliveira, Gabriel Bianchin and Pedrini, Helio and Dias, Zanoni","Luo, Jueling and Long, Hui and Xie, Si and Zhang, Yalu and Ma, Haodong and Meng, Guangyao","Debnath, Chandrima and Aishwaryaprajna and Hait, Swati Rani and Guha, Debashree and Chakraborty, Debjani","Kianifar, Mohammad Ali and Motallebi, Hassan and Bardsiri, Vahid Khatibi","Ping, Mingtian and Pi, Dechang and Chen, Zhiwei and Wang, Junlong","Hu, Jianhua and Pan, Kejin and Song, Yan and Wei, Guoliang and Shen, Chungen","Saxena, Rohit and Arora, Deepak and Nagar, Vishal","Cai, Hongmin and Ng, Michael","Upadhyay, Rishabh and Pasi, Gabriella and Viviani, Marco","Liang, Bin and Li, Xiang and Gui, Lin and Fu, Yonghao and He, Yulan and Yang, Min and Xu, Ruifeng","Channing, Georgia and Patel, Ria and Olaya, Paula and Rorabaugh, Ariel and Miyashita, Osamu and Caino-Lores, Silvina and Schuman, Catherine and Tama, Florence and Taufer, Michela","Konduru, S. and Amiruzzaman, M. and Avina, V. and Islam, M. R.","NaN","NaN","Boratto, Tales H.A. and Cury, Alexandre A. and Goliatt, Leonardo","Gupta, Subhash Chandra and Goel, Noopur","Chen, Junming","Zidi, Salah and Mihoub, Alaeddine and Mian Qaisar, Saeed and Krichen, Moez and Abu Al-Haija, Qasem","Li, Jiazhi and Abd-Almageed, Wael","Retta, Ephrem Afele and Almekhlafi, Eiad and Sutcliffe, Richard and Mhamed, Mustafa and Ali, Haider and Feng, Jun","Masoumi, Najmeh and Khajavi, Reza","Khan, Ayesha Tooba and Khajuria, Aayushi and Mukherjee, Biswarup and Joshi, Deepak","Huo, Xiaoyang and Zeng, Xiangping and Wu, Si and Shen, Wenjun and Wong, Hau-San","Saikia, Rabul and Devi, Salam Shuleenda","Berghouse, Marc and Bebis, George and Tavakkoli, Alireza","Thome, N. and Vacavant, A.","Praschl, Christoph and Auersperg-Castell, Philipp and Forster-Heinlein, Brigitte and Zwettler, Gerald Adam","Teixeira, Ana Clara and Yazdanpanah, Hamed and Pezente, Aline and Ghassemi, Mohammad","Pourali, Hadiseh and Omranpour, Hesam","NaN","Lopez Uroz, Lorenzo and Benoit, Alexandre and Yan, Yajing and Lin-Kwong-Chon, Christophe and Giffard-Roisin, Sophie and Rabatel, Antoine","NaN","Tzudir, Moakala and Sadashiv T.N., Rishith and Agarwal, Ayush and Prasanna, S. R. Mahadeva","Caceres-Hernandez, Danilo and Gutierrez, Ricardo and Kung, Kelvin and Rodriguez, Juan and Lao, Oscar and Contreras, Kenji and Jo, Kang-Hyun and Sanchez-Galan, Javier E.","Yang, Qiujuan and Zhang, Jiaxiao","NaN","Anand, Diya and Mavromatis, Ioannis and Carnelli, Pietro and Khan, Aftab","Dipto, Imran Chowdhury and Cassidy, Bill and Kendrick, Connah and Reeves, Neil D. and Pappachan, Joseph M. and Chandrabalan, Vishnu and Yap, Moi Hoon","NaN","H. Padmanabha, Sharath and Shaikh, Fahad and Bansal, Mayank and Chatterjee, Debanjan and Singh, Preeti and Karkare, Amey and Kar, Purushottam","NaN","Ke, Ting and Liao, Yangyang and Wu, Mengyan and Ge, Xuechun and Huang, Xinyi and Zhang, Chuanlei and Li, Jianrong","Zhang, Ming-Liang and Zhang, Xu-Yao and Wang, Chuang and Liu, Cheng-Lin","Guo, Yeang and Tao, Tan and Ronglin, Ronglin and Xiao, Liangfen and Ding, Lijuan and Li, Qing and Xie, Hui","Samui, Suman and Garai, Soumen and Ghosh, Anindya and Mukhopadhyay, Anand Kumar","Karimi Monsefi, Amin and Shiri, Pouya and Mohammadshirazi, Ahmad and Karimi Monsefi, Nastaran and Davies, Ron and Moosavi, Sobhan and Ramnath, Rajiv","Uslu, Tolga and Mehler, Alexander and Baumartz, Daniel","Zhang, Dell and Sensoy, Murat and Makrehchi, Masoud and Taneva-Popova, Bilyana and Gui, Lin and He, Yulan","Du, Minghao and Zhang, Wenquan and Wang, Tao and Liu, Shuang and Ming, Dong","Duong, Huong T. and Ho, Van H. and Do, Phuc","Yang, Yang and Tu, Yanlun and Lei, Houchao and Long, Wei","Shi, Xintong and Cao, Wenzhi and Raschka, Sebastian","B., Sowkarthika and Gyanchandani, Manasi and Wadhvani, Rajesh and Shukla, Sanyam","Fanai, Hosein and Abbasimehr, Hossein","Xu, Mingjing and Baraldi, Piero and Yang, Zhe and Zio, Enrico","Bandi, Raswitha and Likhit, M. Sai Surya and Reddy, S. Rajavardhan and Bodla, Sathwik Raj and Venkat, Vempati Sai","Sun, Jiaxi and Zhang, Jiguang and Xu, Shibiao and Meng, Weiliang and Zhang, Xiaopeng","Liu, Yanchen and Lai, King Wai Chiu","Ye, Ze and Liu, Dantong and Pavani, Kaushik and Dasgupta, Sunny","Zhou, Wenjie and Li, Piji and Han, Zhaoyang and Lu, Xiaozhen and Li, Juan and Ren, Zhaochun and Liu, Zhe","Amgoud, Leila","Okkalioglu, Murat","H. Padmanabha, Sharath and Shaikh, Fahad and Bansal, Mayank and Chatterjee, Debanjan and Singh, Preeti and Karkare, Amey and Kar, Purushottam","Zhou, Zhe and Zhao, Weibin and Shang, Lin","Du, Yinan and Rui, Ting and Li, Hongwei and Yang, Chengsong and Wang, Dong","Jang, Jaeyeon and Lee, Gyeong Taek","Hikmawati, Erna and Maulidevi, Nur Ulfa and Surendro, Kridanto","Joaquim, Carlos Eduardo de Lima and Faleiros, Thiago de Paulo","NaN","Charoenkwan, Phasit and Pipattanaboon, Chonlatip and Nantasenamat, Chanin and Hasan, Md Mehedi and Moni, Mohammad Ali and Lio\u2019, Pietro and Shoombuatong, Watshara","Zou, Wang and Zhang, Wubo and Tian, Zhuofeng and Wu, Wenhuan","Hayati, Mira and Muchtar, Kahlil and Roslidar and Maulina, Novi and Syamsuddin, Irfan and Elwirehardja, Gregorius Natanael and Pardamean, Bens","NaN","Karunachandra, Bryan and Putera, Nathaniel and Wijaya, Stephen Rian and Suryani, Dewi and Wesley, Julian and Purnama, Yudy","Wang, Kang and Zhang, Ji and Zhang, Jie","Li, Guiling and Xu, Shaolin and Wang, Senzhang and Yu, Philip S.","NaN","Al-wajih, Ebrahim and Ghazali, Rozaida","Haarika, Raye and Babu, Tina and Nair, Rekha R","Zhang, Yuanpeng and Ding, Weiping","Allu, Ramakrishna and Padmanabhuni, Venkata Nageswara Rao","Raundahl, Jakob and Loog, Marco and Pettersen, Paola and Nielsen, Mads","NaN","Sun, Shaolun and Yu, Zejun and Zhang, Sen and Xiao, Wendong and Yang, Yongliang","Tiwary, Sanjeeb and Darshana, Subhashree and Mohanty, Debabrata and Dash, Adyasha and Rupsa, Potnuru and Barik, Rabindra K","Li, Liangping and Gong, Xun and Wang, Chenzhong and Kong, Weiji","NaN","Huang, Teng and Jia, Bin-Bin and Zhang, Min-Ling","Jadhav, Preet and Wanjale, Kirti and Chitre, Abhijit and Vaidya, Vedmani","Liu, Tong and Yuan, Xiaochen","Younis, Raneen and Ahmadi, Zahra and Hakmeh, Abdul and Fisichella, Marco","Upadhyaya, Apoorva and Fisichella, Marco and Nejdl, Wolfgang","NaN","Fakhrahmad, S. M. and Zare, A. and Jahromi, M. Zolghadri","Tripathi, Ashish and Misra, Anuradha and Kumar, Kuldeep and Chaurasia, Brijesh Kumar","Su, Zuanxian and Liu, Lunyang and Li, Yunqi and Chen, Houbin","Ali, Waqar and Vascon, Sebastiano and Stadelmann, Thilo and Pelillo, Marcello","Cao, Chunhong and Duan, Hongxuan and Gao, Xieping","NaN","Lai, Shenqi and Du, Xi and Guo, Jia and Zhang, Kaipeng","Kumar Panda, Deepak and Das, Saptarshi and Townley, Stuart","Qi, Haochen and Ji, Huiyan and Zhang, Jiqiang and Cheng, Liu and Kong, Xiangwei","Gehad Ismail Sayed and Aboul Ella Hassanein","Ninh, Quoc-Bao and Nguyen, Hai-Chan and Huynh, Triet and Tran, Minh-Triet and Le, Trung-Nghia","NaN","NaN","Ma, Mengdan and Xu, Yitian","Pandey, Suraj Kumar and Nair, Shivashankar B.","Liu, Na and Zhang, Fan and Chang, Liang and Duan, Fuqing","Ren, Hongjia and Guo, Qiulin","Lv, Zhi and Lin, Bo and Liang, Siyuan and Wang, Lihua and Yu, Mochen and Tang, Yao and Liang, Jiajun","NaN","Jiang, Tao and Zong, Ming and Ma, Yujun and Hou, Feng and Wang, Ruili","Zhang, Jia and Wu, Hanrui and Jiang, Min and Liu, Jinghua and Li, Shaozi and Tang, Yong and Long, Jinyi","Wu, Chao and Sang, Yu and Gao, Yakun","Angiulli, Fabrizio and Fionda, Valeria and Rombo, Simona E.","Tong, Xuming and Zhao, Zhisheng and Liang, Junhua and Ding, Lihua and Jia, Caijun and Yuan, Yanhong","Wang, Siqi and Zhou, Dongmei and Cheng, Yongjian and Jiang, Meiqi","Nayfeh, Mohammad and Li, Yuchen and Shamaileh, Khair Al and Devabhaktuni, Vijay and Kaabouch, Naima","Teynor, Alexandra and Burkhardt, Hans","NaN","Badhon, Ariful Islam Mahmud and Hasan, Md Sadman and Haque, Md. Samiul and Pranto, Md. Shafayat Hossain and Ghosh, Saurav and Alam, Md. Golam Rabiul","Tlachac, ML and Ogden, Samuel S.","Han, Peng and Chen, Zhiming and Jiang, Fei and Si, Jiaxin","NaN","Abbas, Gazy and Farooq, Umar and Singh, Parvinder and Khurana, Surinder Singh and Singh, Paramjeet","NaN","Kumar, Wahengbam Kanan and Paidimarri, Manoj and Sur, Arijit","He, Yang-Hui and Lee, Kyu-Hwan and Oliver, Thomas","NaN","NaN","NaN","St-Vincent Villeneuve, Alexandre and Plaisent, Michel","Luo, Juanjuan and Hu, Defa and Yang, Zaoli","Jung, Sang-Hack and Guo, Yanlin and Sawhney, Harpreet and Kumar, Rakesh","NaN","Jia, Jingyun and Chan, Philip K.","Sharma, Saurabh and Xian, Yongqin and Yu, Ning and Singh, Ambuj","Daniel and Cenggoro, Tjeng Wawan and Pardamean, Bens","Hoang, Ngan Dao and Tran-Anh, Dat and Luong, Manh and Tran, Cong and Pham, Cuong","Hast, Anders","Kleyko, Denis and Rachkovskij, Dmitri and Osipov, Evgeny and Rahimi, Abbas","Ghosh Dastidar, Kanishka and Siblini, Wissam and Granitzer, Michael","Fatima, Zainab and Doulani, Khushbu and Adhikari, Mainak","Francis, Sumam and Uma, Kanimozhi and Moens, Marie-Francine","Xie, Baijun and Park, Chung Hyuk","Jiang, Chenzhi and Jin, Yin and Wang, Ningtao and Wu, Ruofan and Fu, Xing and Wang, Weiqiang","Zheng, Huilin and Waqar, Malik Muhammad and Arif, Saba and Sherazi, Syed Waseem Abbas and Son, Sang Hyeok and Lee, Jong Yun","Junttila, Jukka and Raunio, Kalle and Kokkonen, Petteri and Saarela, Olli","Xia, Guoxuan and Bouganis, Christos-Savvas","Wang, Jianhui and Jie, Biao and Zhang, Xingyu and Li, Wen and Wu, Zhaoxiang and Yang, Yang","Sun, Xiaowu and Cheng, Li-Hsin and van der Geest, Rob J.","Lu, Xun and Feng, Songhe and Lyu, Gengyu and Jin, Yi and Lang, Congyan","Cui, Yuwei and Song, Qingzeng and Xue, Yongjiang and Yu, Jing","Yu, Guolin and Ma, Jun and Xie, Chenzhen","Ravi, T. and Sathish Kumar, K. and Dhanamjayulu, C. and Khan, Baseem and Pai, Ping-Feng","Salazar, Addisson and Vergara, Luis and Vidal, Enrique","Kim, Dohyung and Park, Sungho and Hwang, Sunhee and Byun, Hyeran","Arasaki, Caio and Wolschick, Lucas and Freire, Willian and Amaral, Aline","Liu, Jinjun","Liu, Li and Liu, Jian and Zhou, Qichao and Huang, De","Dahiya, Kunal and Gupta, Nilesh and Saini, Deepak and Soni, Akshay and Wang, Yajun and Dave, Kushal and Jiao, Jian and K, Gururaj and Dey, Prasenjit and Singh, Amit and Hada, Deepesh and Jain, Vidit and Paliwal, Bhawna and Mittal, Anshul and Mehta, Sonu and Ramjee, Ramachandran and Agarwal, Sumeet and Kar, Purushottam and Varma, Manik","Roul, Rajendra Kumar and Sahoo, Jajati Keshari and Satyanath, Gaurav","Runchi, Zhang and Liguo, Xue and Qin, Wang","Liu, Bo and Li, Weibin and Xiao, Yanshan and Chen, Xiaodong and Liu, Laiwang and Liu, Changdong and Wang, Kai and Sun, Peng","Sachan, Shivangi and Doulani, Khushbu and Adhikari, Mainak","Huang, Siteng and Wei, Qiyao and Wang, Donglin","Geng, Yuxia and Chen, Jiaoyan and Zhuang, Xiang and Chen, Zhuo and Pan, Jeff Z. and Li, Juan and Yuan, Zonggang and Chen, Huajun","He, Huijie and Lai, Yingxu and Wang, Yipeng and Le, Siqi and Zhao, Zijian","Fu, Xingcheng and Wei, Yuecen and Sun, Qingyun and Yuan, Haonan and Wu, Jia and Peng, Hao and Li, Jianxin","Jiao, Jiajia and Chen, Bo","Monteiro, Marcos and Britto, Alceu S. and Barddal, Jean P. and Oliveira, Luiz S. and Sabourin, Robert","Liuliakov, Aleksei and Hermes, Luca and Hammer, Barbara","Wang, Xiaofang and Qiu, Yanhua and Chen, Xin and Wu, Jialing and Zou, Qianying and Mu, Nan","Shiraishi, Hiroki and Hayamizu, Yohei and Hashiyama, Tomonori","Valero-Carreras, Daniel and Alcaraz, Javier and Landete, Mercedes","Bansal, Anam and Garg, Naresh Kumar","Yu, Ying and Duan, Junwen and Jiang, Han and Wang, Jianxin","Keles, Tugce and Yildiz, Arif Metehan and Barua, Prabal Datta and Dogan, Sengul and Baygin, Mehmet and Tuncer, Turker and Demir, Caner Feyzi and Ciaccio, Edward J. and Acharya, U. Rajendra","NaN","Dave, Saransh and Basu, Ritam and Gandhi, Vineet","Bhattacharya, Indranil and Ye, Ze and Pavani, Kaushik and Dasgupta, Sunny","Wei, Jiaheng and Zhu, Zhaowei and Luo, Tianyi and Amid, Ehsan and Kumar, Abhishek and Liu, Yang","Er-Rahmadi, Btissam and Oncevay, Arturo and Ji, Yuanyi and Pan, Jeff Z.","Wei, Xiao and Huang, Jianbao and Zhao, Rui and Yu, Hang and Xu, Zheng","Bei, Yijun and Geng, Jinsong and Liu, Erteng and Gao, Kewei and Huang, Wenqi and Feng, Zunlei","Cheng, Lei and Khalitov, Ruslan and Yu, Tong and Zhang, Jing and Yang, Zhirong","NaN","Shu, Wenhao and Yu, Jianhui and Yan, Zhenchao and Qian, Wenbin","NaN","Aryal, Saurav K. and Prioleau, Howard and Shah, Ujjawal and Acharya, Sameer","Feng, Jianzhou and Wei, Qikai and Cui, Jinman","NaN","Babu, V. Suresh and Viswanath, P.","Chung, Euisuk and Park, Kyoungchan and Kang, Pilsung","Sharma, Rahul and Singh, Amar","NaN","NaN","Huang, Dan and Zhang, Qinli and Li, Zhaowen","NaN","Khokhlova, Maria V. and Blinova, Olga V. and Bogdanova-Beglarian, Natalia and Sherstinova, Tatiana","Luo, Xincheng and Li, Daiwei and Zhang, Haiqing and Xu, Lang and Cai, Bo and Deng, Junyu","Ding, Yifei and Jia, Minping and Cao, Yudong and Ding, Peng and Zhao, Xiaoli and Lee, Chi-Guhn","Frati, Lapo and Traft, Neil and Cheney, Nick","Xiao, Ruixuan and Dong, Yiwen and Wang, Haobo and Feng, Lei and Wu, Runze and Chen, Gang and Zhao, Junbo","Al-Nima, Raid Rafi Omar and Al-Hatab, Marwa Mawfaq Mohamedsheet and Qasim, Maysaloon Abed and Mandeep, Jit S.","Bassani, Cristiano N. de O. and Saito, Prisicla T. M. and Bugatti, Pedro H.","Rachdi, E. and El khadiri, I. and El merabet, Y. and Rhazi, Y. and Meurie, C.","Nguyen, Khang Hoang and Nguyen, Huynh Vu Nhu and Tran, Hoang Ngoc and Quach, Luyl-Da","Laroca, Rayson and Zanlorensi, Luiz A. and Estevam, Valter and Minetto, Rodrigo and Menotti, David","Gupta, Kapil and Bajaj, Varun and Ansari, Irshad Ahmad","Vasilakakis, Michael D. and Iakovidis, Dimitris K.","Dholey, Moumita and Santosham, Ritesh J. M. and Ray, Soumendranath and Das, Jayanta and Chatterjee, Sanjoy and Ahmed, Rosina and Mukherjee, Jayanta","Chandna, Kshitij","Liu, Yifei and Wu, Yiquan and Zhang, Yating and Sun, Changlong and Lu, Weiming and Wu, Fei and Kuang, Kun","Mooijman, Paul and Catal, Cagatay and Tekinerdogan, Bedir and Lommen, Arjen and Blokland, Marco","Liu, Chang and Shmilovici, Armin and Last, Mark","Lee, Junho and Song, Hyeonho and Lee, Dongjoon and Kim, Sundong and Sim, Jisoo and Cha, Meeyoung and Park, Kyung-Ryul","Raj, Rishi and Mathew, Jimson and Kannath, Santhosh Kumar and Rajan, Jeny","Pei, Guanxiong and Fan, Cunhang and Li, Taihao and Jin, Jia and Wang, Rui","Wang, Yiwei and Hooi, Bryan and Liu, Yozen and Shah, Neil","Llopis-Ibor, Laura and Beltran-Royo, Cesar and Cuesta-Infante, Alfredo and Pantrigo, Juan J.","Xu, Xun and Huang, Thomas S.","Tajalli, Behrad and Abad, Gorka and Picek, Stjepan","Zhang, Qin and Shi, Zelin and Zhang, Xiaolin and Chen, Xiaojun and Fournier-Viger, Philippe and Pan, Shirui","Montalbo, Francis Jesmar P.","Szczepaniak, P. S. and Tomczyk, A. and Pryczek, M.","Zhai, Jun-Hai and Wang, Xi-Zhao and Wang, Hua-Chao","Park, Jinseong and Choi, Yujin and Byun, Junyoung and Lee, Jaewook and Park, Saerom","Ilani, Arnon and Dolev, Shlomi","Wang, Chenhan","Van Gompel, Jonas and Spina, Domenico and Develder, Chris","Lin, Yuxin and Ling, Bingo Wing-Kuen and Li, Caijun and Liao, Guozhao","Sood, Meenakshi and Jain, Shruti and Dogra, Jyotsna","NaN","Walkowiak, Tomasz","Nguyen, Anh Tien and Kwak, Jin Tae","Rashidpoor Toochaei, Masoomeh and Moeini, Farzad","Przybyszewski, Andrzej W.","Nair, Ajith N and Tanwar, Harshwardhan and Arjunan, Pandarasamy and Anand, Prashant and Mahdavi, Ardeshir","Prakash, Jainendra and Ghorai, Mrinmoy and Sanodiya, Rakesh","Zou, Jiahui and Yuan, Chaoxia and Zhang, Xinyu and Zou, Guohua and Wan, Alan T. K.","Moon, Hyung-Jun and Cho, Sung-Bae","Joshy, Amlu Anna and Rajan, Rajeev","Zhang, Zhifei and Wang, Ruizhi","J., Padmapriya and T., Sasilatha","NaN","Gupta, Ashish and Gupta, Hari Prabhat and Das, Sajal K.","Cao, Alexander and Utke, Jean and Klabjan, Diego","Rajabi, Hamid and Ding, Xianzhong and Du, Wan and Cerpa, Alberto","Goshvarpour, Atefeh and Goshvarpour, Ateke","Moore, Stephen and Bowden, Richard","Achmamad, Abdelouahad and Elfezazi, Mohamed and Chehri, Abdellah and Ahmed, Imran and Jbari, Atman and Saadane, Rachid","Liang, Huanwen and Lu, Yu","Fathan, Abderrahim and Alam, Jahangir and Zhu, Xiaolin","Guo, Xiaoyi and Tiwari, Prayag and Zou, Quan and Ding, Yijie","Mayo, Michael","Domschot, Eva and Ramyaa, Ramyaa and Smith, Michael R.","Vedula, Nikhita and Collins, Marcus and Rokhlenko, Oleg","NaN","Cao, Churan and Han, Peng and Qiu, Jian and Liu, Dongmei and Peng, Li and Luo, Kaiqing","NaN","Jitpakdeebodin, Worawit and Sinapiromsaran, Krung","Wang, Huajun and Shao, Yuanhai","Khaked, Azhar Ali and Oishi, Nobuyuki and Roggen, Daniel and Lago, Paula","Min, Jun-Ki and Cho, Sung-Bae","Hossain Molla, Jakir and Basak, Sandip Kumar and Obaidullah, Sk Md and Alam, Parveen Ahmed and Goto, Takaaki and Sen, Soumya","Pineda Arango, Sebastian and Grabocka, Josif","Heusinger, Moritz and Schleif, Frank-Michael","Nath, Vikanksh and Chattopadhyay, Chiranjoy and Desai, K.A.","Lai, Kwei-Herng and Zha, Daochen and Chen, Huiyuan and Bendre, Mangesh and Chen, Yuzhong and Das, Mashweta and Yang, Hao and Hu, Xia","Gunter, Andrew David and Wilton, Steven","Nkongolo Wa Nkongolo, Mike and Bi, Yaxin","Cui, Xiyong and Fang, Wei","Khan, Junaid and Kim, Kyungsup","Bharathi, G. and Anandharaj, G.","Athithan, Senthil and Sachi, Savya and Singh, Ajay Kumar","Shifat-E-Rabbi, Mohammad and Zhuang, Yan and Li, Shiying and Rubaiyat, Abu Hasnat Mohammad and Yin, Xuwang and Rohde, Gustavo K.","Han, Xin and Pan, Junjun and Liu, Zhimin and Zhao, Yuanzhe and Jiang, Caichao and Chen, Shiyong and Liu, Sheng and Xie, Yahong","Parmar, Jitendra and Chouhan, Satyendra Singh and Raychoudhury, Vaskar","Dwivedi, Kountay and Rajpal, Ankit and Rajpal, Sheetal and Agarwal, Manoj and Kumar, Virendra and Kumar, Naveen","Alsaif, Suleiman Ali and Erdebilli, Babek","Napier, Thomas and Ahn, Euijoon and Allen-Ankins, Slade and Lee, Ickjai","Yang, Zaoli and Zhang, Weijian and Han, Chunjia and Li, Yuchen and Yang, Mu and Ieromonachou, Petros","Che, Yongjuan and An, Yuexuan and Xue, Hui","Shi, Feng and Liu, Yong and Jiang, Tianzi and Zhou, Yuan and Zhu, Wanlin and Jiang, Jiefeng and Liu, Haihong and Liu, Zhening","Ru, Guo and Sheng, Peng and Tong, Anyang and Li, Zhenyuan","Pham, Nhat Truong and Phan, Le Thi and Dang, Duc Ngoc Minh and Manavalan, Balachandran","NaN","NaN","Zhang, Shirong and Zhou, Kuanjiu and Tian, Yuan","Liu, Boting and Guan, Weili and Yang, Changjin and Fang, Zhijie","Lv, Bailin and Wang, Sijia and Xia, Kaijian and Jiang, Yizhang","Zaidi, Tehreem Fatima and Farooq, Omar","Avudaiammal, R. and Rajangam, Vijayarajan and Durai Raji V. and Senthil Kumar S.","Piet, Julien and Nwoji, Dubem and Paxson, Vern","NaN","Wang, Dengke and Xu, Yajing and Luo, Yicheng and Qian, Qifeng and Yuan, Lv","Nemade, Varsha and Fegade, Vishal","Liu, Weike and Zhu, Cheng and Ding, Zhaoyun and Zhang, Hang and Liu, Qingbao","Zeng, Huimin and Yue, Zhenrui and Kou, Ziyi and Zhang, Yang and Shang, Lanyu and Wang, Dong","NaN","NaN","Chen, Keke and Gu, Yuechun and Sharma, Sagar","Weishampel, Anthony and Staicu, Ana-Maria and Rand, William","Wang, Haidong and He, Xuan and Li, Zhiyong and Yuan, Jin and Li, Shutao","NaN","Chen, Zhaoliang and Fu, Lele and Xiao, Shunxin and Wang, Shiping and Plant, Claudia and Guo, Wenzhong","Aseeri, Ahmad O.","Jang, Jaehee and Ha, Heoneok and Jung, Dahuin and Yoon, Sungroh","Hort, Max and Chen, Zhenpeng and Zhang, Jie M. and Harman, Mark and Sarro, Federica","Pusuluri, Aditya and Kachhi, Aastha and Patil, Hemant A.","Zhou, Hai and Xue, Zhe and Liu, Ying and Li, Boang and Du, Junping and Liang, Meiyu and Qi, Yuankai","Themeli, Chrysoula and Giannakopoulos, George and Pittaras, Nikiforos","NaN","Solares, Cristina and Sanz, Ana Maria","Mohamadi, Salman and Doretto, Gianfranco and Adjeroh, Donald A.","NaN","Wang, Qianyu and Cao, Dong and Zhang, Shuyuan and Zhou, Yuzan and Yao, Lina and Sarker, Subrata Kumar","Sridevi, S. and Karpagam, G.R. and Kumar, B. Vinoth","Deepak, S. and Ameer, P.M.","Tu, Lingling and Cai, Gaoyan and Liang, Bingji and Mao, Weining","d\u2019Aloisio, Giordano and D\u2019Angelo, Andrea and Di Marco, Antinisca and Stilo, Giovanni","Li, Ruizhe and Chen, Xin","Ahmed, Zahid and Das, Sufal","Minnoor, Manas and Baths, Veeky","Reddy, Satti R.G. and Varma, G.P. Saradhi and Davuluri, Rajya Lakshmi","Alfano, Bruno and Brunetti, Arturo and De Pietro, Giuseppe and Esposito, Amalia","Tan, Weijun and Yao, Qi and Liu, Jingfeng","Parlak, Bekir and Uysal, Alper Kursat","Shi, Jiechuan and Liu, Kun and Yuan, Hao and Wang, Can and Yang, Bo","NaN","Filipovych, Roman and Ribeiro, Eraldo","Chaganti, Rajasekhar and Ravi, Vinayakumar and Pham, Tuan D.","Ferreira, Alejandro and Curilem, Millaray and Gomez, Walter and Rios, Ricardo","Kongnim, Pongjit and Cooharojananone, Nagul and Chavarnakul, Thira","Vedula, Nikhita and Collins, Marcus and Agichtein, Eugene and Rokhlenko, Oleg","Yin, Yu Jia and Lv, Yan and Guo, Wenwen and Bai, Lan","Ji, Xuan and Peng, Shige and Yang, Shuzhen","Gil-Pita, R. and Yao, X.","Chakka, Sai Pradeep and Sinha, Neelam","Dibaji, Mahsa and Gianchandani, Neha and Nair, Akhil and Singhal, Mansi and Souza, Roberto and Bento, Mariana","Patel, Bharati and Sharaff, Aakanksha","Abdelfattah, Rabab and Zhang, Xin and Wu, Zhenyao and Wu, Xinyi and Wang, Xiaofeng and Wang, Song","Huang, Bo and Li, Peipei and Fang, Zhijun and Lei, Lv and Wang, Chenming","NaN","Luo, Xiaoyan and Li, Sen and Wang, Yu and Zhan, Tiancheng and Shi, Xiaofeng and Liu, Bo","NaN","Albarico, Jennifer P. and La Rosa, Glaiza Rein F. and Santos, Regina Anne DJ. and Tesorero, Alona Jane M. and Magboo, Ma. Sheila A. and Magboo, Vincent Peter C.","Tan, Taizhe and Zhan, Yinwei and Ding, Lei and Sheng, Sun","Jiang, Songhao and Chu, Yan and Wang, Zhengkui and Ma, Tianxing and Wang, Hanlin and Lu, Wenxuan and Zang, Tianning and Wang, Bo","Liu, Hanmo and Di, Shimin and Chen, Lei","Miao, Guijun and Wu, Yifan and Qiu, Xianbo and Li, Fang","Jia, Bin-Bin and Liu, Jun-Ying and Hang, Jun-Yi and Zhang, Min-Ling","Zhang, Yang and Zhang, Jianwu and Zhang, Guanhong and Li, Hong","Narayan, Subhashini and Sathiyamoorthy E.","Taheri, Khalil and Moradi, Hadi and Tavassolipour, Mostafa","Kim, Sungwon and Lee, Junseok and Lee, Namkyeong and Kim, Wonjoong and Choi, Seungyoon and Park, Chanyoung","Pham, Tuan D.","Echtioui, Amira and Zouch, Wassim and Ghorbel, Mohamed and Mhiri, Chokri","Aryal, Saurav K. and Prioleau, Howard and Aryal, Surakshya and Washington, Gloria","NaN","Kenesei, Tamas and Roubos, Johannes A. and Abonyi, Janos","Yu, Wei and Yang, Haiyan and Wang, Mengzhu and Wang, Xiaodong","Fattahi, Mahboubeh and Moattar, Mohammad Hossein and Forghani, Yahya","Zhang, Yun and Hua, Qinglong and Wang, Haotian and Ji, Zhenyuan and Wang, Yong","Lang, Shinan and Zhang, Jizhong and Chen, Fangyi and Cai, Yiheng and Wu, Qiang","He, Shuo and Feng, Lei and Yang, Guowu","NaN","Kumar, Pankaj and Jayaraman, V. K. and Kulkarni, B. D.","Takai, Kohichi and Hattori, Gen and Yasuda, Keiji and Heracleous, Panikos and Ishikawa, Akio and Matsumoto, Kazunori and Sugaya, Fumiaki","Zhang, Zheyu and Zhang, Tianping and Li, Jian","Niranjan, K and Shankar Kumar, S and Vedanth, S and Chitrakala, Dr. S.","Mousavi, Hamid and Loni, Mohammad and Alibeigi, Mina and Daneshtalab, Masoud","Abebe Fenta, Anduamlak and Gebeyehu, Seffi","Sheng, Haiyang and Yu, Guan","Zhao, Shulin and Sun, Xiaoting and Gai, Lingyun","NaN","Huang, Baojin and Wang, Zhongyuan and Wang, Guangcheng and Jiang, Kui and Han, Zhen and Lu, Tao and Liang, Chao","NaN","Rana, Mashud and Rahman, Ashfaqur and Almashor, Mahathir and McCulloch, John and Sethuvenkatraman, Subbu","Duan, Xueyuan and Fu, Yu and Wang, Kun","Depto, Deponker Sarker and Rizvee, Md. Mashfiq and Rahman, Aimon and Zunair, Hasib and Rahman, M. Sohel and Mahdy, M.R.C.","Kuo, Tzu-Ming and Hung, Hsuan-Yu and Wu, Chienhsing","NaN","Zou, Xin and Tang, Chang and Zheng, Xiao and Li, Zhenglai and He, Xiao and An, Shan and Liu, Xinwang","NaN","Ren, Genqiang and Chen, Yufei and Qi, Shuai and Fu, Yujie and Zhang, Qi","Cheng, Xiang and Han, Xuan and Song, Yu and Zhang, Tielin and Xu, Bo","NaN","Flores, Christopher A. and Verschae, Rodrigo","Hong, Geonkyo and Suh, Dongjun","Xu, Qinghua and Ali, Shaukat and Yue, Tao","NaN","Azawi, Nidhal","Proskura, Polina and Zaytsev, Alexey","Alreshidi, Ibrahim Mohammad and Moulitsas, Irene and Jenkins, Karl W.","Khan, Anupam and Ghosh, Soumya K.","Wang, Yongsheng and Zheng, Xuefeng"],"cluster":[0,9,0,0,8,5,5,8,0,0,5,8,5,0,5,5,5,0,0,5,0,0,9,0,0,4,5,3,0,8,5,8,0,4,7,7,0,0,0,8,5,4,4,6,4,0,5,5,6,5,5,5,8,5,2,2,0,0,7,0,1,0,0,5,5,10,0,0,0,5,4,5,1,8,5,0,5,5,0,5,5,5,0,8,0,5,5,5,3,3,7,0,5,0,6,0,5,5,0,5,0,5,0,4,0,0,5,3,5,0,3,4,6,0,0,3,0,7,6,5,9,6,5,0,0,0,5,5,5,5,1,0,4,9,4,0,8,5,8,7,5,5,0,5,0,3,5,10,4,0,5,5,5,10,0,0,5,5,0,8,7,5,8,4,5,5,7,4,0,7,5,4,5,5,5,10,7,7,0,5,1,5,5,0,0,4,0,0,5,0,0,6,7,6,6,1,5,0,1,4,5,0,0,1,7,5,0,1,5,0,7,7,8,5,10,5,5,5,8,0,8,5,0,3,5,5,1,5,5,0,5,0,0,9,6,1,5,5,5,4,10,0,7,5,5,0,5,0,0,10,5,0,0,0,5,1,0,8,0,0,5,5,5,0,3,1,4,8,8,5,8,7,5,6,0,4,0,8,1,0,0,7,5,9,3,5,0,0,4,0,4,7,0,5,0,8,3,8,5,0,9,0,2,10,5,7,7,6,9,0,0,0,4,5,4,7,0,5,5,5,2,5,0,0,0,5,0,0,4,10,5,0,0,5,0,5,5,0,0,5,5,8,0,0,5,5,0,5,5,8,7,5,2,5,5,5,0,8,5,9,7,5,3,8,5,5,8,5,0,5,5,0,0,0,6,6,5,5,1,5,0,0,5,5,0,5,5,3,0,6,7,5,5,10,5,1,10,4,5,5,5,4,8,5,4,0,0,5,5,5,5,0,8,0,5,0,9,10,4,5,7,1,6,5,0,0,0,5,0,5,0,0,7,5,5,5,5,0,0,0,5,8,10,6,4,0,0,9,0,3,0,9,5,0,0,8,0,4,0,5,5,4,8,9,5,5,6,1,7,1,5,0,9,3,0,1,3,4,0,5,5,0,4,1,5,5,4,0,0,7,0,8,5,4,1,5,4,5,0,5,4,0,0,5,5,5,5,8,5,7,7,0,9,3,5,0,4,9,5,9,0,5,4,0,0,0,5,8,0,0,1,0,0,2,0,4,7,0,0,1,5,0,5,5,0,5,5,0,0,3,6,0,4,5,4,1,0,7,10,2,9,6,0,0,5,8,5,5,4,7,6,0,0,0,1,0,0,0,4,0,5,5,7,1,5,0,1,1,0,1,5,0,0,5,5,5,0,0,5,0,0,5,8,8,0,5,4,5,10,5,5,10,4,0,5,0,5,5,5,8,4,2,6,5,10,0,8,4,5,0,5,5,7,0,10,5,0,0,5,5,0,7,0,0,5,1,7,1,1,0,0,0,5,4,5,4,0,4,5,1,0,0,5,5,4,0,9,0,4,6,5,0,4,0,0,7,0,0,5,2,0,2,0,4,0,0,5,0,0,4,7,5,5,5,5,5,7,0,5,0,0,1,0,0,0,5,5,4,5,9,0,0,0,0,0,8,0,5,8,0,5,8,5,5,2,5,8,0,0,4,5,6,10,5,5,0,5,0,1,0,8,5,5,1,6,4,8,5,4,0,0,4,7,9,3,7,0,10,9,5,6,1,1,4,5,5,0,5,10,0,0,0,1,0,5,1,5,5,7,1,2,0,4,10,0,5,4,5,4,5,5,8,4,0,0,0,9,6,5,4,8,10,0,0,7,0,5,1,5,0,5,5,4,1,5,5,8,5,3,0,1,0,0,5,0,4,3,0,8,8,0,0,0,4,8,3,0,0,0,5,2,0,4,5,0,0,0,8,5,4,0,2,5,4,0,2,0,7,4,4,1,5,0,5,5,0,0,5,5,8,5,0,5,0,0,0,5,0,0,5,3,0,5,0,5,0,5,10,7,5,5,0,8,0,0,0,5,0,5,4,8,6,4,7,0,5,8,5,4,5,0,0,5,5,0,5,8,0,0,8,6,4,0,5,0,0,5,0,5,5,0,0,10,1,7,6,5,2,0,0,7,5,0,0,0,0,8,0,5,5,10,0,3,0,1],"labels":["C-0","C-9","C-0","C-0","C-8","C-5","C-5","C-8","C-0","C-0","C-5","C-8","C-5","C-0","C-5","C-5","C-5","C-0","C-0","C-5","C-0","C-0","C-9","C-0","C-0","C-4","C-5","C-3","C-0","C-8","C-5","C-8","C-0","C-4","C-7","C-7","C-0","C-0","C-0","C-8","C-5","C-4","C-4","C-6","C-4","C-0","C-5","C-5","C-6","C-5","C-5","C-5","C-8","C-5","C-2","C-2","C-0","C-0","C-7","C-0","C-1","C-0","C-0","C-5","C-5","C-10","C-0","C-0","C-0","C-5","C-4","C-5","C-1","C-8","C-5","C-0","C-5","C-5","C-0","C-5","C-5","C-5","C-0","C-8","C-0","C-5","C-5","C-5","C-3","C-3","C-7","C-0","C-5","C-0","C-6","C-0","C-5","C-5","C-0","C-5","C-0","C-5","C-0","C-4","C-0","C-0","C-5","C-3","C-5","C-0","C-3","C-4","C-6","C-0","C-0","C-3","C-0","C-7","C-6","C-5","C-9","C-6","C-5","C-0","C-0","C-0","C-5","C-5","C-5","C-5","C-1","C-0","C-4","C-9","C-4","C-0","C-8","C-5","C-8","C-7","C-5","C-5","C-0","C-5","C-0","C-3","C-5","C-10","C-4","C-0","C-5","C-5","C-5","C-10","C-0","C-0","C-5","C-5","C-0","C-8","C-7","C-5","C-8","C-4","C-5","C-5","C-7","C-4","C-0","C-7","C-5","C-4","C-5","C-5","C-5","C-10","C-7","C-7","C-0","C-5","C-1","C-5","C-5","C-0","C-0","C-4","C-0","C-0","C-5","C-0","C-0","C-6","C-7","C-6","C-6","C-1","C-5","C-0","C-1","C-4","C-5","C-0","C-0","C-1","C-7","C-5","C-0","C-1","C-5","C-0","C-7","C-7","C-8","C-5","C-10","C-5","C-5","C-5","C-8","C-0","C-8","C-5","C-0","C-3","C-5","C-5","C-1","C-5","C-5","C-0","C-5","C-0","C-0","C-9","C-6","C-1","C-5","C-5","C-5","C-4","C-10","C-0","C-7","C-5","C-5","C-0","C-5","C-0","C-0","C-10","C-5","C-0","C-0","C-0","C-5","C-1","C-0","C-8","C-0","C-0","C-5","C-5","C-5","C-0","C-3","C-1","C-4","C-8","C-8","C-5","C-8","C-7","C-5","C-6","C-0","C-4","C-0","C-8","C-1","C-0","C-0","C-7","C-5","C-9","C-3","C-5","C-0","C-0","C-4","C-0","C-4","C-7","C-0","C-5","C-0","C-8","C-3","C-8","C-5","C-0","C-9","C-0","C-2","C-10","C-5","C-7","C-7","C-6","C-9","C-0","C-0","C-0","C-4","C-5","C-4","C-7","C-0","C-5","C-5","C-5","C-2","C-5","C-0","C-0","C-0","C-5","C-0","C-0","C-4","C-10","C-5","C-0","C-0","C-5","C-0","C-5","C-5","C-0","C-0","C-5","C-5","C-8","C-0","C-0","C-5","C-5","C-0","C-5","C-5","C-8","C-7","C-5","C-2","C-5","C-5","C-5","C-0","C-8","C-5","C-9","C-7","C-5","C-3","C-8","C-5","C-5","C-8","C-5","C-0","C-5","C-5","C-0","C-0","C-0","C-6","C-6","C-5","C-5","C-1","C-5","C-0","C-0","C-5","C-5","C-0","C-5","C-5","C-3","C-0","C-6","C-7","C-5","C-5","C-10","C-5","C-1","C-10","C-4","C-5","C-5","C-5","C-4","C-8","C-5","C-4","C-0","C-0","C-5","C-5","C-5","C-5","C-0","C-8","C-0","C-5","C-0","C-9","C-10","C-4","C-5","C-7","C-1","C-6","C-5","C-0","C-0","C-0","C-5","C-0","C-5","C-0","C-0","C-7","C-5","C-5","C-5","C-5","C-0","C-0","C-0","C-5","C-8","C-10","C-6","C-4","C-0","C-0","C-9","C-0","C-3","C-0","C-9","C-5","C-0","C-0","C-8","C-0","C-4","C-0","C-5","C-5","C-4","C-8","C-9","C-5","C-5","C-6","C-1","C-7","C-1","C-5","C-0","C-9","C-3","C-0","C-1","C-3","C-4","C-0","C-5","C-5","C-0","C-4","C-1","C-5","C-5","C-4","C-0","C-0","C-7","C-0","C-8","C-5","C-4","C-1","C-5","C-4","C-5","C-0","C-5","C-4","C-0","C-0","C-5","C-5","C-5","C-5","C-8","C-5","C-7","C-7","C-0","C-9","C-3","C-5","C-0","C-4","C-9","C-5","C-9","C-0","C-5","C-4","C-0","C-0","C-0","C-5","C-8","C-0","C-0","C-1","C-0","C-0","C-2","C-0","C-4","C-7","C-0","C-0","C-1","C-5","C-0","C-5","C-5","C-0","C-5","C-5","C-0","C-0","C-3","C-6","C-0","C-4","C-5","C-4","C-1","C-0","C-7","C-10","C-2","C-9","C-6","C-0","C-0","C-5","C-8","C-5","C-5","C-4","C-7","C-6","C-0","C-0","C-0","C-1","C-0","C-0","C-0","C-4","C-0","C-5","C-5","C-7","C-1","C-5","C-0","C-1","C-1","C-0","C-1","C-5","C-0","C-0","C-5","C-5","C-5","C-0","C-0","C-5","C-0","C-0","C-5","C-8","C-8","C-0","C-5","C-4","C-5","C-10","C-5","C-5","C-10","C-4","C-0","C-5","C-0","C-5","C-5","C-5","C-8","C-4","C-2","C-6","C-5","C-10","C-0","C-8","C-4","C-5","C-0","C-5","C-5","C-7","C-0","C-10","C-5","C-0","C-0","C-5","C-5","C-0","C-7","C-0","C-0","C-5","C-1","C-7","C-1","C-1","C-0","C-0","C-0","C-5","C-4","C-5","C-4","C-0","C-4","C-5","C-1","C-0","C-0","C-5","C-5","C-4","C-0","C-9","C-0","C-4","C-6","C-5","C-0","C-4","C-0","C-0","C-7","C-0","C-0","C-5","C-2","C-0","C-2","C-0","C-4","C-0","C-0","C-5","C-0","C-0","C-4","C-7","C-5","C-5","C-5","C-5","C-5","C-7","C-0","C-5","C-0","C-0","C-1","C-0","C-0","C-0","C-5","C-5","C-4","C-5","C-9","C-0","C-0","C-0","C-0","C-0","C-8","C-0","C-5","C-8","C-0","C-5","C-8","C-5","C-5","C-2","C-5","C-8","C-0","C-0","C-4","C-5","C-6","C-10","C-5","C-5","C-0","C-5","C-0","C-1","C-0","C-8","C-5","C-5","C-1","C-6","C-4","C-8","C-5","C-4","C-0","C-0","C-4","C-7","C-9","C-3","C-7","C-0","C-10","C-9","C-5","C-6","C-1","C-1","C-4","C-5","C-5","C-0","C-5","C-10","C-0","C-0","C-0","C-1","C-0","C-5","C-1","C-5","C-5","C-7","C-1","C-2","C-0","C-4","C-10","C-0","C-5","C-4","C-5","C-4","C-5","C-5","C-8","C-4","C-0","C-0","C-0","C-9","C-6","C-5","C-4","C-8","C-10","C-0","C-0","C-7","C-0","C-5","C-1","C-5","C-0","C-5","C-5","C-4","C-1","C-5","C-5","C-8","C-5","C-3","C-0","C-1","C-0","C-0","C-5","C-0","C-4","C-3","C-0","C-8","C-8","C-0","C-0","C-0","C-4","C-8","C-3","C-0","C-0","C-0","C-5","C-2","C-0","C-4","C-5","C-0","C-0","C-0","C-8","C-5","C-4","C-0","C-2","C-5","C-4","C-0","C-2","C-0","C-7","C-4","C-4","C-1","C-5","C-0","C-5","C-5","C-0","C-0","C-5","C-5","C-8","C-5","C-0","C-5","C-0","C-0","C-0","C-5","C-0","C-0","C-5","C-3","C-0","C-5","C-0","C-5","C-0","C-5","C-10","C-7","C-5","C-5","C-0","C-8","C-0","C-0","C-0","C-5","C-0","C-5","C-4","C-8","C-6","C-4","C-7","C-0","C-5","C-8","C-5","C-4","C-5","C-0","C-0","C-5","C-5","C-0","C-5","C-8","C-0","C-0","C-8","C-6","C-4","C-0","C-5","C-0","C-0","C-5","C-0","C-5","C-5","C-0","C-0","C-10","C-1","C-7","C-6","C-5","C-2","C-0","C-0","C-7","C-5","C-0","C-0","C-0","C-0","C-8","C-0","C-5","C-5","C-10","C-0","C-3","C-0","C-1"],"publication_date":["2023-03-14","2023-03-15","2023-10-18","2023-02-01","2023-01-01","2023-01-01","2023-01-01","2023-05-01","2023-10-21","2023-05-01","2023-03-01","2023-11-27","2023-03-01","2023-11-03","2023-09-25","2023-09-18","2023-04-30","2023-10-21","2023-01-01","2023-01-01","2023-04-30","2023-10-21","2023-02-28","2023-08-29","2023-01-01","2023-01-01","2023-02-26","2023-01-04","2023-01-01","2023-12-22","2023-02-01","2023-02-03","2023-04-01","2023-01-01","2023-03-01","2023-01-15","2023-09-13","2023-03-15","2023-11-02","2023-01-01","2023-05-01","2023-01-27","2023-01-01","2023-02-01","2023-05-01","2023-03-07","2023-11-28","2023-08-02","2023-05-30","2023-03-01","2023-01-01","2023-03-15","2023-11-27","2023-10-18","2023-01-01","2023-06-15","2023-08-29","2023-01-10","2023-03-02","2023-10-24","2023-02-01","2023-08-19","2023-09-28","2023-10-21","2023-01-01","2023-12-01","2023-04-01","2023-08-16","2023-02-01","2023-02-01","2023-11-01","2023-07-22","2023-10-08","2023-02-10","2023-03-15","2023-02-26","2023-01-01","2023-02-14","2023-04-01","2023-02-12","2023-06-20","2023-03-01","2023-09-13","2023-11-15","2023-01-01","2023-01-18","2023-01-01","2023-08-04","2023-08-19","2023-06-27","2023-10-12","2023-01-01","2023-10-04","2023-01-01","2023-01-13","2023-02-01","2023-11-22","2023-01-01","2023-10-12","2023-09-07","2023-02-01","2023-02-10","2023-10-02","2023-03-15","2023-03-13","2023-11-22","2023-08-19","2023-12-13","2023-12-13","2023-07-03","2023-09-30","2023-09-11","2023-06-04","2023-10-27","2023-03-15","2023-11-22","2023-11-27","2023-02-01","2023-03-07","2023-12-02","2023-01-25","2023-03-15","2023-08-04","2023-10-21","2023-03-15","2023-08-20","2023-02-26","2023-02-15","2023-08-19","2023-11-29","2023-07-03","2023-04-01","2023-04-01","2023-01-01","2023-05-01","2023-11-14","2023-01-18","2023-02-14","2023-11-02","2023-03-01","2023-11-22","2023-10-02","2023-01-01","2023-01-01","2023-09-28","2023-05-01","2023-01-01","2023-01-01","2023-10-26","2023-01-01","2023-08-04","2023-10-08","2023-03-01","2023-06-27","2023-08-19","2023-03-01","2023-07-11","2023-05-01","2023-01-01","2023-04-12","2023-01-18","2023-08-19","2023-03-01","2023-01-01","2023-02-12","2023-03-01","2023-05-03","2023-07-21","2023-04-24","2023-02-01","2023-02-12","2023-07-23","2023-08-19","2023-01-01","2023-02-01","2023-01-29","2023-02-21","2023-10-08","2023-12-13","2023-12-12","2023-03-01","2023-09-07","2023-11-20","2023-05-01","2023-03-07","2023-05-22","2023-10-27","2023-11-19","2023-01-01","2023-06-20","2023-01-01","2023-03-15","2023-02-25","2023-03-07","2023-10-16","2023-10-16","2023-01-25","2023-05-15","2023-10-09","2023-03-01","2023-02-14","2023-03-15","2023-10-06","2023-08-04","2023-10-08","2023-05-01","2023-08-09","2023-02-26","2023-08-19","2023-11-15","2023-01-05","2023-01-01","2023-01-24","2023-07-26","2023-12-12","2023-08-09","2023-12-01","2023-10-11","2023-06-23","2023-01-01","2023-01-01","2023-03-01","2023-01-01","2023-03-14","2023-10-17","2023-10-21","2023-01-29","2023-08-02","2023-04-15","2023-06-07","2023-07-18","2023-03-15","2023-03-10","2023-03-01","2023-03-07","2023-08-19","2023-03-01","2023-03-15","2023-02-01","2023-01-01","2023-12-11","2023-08-04","2023-03-15","2023-09-06","2023-02-12","2023-11-01","2023-01-01","2023-02-22","2023-12-01","2023-04-01","2023-02-01","2023-03-04","2023-01-01","2023-01-01","2023-11-22","2023-10-09","2023-03-03","2023-11-25","2023-12-12","2023-03-01","2023-02-26","2023-11-27","2023-02-26","2023-08-07","2023-04-01","2023-01-01","2023-11-27","2023-08-20","2023-11-17","2023-02-01","2023-03-10","2023-03-15","2023-08-19","2023-01-01","2023-12-19","2023-12-01","2023-10-02","2023-12-07","2023-01-05","2023-03-07","2023-11-21","2023-06-27","2023-05-01","2023-02-17","2023-11-27","2023-11-17","2023-09-28","2023-01-01","2023-03-01","2023-10-12","2023-01-01","2023-02-26","2023-01-14","2023-04-01","2023-07-12","2023-05-26","2023-11-28","2023-08-19","2023-03-15","2023-02-01","2023-02-18","2023-11-15","2023-10-21","2023-03-01","2023-03-01","2023-01-01","2023-01-01","2023-03-15","2023-08-04","2023-01-01","2023-01-01","2023-04-01","2023-05-10","2023-03-01","2023-02-01","2023-04-24","2023-03-08","2023-02-18","2023-04-30","2023-03-07","2023-05-09","2023-01-01","2023-08-29","2023-01-01","2023-01-01","2023-10-04","2023-03-01","2023-03-15","2023-03-15","2023-12-07","2023-08-19","2023-08-05","2023-05-01","2023-01-01","2023-01-01","2023-01-01","2023-03-15","2023-09-28","2023-04-01","2023-02-19","2023-11-13","2023-01-13","2023-01-01","2023-05-01","2023-02-26","2023-10-27","2023-03-01","2023-10-31","2023-03-15","2023-11-20","2023-02-01","2023-03-01","2023-03-01","2023-04-01","2023-10-11","2023-12-04","2023-08-21","2023-07-23","2023-02-10","2023-01-13","2023-08-19","2023-04-30","2023-01-01","2023-03-30","2023-08-04","2023-12-01","2023-03-15","2023-01-01","2023-05-01","2023-01-01","2023-01-01","2023-01-01","2023-02-01","2023-01-01","2023-03-15","2023-01-29","2023-01-01","2023-01-01","2023-10-18","2023-02-10","2023-01-01","2023-01-01","2023-03-01","2023-10-21","2023-08-19","2023-12-04","2023-10-21","2023-08-19","2023-11-29","2023-05-01","2023-02-08","2023-01-14","2023-03-15","2023-08-20","2023-08-25","2023-07-27","2023-11-16","2023-01-25","2023-12-12","2023-02-15","2023-05-01","2023-03-14","2023-02-26","2023-10-21","2023-01-25","2023-02-01","2023-01-24","2023-10-27","2023-07-26","2023-09-27","2023-02-27","2023-01-01","2023-01-01","2023-03-01","2023-10-27","2023-01-01","2023-04-24","2023-02-01","2023-11-02","2023-01-28","2023-03-01","2023-01-01","2023-12-13","2023-08-04","2023-01-01","2023-01-01","2023-12-01","2023-01-13","2023-03-15","2023-01-01","2023-03-13","2023-02-24","2023-08-04","2023-08-19","2023-07-17","2023-03-01","2023-11-11","2023-03-14","2023-01-01","2023-04-12","2023-01-01","2023-08-29","2023-09-27","2023-12-13","2023-07-24","2023-05-01","2023-01-01","2023-02-12","2023-01-01","2023-08-04","2023-03-15","2023-01-01","2023-10-23","2023-11-27","2023-01-01","2023-02-26","2023-01-01","2023-02-09","2023-03-06","2023-07-24","2023-02-11","2023-12-15","2023-06-27","2023-08-12","2023-01-01","2023-02-26","2023-01-21","2023-02-14","2023-06-20","2023-01-20","2023-12-06","2023-11-07","2023-08-19","2023-01-01","2023-03-02","2023-01-01","2023-03-15","2023-11-14","2023-02-01","2023-03-01","2023-10-21","2023-01-27","2023-01-01","2023-02-01","2023-02-26","2023-09-07","2023-01-21","2023-02-01","2023-03-01","2023-03-15","2023-04-01","2023-02-15","2023-08-29","2023-06-12","2023-01-01","2023-03-09","2023-08-27","2023-08-19","2023-03-15","2023-07-07","2023-02-12","2023-06-16","2023-03-01","2023-01-27","2023-06-26","2023-02-01","2023-03-15","2023-03-06","2023-03-15","2023-03-15","2023-06-08","2023-03-14","2023-02-01","2023-07-27","2023-02-01","2023-10-12","2023-02-15","2023-08-19","2023-07-31","2023-05-01","2023-12-13","2023-10-29","2023-01-01","2023-11-19","2023-01-01","2023-02-08","2023-02-01","2023-03-30","2023-12-01","2023-11-01","2023-03-15","2023-03-01","2023-11-14","2023-02-10","2023-01-01","2023-01-01","2023-01-01","2023-03-10","2023-01-01","2023-11-13","2023-11-19","2023-01-01","2023-08-09","2023-03-14","2023-03-01","2023-02-28","2023-02-24","2023-01-01","2023-01-10","2023-09-26","2023-01-23","2023-02-01","2023-01-01","2023-11-04","2023-11-14","2023-08-19","2023-01-24","2023-12-15","2023-10-30","2023-01-01","2023-11-20","2023-02-01","2023-01-01","2023-03-03","2023-09-04","2023-01-31","2023-09-13","2023-10-01","2023-03-15","2023-01-05","2023-04-01","2023-01-01","2023-08-09","2023-01-01","2023-02-12","2023-05-26","2023-01-11","2023-03-01","2023-01-01","2023-01-01","2023-12-01","2023-03-15","2023-02-10","2023-11-25","2023-01-01","2023-11-22","2023-12-30","2023-03-15","2023-11-29","2023-03-14","2023-07-11","2023-02-01","2023-02-13","2023-02-12","2023-10-16","2023-02-23","2023-01-01","2023-01-01","2023-01-01","2023-12-15","2023-12-12","2023-11-29","2023-02-26","2023-07-18","2023-03-15","2023-10-13","2023-04-01","2023-06-27","2023-04-15","2023-05-01","2023-03-01","2023-07-06","2023-01-03","2023-05-01","2023-10-21","2023-10-21","2023-04-01","2023-05-01","2023-02-23","2023-03-03","2023-01-01","2023-04-01","2023-06-20","2023-02-05","2023-06-01","2023-01-01","2023-01-01","2023-01-01","2023-03-09","2023-01-01","2023-05-29","2023-03-01","2023-03-15","2023-01-10","2023-01-01","2023-03-05","2023-11-01","2023-03-15","2023-07-03","2023-04-01","2023-09-28","2023-11-28","2023-02-01","2023-08-19","2023-05-30","2023-05-15","2023-08-04","2023-04-30","2023-01-05","2023-03-15","2023-06-22","2023-02-01","2023-06-07","2023-02-01","2023-01-01","2023-08-19","2023-03-15","2023-01-17","2023-12-01","2023-12-07","2023-03-01","2023-01-01","2023-02-01","2023-02-04","2023-11-25","2023-01-25","2023-02-19","2023-12-13","2023-02-04","2023-03-01","2023-07-22","2023-03-15","2023-06-18","2023-10-26","2023-03-01","2023-03-15","2023-03-01","2023-06-26","2023-04-24","2023-11-20","2023-03-15","2023-07-29","2023-08-25","2023-05-12","2023-03-01","2023-11-22","2023-03-15","2023-02-01","2023-12-06","2023-01-01","2023-03-15","2023-03-15","2023-09-26","2023-08-19","2023-01-01","2023-09-27","2023-11-14","2023-01-16","2023-03-10","2023-09-28","2023-04-30","2023-03-13","2023-10-21","2023-06-26","2023-10-17","2023-02-26","2023-06-04","2023-01-28","2023-02-20","2023-10-03","2023-03-01","2023-01-01","2023-04-01","2023-01-21","2023-09-25","2023-06-21","2023-03-01","2023-02-27","2023-01-18","2023-02-01","2023-05-01","2023-09-28","2023-06-12","2023-01-01","2023-01-01","2023-04-30","2023-12-01","2023-01-01","2023-01-01","2023-02-01","2023-07-12","2023-04-01","2023-01-01","2023-01-01","2023-03-01","2023-03-01","2023-05-12","2023-10-21","2023-08-04","2023-07-18","2023-05-01","2023-11-20","2023-01-21","2023-03-03","2023-02-01","2023-01-01","2023-10-01","2023-01-01","2023-05-11","2023-03-15","2023-02-01","2023-01-01","2023-01-28","2023-03-21","2023-03-01","2023-03-01","2023-11-29","2023-08-20","2023-02-15","2023-07-12","2023-08-19","2023-01-01","2023-01-24","2023-01-01","2023-07-13","2023-11-27","2023-03-15","2023-05-01","2023-12-12","2023-02-15","2023-07-18","2023-01-01","2023-02-14","2023-08-19","2023-03-01","2023-06-13","2023-02-27","2023-02-01","2023-03-15","2023-11-21","2023-08-19","2023-01-01","2023-03-15","2023-03-07","2023-01-01","2023-06-20","2023-09-20","2023-06-16","2023-05-10","2023-01-01","2023-03-15","2023-01-24","2023-12-01","2023-03-01","2023-03-15","2023-11-15","2023-12-12","2023-08-08","2023-11-22","2023-02-01","2023-02-08","2023-03-01","2023-03-15","2023-05-09","2023-10-09","2023-11-15","2023-03-01","2023-03-15","2023-12-14","2023-02-01","2023-11-29","2023-03-05","2023-03-15","2023-05-17","2023-07-18","2023-04-01","2023-03-06","2023-01-01","2023-06-16","2023-01-25","2023-10-08","2023-03-15","2023-01-26","2023-08-04","2023-01-24","2023-01-01","2023-10-21","2023-02-12","2023-01-01","2023-03-06","2023-04-20","2023-01-01","2023-07-29","2023-05-01","2023-08-29","2023-03-01","2023-02-01","2023-01-01","2023-11-28","2023-04-01","2023-08-19","2023-03-15","2023-01-13","2023-12-07","2023-02-05","2023-01-01","2023-03-15","2023-01-01","2023-01-01","2023-02-01","2023-12-01","2023-09-01","2023-03-15","2023-02-23","2023-01-01","2023-01-01","2023-03-15","2023-03-15","2023-03-15","2023-08-21","2023-03-01","2023-02-03","2023-01-01","2023-08-10","2023-05-17","2023-01-13","2023-11-01","2023-11-29","2023-10-27","2023-02-26","2023-02-01","2023-03-15","2023-03-11","2023-01-23","2023-01-01","2023-01-01","2023-02-01","2023-01-03","2023-03-01","2023-01-28","2023-11-21","2023-01-01","2023-01-01","2023-03-15","2023-02-12","2023-02-01","2023-12-04","2023-03-15","2023-03-15","2023-02-01","2023-09-13","2023-09-13","2023-02-27","2023-11-03","2023-04-01","2023-03-15","2023-05-12","2023-10-12","2023-05-04","2023-02-16","2023-01-01","2023-04-15","2023-02-01","2023-03-15","2023-01-01","2023-03-15","2023-08-19","2023-05-30","2023-04-04","2023-07-11","2023-01-13","2023-01-13","2023-03-01","2023-08-04","2023-03-15","2023-05-04","2023-10-01","2023-11-28","2023-03-15","2023-03-25","2023-01-10","2023-01-21","2023-01-07","2023-08-04","2023-07-03","2023-03-15","2023-02-26","2023-08-19","2023-01-01","2023-09-09","2023-08-23","2023-01-01","2023-01-01","2023-03-15","2023-03-01","2023-03-15","2023-11-28","2023-01-15","2023-01-01","2023-10-16","2023-06-14","2023-10-27","2023-11-27","2023-01-10","2023-05-22","2023-09-19","2023-11-22","2023-05-01","2023-07-22","2023-08-19","2023-03-01","2023-05-16","2023-01-12","2023-04-01","2023-02-13"],"title":["Machine Learning Applications for Urban Photovoltaic Potential Estimation: A Survey","Semantic-Aware Alignment and Label Propagation for Cross-Domain Arrhythmia Classification","Examining the Impact of FMRI Preprocessing Steps on Machine Learning-Based Classification of Autism Spectrum Disorder","An Intelligent Technique for Fault Detection and Localization of Three-Level ANPC Inverter with NP Connection for Electric Vehicles","Attention-Enabled Gated Spiking Neural P Model for Aspect-Level Sentiment Classification","Accelerate Adversarial Training with Loss Guided Propagation for Robust Image Classification","Generative Adversarial Network Based Data Augmentation and Quantum Based Convolution Neural Network for the Classification of Indian Classical Dance Forms","MSRL-Net: A Multi-Level Semantic Relation-Enhanced Learning Network for Aspect-Based Sentiment Analysis","Practical Lessons Learned From Detecting, Preventing and Mitigating Harmful Experiences on Facebook","Neural Network for Ordinal Classification of Imbalanced Data by Minimizing a Bayesian Cost","Graph Neural Networks Induced by Concept Lattices for Classification","Deep Modular Co-Attention Shifting Network for Multimodal Sentiment Analysis","A Novel Automated CNN Arrhythmia Classifier with Memory-Enhanced Artificial Hummingbird Algorithm","Fast Twin Support Vector Classification for Large Scale Problems","Meta-MMFNet: Meta-Learning-Based Multi-Model Fusion Network for Micro-Expression Recognition","Counterfactual Explanations for Remote Sensing Time Series Data: An Application to Land Cover Classification","Node-Wise Diffusion for Scalable Graph Learning","Unlocking the Potential of Non-PSD Kernel Matrices: A Polar Decomposition-Based Transformation for Improved Prediction Models","Texture Feature Dimensionality Reduction-Based Mammography Classification Using Random Forest","A Multi-Tasking Model of Speaker-Keyword Classification for Keeping Human in the Loop of Drone-Assisted Inspection","Same Same, But Different: Conditional Multi-Task Learning for Demographic-Specific Toxicity Detection","Retention is All You Need","A New Multi-Source Transfer Learning Method Based on Two-Stage Weighted Fusion","Mitigating Voter Attribute Bias for Fair Opinion Aggregation","A Multi Stage Approach to Handle Class Imbalance: An Ensemble Method","A Capsule Network for Hierarchical Multi-Label Image Classification","A Neural Network Classifier Based on Dependency Tree for English-Vietnamese Statistical Machine Translation","Semi-Supervised Hierarchical Classification Based on Local Information","A Three-Way Classification with Fuzzy Decision Trees","Graph-Based Text Classification by Contrastive Learning with Text-Level Graph Augmentation","A Novel Soft-Coded Error-Correcting Output Codes Algorithm","Syntax-Aware Transformer for Sentence Classification","Subspace-Based Minority Oversampling for Imbalance Classification","Classifying the Stages of Alzheimer's Disease by Using Multi Layer Feed Forward Neural Network","Modality-Invariant Temporal Representation Learning for Multimodal Sentiment Classification","Automatic Estimation of Annual Ring Profiles in Norway Spruce Timber Boards Using Optical Scanning and Deep Learning","GBSMOTE: A Robust Sampling Method Based on Granular-Ball Computing and SMOTE for Class Imbalance","Robust Alternating AdaBoost","Machine Learning Algorithm-Based Prediction of Machined Surface Quality in End Milling Operation","Augmenting Feature Representation with Gradient Penalty for Robust Text Categorization","A Multi-Task Approach for Contrastive Learning of Handwritten Signature Feature Representations","Feature Extraction From Single-Channel EEG Using Tsfresh and Stacked Ensemble Approach for Sleep Stage Classification","Neutrosophic-CNN-Based Image and Text Fusion for Multimodal Classification","Preictal Phase Detection on EEG Signals Using Hybridized Machine Learning Classifiers with a Novel Feature Selection Procedure Based GAs and ICOMP","Coupled Adversarial Learning for Fusion Classification of Hyperspectral and LiDAR Data","A Two-Step Anomaly Detection Based Method for PU Classification in Imbalanced Data Sets","COVID-19 Fake News Detection Using Cross-Domain Classification Techniques","DWT-CompCNN: Deep Image Classification Network for High Throughput JPEG 2000 Compressed Documents","FEAST: A Communication-Efficient Federated Feature Selection Framework for Relational Data","Evolving Multi-User Fuzzy Classifier System with Advanced Explainability and Interpretability Aspects","Classification of Mild Cognitive Impairment Based on Handwriting Dynamics and QEEG","Classification of Image Encoded SSVEP-Based EEG Signals Using Convolutional Neural Networks","Bipartite Graph Coarsening for Text Classification Using Graph Neural Networks","Machine Learning for Leaf Disease Classification: Data, Techniques and Applications","Traffic Condition Classification Model Based on Traffic-Net","Accurate Open-Set Recognition for Memory Workload","Learning Optimal Fair Decision Trees: Trade-Offs Between Interpretability, Fairness, and Accuracy","Uncertainty Guided Pruning of Classification Model Tree","GB-CosFace: Rethinking Softmax-Based Face Recognition from the Perspective of Open Set Classification","Flocking to Mastodon: Tracking the Great Twitter Migration","Application of Machine Learning Models in the Behavioral Study of Forest Fires in the Brazilian Federal District Region","Building Concise Logical Patterns by Constraining Tsetlin Machine Clause Size","Credit Card Fraud Detection Using XGBoost for Imbalanced Data Set","Time-Series Shapelets with Learnable Lengths","Dry Bean Cultivars Classification Using Deep Cnn Features and Salp Swarm Algorithm Based Extreme Learning Machine","BC-Net: Early Diagnostics of Breast Cancer Using Nested Ensemble Technique of Machine Learning","Twin SVM for Conditional Probability Estimation in Binary and Multiclass Classification","Comparative Study and Analysis on Skin Cancer Detection Using Machine Learning and Deep Learning Algorithms","ML-DSVM+: A Meta-Learning Based Deep SVM+ for Computer-Aided Diagnosis","Online Transfer Learning with Partial Feedback","Checkpoint Classifier for CNN Image Classification","A Novel Deep Learning Automatic Modulation Classifier with Fusion of Multichannel Information Using GRU","Private, Fair and Secure Collaborative Learning Framework for Human Activity Recognition","Shap-PreBiNT: A Sentiment Analysis Model Based on Optimized Transformer","Spoken Language Identification for Indian Languages Using Split and Merge EM Algorithm","Comparing Complexities of Decision Boundaries for Robust Training: A Universal Approach","Leveraging Class Hierarchy for Detecting Missing Annotations on Hierarchical Multi-Label Classification","Training with Scaled Logits to Alleviate Class-Level over-Fitting in Few-Shot Learning","LS-GNHSVM: A Novel Joint Geometrical Nonparallel Hyperplane Support Vector Machine","Active Learning for Imbalanced Civil Infrastructure Data","Steered Training Data Generation for Learned Semantic Type Detection","Identification and Classification of Exfoliated Graphene Flakes from Microscopy Images Using a Hierarchical Deep Convolutional Neural Network","An Adaptive Granular Ball Classifier Based on Natural Neighbor","Image and Text Aspect Level Multimodal Sentiment Classification Model Using Transformer and Multilayer Attention Interaction","Comparative Analysis of Weka-Based Classification Algorithms on Medical Diagnosis Datasets","Landslide Classification Using Deep Convolutional Neural Network with Synthetic Minority Oversampling Technique","Data Augmentation on Graphs for Table Type Classification","Revisiting Hate Speech Benchmarks: From Data Curation to System Deployment","A Noisy-Label-Learning Formulation for Immune Repertoire Classification and Disease-Associated Immune Receptor Sequence Identification","SCALA: Scaling Algorithm for Multi-Class Imbalanced Classification: A Novel Algorithm Specifically Designed for Multi-Class Multiple Minority Imbalanced Data Problems.","Automated Hand Joint Classification of Psoriatic Arthritis Patients Using Routinely Acquired Near Infrared Fluorescence Optical Imaging","Vehicle Leasing Credit Risk Assessment Modeling by Applying Extended Logistic Regression","An Ensemble Machine Learning Approach for Benchmarking and Selection of ScRNA-Seq Integration Methods","Heart Sound Classification Using Wavelet Scattering Transform and Support Vector Machine","A Novel Incremental Framework for Building Classifier Using Constraint Class Association Rules","Machine Learning Application to Layer Counting in Speleothems","FETCH: A Memory-Efficient Replay Approach for Continual Learning in Image Classification","Expressing Uncertainty in Information Systems Analytics Research: A Demonstration of Bayesian Analysis Applied to Binary Classification Problems","Revisiting Skin Tone Fairness in Dermatological Lesion Classification","Highly Imbalanced Baggage Threat Classification","A Framework Based on Local Cores and Synthetic Examples Generation for Self-Labeled Semi-Supervised Classification","Continuous Time Normalized Signal Trains for a Better Classification of Myoelectric Signals","A Joint Analysis of Input Resolution and Quantization Precision in Deep Learning","Automatic Image Annotation Using a Semi-Supervised Ensemble of Classifiers","Towards Automated Analysis of Rhetorical Categories in Students Essay Writings Using Bloom\u2019s Taxonomy","Enhanced SVM-SMOTE with Cluster Consistency for Imbalanced Data Classification","Unreliable Partial Label Learning with Recursive Separation","Improving SMOTE with Fuzzy Rough Prototype Selection to Detect Noise in Imbalanced Classification Data","On the Estimation of Predictive Evaluation Measure Baselines for Multi-Label Learning","Distributed Adaptive Nearest Neighbor Classifier: Algorithm and Theory","A Multi-Class Partial Hinge Loss for Partial Label Learning","MMF-DRL: Multimodal Fusion-Deep Reinforcement Learning Approach with Domain-Specific Features for Classifying Time Series Data","A High-Quality Feature Selection Method Based on Frequent and Correlated Items for Text Classification","Towards Fast and Stable Federated Learning: Confronting Heterogeneity via Knowledge Anchor","A Comparative Study of Local Classifiers Based on Clustering Techniques and One-Layer Neural Networks","Instance Selection Techniques for Large Volumes of Data","FLIPS: Federated Learning Using Intelligent Participant Selection","A Lie Algebra Representation for Efficient 2D Shape Classification","Multi-Granulation Ensemble Classification for Incomplete Data","Harnessing the Potential of Deep Learning for Total Shoulder Implant Classification: A Comparative Study","Semi-Supervised Learning with Pseudo-Negative Labels for Image Classification","Classifier Selection Strategies for Label Fusion Using Large Atlas Databases","Binary Classifier Evaluation on Unlabeled Segments Using Inverse Distance Weighting with Distance Learning","Beyond Trading Data: The Hidden Influence of Public Awareness and Interest on Cryptocurrency Volatility","A Novel Hybrid Taguchi-Grey-Based Method for Feature Subset Selection","Identification of High-Risk Areas for Geological Disasters Using Classification Methods under Complex Environmental Conditions","Contrastive Learning of Emoji-Based Representations for Resource-Poor Languages","Latent Feature Learning via Autoencoder Training for Automatic Classification Configuration Recommendation","Leveraging Argumentation for Generating Robust Sample-Based Explanations","Investigating the Effect of Data Impurity on the Detection Performances of Mental Disorders Through Spoken Dialogues","Multimodal Emotion Classification Supported in the Aggregation of Pre-Trained Classification Models","Joint Classification and Prediction of Random Curves Using Heavy\u2010tailed Process Functional Regression","Feature Learning Network with Transformer for Multi-Label Image Classification","Enhancing Classification Performance through Multi-Source Online Transfer Learning Algorithm with Oversampling","An Evidential Combination Method with Multi-Color Spaces for Remote Sensing Image Scene Classification","Semi-Supervised Classification and Segmentation of Forest Fire Using Autoencoders","Fine-Tuning of Multilingual Models for Sentiment Classification in Code-Mixed Indian Language Texts","An Optimized Fuzzy Deep Learning Model for Data Classification Based on NSGA-II","Heterogeneous-Training: A Semi-Supervised Text Classification Method","Building for Tomorrow: Assessing the Temporal Persistence of Text Classifiers","Threshold-Based Classification to Enhance Confidence in Open Set of Legal Texts","Improved Decision Module Selection for Hierarchical Inference in Resource-Constrained Edge Devices","Maximum Decentral Projection Margin Classifier for High Dimension and Low Sample Size Problems","Text Classification Using Improved IWO-HAN","An Improved Oversampling Algorithms Based on Informative Sample Selection Strategy Solving Imbalance","A Hybrid Imbalanced Classification Model Based on Data Density","Heart Diseases Prediction Based on Stacking Classifiers Model","Exploring Deep Ensemble Model for Insect and Pest Detection from Images","Multi-Class Wall Recognition in Complex Architectural Floor Plan Images Using a Convolutional Network","Local Mean Imputation for Handling Missing Value to Provide More Accurate Facies Classification","DyGen: Learning from Noisy Labels via Dynamics-Enhanced Generative Modeling","GSR Based Generic Stress Prediction System","Discriminative Singular Spectrum Classifier with Applications on Bioacoustic Signal Recognition","Optimizing the Selection of Base Learners for Multiple Classifier System in Liver Cancer Identification Using Contribution-Based Iterative Removal Algorithm","Interpretability and Fairness in Machine Learning: A Formal Methods Approach","Screening Model of Candidate Drugs for Breast Cancer Based on Ensemble Learning Algorithm and Molecular Descriptor","An Effective DeepWINet CNN Model for Off-Line Text-Independent Writer Identification","Chinese Named Entity Recognition Method for the Finance Domain Based on Enhanced Features and Pretrained Language Models","A DT-CWT and Data Mining Based Approach for High Impedance Fault Diagnosis in Micro-Grid System","Emotion Recognition in Hindi Text Using Multilingual BERT Transformer","AIS Data Driven CNN-BiGRU Model for Ship Target Classification","Stochastic Feature Averaging for Learning with Long-Tailed Noisy Labels","Generating Effective Label Description for Label-Aware Sentiment Classification","Fault-Level Grading of Photovoltaic Cells Employing Lightweight Deep Learning Models","Model-Assisted Labeling via Explainability for Visual Inspection of Civil Infrastructures","Multilabel Prototype Generation for Data Reduction in K-Nearest Neighbour Classification","CsFEVER and CTKFacts: Acquiring Czech Data for Fact Verification","T-ADAF: Adaptive Data Augmentation Framework for Image Classification Network Based on Tensor T-Product Operator","BERT Variants for Depression Screening with Typed and Transcribed Responses","In-Field Classification of the Asymptomatic Biotrophic Phase of Potato Late Blight Based on Deep Learning and Proximal Hyperspectral Imaging","Boosting COVID-19 Severity Detection with Infection-Aware Contrastive Mixup Classification","On the Performance of New Higher Order Transformation Functions for Highly Efficient Dense Layers","Multi-View Robust Graph Representation Learning for Graph Classification","Holistic Approaches to Music Genre Classification Using Efficient Transfer and Deep Learning Techniques","Towards Open-Set Text Recognition via Label-to-Prototype Learning","Extremely Randomized Tree Based Sentiment Polarity Classification on Online Product Reviews","Isolated Arabic Sign Language Recognition Using A Transformer-Based Model and Landmark Keypoints","Multimodal Sensor Data Fusion and Ensemble Modeling for Human Locomotion Activity Recognition","Enhancing the Performance of SVM on Skewed Data Sets by Exciting Support Vectors","Multi-Modal Multi-Class Parkinson Disease Classification Using CNN and Decision Level Fusion","A Novel Dropout Mechanism with Label Extension Schema toward Text Emotion Classification","Surrogate Deep Learning to Estimate Uncertainties for Driver Intention Recognition","TwiSP: A Framework for Exploring Polarized Issues in Twitter","Maximizing AUC to Learn Weighted Naive Bayes for Imbalanced Data Classification","Cost-Sensitive Learning with Modified Stein Loss Function","Object Detection Algorithm Based on Coordinate Attention and Context Feature Enhancement","FedGH: Heterogeneous Federated Learning with Generalized Global Header","Evidential Generative Adversarial Networks for Handling Imbalanced Learning","An Automated Approach to Reuse Machining Knowledge through 3D \u2013 CNN Based Classification of Voxelized Geometric Features","A Fraud Detection System Using Decision Trees Classification in An Online Transactions","Capped Asymmetric Elastic Net Support Vector Machine for Robust Binary Classification","Mixed Data Object Selection Based on Clustering and Border Objects","On Modality Bias Recognition and Reduction","Multi-Label Feature Selection with Fuzzy Rough Sets","Multi-Label Feature Selection via Maximum Dynamic Correlation Change and Minimum Label Redundancy","Classification Model for NAVTEX Navigational Warning Messages Based on Adaptive Weighted TF-IDF","Two-Stage Natural Scene Image Classification with Noise Discovering and Label-Correlation Mining","Viral Genome Prediction from Raw Human DNA Sequence Samples by Combining Natural Language Processing and Machine Learning Techniques","Performance Exploration of RNN Variants for Recognizing Daily Life Stress Levels by Using Multimodal Physiological Signals","A Novel Multi-Branch Wavelet Neural Network for Sparse Representation Based Object Classification","Analysis of Automatic Image Classification Methods for Urticaceae Pollen Classification","An Unbalanced Data Classification Model Using Hybrid Sampling Technique for Fraud Detection","A Review of the F-Measure: Its History, Properties, Criticism, and Alternatives","ST-IFGSM: Enhancing Robustness of Human Mobility Signature Identification Model via Spatial-Temporal Iterative FGSM","Summary of SHL Challenge 2023: Recognizing Locomotion and Transportation Mode from GPS and Motion Sensors","A Two-Phase Projective Dictionary Pair Learning-Based Classification Scheme for Positive and Unlabeled Learning","A Multi-Temporal Analysis of Archaeological Site Destruction Using Landsat Satellite Data and Machine Learning, Moche Valley, Peru","Named Entity Recognition by Character-Based Word Classification Using a Domain Specific Dictionary","SmartBERT: A Promotion of Dynamic Early Exiting Mechanism for Accelerating BERT Inference","Heterogeneous Analysis for Clustered Data Using Grouped Finite Mixture Models","Automatically Tagging the \u201cAAA\u201d Pattern in Unit Test Cases Using Machine Learning Models","Continual Learning with Attentive Recurrent Neural Networks for Temporal Data Classification","Short Texts Representations for Legal Domain Classification","Automatic Classification of TEDS Monitoring Operation Technology Research","Multi-Criteria Decision-Making Based Classifier Ensemble by Using Prioritized Aggregation Operator","Low Resource Language Analysis Using Deep Learning Algorithm for Gender Classification","Explainable Product Classification for Customs","Compressed, Real-Time Voice Activity Detection with Open Source Implementation for Small Devices","Low-Dimensional Text Representations for Sentiment Analysis NLP Tasks","Performance Analysis of Averaged Perceptron Machine Learning Classifier for Breast Cancer Detection","Imbalanced Aspect Categorization Using Bidirectional Encoder Representation from Transformers","Deep Multi-View Multiclass Twin Support Vector Machines","Affinity Based Fuzzy Kernel Ridge Regression Classifier for Binary Class Imbalance Learning","Data Reduction via Multi-Label Prototype Generation","Bibliographic Reference Classification in Historiographic Documents Using Supervised Machine Learning and Grammatical Features","Data-Driven Operator Functional State Classification in Smart Manufacturing","A Distributed Ensemble Machine Learning Technique for Emotion Classification from Vocal Cues","A Robust Joint-Training Graph Neural Networks Model for Event Detection with Noisy Labels","Hierarchical Belief Rule-Based Model for Imbalanced Multi-Classification","Estimating Phenotypic Characteristics of Tuberculosis Bacteria","Learning Through Interpolative Augmentation of Dynamic Curvature Spaces","A Sparse Bayesian Position Weighted Bio-Kernel Network","A Game Theoretic Flavoured Decision Tree for Classification","Topic Driven Adaptive Network for Cross-Domain Sentiment Classification","Feature Selection for Multi-Label Learning Using Mutual Information and GA","Unbiased Risk Estimator to Multi-Labeled Complementary Label Learning","Madhubani Art Classification Using Transfer Learning with Deep Feature Fusion and Decision Fusion Based Techniques","TaxonPrompt: Taxonomy-Aware Curriculum Prompt Learning for Few-Shot Event Classification","Hierarchical Concept Bottleneck Models for Vision and Their Application to Explainable Fine Classification and Tracking","Alzheimer\u2019s Disease Classification Based on Graph Kernel SVMs Constructed with 3D Texture Features Extracted from MR Images","An IoT and Deep Learning-Based Smart Healthcare Framework for Thyroid Cancer Detection","Navigating Alignment for Non-Identical Client Class Sets: A Label Name-Anchored Federated Learning Framework","Edge Detection in Ventriculograms Using Support Vector Machine Classifiers and Deformable Models","Same or Different? Diff-Vectors for Authorship Analysis","A Geometric-Relational Deep Learning Framework for BIM Object Classification","Comparing Different Oversampling Methods in Predicting Multi-Class Educational Datasets Using Machine Learning Techniques","Uncertainty-Driven Ensembles of Multi-Scale Deep Architectures for Image Classification","Random Forest Based Multiclass Classification Approach for Highly Skewed Particle Data","Probabilistic Local Equivalence Certification for Robustness Evaluation","Dynamic Ensemble Learning for Multi-Label Classification","An Investigation of Fusion Strategies for Boosting Pig Cough Sound Recognition","DecisioNet: A Binary-Tree Structured Neural Network","A Clustered Borderline Synthetic Minority Over-Sampling Technique for Balancing Quick Access Recorder Data","Automatic Gender and Unilateral Load State Recognition for Biometric Purposes","Preliminary Study on Unexploded Ordnance Classification in Underwater Environment Based on the Raw Magnetometry Data","Interpreting Sign Language Recognition Using Transformers and MediaPipe Landmarks","A Method to Reduce Boundary Regions in Three-Way Decision Theory","FinBERT-FOMC: Fine-Tuned FinBERT Model with Sentiment Focus Method for Enhancing Sentiment Analysis of FOMC Minutes","Handling Small Disjuncts and Class Skew Using Sequential Ellipsoidal Partitioning","Bagging Supervised Autoencoder Classifier for Credit Scoring","Multi-Task Learning for Detecting Stance in Tweets","Detection of Covid-19 in Chest X-Ray Images Using Percolation Features and Hermite Polynomial Classification","On the Use of Dependencies in Relation Classification of Text with Deep Learning","A Study on An Automatic Self-Training Model for Mango Segmentation of Sorting System","Multi-Label Weak-Label Learning via Semantic Reconstruction and Label Correlations","Facial Emotion Recognition and Classification Using the Convolutional Neural Network-10 (CNN-10)","Weeds Classification with Deep Learning: An Investigation Using CNN, Vision Transformers, Pyramid Vision Transformers, and Ensemble Strategy","Fusion Local and Global Aspect-Based Sentiment Analysis","Text Sentiment Classification Model Based on Fusion of DualChannel Features of CNN and BiLSTM","An Angular Shrinkage BERT Model for Few-Shot Relation Extraction with None-of-the-above Detection","A Comparison of SVM Against Pre-Trained Language Models (PLMs) for Text Classification Tasks","Shape Representation and Classification Using Boundary Radius Function","Label Specific Multi-Semantics Metric Learning for Multi-Label Classification: Global Consideration Helps","Effective Hybrid Feature Subset Selection for Multilevel Datasets Using Decision Tree Classifiers","Fruit Calories Estimation Using Convolutional Neural Network","Cellular Features Based Interpretable Network for Classifying Cell-Of-Origin from Whole Slide Images for Diffuse Large B-Cell Lymphoma Patients","Machine Learning for the Classification of Obesity Levels Based on Lifestyle Factors","A Deep Learning Emotion Classification Framework for Low Resource Languages","Fine-Grained Human Activity Recognition - A New Paradigm","On the Choice of Kernel and Labelled Data in Semi-Supervised Learning Methods","Verifiable Learning for Robust Tree Ensembles","Action Classification in Human Robot Interaction Cells in Manufacturing: Moving Towards Mutual Performance Monitoring Capacity","Semi-Supervised Node Classification via Fine-Grained Graph Auxiliary Augmentation Learning","Kurcuma: A Kitchen Utensil Recognition Collection for Unsupervised Domain Adaptation","Presumably Correct Undersampling","An Ensemble Pneumonia Prediction and Classification Model Including InceptionNeXtPneumonia Prediction and Classification","Review of Machine Learning Techniques for Crop Recommendation","A Widespread Survey on Machine Learning Techniques and User Substantiation Methods for Credit Card Fraud Detection","Feature Augmentation Based on Manifold Ranking and LSTM for Image Classification\u25aa","An Investigation into Race Bias in Random Forest Models Based on Breast DCE-MRI Derived Radiomics Features","A Image Fusion and U-Net Approach to Improving Crop Planting Structure Multi-Category Classification in Irrigated Area","Learning and Transforming General Representations to Break Down Stability-Plasticity Dilemma","Robust Projection Twin Extreme Learning Machines with Capped L 1-Norm Distance Metric","XRR: Extreme Multi-Label Text Classification with Candidate Retrieving and Deep Ranking","Optimizing Fairness Tradeoffs in Machine Learning with Multiobjective Meta-Models","Event Detection via Context Understanding Based on Multi-Task Learning","Improving CCA Algorithms on SSVEP Classification with Reinforcement Learning Based Temporal Filtering","Learning Few-Shot Sample-Set Operations for Noisy Multi-Label Aspect Category Detection","Cepstral Domain Teager Energy for Identifying Perceptually Similar Languages","Binary Feature Learning with Local Spectral Context-Aware Attention for Classification of Hyperspectral Images","Which Expert Knows Best? Modulating Soft Learning with Online Batch Confidence for Domain Adaptive Person Re-Identification","Classification Prediction of Lung Cancer Based on Machine Learning Method","WOT-Class: Weakly Supervised Open-World Text Classification","RGAN-EL: A GAN and Ensemble Learning-Based Hybrid Approach for Imbalanced Data Classification","CARES: A Corpus for Classification of Spanish Radiological Reports","Learning Representational Invariances for Data-Efficient Action Recognition","Textual Entailment Classification Using Syntactic Structures and Semantic Relations","Filter Methods for Feature Selection \u2013 A Comparative Study","A Collaborative Transfer Learning Framework for Cross-Domain Recommendation","Inverse Free Reduced Universum Twin Support Vector Machine for Imbalanced Data Classification","Classification of Hundreds of Classes: A Case Study in a Bank Internal Control Department","An Imbalanced Binary Classification Method via Space Mapping Using Normalizing Flows with Class Discrepancy Constraints","Hyperspectral Image Classification Using K-Plane Clustering and Kernel Principal Component Analysis","Complexity Measures and Features for Times Series Classification","ReNAP: Relation Network with Adaptiveprototypical Learning for Few-Shot Classification","Temporal Facial Features for Depression Screening","On Exact Distribution for Multivariate Weighted Distributions and Classification","Probing Contextual Diversity for Dense Out-of-Distribution Detection","Cross-Lingual Search for e-Commerce Based on Query Translatability and Mixed-Domain Fine-Tuning","A Deep Interpretation of Classifier Chains","Intellectual Lidar-Based Object Classification for V2V Communication Technology Implementation","Fusion of Vertical and Oblique Images Using Intra-Cluster-Classification for Building Damage Assessment","Fairness Implications of Encoding Protected Categorical Attributes","Random Vector Functional Link Forests and Extreme Learning Forests Applied to UAV Automatic Target Recognition","Credit-Risk Prediction Model Using Hybrid Deep\u2014Machine-Learning Based Algorithms","CBOEP: Generating Negative Enhancer-Promoter Interactions to Train Classifiers","The Automation of the Development of Classification Models and Improvement of Model Quality Using Feature Engineering Techniques","Machine Learning Classification of Non-Specifically Trained Muscle between Endurance and Power Athletes","Weighted Principal Geodesic Analysis for Facial Gender Classification","Deep Learning Hierarchical Methods for Insect Pest Recognition on Plants","Less Learn Shortcut: Analyzing and Mitigating Learning of Spurious Feature-Label Correlation","A Multi-Metric Small Sphere Large Margin Method for Classification","On the Optimal Binary Classifier with an Application","A Predictive Approach for Enhancing Outcomes Performance in SAW Process","The Choice of Scaling Technique Matters for Classification Performance","TomConv: An Improved CNN Model for Diagnosis of Diseases in Tomato Plant Leaves","Complementary Features Based Prototype Self-Updating for Few-Shot Learning","Insider Threat Detection: Using Classification Models","Effect of Inconsistency Rate of Granulated Datasets on Classification Performance: An Experimental Approach","NICO Challenge: Out-of-Distribution Generalization for Image Recognition Challenges","Understanding Any Time Series Classifier with a Subsequence-Based Explainer","Research on the Construction of an Accurate Procurement System for Library E-Resources in Foreign Language Under Dig Data Analysis","Reliable Prediction of Cannabinoid Receptor 2 Ligand by Machine Learning Based on Combined Fingerprints","Fentanyl Analogs Classification via Siamese Network and Mass Spectral Library Searching","Meta-Prototype Decoupled Training for Long-Tailed Learning","Automatic Audio Augmentation for Requests Sub-Challenge","An Adaptive Machine Learning Algorithm for the Resource-Constrained Classification Problem","Multimodal Fuzzy Granular Representation and Classification","Gender Classification Based on Fusion of Multi-View Gait Sequences","Empirical Analysis of Multi-Label Classification on GitterCom Using BERT and ML Classifiers","Three-Classification Face Manipulation Detection Using Attention-Based Feature Decomposition","Hierarchical Multiple Proxy Loss for Deep Metric Learning\u25aa","Traffic Classification Using Distributions of Latent Space in Software-Defined Networks: An Experimental Evaluation","A Novel Semi-Supervised Classification Approach for Evolving Data Streams","Classification of Freezing of Gait Using Accelerometer Data: A Systematic Performance Evaluation Approach","Multitask, Cross-Lingual Recipe Classification Using Joint Fine-Tuning Mechanisms","Condition Monitoring of an Autonomous Electric Drive Train by Using Machine Learning Methods","Fine-Tuning Language Models to Recognize Semantic Relations","Universum-Inspired Supervised Contrastive Learning","Semi-Supervised Learning with Nearest-Neighbor Label and Consistency Regularization","Automatic Recognition of the General-Purpose Communicative Functions Defined by the ISO 24617-2 Standard for Dialog Act Annotation (Extended Abstract)","Aspect-Based Summarization of Legal Case Files Using Sentence Classification","A Unifying View of Class Overlap and Imbalance: Key Concepts, Multi-View Panorama, and Open Avenues for Research","Research on Chinese Short Text Semantic Matching Based on Lightweight ERNIE","Efficient and Effective Edge-Wise Graph Representation Learning","Communication-Efficient Federated Skin Lesion Classification with Generalizable Dataset Distillation","Readability Factors of Japanese Text Classification","Weight-Guided Loss for Long-Tailed Object Detection and Instance Segmentation","Non-Parallel Bounded Support Matrix Machine and Its Application in Roller Bearing Fault Diagnosis","Improved Training for 3D Point Cloud Classification","IRMAC: Interpretable Refined Motifs in Binary Classification for Smart Grid Applications","A Deep Learning Approach to Automated Sleep Stages Classification Using Multi-Modal Signals","An Inverse Classification Framework with Limited Budget and Maximum Number of Perturbed Samples","IGPred-HDnet: Prediction of Immunoglobulin Proteins Using Graphical Features and the Hierarchal Deep Learning-Based Approach","Image Classification from Small Sample, with Distance Learning and Feature Selection","A Novel Feature Selection Based Text Classification Using Multi-Layer ELM","LongReMix: Robust Learning with High Confidence Samples in a Noisy Label Environment","Efficient Neural Net Approaches in Metal Casting Defect Detection","Predicting Autism from Head Movement Patterns during Naturalistic Social Interactions","ToSA: A Top-Down Tree Structure Awareness Model for Hierarchical Text Classification","Detection and Classification of Cracks and Potholes in Road Images Using Texture Descriptors","Development of Flood-Prone Area Classification Program Using Linear Classifier Method Based on Geomorphic Flood Index and Land Cover","Classification of Single-View Object Point Clouds","Explainable and Accurate Natural Language Understanding for Voice Assistants and Beyond","Optimal Decision Trees for Interpretable Clustering with Constraints","A Machine Learning Approach to Enterprise Matchmaking Using Multilabel Text Classification Based on Semi-Structured Website Content","Accelerating Concept Learning via Sampling","Context-Aware Feature Selection and Classification","Preliminary Analysis of Lambani Vowels and Vowel Classification Using Acoustic Features","A Novel Multi-Label Feature Selection Method with Association Rules and Rough Set","Rough Classification Based on Correlation Clustering","An Imbalanced Binary Classification Method Based on Contrastive Learning Using Multi-Label Confidence Comparisons within Sample-Neighbors Pair","Automatic Document Classification Based on Latent Semantic Analysis","Adaptive Weighted Ensemble Classifier for Improving Breast Tumors Classification Based on Ultrasound RF Data","DocLangID: Improving Few-Shot Training to Identify the Language of Historical Documents","Human Activity Classification Based on Data Analysis and Feature Extraction","A Voting Ensemble-Based Model to Predict the Risk of Cardiovascular Disease in Ordinary People","Granular Neural Networks with a Reference Frame","A Novel Network Architecture for Microplankton Classification in Digital Holographic Images","How Well Do Vision Transformers (VTs) Transfer to the Non-Natural Image Domain? An Empirical Study Involving Art Classification","Soft Labelling Based on Triangular Distributions for Ordinal Classification","Semi-Supervised Feature Learning for Disjoint Hyperspectral Imagery Classification","Multilabel Text Classification of Unbalanced Datasets: Two-Pass NNMF","Hierarchical Meta-Learning with Hyper-Tasks for Few-Shot Learning","Automated Accurate Detection of Depression Using Twin Pascal\u2019s Triangles Lattice Pattern with EEG Signals","Assessing Machine Learning Approaches for Predicting Failures of Investigational Drug Candidates during Clinical Trials","Using Automatic Programming to Improve Gradient Boosting for Classification","Multi-Label Emotion Analysis in Conversation via Multimodal Knowledge Distillation","GF-CNN: An Enhanced Deep Learning Model with Gabor Filters for Maize Disease Classification","Privacy against Real-Time Speech Emotion Detection via Acoustic Adversarial Evasion of Machine Learning","Position-Aware Subgraph Neural Networks with Data-Efficient Learning","Grouping-Based Oversampling in Kernel Space for Imbalanced Data Classification","Automated Data Mapping Based on FastText and LSTM for Business Systems","Prediction of Pipe Failures in Water Supply Networks for Longer Time Periods through Multi-Label Classification","Orthogonal Uncertainty Representation of Data Manifold for Robust Long-Tailed Learning","Nonlinear Desirability as a Linear Classification Problem","Investigating Domain-Agnostic Performance in Activity Recognition Using Accelerometer Data","Crop Type Mapping in the Central Part of the North China Plain Using Sentinel-2 Time Series and Machine Learning","Research on Bayesian Network Garbage Classification Based on Multi-Source Information Fusion","Deep Learning Based Classification and Segmentation for Cardiac Magnetic Resonance Imaging with Respiratory Motion Artifacts","A Hybrid Approach for Bangla Sign Language Recognition Using Deep Transfer Learning Model with Random Forest Classifier","Weighted Competitive-Collaborative Representation Based Classifier for Imbalanced Data Classification","Nearest Prototype Classification of Special School Families Based on Hierarchical Compact Sets Clustering","Deep Encoders with Auxiliary Parameters for Extreme Classification","Tropical Support Vector Machines: Evaluations and Extension to Function Spaces","Japanese Black Cattle Call Patterns Classification Using Multiple Acoustic Features and Machine Learning Models","Classification of Services through Feature Selection and Machine Learning in 5G Networks","Robust Remote Sensing Scene Classification with Multi-View Voting and Entropy Ranking","MP-Polynomial Kernel for Training Support Vector Machines","Selection of the Best Hybrid Spectral Similarity Measure for Characterising Marine Oil Spills from Multi-Platform Hyperspectral Datasets","Cluster-Based Performance of Student Dropout Prediction as a Solution for Large Scale Models in a Moodle LMS","Classifying Malignant and Benign Tumors of Breast Cancer: A Comparative Investigation Using Machine Learning Techniques","Removing Camouflage and Revealing Collusion: Leveraging Gang-Crime Pattern in Fraudster Detection","Contrastive Label Enhancement","Learning from Uncurated Regular Expressions for Semantic Type Classification","Class Token and Knowledge Distillation for Multi-Head Self-Attention Speaker Verification Systems","LIDACS: A Lightweight Domain Adaptive Cell Segmentation Framework","Customer Churn Combination Prediction Model Based on Convolutional Neural Network and Gradient Boosting Decision Tree","Unbalanced Credit Card Fraud Detection Data: A Machine Learning-Oriented Comparative Study of Balancing Techniques","Physiological-Based Difficulty Assessment for Virtual Reality Rehabilitation Games","A Non-Invasive Method for Prediction of Neurodegenerative Diseases Using Gait Signal Features","Text Classification Based on Natural Language Processing and Machine Learning in Multi Label Corpus","A Geometric Framework for Multiclass Ensemble Classifiers","Prototype Selection with Compact Sets and Extended Rough Sets","Extending Tree-Based Automated Machine Learning to Biomedical Image and Text Data Using Custom Feature Extractors","Functional Classification of Bitcoin Addresses","A Kernel-Free Laplacian Quadratic Surface Optimal Margin Distribution Machine with Application to Credit Risk Assessment","Domain-Conditioned Normalization for Test-Time Domain Generalization","Tree Based Fault Classification in Underground Cable","Extreme Multi-Label Classification for Ad Targeting Using Factorization Machines","A Comparison of One-Class Classifiers for Novelty Detection in Forensic Case Data","Better Pseudo-Label: Joint Domain-Aware Label and Dual-Classifier for Semi-Supervised Domain Generalization","WCDForest: A Weighted Cascade Deep Forest Model toward the Classification Tasks","Stingless Bee Classification: A New Dataset and Baseline Results","A Machine Learning Method for Improving Liver Cancer Staging","Comparison of Text Classification Methods Using Deep Learning Neural Networks","Local Differential Privacy for Private Construction of Classification Algorithms","Label Contrastive Learning for Image Classification","I Don\u2019t Feel so Good! Detecting Depressive Tendencies Using Transformer-Based Multimodal Frameworks","Enhancing Siamese Neural Networks for Multi-Class Classification: An Immuno-Inspired Approach","Siamese Basis Function Networks for Data-Efficient Defect Classification in Technical Domains","A Hybrid Self-Supervised Learning Framework For Hyperspectral Image Classification","Random Division: An Effective Method for Chinese Text Classification","Domain Adaptation for Learning from Label Proportions Using Domain-Adversarial Neural Network","Margin Embedding Net for Robust Margin Collaborative Representation-Based Classification","Classifiers for Yelp-Reviews Based on GMDH-Algorithms","Feature Selection with Scalable Variational Gaussian Process via Sensitivity Analysis Based on L 2 Divergence","Task Grouping for Multilingual Text Recognition","Cascading Global and Local Deep Features for Smartphone-Based Human Activity Classification","More Sustainable Text Classification via Uncertainty Sampling and a Human-in-the-Loop","Predicting Blood Drop Height and Volume Using Physics Equations, VGG-19, and XGBoost","Patch-Based Deep Learning Models for Breast Mammographic Mass Classification","Meta-Tsallis-Entropy Minimization: A New Self-Training Approach for Domain Adaptation on Text Classification","Retargeted Regression Methods for Multi-Label Learning","A Mandarin Tone Recognition Algorithm Based on Random Forest and Features Fusion","Speech Emotion Classification Using Ensemble Models with MFCC","Robust Bag Classification Approach for Multi-Instance Learning via Subspace Fuzzy Clustering","Texture-Based Data Augmentation for Small Datasets","Computer-Aided Breast Cancer Detection and Classification in Mammography: A Comprehensive Review","An End-to-End Deep Generative Approach with Meta-Learning Optimization for Zero-Shot Object Classification","Disparity, Inequality, and Accuracy Tradeoffs in Graph Neural Networks for Node Classification","Adaptive Sampling for Weighted Log-Rank Survival Trees Boosting","Convolutional Neural Networks for Identification of Moving Combustion Chambers Entering a Brazing Process","Remote Sensing Scene Classification under Scarcity of Labelled Samples\u2014A Survey of the State-of-the-Arts","Effectiveness of Self Normalizing Neural Networks for Text Classification","Uncovering Trauma in Genocide Tribunals: An NLP Approach Using the Genocide Transcript Corpus","Classification of Natural Images Inspired by the Human Visual System","TJU-DNN: A Trajectory-Unified Framework for Training Deep Neural Networks and Its Applications","Ensemble Feature Selection and Classification Methods for Machine Learning-Based Coronary Artery Disease Diagnosis","On Using a Pre-Clustering Technique to Optimize LDA-Based Classifiers for Appearance-Based Face Recognition","Minority-Prediction-Probability-Based Oversampling Technique for Imbalanced Learning","Supervised Term-Category Feature Weighting for Improved Text Classification","Exploring Federated Learning for Speech-Based Parkinson\u2019s Disease Detection","Autoencoder-Based Data Augmentation for Deepfake Detection","Classification of Earthquakes, Explosions and Mining-Induced Earthquakes Based on XGBoost Algorithm","CLUE: Consolidating Learned and Undergoing Experience in Domain-Incremental Classification","Evaluating Autoencoders for Dimensionality Reduction of MRI-Derived Radiomics and Classification of Malignant Brain Tumors","Regularisation for Efficient Softmax Parameter Generation in Low-Resource Text Classifiers","Robust Approach for Estimating Probabilities in Naive-Bayes Classifier","Arrhythmia Detection Using ECG-Based Classification with Prioritized Feature Subset Vector-Associated Generative Adversarial Network","Multi-Scale Attention-Based Multiple Instance Learning for Classification of Multi-Gigapixel Histology Images","Classification of Inter-Patient\u2019s Cardiac Arrhythmias in ECG Signals with Enhanced Jaya Optimized TQWT Parameters and Stacked Ensemble Algorithm","On Multivariate Randomized Classification Trees: L 0-Based Sparsity, VC Dimension and Decomposition Methods","Gesture Recognition and Multi-Modal Fusion on a New Hand Gesture Dataset","Protein Sequence Classification Using Bidirectional Encoder Representations from Transformers (BERT) Approach","Stable and Compact Face Recognition via Unlabeled Data Driven Sparse Representation-Based Classification","A Family of Novel Graph Kernels for Structural Pattern Recognition","Chinese News Title Classification Model Based on ERNIE-TextRCNN","Information Extraction and Classification from Free Text Using a Neural Approach","Is Pinocchio\u2019s Nose Long or His Head Small? Learning Shape Distances for Classification","Multimodal Cascaded Framework with Metric Learning Robust to Missing Modalities for Person Classification","Infrared Detector Fault Classification and Prediction Technology Based on Sensitive Parameter Learning","Domain Generalization by Joint-Product Distribution Alignment","Feature Selection Using Gravitational Search Algorithm in Customer Churn Prediction","Multiscale Echo Self-Attention Memory Network for Multivariate Time Series Classification","A Study of Age and Sex Bias in Multiple Instance Learning Based Classification of Acute Myeloid Leukemia Subtypes","Cross-Domain Meta Learning Fault Diagnosis Based on Multi-Scale Dilated Convolution and Adaptive Relation Module","Generalization Guarantees of Self-Training of Halfspaces under Label Noise Corruption","Training a Multi-Task Model for Classification and Grasp Detection of Surgical Tools Using Transfer Learning","Unsupervised Multimodal Domain Adversarial Network for Time Series Classification","Detecting Survival Patterns in Women with Invasive Cervical Cancer with Decision Trees","Unsupervised Segmentation of Haze Regions as Hard Attention for Haze Classification","Deep Residual SVM: A Hybrid Learning Approach to Obtain High Discriminative Feature for Land Use and Land Cover Classification","Cautious Decision-Making for Tree Ensembles","IMBoost: A New Weighting Factor for Boosting to Improve the Classification Performance of Imbalanced Data","Nearest Neighbor Condensation Based on Fuzzy Rough Set for Classification","UncertaintyFuseNet: Robust Uncertainty-Aware Hierarchical Feature Fusion Model with Ensemble Monte Carlo Dropout for COVID-19 Detection","A Sentiment Analysis Classification of Product Reviews through Convolutional Neural Networks (CNN)","\nKNNHI: Resilient KNN Algorithm for Heterogeneous Incomplete Data Classification and K Identification Using Rough Set Theory","Generalization Bounds in the Predict-Then-Optimize Framework","An Empirical Study of the Behavior of Classifiers on Imbalanced and Overlapped Data Sets","A Weighted Distance-Based Approach with Boosted Decision Trees for Label Ranking","Improving PPP-RTK-Based Vehicle Navigation in Urban Environments via Multilayer Perceptron-Based NLOS Signal Detection","WSNet: A Wrapper-Based Stacking Network for Multi-Scenes Classification of DApps","Open-Set Recognition of Breast Cancer Treatments","Classification of Brain Tumor Images Using CNN","The Role of Prescriptive Data and Non-Linear Dimension-Reduction Methods in Spare Part Classification","Tree-Based Credit Card Fraud Detection Using Isolation Forest, Spectral Residual, and Knowledge Graph","RuleCOSI+: Rule Extraction for Interpreting Classification Tree Ensembles","GeoFormer: Predicting Human Mobility Using Generative Pre-Trained Transformer (GPT)","Multi-Label Classification of Mobile Application User Reviews Using Neural Language Models","Secure Two-Party Decision Tree Classification Based on Function Secret Sharing","A Theoretical Analysis of Out-of-Distribution Detection in Multi-Label Classification","Research on Ship Target Recognition Based on Attention Mechanism","A Novel Two-Phase Clustering-Based under-Sampling Method for Imbalanced Classification Problems","Sparse Projection Infinite Selection Ensemble for Imbalanced Classification","Continual Recognition with Adaptive Memory Update","Rebalance Weights AdaBoost-SVM Model for Imbalanced Data","A Real-Time Adaptive Model for Bearing Fault Classification and Remaining Useful Life Estimation Using Deep Neural Network","Adaptive Embedding and Distribution Re-Margin for Long-Tail Recognition","Scalable Classifier-Agnostic Channel Selection for Multivariate Time Series Classification","Hierarchical Clustered Multiclass Discriminant Analysis via Cross-Validation","Automatic Diagnosis of COVID-19 with MCA-Inspired TQWT-Based Classification of Chest X-Ray Images","A Transfer Learning Approach to Interdisciplinary Document Classification with Keyword-Based Explanation","Pyramid Swin Transformer for Multi-Task: Expanding to More Computer Vision Tasks","Find Rhinos without Finding Rhinos: Active Learning with Multimodal Imagery of South African Rhino Habitats","Prediction of Protein Molecular Functions Using Transformers","Improving Table Tennis Training and Technique Analysis: Accurate Classification of Actions with Informer Encoder","Evolutionary Ensembles Based on Prioritized Aggregation Operator","Long-Term Traffic Pattern Forecasting Using Dynamic Classifier Selection","Cross-Domain Bearing Fault Diagnosis Method Using Hierarchical Pseudo Labels","An Improved Feature Selection Method for Classification on Incomplete Data: Non-Negative Latent Factor-Incorporated Duplicate MIC","Classifying Blockchain Cybercriminal Transactions Using Hyperparameter Tuned Supervised Machine Learning Models","Optimal Combination of Feature Weight Learning and Classification Based on Local Approximation","Leveraging Socio-Contextual Information in BERT for Fake Health News Detection in Social Media","Few-Shot Aspect Category Sentiment Analysis via Meta-Learning","Composable Workflow for Accelerating Neural Architecture Search Using In Situ Analytics for Protein Classification","Plant Disease Detection and Classification Using Deep Learning Models","Classification of High Resolution Satellite Images Using Texture from the Panchromatic Band","Transformer-Based Recognition of Activities of Daily Living from Wearable Sensor Data","Machine Learning-Based Classification of Bronze Alloy Cymbals from Microphone Captured Data Enhanced with Feature Selection Approaches","Predictive Modeling and Analytics for Diabetes Using Hyperparameter Tuned Machine Learning Techniques","Classification of Theoretical Extracellular Action Potentials Based on Unsupervised Machine-Learning","Theft Detection Dataset for Benchmarking and Machine Learning Based Classification in a Smart Grid Environment","CAT: Controllable Attribute Translation for Fair Facial Attribute Classification","A New Amharic Speech Emotion Dataset and Classification Benchmark","A Fuzzy Classifier for Evaluation of Research Topics by Using Keyword Co-Occurrence Network and Sponsors Information","A Classification Framework for Investigating Neural Correlates of the Limit of Stability during Weight-Shifting in Lower Limb Amputees","Collaborative Learning with Unreliability Adaptation for Semi-Supervised Image Classification","White Blood Cell Classification Based on Gray Level Co-Occurrence Matrix with Zero Phase Component Analysis Approach","Investigating the Impact of Attention on Mammogram Classification","A Combined Statistical-Structural Strategy for Alphanumeric Recognition","Segmentation and Multi-Facet Classification of Individual Logs in Wooden Piles","Bayesian Networks Improve Out-of-Distribution Calibration for Agribusiness Delinquency Risk Assessment","CSP-Ph-PS: Learning CSP-Phase Space and Poincare Sections Based on Evolutionary Algorithm for EEG Signals Recognition","Language Models for Automatic Distribution of Review Notes in Movie Production","Deep Learning Multimodal Methods for Geophysical Inversion\u202f: Application to Glacier Ice Thickness Estimation","Comparing a Transferable Belief Model Capable of Recognizing Facial Expressions with the Latest Human Data","Dialect Identification in Ao Using Modulation-Based Representation","Recent Advances in Automatic Feature Detection and Classification of Fruits Including with a Special Emphasis on Watermelon (Citrillus Lanatus): A Review","Sentiment Distribution of Topic Discussion in Online English Learning: An Approach Based on Clustering Algorithm and Improved CNN","Gaussian Conditional Random Fields for Classification","A Federated Learning-Enabled Smart Street Light Monitoring Application: Benefits and Future Challenges","Quantifying the Effect of Image Similarity on Diabetic Foot Ulcer Classification","Intuitive Human-Swarm Interaction with Gesture Recognition and Machine Learning","Advances in Automated Pedagogical Compile-Time Error Repair","A One-Class Classification Method Based on Expanded Non-Convex Hulls","Maximal Margin Hyper-Sphere SVM for Binary Pattern Classification","Towards Prior Gap and Representation Gap for Long-Tailed Recognition","SVM in Classification of Stage 0~II and III~IV with Breast Cancer\u202f: A Retrospective Cohort Study on a Bicentric Cohort","Heterogeneous Stacked Ensemble Framework for Surface Electromyography Signal Classification","CrashFormer: A Multimodal Architecture to Predict the Risk of Crash","Computing Classifier-Based Embeddings with the Help of Text2ddc","Uncertainty Quantification for Text Classification","An Automatic Depression Recognition Method from Spontaneous Pronunciation Using Machine Learning","Fact-Checking Vietnamese Information Using Knowledge Graph, Datalog, and KG-BERT","HAMIL: Hierarchical Aggregation-Based Multi-Instance Learning for Microscopy Image Classification","Deep Neural Networks for Rank-Consistent Ordinal Regression Based on Conditional Probabilities","Data Complexity-Based Dynamic Ensembling of SVMs in Classification","A Novel Combined Approach Based on Deep Autoencoder and Deep Classifiers for Credit Card Fraud Detection","A Two-Stage Estimation Method Based on Conceptors-Aided Unsupervised Clustering and Convolutional Neural Network Classification for the Estimation of the Degradation Level of Industrial Equipment","Voting Classifier-Based Crop Recommendation","MFFNet: Multi-Receptive Field Fusion Net for Microscope Steel Grain Grading","The Performance Index of Convolutional Neural Network-Based Classifiers in Class Imbalance Problem","LAMM: Language Aware Active Learning for Multilingual Models","Privacy-Preserving Federated Learning via Disentanglement","Explaining Black-Box Classifiers: Properties and Functions","TF-IGM Revisited: Imbalance Text Classification with Relative Imbalance Ratio","PRIORITY: An Intelligent Problem Indicator Repository","Sentiment Analysis with Automatically Constructed Lexicon and Three-Way Decision","DeepBP: A Bilinear Model Integrating Multi-Order Statistics for Fine-Grained Recognition","Decision Fusion Approach for Detecting Unknown Wafer Bin Map Patterns Based on a Deep Multitask Learning Model","Improved Classification Accuracy by Feature Selection Using Adaptive Support Method","BERT Self-Learning Approach with Limited Labels for Document Classification","Active Weighted Aging Ensemble for Drifted Data Stream Classification","PSRTTCA: A New Approach for Improving the Prediction and Characterization of Tumor T Cell Antigens Using Propensity Score Representation Learning","A Hybrid Model for Text Classification Using Part-of-Speech Features","Impact of CLAHE-Based Image Enhancement for Diabetic Retinopathy Classification through Deep Learning","Mesoscale Events Classification in Sea Surface Temperature Imagery","On the Benefits of Machine Learning Classification in Cashback Fraud Detection","Discrimination of Seismic and Non-Seismic Signal Using SCOUTER","Forest Based on Interval Transformation (FIT): A Time Series Classifier with Adaptive Features","Boosting with Temporal Consistent Learners: An Application to Human Activity Recognition","Threshold Center-Symmetric Local Binary Convolutional Neural Networks for Bilingual Handwritten Digit Recognition","Insect Classification Framework Based on a Novel Fusion of High-Level and Shallow Features","Motor Imagery Classification via Stacking-Based Takagi\u2013Sugeno\u2013Kang Fuzzy Classifier Ensemble","Convex Least Angle Regression Based LASSO Feature Selection and Swish Activation Function Model for Startup Survival Rate","Quantifying Effect-Specific Mammographic Density","Exploring the Capabilities of Quantum Support Vector Machines for Image Classification on the MNIST Benchmark","Reconstruction and Classification of 3D Burden Surfaces Based on Two Model Drived Data Fusion","Prediction of Algae Growth: A Machine Learning Perspective","Part-Aware Prototype-Aligned Interpretable Image Classification with Basic Feature Domain","Support Subsets Estimation for Support Vector Machines Retraining","Progressive Label Propagation for Semi-Supervised Multi-Dimensional Classification","Monuments Identification Using Satellite Images: A CNN Based Approach","Paralinguistic and Spectral Feature Extraction for Speech Emotion Classification Using Machine Learning Techniques","FLAMES2Graph: An Interpretable Federated Multivariate Time Series Classification Framework","A Multi-Task Model for Emotion and Offensive Aided Stance Detection of Climate Change Tweets","Discovering Behavioural Predispositions in Data to Improve Human Activity Recognition","Constructing Accurate Fuzzy Rule-Based Classification Systems Using Apriori Principles and Rule-Weighting","Optimized Machine Learning for Classifying Colorectal Tissues","Predicting Flower Induction of Litchi (Litchi Chinensis Sonn.) with Machine Learning Techniques","Quasi-CliquePool: Hierarchical Graph Pooling for Graph Classification","Hyperspectral Image Classification Based on Three-Dimensional Adaptive Sampling and Improved Iterative Shrinkage-Threshold Algorithm","A Multi-Class Classification Model with Parametrized Target Outputs for Randomized-Based Feedforward Neural Networks","RaMLP: Vision MLP via Region-Aware Mixing","Hyperparameter Optimized Classification Pipeline for Handling Unbalanced Urban and Rural Energy Consumption Patterns","Defect Classification Method of X-Ray Images Based on Improved U-Net","Air Pollutants Classification Using Optimized Neural Network Based on War Strategy Optimization Algorithm","Multi-Branch Network for Imagery Emotion Prediction","False Alarm Detection in Wind Turbine by Classification Models","Evaluation of Banknote Identification Methodologies Based on Local and Deep Features","A Two-Stage Gap Safe Screening Rule for Multi-Label Optimal Margin Distribution Machine","Immuno-Inspired Augmentation of Siamese Neural Network for Multi-Class Classification","Scattering-Based Hybrid Network for Facial Attribute Classification","NaN","SimpleDG: Simple Domain Generalization Baseline Without Bells and Whistles","Hierarchical Classification of Gene Ontology with Learning Classifier Systems","MobileACNet: ACNet-Based Lightweight Model for Image Classification","Group-Preserving Label-Specific Feature Selection for Multi-Label Learning","Extreme Learning Machine Combining Hidden-Layer Feature Weighting and Batch Training for Classification","Protein Data Condensation for Effective Quaternary Structure Classification","A Bayesian Convolutional Neural Network Model with Uncertainty for Multi-Label Text Classification on Mechanisms of Action (MoA) Prediction","Classification Algorithm for Liquid Dangerous Goods Based on WT-AE and Attention-GRU Network","Machine Learning Modeling of GPS Features with Applications to UAV Location Spoofing Detection and Classification","Fast Codebook Generation by Sequential Data Analysis for Object Classification","Cautious Weighted Random Forests","Diagnosing Prostate Cancer: An Implementation of Deep Machine Learning Fusion Network in MRI Using a Transfer Learning Approach","Left on Read: Reply Latency for Anxiety \\&amp; Depression Screening","Active Learning for Open-Set Annotation Using Contrastive Query Strategy","Out of Bootstrap Estimation of Generalization Error Curves in Bagging Ensembles","Feature Engineering and Ensemble Learning-Based Classification of VPN and Non-VPN-Based Network Traffic over Temporal Features","An Evaluation of Handwritten Text Recognition Methods for Historical Ciphered Manuscripts","A Globally-Connected and Trainable Hierarchical Fine-Attention Generative Adversarial Network Based Adversarial Defense","Machine Learning Invariants of Arithmetic Curves","Complexity-Driven Sampling for Bagging","Confusion Matrix Disagreement for Multiple Classifiers","Fire Images Classification Based on a Handcraft Approach","Framework for Choosing a Supervised Machine Learning Method for Classification Based on Object Categories\u202f: Classifying Subjectivity of Online Comments by Product Categories","An Image Classification Method Based on Adaptive Attention Mechanism and Feature Extraction Network","Multiple Cue Integrated Action Detection","Modification of the Growing Neural Gas Algorithm for Cluster Analysis","GII: A Unified Approach to Representation Learning in Open Set Recognition with Novel Category Discovery","Learning Prototype Classifiers for Long-Tailed Recognition","A Systematic Literature Review of Machine Learning Application in COVID-19 Medical Image Classification","Federated Few-Shot Learning for Cough Classification with Edge Devices","Age-Invariant Face Recognition Using Face Feature Vectors and Embedded Prototype Subspace Classifiers","A Survey on Hyperdimensional Computing Aka Vector Symbolic Architectures, Part II: Applications, Cognitive Models, and Challenges","Expressive and Intuitive Models for Automated Context Representation Learning in Credit-Card Fraud Detection","SVM Kernel and It\u2019s Aggregation Using Stacking on Imbalanced Dataset","Understanding the Impact of Label Skewness and Optimization on Federated Learning for Text Classification","\"Can You Guess My Moves? Playing Charades with a Humanoid Robot Employing Mutual Learning with Emotional Intelligence","A Momentum Loss Reweighting Method for Improving Recall","An Explainable Machine Learning-Based Prediction Model for In-Hospital Mortality in Acute Myocardial Infarction Patients with Typical Chest Pain","Feature Estimation for Punching Tool Wear at the Edge","Augmenting Softmax Information for Selective Classification with Out-of-Distribution Data","Sparse-Learning-Based High-Order Dynamic Functional Connectivity Networks for Brain Disease Classification","Combination Special Data Augmentation and Sampling Inspection Network for Cardiac Magnetic Resonance Imaging Quality Classification","Distance-Preserving Embedding Adaptive Bipartite Graph Multi-View Learning with Application to Multi-Label Classification","Research on Multi-Domain Sample Classification Method Based on Baidu API","Hessian Scatter Regularized Twin Support Vector Machine for Semi-Supervised Classification","Utilization of Stockwell Transform and Random Forest Algorithm for Efficient Detection and Classification of Power Quality Disturbances","A Proxy Learning Curve for the Bayes Classifier","Fair Classification by Loss Balancing via Fairness-Aware Batch Sampling","Feature Selection in an Interactive Search-Based PLA Design Approach","Sentiment Classification of Social Network Text Based on AT-BiLSTM Model in a Big Data Environment","Influence of Sample Attributes on Generalization Performance of Machine Learning Models for Windage Alteration Fault Diagnosis of the Mine Ventilation System","NGAME: Negative Mining-Aware Mini-Batching for Extreme Classification","Text Classification Using Correlation Based Feature Selection on Multi-Layer ELM Feature Space","An Ensemble Credit Scoring Model Based on Logistic Regression with Heterogeneous Balancing and Weighting Effects","Multi-View Multi-Label Learning with High-Order Label Correlation","Semantic Analysis and Classification of Emails through Informative Selection of Features and Ensemble AI Model","Reference-Limited Compositional Zero-Shot Learning","Benchmarking Knowledge-Driven Zero-Shot Learning","A Data Skew-Based Unknown Traffic Classification Approach for TLS Applications","Hyperbolic Geometric Graph Representation Learning for Hierarchy-Imbalance Node Classification","A GCN- and Deep Biaffine Attention-Based Classification Model for Course Review Sentiment","Exploring Diversity in Data Complexity and Classifier Decision Spaces for Pool Generation","AutoML Technologies for the Identification of Sparse Classification and Outlier Detection Models","Decision Tree Fusion and Improved Fundus Image Classification Algorithm","Fuzzy-UCS Revisited: Self-Adaptation of Rule Representations in Michigan-Style Learning Fuzzy-Classifier Systems","Comparing Two SVM Models through Different Metrics Based on the Confusion Matrix","Environmental Sound Classification Using Hybrid Ensemble Model","Automatic ICD Coding Based on Multi-Granularity Feature Fusion","A New One-Dimensional Testosterone Pattern-Based EEG Sentence Classification Method","Optimum Bayesian Thresholds for Rebalanced Classification Problems Using Class-Switching Ensembles","Cross-Domain Class-Contrastive Learning: Finding Lower Dimensional Representations for Improved Domain Generalization","RT2S: A Framework for Learning with Noisy Labels","To Aggregate or Not? Learning with Separate Noisy Labels","KATIE: A System for Key Attributes Identification in Product Knowledge Graph Construction","Multi-Label Text Classification Model Based on Multi-Level Constraint Augmentation and Label Association Attention","CKR-Calibrator: Convolution Kernel Robustness Evaluation and Calibration","Classification of Long Sequential Data Using Circular Dilated Convolutional Neural Networks","Information Classification Issues","Semi-Supervised Feature Selection for Partially Labeled Mixed-Type Data Based on Multi-Criteria Measure Approach","Maximal Independent Vertex Set Applied to Graph Pooling","Term Frequency Features vs Transformers: A Comparision for Sentiment Classification of African Languages","Prototypical Networks Relation Classification Model Based on Entity Convolution","The Classification of Wheat Yellow Rust Disease Based on a Combination of Textural and Deep Features","Weighted K-Nearest Leader Classifier for Large Data Sets","Fault Classification and Timing Prediction Based on Shipment Inspection Data and Maintenance Reports for Semiconductor Manufacturing Equipment","VGG16 Feature Selection Using PCA-Big Bang Big Algorithm","Post-Infarction Risk Prediction with Mesh Classification Networks","Operational Domain Name Classification: From Automatic Ground Truth Generation to Adaptation to Missing Values","Semi-Supervised Attribute Reduction for Partially Labeled Categorical Data Based on Predicted Label","Imbalance Example-Dependent Cost Classification: A Bayesian Based Method","On the Most Frequent Sequences of Words in Russian Spoken Everyday Language (Bigrams and Trigrams): An Experience of Classification","Multi-Classification Data Stream Algorithm Based on One-Vs-Rest Strategy","Domain Generalization via Adversarial Out-Domain Augmentation for Remaining Useful Life Prediction of Bearings under Unseen Conditions","OmnImage: Evolving 1k Image Cliques for Few-Shot Learning","ProMix: Combating Label Noise via Maximizing Clean Sample Utility","An Artificial Intelligence Approach for Verifying Persons by Employing the Deoxyribonucleic Acid (DNA) Nucleotides","Assessment of Semi-Supervised Approaches Applied to Convolutional Neural Networks","Texture and Material Classification with Multi-Scale Ternary and Septenary Patterns","Combining Autoencoder and Yolov6 Model for Classification and Disease Detection in Chickens","Leveraging Model Fusion for Improved License Plate Recognition","A Support System for Automatic Classification of Hypertension Using BCG Signals","Fuzzy Similarity Phrases for Interpretable Data Classification","Ensemble Methods With [18F]FDG-PET/CT Radiomics In Breast Cancer Response Prediction","Improving Adversarial Robustness by Penalizing Natural Accuracy","ML-LJP: Multi-Law Aware Legal Judgment Prediction","The Effects of Data Balancing Approaches: A Case Study","MND: A New Dataset and Benchmark of Movie Scenes Classified by Their Narrative Function","Machine Learning Driven Aid Classification for Sustainable Development","StrokeViT with AutoML for Brain Stroke Classification","EEG-Based Emotion Recognition in Immersive Virtual Reality: Meeting the Requirement of Accuracy and Computational Efficiency","Graph Explicit Neural Networks: Explicitly Encoding Graphs for Efficient and Accurate Inference","Fast Incremental Learning by Transfer Learning and Hierarchical Sequencing","SODA-Boosting and Its Application to Gender Recognition","Poster: Backdoor Attack on Extreme Learning Machines","G2Pxy: Generative Open-Set Node Classification on Graphs with Proxy Unknowns","Machine-Based Mosquito Taxonomy with a Lightweight Network-Fused Efficient Dual ConvNet with Residual Learning and Knowledge Distillation","Supervised Web Document Classification Using Discrete Transforms, Active Hypercontours and Expert Knowledge","Dynamic Ensemble of Rough Set Reducts for Data Classification","Efficient Differentially Private Kernel Support Vector Classifier for Multi-Class Classification","Invited Paper: Common Public Knowledge for Enhancing Machine Learning Data Sets","Federated Learning with ResNet-18 for Medical Image Diagnosis","Graph Neural Networks for Fault Diagnosis of Geographically Nearby Photovoltaic Systems","Multivariate Two Dimensional Singular Spectrum Analysis Based Fusion Method for Four View Image Based Object Classification","Classification and Pathologic Diagnosis of Gliomas in MR Brain Images","Classification of Structural Cartographic Objects Using Edge-Based Features","Author Attribution of Literary Texts in Polish by the Sequence Averaging","GPC: Generative and General Pathology Image Classifier","Evaluating the Performance of Ensemble Classifiers in Stock Returns Prediction Using Effective Features","Rough Set Theory of Pattern Classification in the Brain","Explainable Occupancy Prediction Using QLattice","Transfer Learning: Kernel-Based Domain Adaptation with Distance-Based Penalization","Model Averaging for Support Vector Classifier by Cross-Validation","A Subgraph Embedded GIN with Attention for Graph Classification","Dysarthria Severity Classification Using Multi-Head Attention and Multi-Task Learning","Applying Three-Way Decisions to Sentiment Classification with Sentiment Uncertainty","Deep Learning Based Multi-Labelled Soil Classification and Empirical Estimation toward Sustainable Agriculture","Fast k Most Similar Neighbor Classifier for Mixed Data Based on a Tree Structure","FedAR+: A Federated Learning Approach to Appliance Recognition with Mislabeled Data in Residential Environments","Early Classifying Multimodal Sequences","TODOS: Thermal SensOr Data-Driven Occupancy Estimation System for Smart Buildings","Matching Pursuit-Based Analysis of FNIRS in Combination with Cascade PCA and ReliefF for Mental Task Recognition","Automatic Facial Expression Recognition Using Boosted Discriminatory Classifiers","ML-Based Identification of Neuromuscular Disorder Using EMG Signals for Emotional Health Application","A CNN-RNN Unified Framework for Intrapartum Cardiotocograph Classification","Multi-Task Learning over Mixup Variants for the Speaker Verification Task","Subspace Projection-Based Weighted Echo State Networks for Predicting Therapeutic Peptides","Random Convolution Ensembles","Improving Automated Labeling for ATT&amp;CK Tactics in Malware Threat Reports","Disentangling User Conversations with Voice Assistants for Online Shopping","Prolog-Based Agnostic Explanation Module for Structured Pattern Classification","Multi-Channel Graph Convolution for Aspect-Level Sentiment Classification of Online Student Reviews","Parkinson\u2019s Disease Classification with CWNN: Using Wavelet Transformations and IMU Data Fusion for Improved Accuracy","Random Forest Algorithm Using Quartile-Pattern Bootstrapping for a Class Imbalanced Problem","Fast Truncated Huber Loss SVM for Large Scale Classification","Investigating the Effect of Orientation Variability in Deep Learning-Based Human Activity Recognition","Multiple Classifier Fusion Using K-Nearest Localized Templates","Assessing Students for Industry Readiness Using Classification Methods","Deep Pipeline Embeddings for AutoML","A Streaming Approach to the Core Vector Machine","On Enhancing Prediction Abilities of Vision-Based Metallic Surface Defect Classification through Adversarial Training","Tackling Diverse Minorities in Imbalanced Classification","Towards a Machine Learning Approach to Predicting the Difficulty of FPGA Routing Problems","News Classification and Categorization with Smart Function Sentiment Analysis","Aspect-Based Sentiment Classification with Dual Cooperative Graph Attention Networks","An Efficient CNN-Based Automated Leukemia Diagnosis Using Microscopic Blood Smear Images and Subtypes Classification","Classification of Review Information Based on Tourist's Places of Interest in India Using Support Vector Machine","Ultrasound-Based Ovarian Cysts Detection with Improved Machine-Learning Techniques and Stage Classification Using Enhanced Classifiers","Invariance Encoding in Sliced-Wasserstein Space for Image Classification with Limited Training Data","MacBERT Classification Model of Memory Attention Mechanism and Its Application to the Power System of EAST Neutral Beam Injection Facility","A Machine Learning Based Framework to Identify Unseen Classes in Open-World Text Classification","An Explainable AI-Driven Biomarker Discovery Framework for Non-Small Cell Lung Cancer Classification","Machine Learning-Based Ransomware Classification of Bitcoin Transactions","An Optimised Grid Search Based Framework for Robust Large-Scale Natural Soundscape Classification","A Semisupervised Classification Algorithm Combining Noise Learning Theory and a Disagreement Cotraining Framework","Boosting Few-Shot Open-Set Recognition with Multi-Relation Margin Loss","Regional Homogeneity and Anatomical Parcellation for FMRI Image Classification: Application to Schizophrenia and Normal Controls","A Method for Residual Network Image Classification with Multi-Scale Feature Fusion","SER-Fuse: An Emotion Recognition Application Utilizing Multi-Modal, Multi-Lingual, and Multi-Feature Fusion","Bounds for Sparse Solutions of K-SVCR Multi-Class Classification Model","Classification Prediction of Breast Cancer Based on Machine Learning","An Improved NN-SVM Based on K Congener Nearest Neighbors Classification Algorithm","Effective Method for Making Chinese Word Vector Dynamic","Multiview Collaboration Learning Classification Model of Stock Data Based on View Weighting Mechanism","EEG Sub-Bands Based Sleep Stages Classification Using Fourier Synchrosqueezed Transform Features","Color Models Aware Dynamic Feature Extraction for Forest Fire Detection Using Machine Learning Classifiers","GGFAST: Automating Generation of Flexible Network Traffic Classifiers","Detecting Semantic Relations Between Nominals Using Support Vector Machines and Linguistic-Based Rules","CIRL: A Category-Instance Representation Learning Framework for Tropical Cyclone Intensity Estimation","Machine Learning Techniques for Breast Cancer Prediction","Multiclass Imbalanced and Concept Drift Network Traffic Classification Framework Based on Online Active Learning","Fairness-Aware Training of Face Attribute Classifiers via Adversarial Robustness","Bagging with Asymmetric Costs for Misclassified and Correctly Classified Examples","Median M-Type Radial Basis Function Neural Network","DisguisedNets: Secure Image Outsourcing for Confidential Model Training in Clouds","Classification of Social Media Users with Generalized Functional Data Analysis","JDAN: Joint Detection and Association Network for Real-Time Online Multi-Object Tracking","Data-Driven Surface Classification for Differential Drive Autonomous Guided Vehicles","Multi-View Graph Convolutional Networks with Differentiable Node Selection","A Problem-Tailored Adversarial Deep Neural Network-Based Attack Model for Feed-Forward Physical Unclonable Functions","FedClassAvg: Local Representation Learning for Personalized Federated Learning on Heterogeneous Neural Networks","Bias Mitigation for Machine Learning Classifiers: A Comprehensive Survey","Constant-Q Based Harmonic And Pitch Features For Normal vs. Pathological Infant Cry Classification","CALM: An Enhanced Encoding and Confidence Evaluating Framework for Trustworthy Multi-View Learning","A Study of Text Representations for Hate Speech Detection","Impact of Dataset Size and Convolutional Neural Network Architecture on Transfer Learning for Carbonate Rock Classification","Different Bayesian Network Models in the Classification of Remote Sensing Images","Deep Active Ensemble Sampling for Image Classification","Efficient Learning of Large Sets of Locally Optimal Classification Rules","The Cable Fault Diagnosis for XLPE Cable Based on 1DCNNs-BiLSTM Network","Knowledge-Based Genetic Algorithm Approach to Optimise Gated Recurrent Unit for Semantic Web Service Classification","Brain Tumor Categorization from Imbalanced MRI Dataset Using Weighted Loss and Deep Feature Fusion","Non-Intrusive Load Identification Based on Complex Spectrum and Support Vector Machine","Debiaser for Multiple Variables to Enhance Fairness in Classification Tasks","Motion-Related Artefact Classification Using Patch-Based Ensemble and Transfer Learning in Cardiac MRI","A Comparative Analysis on Recent Methods for Addressing Imbalance Classification","Diagnosis of Breast Cancer Using Random Forests","Resnet-Based Modified Red Deer Optimization with DLCNN Classifier for Plant Disease Identification and Classification","An Ontology Approach for Classification of Abnormal White Matter in Patients with Multiple Sclerosis","Two-Stage COVID19 Classification Using BERT Features","A Novel Filter Feature Selection Method for Text Classification: Extensive Feature Selector","Fu-W:A Hyperspectral Image Classification Algorithm Combining Mini Graph Convolutional Networks and Convolutional Neural Network","Joint Leaf-Refinement and Ensemble Pruning through L1 Regularization","Probabilistic Combination of Visual Cues for Object Classification","A Multi-View Feature Fusion Approach for Effective Malware Classification Using Deep Learning","NaN","Forecasting Dining Times in a Full-Service Thai Hotpot Restaurant Using Random Forest Classifier","Generating Explainable Product Comparisons for Online Shopping","Fast Support Vector Classifier with Quantile","Imbalanced Binary Classification under Distribution Uncertainty","Using a Genetic Algorithm for Editing K-Nearest Neighbor Classifiers","Supervised Contrastive Multi-Tasking Learning Based Hierarchical Yoga Pose Classification Using CNNs","Studying the Effects of Sex-Related Differences on Brain Age Prediction Using Brain MR Imaging","Rice Variety Classification \\&amp; Yield Prediction Using Semantic Segmentation of Agro-Morphological Characteristics","PLMCL: Partial-Label Momentum Curriculum Learning for Multi-Label Image Classification","Heterogeneous Graph Convolutional Neural Network for Short Text Classification","Multilabel All-Relevant Feature Selection Using Lower Bounds of Conditional Mutual Information","MaMiNet: Memory-Attended Multi-Inference Network for Surface-Defect Detection","Multi-Class Binary Object Categorization Using Blurred Shape Models","Roses Greenhouse Cultivation Classification Using Machine Learning Techniques","Fingerprint Classification Method Based on Analysis of Singularities and Geometric Framework","Explainable Text Classification via Attentive and Targeted Mixing Data Augmentation","Incremental Tabular Learning on Heterogeneous Feature Space","An Unsupervised HIV Lateral Flow Immunochromatography Detection Algorithm Based on K-Means++","Learning Label-Specific Features for Decomposition-Based Multi-Class Classification","An Improved Conv-LSTM Method for Gear Fault Detection","Early Prediction of Heart Diseases Using Naive Bayes Classification Algorithm and Laplace Smoothing Technique","Collaboration Graph for Feature Set Partitioning in Data Classification","Task-Equivariant Graph Few-Shot Learning","Mass Spectrometry Based Cancer Classification Using Fuzzy Fractal Dimensions","Convolutional Neural Network with Support Vector Machine for Motor Imagery EEG Signal Classification","Baselining Performance for Multilingual Codeswitching Sentiment Classification","Systematic Analysis of the Impact of Label Noise Correction on ML Fairness","A Combination-of-Tools Method for Learning Interpretable Fuzzy Rule-Based Classifiers from Support Vector Machines","Bravely Say I Don\u2019t Know: Relational Question-Schema Graph for Text-to-SQL Answerability Classification","Locally Alignment Based Manifold Learning for Simultaneous Feature Selection and Extraction in Classification Problems","Gaussian-Type Activation Function with Learnable Parameters in Complex-Valued Convolutional Neural Network and Its Application for PolSAR Classification","Material Classification of Polishing and Convex Surface Objects Based on Photon Accumulation Point Spread Function (PAPSF) from Imaging Model of Binocular Pulsed Time-of-Flight Camera","Partial-Label Learning with Mixed Closed-Set and Open-Set Out-of-Candidate Examples","Prediction of Casting Mechanical Parameters Based on Direct Microstructure Image Analysis Using Deep Neural Network and Graphite Forms Classification","Granular Support Vector Machine Based Method for Prediction of Solubility of Proteins on Overexpression in Escherichia Coli","Automatic Method to Build a Dictionary for Class-Based Translation Systems","Unbiased Gradient Boosting Decision Tree with Unbiased Feature Importance","An Explainable AI Driven Decision Support System for COVID-19 Diagnosis Using Fused Classification and Segmentation","DASS: Differentiable Architecture Search for Sparse Neural Networks","Automatic Idiom Identification Model for Amharic Language","TNN: A Transfer Learning Classifier Based on Weighted Nearest Neighbors","Data Enhancement and Multi-Feature Learning Model for Pest Classification","An Analysis of Automatic Gender Classification","PLFace: Progressive Learning for Face Recognition with Mask Bias","Object Selection Based on Subclass Error Correcting for ALVOT","Automatic Classification of Sensors in Buildings: Learning from Time Series Data","Network Traffic Anomaly Detection Method Based on Multi-Scale Residual Classifier","Quantifying Imbalanced Classification Methods for Leukemia Detection","Does Removal of Noisy Granulated Datasets Matter to Performance of Decision Tree Generation?","Automatic Data-Driven Software Change Identification via Code Representation Learning","DPNET: Dynamic Poly-Attention Network for Trustworthy Multi-Modal Classification","DIF-SR: A Differential Item Functioning-Based Sample Reweighting Method","Feature Patch Based Attention Model for Dental Caries Classification","Artificial Neural Network-Assisted Amplitude Thresholding Improves Spike Detection","WEASEL 2.0: A Random Dilated Dictionary Transform for Fast, Accurate and Memory Constrained Time Series Classification","Combining Regular Expressions and Supervised Algorithms for Clinical Text Classification","Mel Spectrogram-Based Advanced Deep Temporal Clustering Model with Unsupervised Data for Fault Diagnosis","Digital Twin-Based Anomaly Detection with Curriculum Learning in Cyber-Physical Systems","Leveraging Domain Knowledge for Inclusive and Bias-Aware Humanitarian Response Entry Classification","Handwritten Digits Recognition Using Transfer Learning","Effective Training-Time Stacking for Ensembling of Deep Neural Networks","Miscellaneous EEG Preprocessing and Machine Learning for Pilots' Mental States Classification: Implications","Machine Assistance for Credit Approval? Random Wheel Can Recommend and Explain","An Efficient BP-Neural Network Classification Model Based on Attribute Reduction"],"x":{"__ndarray__":"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","dtype":"float64","shape":[955]},"x_backup":{"__ndarray__":"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","dtype":"float64","shape":[955]},"y":{"__ndarray__":"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","dtype":"float64","shape":[955]},"y_backup":{"__ndarray__":"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","dtype":"float64","shape":[955]}},"selected":{"id":"1071","type":"Selection"},"selection_policy":{"id":"1072","type":"UnionRenderers"}},"id":"1011","type":"ColumnDataSource"},{"attributes":{"fill_alpha":{"value":0.1},"fill_color":{"value":"#1f77b4"},"line_alpha":{"value":0.1},"line_color":{"value":"#1f77b4"},"size":{"units":"screen","value":5},"x":{"field":"x"},"y":{"field":"y"}},"id":"1051","type":"Scatter"},{"attributes":{},"id":"1055","type":"BasicTickFormatter"},{"attributes":{"text":"&lt;a href=\"clusters_last_year.html\" target=\"_blank\"&gt;&lt;button&gt;Last Year&lt;/button&gt;&lt;/a&gt;"},"id":"1005","type":"Div"},{"attributes":{"overlay":{"id":"1059","type":"BoxAnnotation"}},"id":"1037","type":"BoxZoomTool"},{"attributes":{"sizing_mode":"stretch_width","style":{"font-family":"Helvetica Neue, Helvetica, Arial, sans-serif;","font-size":"1.1em"},"text":"&lt;h3&gt;Filter by the Clusters:&lt;/h3&gt;&lt;p1&gt;The slider below can be used to filter the target cluster. \nSimply slide the slider to the desired cluster number to display the plots that belong to that cluster. \nSlide back to the last cluster to show all the plots.&lt;/p1&gt;"},"id":"1008","type":"Div"},{"attributes":{},"id":"1039","type":"SaveTool"}],"root_ids":["1087"]},"title":"Bokeh Application","version":"1.4.0"}}
        </script>
        <script type="text/javascript">
          (function() {
            var fn = function() {
              Bokeh.safely(function() {
                (function(root) {
                  function embed_document(root) {
                    
                  var docs_json = document.getElementById('1176').textContent;
                  var render_items = [{"docid":"946ecee8-9104-46f9-a7fd-10758e4f2034","roots":{"1087":"8ee57938-c867-4570-88eb-ff975b2ca1b4"}}];
                  root.Bokeh.embed.embed_items(docs_json, render_items);
                
                  }
                  if (root.Bokeh !== undefined) {
                    embed_document(root);
                  } else {
                    var attempts = 0;
                    var timer = setInterval(function(root) {
                      if (root.Bokeh !== undefined) {
                        clearInterval(timer);
                        embed_document(root);
                      } else {
                        attempts++;
                        if (attempts > 100) {
                          clearInterval(timer);
                          console.log("Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing");
                        }
                      }
                    }, 10, root)
                  }
                })(window);
              });
            };
            if (document.readyState != "loading") fn();
            else document.addEventListener("DOMContentLoaded", fn);
          })();
        </script>
    
  </body>
  
</html>